{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Hat USA Training (Early draft)\n",
    "\n",
    "## Lab 2: Dynamic Malware Detection using Machine Learning with PyTorch\n",
    "\n",
    "We will follow a \"Top-Down\" teaching methodology: We will start with higher level concepts familiar to our students in the cybersecurity domain, for instance, by introducing a specific library and demonstrating its use. Then, we delve deeper into the methods and parameters of these applications. Finally, we explore the underlying fundamentals, such as the specific PE format properties or mathematical concepts at the core of these ideas.\n",
    "\n",
    "**NOTE: This is a raw draft that will be populated with more material (especially visual) and explanations, especially, facilitating more gradual AI/ML concept introduction.**\n",
    "\n",
    "Contents:\n",
    "\n",
    "- Download PlugX Sample\n",
    "- Machine Learning with Dynamic Malware Analysis \n",
    "- Speakeasy Emulator\n",
    "- PyTorch Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download PlugX Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'MZP\\x00\\x02\\x00\\x00\\x00\\x04\\x00\\x0f\\x00\\xff\\xff\\x00\\x00\\xb8\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import py7zr\n",
    "import os\n",
    "\n",
    "def download_archive(link: str) -> bytes:\n",
    "    assert link.endswith(\".7z\"), \"link must end with .7z\"\n",
    "    archive_name = link.split(\"/\")[-1]\n",
    "    archive_name_no_ext = archive_name.replace(\".7z\", \"\")\n",
    "    archive = requests.get(link).content\n",
    "    with open(archive_name, \"wb\") as f:\n",
    "        f.write(archive)\n",
    "    with py7zr.SevenZipFile(archive_name, \"r\", password='infected') as archive:\n",
    "        content = archive.read(targets=archive_name_no_ext)[archive_name_no_ext].read()\n",
    "    os.remove(archive_name)\n",
    "    return content\n",
    "\n",
    "vx_link = \"https://samples.vx-underground.org/Samples/Families/PlugX/0D219AA54B1D417DA61BD4AED5EEB53D6CBA91B3287D53186B21FED450248215.7z\"\n",
    "plugx_rat_bytez = download_archive(vx_link)\n",
    "print(plugx_rat_bytez[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model with Dynamic Malware Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/dtrizna/nebula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 API calls invoked by PlugX RAT:\n",
      "\n",
      "{'api_name': 'kernel32.GetModuleHandleA', 'args': ['0x0'], 'ret_val': '0x400000'}\n",
      "{'api_name': 'user32.GetKeyboardType', 'args': ['0x0'], 'ret_val': '0x4'}\n",
      "{'api_name': 'kernel32.GetCommandLineA', 'args': [], 'ret_val': '0x45f0'}\n",
      "{'api_name': 'kernel32.GetStartupInfoA', 'args': ['0x1211f30'], 'ret_val': None}\n",
      "{'api_name': 'kernel32.GetVersion', 'args': [], 'ret_val': '0x1db10106'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import nebula\n",
    "\n",
    "nebula_transformer = nebula.Nebula(tokenizer='bpe')\n",
    "plugx_rat_report = nebula_transformer.dynamic_analysis_pe_file(plugx_rat_bytez)\n",
    "\n",
    "print(\"First 5 API calls invoked by PlugX RAT:\\n\")\n",
    "_ = [print(val) for val in plugx_rat_report['apis'][0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dynamic features: (1, 512)\n",
      "\n",
      "First 5 dynamic features:\n",
      "\n",
      "[ 2235  1036   530  1203   778  1078   125  1103 15492 49966   932   530\n",
      "  1203   778   560  1176  1103 15492 49966   932]\n"
     ]
    }
   ],
   "source": [
    "plugx_dynamic_features = nebula_transformer.preprocess(plugx_rat_report)\n",
    "\n",
    "print(f\"Shape of dynamic features: {plugx_dynamic_features.shape}\\n\")\n",
    "\n",
    "print(f\"First 5 dynamic features:\\n\\n{plugx_dynamic_features[0, 0:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of '0d219aa54b1d417da61bd4aed5eeb53d6cba91b3287d53186b21fed450248215' being malware: 93.04%\n",
      "Probability of 'calc.exe' being malware: 0.04%\n"
     ]
    }
   ],
   "source": [
    "from torch import manual_seed\n",
    "manual_seed(0)\n",
    "prob = nebula_transformer.predict_proba(plugx_dynamic_features)\n",
    "\n",
    "hhash = vx_link.split(\"/\")[-1].split(\".\")[0].lower()\n",
    "print(f\"Probability of '{hhash}' being malware: {prob*100:.2f}%\")\n",
    "\n",
    "if os.path.exists(r\"C:\\windows\\system32\\calc.exe\"):\n",
    "    with open (r\"C:\\windows\\system32\\calc.exe\", \"rb\") as f:\n",
    "        calc_bytez = f.read()\n",
    "    report = nebula_transformer.dynamic_analysis_pe_file(calc_bytez)\n",
    "    dynamic_features = nebula_transformer.preprocess(report)\n",
    "    manual_seed(0)\n",
    "    prob = nebula_transformer.predict_proba(dynamic_features)\n",
    "    print(f\"Probability of 'calc.exe' being malware: {prob*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speakeasy Emulator\n",
    "\n",
    "How this is achieved under the hood?\n",
    "\n",
    "Speakeasy is a Python-based emulator build and actively maintained by Mandiant.\n",
    "\n",
    "It is built on top of the Unicorn emulator framework, and emulated x86 architecture solely with a focus on malware analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speakeasy\n",
    "emulator = speakeasy.Speakeasy()\n",
    "\n",
    "module = emulator.load_module(data=plugx_rat_bytez)\n",
    "emulator.run_module(module)\n",
    "report = emulator.get_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pc': '0x4069ad', 'api_name': 'kernel32.GetModuleHandleA', 'args': ['0x0'], 'ret_val': '0x400000'}\n",
      "{'pc': '0x40398a', 'api_name': 'user32.GetKeyboardType', 'args': ['0x0'], 'ret_val': '0x4'}\n",
      "{'pc': '0x406870', 'api_name': 'kernel32.GetCommandLineA', 'args': [], 'ret_val': '0x45f0'}\n",
      "{'pc': '0x401333', 'api_name': 'kernel32.GetStartupInfoA', 'args': ['0x1211f30'], 'ret_val': None}\n",
      "{'pc': '0x406884', 'api_name': 'kernel32.GetVersion', 'args': [], 'ret_val': '0x1db10106'}\n"
     ]
    }
   ],
   "source": [
    "_ = [print(val) for val in report['entry_points'][0]['apis'][0:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the same values as the ones we got from the `nebula` analysis above. In reality, Speakeasy has extra fields that are a potential source of information for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'open_key',\n",
       "  'path': 'HKEY_CURRENT_USER\\\\Software\\\\Borland\\\\Locales'},\n",
       " {'event': 'open_key',\n",
       "  'path': 'HKEY_LOCAL_MACHINE\\\\Software\\\\Borland\\\\Locales'},\n",
       " {'event': 'open_key',\n",
       "  'path': 'HKEY_CURRENT_USER\\\\Software\\\\Borland\\\\Delphi\\\\Locales'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report['entry_points'][0]['registry_access']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speakeasy is adjustable tool and supports a variety of configurations, for instance, it is possible to modify environment variables, user and domain information, simulate network state, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comspec': 'C:\\\\Windows\\\\system32\\\\cmd.exe',\n",
       " 'systemroot': 'C:\\\\Windows',\n",
       " 'windir': 'C:\\\\Windows',\n",
       " 'temp': 'C:\\\\Windows\\\\temp\\\\',\n",
       " 'userprofile': 'C:\\\\Users\\\\dtrizna',\n",
       " 'systemdrive': 'C:',\n",
       " 'allusersprofile': 'C:\\\\ProgramData',\n",
       " 'programfiles': 'C:\\\\Program Files'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "speakeasy_config = os.path.join(os.path.dirname(nebula.__file__), \"objects\", \"speakeasy_config.json\")\n",
    "\n",
    "with open(speakeasy_config, \"r\") as f:\n",
    "    speakeasy_config = json.load(f)\n",
    "\n",
    "speakeasy_config['env']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo.bar\n",
      "{'name': 'dtrizna', 'is_admin': True}\n"
     ]
    }
   ],
   "source": [
    "print(speakeasy_config['domain'])\n",
    "print(speakeasy_config['user'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dns': {'names': {'work.foo.bar': '127.0.0.1',\n",
       "   'foo.bar': '10.1.2.3',\n",
       "   'default': '10.1.2.3',\n",
       "   'google.com': '8.8.8.8',\n",
       "   'localhost': '127.0.0.1'},\n",
       "  'txt': [{'name': 'default', 'path': '$ROOT$/resources/web/default.bin'}]},\n",
       " 'http': {'responses': [{'verb': 'GET',\n",
       "    'files': [{'mode': 'default', 'path': '$ROOT$/resources/web/default.bin'},\n",
       "     {'mode': 'by_ext',\n",
       "      'ext': 'gif',\n",
       "      'path': '$ROOT$/resources/web/decoy.gif'},\n",
       "     {'mode': 'by_ext',\n",
       "      'ext': 'jpg',\n",
       "      'path': '$ROOT$/resources/web/decoy.jpg'}]}]},\n",
       " 'winsock': {'responses': [{'mode': 'default',\n",
       "    'path': '$ROOT$/resources/web/stager.bin'}]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakeasy_config['network']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "Nebula uses Transformer model to classify malware, the same architecture used in GPT. Transformer is a deep learning model that is based on the attention mechanism. To understand how it works, we need to grasp basics of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderChunks(\n",
       "  (encoder): Embedding(50001, 64)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.3, inplace=False)\n",
       "        (dropout2): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ffnn): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=32768, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classifier_head): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nebula_transformer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Introduction\n",
    "\n",
    "PyTorch is a Python library for implementing Deep Learning models. Deep Learning is a subfield of Machine Learning that uses **Neural Networks** to learn complex patterns in data. \n",
    "\n",
    "<img src=\"./img/ai_ml_dl.png\" width=\"400\">\n",
    "\n",
    "During last years PyTorch became a de-facto standard for Deep Learning research, substituting the previous leader TensorFlow. PyTorch is a very flexible library that allows to implement complex models with a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "PyTorch operates with tensors:\n",
    "\n",
    "<img src=\"./img/tensors.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_a = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(tensor_a.shape)\n",
    "tensor_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_b = tensor_a.reshape(3, 3)\n",
    "print(tensor_b.shape)\n",
    "tensor_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 dimensional tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [7., 8., 9.]],\n",
       "\n",
       "        [[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [7., 8., 9.]],\n",
       "\n",
       "        [[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [7., 8., 9.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_c = torch.vstack([tensor_a, tensor_a, tensor_a]).reshape(3, 3, 3)\n",
    "print(tensor_c.shape)\n",
    "tensor_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing\n",
    "tensor_c[2, 1, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2902],\n",
       "        [0.8687],\n",
       "        [1.4472]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "layer_a = nn.Linear(3, 1)\n",
    "layer_a(tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
