{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Cnn1DLinear(nn.Module):\n",
    "    def __init__(self, \n",
    "                # embedding params\n",
    "                vocabSize = None,\n",
    "                embeddingDim = 64,\n",
    "                paddingIdx = 0,\n",
    "                # conv params\n",
    "                filterSizes = [2, 3, 4, 5],\n",
    "                numFilters = [128, 128, 128, 128],\n",
    "                batchNormConv = False,\n",
    "                convPadding = 0, # default\n",
    "                # ffnn params\n",
    "                hiddenNeurons = [512, 256],\n",
    "                batchNormFFNN = False,\n",
    "                dropout = 0.5,\n",
    "                # pretrain layers\n",
    "                pretrainLayers = [512, 1024],\n",
    "                numClasses = 1): # binary classification\n",
    "        super().__init__()\n",
    "        self.__name__ = \"Cnn1DLinear\"\n",
    "        # embdding\n",
    "        self.embedding = nn.Embedding(vocabSize, \n",
    "                                  embeddingDim, \n",
    "                                  padding_idx=paddingIdx)\n",
    "\n",
    "        # convolutions\n",
    "        self.conv1dModule = nn.ModuleList()\n",
    "        for i in range(len(filterSizes)):\n",
    "                if batchNormConv:\n",
    "                    module = nn.Sequential(\n",
    "                                nn.Conv1d(in_channels=embeddingDim,\n",
    "                                    out_channels=numFilters[i],\n",
    "                                    kernel_size=filterSizes[i]),\n",
    "                                nn.BatchNorm1d(numFilters[i])\n",
    "                            )\n",
    "                else:\n",
    "                    module = nn.Conv1d(in_channels=embeddingDim,\n",
    "                                    out_channels=numFilters[i],\n",
    "                                    kernel_size=filterSizes[i],\n",
    "                                    #padding=filterSizes[i]//2\n",
    "                                    padding=convPadding\n",
    "                                )\n",
    "                self.conv1dModule.append(module)\n",
    "        convOut = np.sum(numFilters)\n",
    "        \n",
    "        # core ffnn\n",
    "        self.ffnn = []\n",
    "        for i,h in enumerate(hiddenNeurons):\n",
    "            self.ffnnBlock = []\n",
    "            if i == 0:\n",
    "                self.ffnnBlock.append(nn.Linear(convOut, h))\n",
    "            else:\n",
    "                self.ffnnBlock.append(nn.Linear(hiddenNeurons[i-1], h))\n",
    "\n",
    "            # add BatchNorm to every layer except last\n",
    "            if batchNormFFNN and i < len(hiddenNeurons)-1:\n",
    "                self.ffnnBlock.append(nn.BatchNorm1d(h))\n",
    "\n",
    "            self.ffnnBlock.append(nn.ReLU())\n",
    "\n",
    "            if dropout:\n",
    "                self.ffnnBlock.append(nn.Dropout(dropout))\n",
    "            \n",
    "            self.ffnn.append(nn.Sequential(*self.ffnnBlock))\n",
    "        self.ffnn = nn.Sequential(*self.ffnn)\n",
    "        \n",
    "        # classification output\n",
    "        self.fcOutput = nn.Linear(hiddenNeurons[-1], numClasses)\n",
    "\n",
    "        # pretrain layers\n",
    "        self.preTrainLayers = []\n",
    "        for i, h in enumerate(pretrainLayers):\n",
    "            self.preTrainBlock = []\n",
    "            if i == 0:\n",
    "                self.preTrainBlock.append(nn.Linear(hiddenNeurons[-1], h))                \n",
    "            else:\n",
    "                self.preTrainBlock.append(nn.Linear(pretrainLayers[i-1], h))\n",
    "            self.preTrainBlock.append(nn.ReLU())\n",
    "            if dropout:\n",
    "                self.preTrainBlock.append(nn.Dropout(dropout))\n",
    "            self.preTrainLayers.append(nn.Sequential(*self.preTrainBlock))\n",
    "        self.preTrainLayers.append(nn.Linear(pretrainLayers[-1], vocabSize))\n",
    "        self.preTrainLayers = nn.Sequential(*self.preTrainLayers)\n",
    "\n",
    "    @staticmethod\n",
    "    def convAndMaxPool(x, conv):\n",
    "        \"\"\"Convolution and global max pooling layer\"\"\"\n",
    "        # conv(x).permute(0, 2, 1) is of shape (batch_size, sequence_length, num_filters)\n",
    "        # max(1)[0] is of shape (batch_size, num_filters)\n",
    "        return F.relu(conv(x).permute(0, 2, 1).max(1)[0])\n",
    "\n",
    "    def core(self, inputs):\n",
    "        embedded = self.embedding(inputs).permute(0, 2, 1)\n",
    "        x_conv = [self.convAndMaxPool(embedded, conv1d) for conv1d in self.conv1dModule]\n",
    "        # x_conv: list of tensors of shape (batch_size, num_filters)\n",
    "        # torch.cat(x_conv, dim=1) is of shape (batch_size, num_filters * len(kernel_sizes))\n",
    "        x_fc = self.ffnn(torch.cat(x_conv, dim=1))\n",
    "        return x_fc\n",
    "\n",
    "    def pretrain(self, inputs):\n",
    "        x_core = self.core(inputs)\n",
    "        return self.preTrainLayers(x_core)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_core = self.core(inputs)\n",
    "        return self.fcOutput(x_core)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Masking sequences...\n",
      "100%|██████████| 4000/4000 [00:01<00:00, 2835.67it/s]\n",
      "WARNING:root:Pre-training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/4000 (0%)]\tLoss: 745.647583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Training pre-trained model on downstream task...\n",
      "WARNING:root:Training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1000 (0%)]\tLoss: 1.003640\tF1: 0.166124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Training new model on downstream task...\n",
      "WARNING:root:Training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1000 (0%)]\tLoss: 0.732999\tF1: 0.292818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Training new model on downstream task on full dataset...\n",
      "WARNING:root:Training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 0.718357\tF1: 0.309973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Evaluating all models on test set...\n",
      "100%|██████████| 68/68 [00:06<00:00, 11.30it/s]\n",
      "100%|██████████| 68/68 [00:06<00:00, 11.33it/s]\n",
      "100%|██████████| 68/68 [00:06<00:00, 11.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from nebula.evaluation import get_tpr_at_fpr\n",
    "from nebula.pretraining import maskSequenceArr\n",
    "\n",
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def pretrain(U_masked, U_target, model, batchSize, pretrinEpochs, device, verbosityBatches=50):\n",
    "    # make a loader from U_masked and U_target\n",
    "    preTrainLoader = torch.utils.data.DataLoader(\n",
    "        # create dataset from U_masked and U_target numpy arrays\n",
    "        torch.utils.data.TensorDataset(torch.from_numpy(U_masked), torch.from_numpy(U_target)),\n",
    "        batch_size=batchSize,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    lossFunction = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # pre-train model\n",
    "    for epoch in range(1, pretrinEpochs+1):\n",
    "        for batch_idx, (data, target) in enumerate(preTrainLoader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            pred_masked_vocab = model.pretrain(data)\n",
    "            loss = lossFunction(pred_masked_vocab, target.float())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % verbosityBatches == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(preTrainLoader.dataset),\n",
    "                    100. * batch_idx / len(preTrainLoader), loss.item()))\n",
    "\n",
    "\n",
    "def downstream(L_x, L_y, model, batchSize, downstreamEpochs, device, verbosityBatches=50):\n",
    "    # make a loader from L_x and L_y\n",
    "    downstreamLoader = torch.utils.data.DataLoader(\n",
    "        # create dataset from L_x and L_y numpy arrays\n",
    "        torch.utils.data.TensorDataset(torch.from_numpy(L_x), torch.from_numpy(L_y)),\n",
    "        batch_size=batchSize,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    logging.warning('Training model...')\n",
    "    lossFunction = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(1, downstreamEpochs+1):\n",
    "        for batch_idx, (data, target) in enumerate(downstreamLoader):\n",
    "            data, target = data.to(device), target.to(device).reshape(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            logits = model(data)\n",
    "            loss = lossFunction(logits, target.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            y_pred_probs = torch.sigmoid(logits).clone().detach().cpu().numpy()\n",
    "            f1 = f1_score(target.cpu().detach().numpy(), y_pred_probs > 0.5, average='macro')\n",
    "            if batch_idx % verbosityBatches == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tF1: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(downstreamLoader.dataset),\n",
    "                    100. * batch_idx / len(downstreamLoader), loss.item(), f1))\n",
    "\n",
    "def evaluate(model, x_test, y_test, fprs, device, batchSize):\n",
    "\n",
    "    testLoader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(torch.from_numpy(x_test).long(), torch.from_numpy(y_test).float()),\n",
    "            batch_size=batchSize,\n",
    "            shuffle=True\n",
    "        )\n",
    "    model.eval()\n",
    "\n",
    "    lossFunction = nn.BCEWithLogitsLoss()\n",
    "    metrics = defaultdict(lambda: defaultdict(list))\n",
    "    for data, target in tqdm(testLoader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(data)\n",
    "\n",
    "        loss = lossFunction(logits, target.float().reshape(-1,1))\n",
    "        y_pred_probs = torch.sigmoid(logits).clone().detach().cpu().numpy()\n",
    "        \n",
    "        target = target.clone().detach().cpu().numpy().reshape(-1,1)\n",
    "        for fpr in fprs:\n",
    "            tpr_at_fpr, threshold_at_fpr = get_tpr_at_fpr(target, y_pred_probs, fpr)\n",
    "            # f1 = f1_score(y[test_index], predicted_probs >= threshold)\n",
    "            f1_at_fpr = f1_score(target, y_pred_probs >= threshold_at_fpr)\n",
    "            metrics[fpr][\"tpr\"].append(tpr_at_fpr)\n",
    "            metrics[fpr][\"f1\"].append(f1_at_fpr)\n",
    "            metrics[fpr][\"loss\"].append(loss.item())\n",
    "    # take the mean of the metrics\n",
    "    for fpr in metrics:\n",
    "        metrics[fpr] = DataFrame(metrics[fpr])\n",
    "        # add std for each metric - tpr, f1, loss\n",
    "        metrics[fpr][\"tpr_std\"] = metrics[fpr][\"tpr\"].std()\n",
    "        metrics[fpr][\"f1_std\"] = metrics[fpr][\"f1\"].std()\n",
    "        metrics[fpr][\"loss_std\"] = metrics[fpr][\"loss\"].std()\n",
    "        # take the mean of the metrics\n",
    "        metrics[fpr] = metrics[fpr].mean(axis=0)\n",
    "        metrics[fpr] = metrics[fpr].to_dict()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def cef_ssl(x, y, x_test, y_test, vocab, modelClass, modelConfig, fprs=[0.0001, 0.001, 0.01, 0.1], test_size=0.2, random_state=42, batchSize=32, pretrinEpochs=3, downstreamEpochs=3, device='cpu', verbosityBatches=50, mask_probability=0.15):\n",
    "    \n",
    "    # split x and y into train and validation sets\n",
    "    U, L_x, _, L_y = train_test_split(x, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # for each sequence in U, mask it\n",
    "    logging.warning('Masking sequences...')\n",
    "    U_masked, U_target = maskSequenceArr(U, vocab, mask_probability=0.15, random_state=None)\n",
    "\n",
    "    # pre-train model\n",
    "    logging.warning('Pre-training model...')\n",
    "    model_Pretrained = modelClass(**modelConfig)\n",
    "    model_Pretrained.to(device)\n",
    "    pretrain(U_masked, U_target, model_Pretrained, batchSize, pretrinEpochs, device, verbosityBatches)\n",
    "\n",
    "    # downstream task for pretrained model\n",
    "    logging.warning('Training pre-trained model on downstream task...')\n",
    "    downstream(L_x, L_y, model_Pretrained, batchSize, downstreamEpochs, device, verbosityBatches)\n",
    "    \n",
    "    # downstream task for new model\n",
    "    logging.warning('Training new model on downstream task...')\n",
    "    model_NonPretrained = modelClass(**modelConfig)\n",
    "    model_NonPretrained.to(device)\n",
    "    downstream(L_x, L_y, model_NonPretrained, batchSize, downstreamEpochs, device, verbosityBatches)\n",
    "\n",
    "    # downstream task for new model on full dataset suitable for benchmarking\n",
    "    logging.warning('Training new model on downstream task on full dataset...')\n",
    "    model_Full = modelClass(**modelConfig)\n",
    "    model_Full.to(device)\n",
    "    downstream(x, y, model_Full, batchSize, downstreamEpochs, device, verbosityBatches)\n",
    "\n",
    "    logging.warning('Evaluating all models on test set...')\n",
    "    # get fpr and f1 on test set on pretrained model\n",
    "    metrics_Pretrained = evaluate(model_Pretrained, x_test, y_test, fprs, device, batchSize)\n",
    "    # get performance metrics on fresh model\n",
    "    metrics_nonPretrained = evaluate(model_NonPretrained, x_test, y_test, fprs, device, batchSize)\n",
    "    # get performance metrics on full model\n",
    "    metrics_full = evaluate(model_Full, x_test, y_test, fprs, device, batchSize)\n",
    "    \n",
    "    return metrics_Pretrained, metrics_nonPretrained, metrics_full\n",
    "\n",
    "train_limit = 5000\n",
    "\n",
    "xTrainFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_trainset_WithAPIargs\\speakeasy_VocabSize_10000_maxLen_2048_x.npy\"\n",
    "xTrain = np.load(xTrainFile)[:train_limit]\n",
    "yTrainFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_trainset_WithAPIargs\\speakeasy_y.npy\"\n",
    "yTrain = np.load(yTrainFile)[:train_limit]\n",
    "xTestFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_testset_WithAPIargs\\speakeasy_VocabSize_10000_maxLen_2048_x.npy\"\n",
    "xTest = np.load(xTestFile)\n",
    "yTestFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_testset_WithAPIargs\\speakeasy_y.npy\"\n",
    "yTest = np.load(yTestFile)\n",
    "\n",
    "vocabFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_trainset_WithAPIargs\\speakeasy_VocabSize_10000.pkl\"\n",
    "with open(vocabFile, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "modelConfig = {\n",
    "    \"vocabSize\": len(vocab)\n",
    "}\n",
    "modelClass = Cnn1DLinear\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "metrics_Pretrained, metrics_nonPretrained, metrics_full = cef_ssl(xTrain, yTrain, xTest, yTest, vocab, modelClass, modelConfig, device=device, pretrinEpochs=1, downstreamEpochs=1, verbosityBatches=100, batchSize=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29 +- 0.06 -- pretrined with MLM\n",
      "0.14 +- 0.05 -- non pretrained, same data\n",
      "0.00 +- 0.00 -- full dataset\n"
     ]
    }
   ],
   "source": [
    "metrics = dict(zip(['pretrined with MLM', 'non pretrained, same data', 'full dataset'], [metrics_Pretrained, metrics_nonPretrained, metrics_full]))\n",
    "for m in metrics:\n",
    "    for fpr in metrics[m]:\n",
    "        print(\"FPR: \", fpr)\n",
    "        print(f\"{metrics[m][fpr]['tpr']:.2f} +- {metrics[m][fpr]['tpr_std']:.2f} -- {m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e123ac7eba4d44924a894b1be2fc564282b1d2645e9d64ed33bc5003b6c2a87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
