01/28/2023 01:46:09 PM  [!] Starting Masked Language Model evaluation over 3 splits!
01/28/2023 01:46:09 PM  [!] Loaded data and vocab. X train size: (76126, 512), X test size: (17407, 512), vocab size: 50002
01/28/2023 01:46:09 PM  [!] Initiating Self-Supervised Learning Pretraining
     Language Modeling MaskedLanguageModel
     Model TransformerEncoderLM with config:
	{'vocabSize': 50002, 'maxLen': 512, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'dropout': 0.3}

01/28/2023 01:46:09 PM  [!] Running pre-training split 1/3
01/28/2023 01:46:11 PM  [!] Pre-training model...
01/28/2023 01:46:12 PM  [*] Masking sequences: iteration 1...
01/28/2023 01:46:29 PM  [*] Started epoch: 1
01/28/2023 01:46:31 PM  [*] Sat Jan 28 13:46:31 2023: Train Epoch: 1 [  0  /60900 (0 %)]	Loss: 469.091492 | Elapsed: 2.31s
01/28/2023 01:46:43 PM  [*] Sat Jan 28 13:46:43 2023: Train Epoch: 1 [6400 /60900 (11%)]	Loss: 240.311493 | Elapsed: 11.76s
01/28/2023 01:46:55 PM  [*] Sat Jan 28 13:46:55 2023: Train Epoch: 1 [12800/60900 (21%)]	Loss: 208.109116 | Elapsed: 11.88s
01/28/2023 01:47:07 PM  [*] Sat Jan 28 13:47:07 2023: Train Epoch: 1 [19200/60900 (32%)]	Loss: 216.385132 | Elapsed: 12.14s
01/28/2023 01:47:19 PM  [*] Sat Jan 28 13:47:19 2023: Train Epoch: 1 [25600/60900 (42%)]	Loss: 216.906372 | Elapsed: 12.19s
01/28/2023 01:47:32 PM  [*] Sat Jan 28 13:47:32 2023: Train Epoch: 1 [32000/60900 (53%)]	Loss: 227.256714 | Elapsed: 12.31s
01/28/2023 01:47:44 PM  [*] Sat Jan 28 13:47:44 2023: Train Epoch: 1 [38400/60900 (63%)]	Loss: 188.191376 | Elapsed: 12.37s
01/28/2023 01:47:56 PM  [*] Sat Jan 28 13:47:56 2023: Train Epoch: 1 [44800/60900 (74%)]	Loss: 211.565796 | Elapsed: 12.39s
01/28/2023 01:48:09 PM  [*] Sat Jan 28 13:48:09 2023: Train Epoch: 1 [51200/60900 (84%)]	Loss: 179.027771 | Elapsed: 12.42s
01/28/2023 01:48:21 PM  [*] Sat Jan 28 13:48:21 2023: Train Epoch: 1 [57600/60900 (95%)]	Loss: 192.293289 | Elapsed: 12.52s
01/28/2023 01:48:30 PM  [*] Sat Jan 28 13:48:30 2023:    1    | Tr.loss: 208.109996 | Elapsed:  120.51  s
01/28/2023 01:48:30 PM [!] Sat Jan 28 13:48:30 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_1_1674910110-model.torch
01/28/2023 01:48:31 PM  [*] Masking sequences: iteration 2...
01/28/2023 01:48:51 PM  [*] Started epoch: 2
01/28/2023 01:48:51 PM  [*] Sat Jan 28 13:48:51 2023: Train Epoch: 2 [  0  /60900 (0 %)]	Loss: 191.396072 | Elapsed: 0.27s
01/28/2023 01:49:03 PM  [*] Sat Jan 28 13:49:03 2023: Train Epoch: 2 [6400 /60900 (11%)]	Loss: 193.474625 | Elapsed: 12.15s
01/28/2023 01:49:15 PM  [*] Sat Jan 28 13:49:15 2023: Train Epoch: 2 [12800/60900 (21%)]	Loss: 195.259720 | Elapsed: 12.17s
01/28/2023 01:49:28 PM  [*] Sat Jan 28 13:49:28 2023: Train Epoch: 2 [19200/60900 (32%)]	Loss: 191.437073 | Elapsed: 12.20s
01/28/2023 01:49:40 PM  [*] Sat Jan 28 13:49:40 2023: Train Epoch: 2 [25600/60900 (42%)]	Loss: 186.817047 | Elapsed: 12.21s
01/28/2023 01:49:52 PM  [*] Sat Jan 28 13:49:52 2023: Train Epoch: 2 [32000/60900 (53%)]	Loss: 205.383102 | Elapsed: 12.25s
01/28/2023 01:50:04 PM  [*] Sat Jan 28 13:50:04 2023: Train Epoch: 2 [38400/60900 (63%)]	Loss: 192.143082 | Elapsed: 12.20s
01/28/2023 01:50:16 PM  [*] Sat Jan 28 13:50:16 2023: Train Epoch: 2 [44800/60900 (74%)]	Loss: 190.170654 | Elapsed: 12.21s
01/28/2023 01:50:29 PM  [*] Sat Jan 28 13:50:29 2023: Train Epoch: 2 [51200/60900 (84%)]	Loss: 179.667679 | Elapsed: 12.22s
01/28/2023 01:50:41 PM  [*] Sat Jan 28 13:50:41 2023: Train Epoch: 2 [57600/60900 (95%)]	Loss: 187.568817 | Elapsed: 12.22s
01/28/2023 01:50:49 PM  [*] Sat Jan 28 13:50:49 2023:    2    | Tr.loss: 187.770602 | Elapsed:  117.92  s
01/28/2023 01:50:49 PM [!] Sat Jan 28 13:50:49 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_2_1674910249-model.torch
01/28/2023 01:50:50 PM  [*] Masking sequences: iteration 3...
01/28/2023 01:51:10 PM  [*] Started epoch: 3
01/28/2023 01:51:11 PM  [*] Sat Jan 28 13:51:11 2023: Train Epoch: 3 [  0  /60900 (0 %)]	Loss: 183.463486 | Elapsed: 0.30s
01/28/2023 01:51:23 PM  [*] Sat Jan 28 13:51:23 2023: Train Epoch: 3 [6400 /60900 (11%)]	Loss: 175.240356 | Elapsed: 12.46s
01/28/2023 01:51:36 PM  [*] Sat Jan 28 13:51:36 2023: Train Epoch: 3 [12800/60900 (21%)]	Loss: 176.177521 | Elapsed: 12.29s
01/28/2023 01:51:48 PM  [*] Sat Jan 28 13:51:48 2023: Train Epoch: 3 [19200/60900 (32%)]	Loss: 168.037888 | Elapsed: 12.39s
01/28/2023 01:52:00 PM  [*] Sat Jan 28 13:52:00 2023: Train Epoch: 3 [25600/60900 (42%)]	Loss: 177.572327 | Elapsed: 12.45s
01/28/2023 01:52:13 PM  [*] Sat Jan 28 13:52:13 2023: Train Epoch: 3 [32000/60900 (53%)]	Loss: 195.283829 | Elapsed: 12.43s
01/28/2023 01:52:25 PM  [*] Sat Jan 28 13:52:25 2023: Train Epoch: 3 [38400/60900 (63%)]	Loss: 201.537674 | Elapsed: 12.43s
01/28/2023 01:52:38 PM  [*] Sat Jan 28 13:52:38 2023: Train Epoch: 3 [44800/60900 (74%)]	Loss: 165.427887 | Elapsed: 12.48s
01/28/2023 01:52:50 PM  [*] Sat Jan 28 13:52:50 2023: Train Epoch: 3 [51200/60900 (84%)]	Loss: 196.306595 | Elapsed: 12.42s
01/28/2023 01:53:03 PM  [*] Sat Jan 28 13:53:03 2023: Train Epoch: 3 [57600/60900 (95%)]	Loss: 148.191681 | Elapsed: 12.42s
01/28/2023 01:53:11 PM  [*] Sat Jan 28 13:53:11 2023:    3    | Tr.loss: 182.881613 | Elapsed:  120.31  s
01/28/2023 01:53:11 PM [!] Sat Jan 28 13:53:11 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_3_1674910391-model.torch
01/28/2023 01:53:12 PM  [*] Masking sequences: iteration 4...
01/28/2023 01:53:34 PM  [*] Started epoch: 4
01/28/2023 01:53:35 PM  [*] Sat Jan 28 13:53:35 2023: Train Epoch: 4 [  0  /60900 (0 %)]	Loss: 192.502625 | Elapsed: 0.44s
01/28/2023 01:53:47 PM  [*] Sat Jan 28 13:53:47 2023: Train Epoch: 4 [6400 /60900 (11%)]	Loss: 191.399200 | Elapsed: 12.34s
01/28/2023 01:53:59 PM  [*] Sat Jan 28 13:53:59 2023: Train Epoch: 4 [12800/60900 (21%)]	Loss: 175.791046 | Elapsed: 12.39s
01/28/2023 01:54:12 PM  [*] Sat Jan 28 13:54:12 2023: Train Epoch: 4 [19200/60900 (32%)]	Loss: 190.402374 | Elapsed: 12.36s
01/28/2023 01:54:24 PM  [*] Sat Jan 28 13:54:24 2023: Train Epoch: 4 [25600/60900 (42%)]	Loss: 174.915955 | Elapsed: 12.42s
01/28/2023 01:54:37 PM  [*] Sat Jan 28 13:54:37 2023: Train Epoch: 4 [32000/60900 (53%)]	Loss: 176.402954 | Elapsed: 12.40s
01/28/2023 01:54:49 PM  [*] Sat Jan 28 13:54:49 2023: Train Epoch: 4 [38400/60900 (63%)]	Loss: 168.030792 | Elapsed: 12.37s
01/28/2023 01:55:01 PM  [*] Sat Jan 28 13:55:01 2023: Train Epoch: 4 [44800/60900 (74%)]	Loss: 169.783737 | Elapsed: 12.39s
01/28/2023 01:55:14 PM  [*] Sat Jan 28 13:55:14 2023: Train Epoch: 4 [51200/60900 (84%)]	Loss: 166.960724 | Elapsed: 12.35s
01/28/2023 01:55:26 PM  [*] Sat Jan 28 13:55:26 2023: Train Epoch: 4 [57600/60900 (95%)]	Loss: 205.754684 | Elapsed: 12.41s
01/28/2023 01:55:34 PM  [*] Sat Jan 28 13:55:34 2023:    4    | Tr.loss: 180.528504 | Elapsed:  120.00  s
01/28/2023 01:55:35 PM [!] Sat Jan 28 13:55:35 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_4_1674910534-model.torch
01/28/2023 01:55:36 PM  [*] Masking sequences: iteration 5...
01/28/2023 01:55:58 PM  [*] Started epoch: 5
01/28/2023 01:55:58 PM  [*] Sat Jan 28 13:55:58 2023: Train Epoch: 5 [  0  /60900 (0 %)]	Loss: 175.823700 | Elapsed: 0.35s
01/28/2023 01:56:11 PM  [*] Sat Jan 28 13:56:11 2023: Train Epoch: 5 [6400 /60900 (11%)]	Loss: 192.821487 | Elapsed: 12.44s
01/28/2023 01:56:23 PM  [*] Sat Jan 28 13:56:23 2023: Train Epoch: 5 [12800/60900 (21%)]	Loss: 155.327942 | Elapsed: 12.41s
01/28/2023 01:56:35 PM  [*] Sat Jan 28 13:56:35 2023: Train Epoch: 5 [19200/60900 (32%)]	Loss: 184.801453 | Elapsed: 12.34s
01/28/2023 01:56:48 PM  [*] Sat Jan 28 13:56:48 2023: Train Epoch: 5 [25600/60900 (42%)]	Loss: 171.905426 | Elapsed: 12.39s
01/28/2023 01:57:00 PM  [*] Sat Jan 28 13:57:00 2023: Train Epoch: 5 [32000/60900 (53%)]	Loss: 202.606232 | Elapsed: 12.37s
01/28/2023 01:57:13 PM  [*] Sat Jan 28 13:57:13 2023: Train Epoch: 5 [38400/60900 (63%)]	Loss: 195.337250 | Elapsed: 12.46s
01/28/2023 01:57:25 PM  [*] Sat Jan 28 13:57:25 2023: Train Epoch: 5 [44800/60900 (74%)]	Loss: 169.400757 | Elapsed: 12.43s
01/28/2023 01:57:37 PM  [*] Sat Jan 28 13:57:37 2023: Train Epoch: 5 [51200/60900 (84%)]	Loss: 170.230804 | Elapsed: 12.40s
01/28/2023 01:57:50 PM  [*] Sat Jan 28 13:57:50 2023: Train Epoch: 5 [57600/60900 (95%)]	Loss: 157.890503 | Elapsed: 12.51s
01/28/2023 01:57:58 PM  [*] Sat Jan 28 13:57:58 2023:    5    | Tr.loss: 178.797218 | Elapsed:  120.41  s
01/28/2023 01:57:59 PM [!] Sat Jan 28 13:57:59 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_5_1674910678-model.torch
01/28/2023 01:58:00 PM  [*] Masking sequences: iteration 6...
01/28/2023 01:58:20 PM  [*] Started epoch: 6
01/28/2023 01:58:21 PM  [*] Sat Jan 28 13:58:21 2023: Train Epoch: 6 [  0  /60900 (0 %)]	Loss: 181.402222 | Elapsed: 0.40s
01/28/2023 01:58:33 PM  [*] Sat Jan 28 13:58:33 2023: Train Epoch: 6 [6400 /60900 (11%)]	Loss: 183.612213 | Elapsed: 12.40s
01/28/2023 01:58:45 PM  [*] Sat Jan 28 13:58:45 2023: Train Epoch: 6 [12800/60900 (21%)]	Loss: 183.712891 | Elapsed: 12.33s
01/28/2023 01:58:58 PM  [*] Sat Jan 28 13:58:58 2023: Train Epoch: 6 [19200/60900 (32%)]	Loss: 155.914459 | Elapsed: 12.49s
01/28/2023 01:59:10 PM  [*] Sat Jan 28 13:59:10 2023: Train Epoch: 6 [25600/60900 (42%)]	Loss: 174.085999 | Elapsed: 12.37s
01/28/2023 01:59:23 PM  [*] Sat Jan 28 13:59:23 2023: Train Epoch: 6 [32000/60900 (53%)]	Loss: 188.935883 | Elapsed: 12.38s
01/28/2023 01:59:35 PM  [*] Sat Jan 28 13:59:35 2023: Train Epoch: 6 [38400/60900 (63%)]	Loss: 160.602005 | Elapsed: 12.38s
01/28/2023 01:59:47 PM  [*] Sat Jan 28 13:59:47 2023: Train Epoch: 6 [44800/60900 (74%)]	Loss: 182.965973 | Elapsed: 12.35s
01/28/2023 02:00:00 PM  [*] Sat Jan 28 14:00:00 2023: Train Epoch: 6 [51200/60900 (84%)]	Loss: 183.776474 | Elapsed: 12.35s
01/28/2023 02:00:12 PM  [*] Sat Jan 28 14:00:12 2023: Train Epoch: 6 [57600/60900 (95%)]	Loss: 172.762665 | Elapsed: 12.37s
01/28/2023 02:00:20 PM  [*] Sat Jan 28 14:00:20 2023:    6    | Tr.loss: 177.592417 | Elapsed:  119.94  s
01/28/2023 02:00:21 PM [!] Sat Jan 28 14:00:21 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_6_1674910820-model.torch
01/28/2023 02:00:22 PM  [*] Masking sequences: iteration 7...
01/28/2023 02:00:42 PM  [*] Started epoch: 7
01/28/2023 02:00:43 PM  [*] Sat Jan 28 14:00:43 2023: Train Epoch: 7 [  0  /60900 (0 %)]	Loss: 165.885681 | Elapsed: 0.46s
01/28/2023 02:00:55 PM  [*] Sat Jan 28 14:00:55 2023: Train Epoch: 7 [6400 /60900 (11%)]	Loss: 159.828018 | Elapsed: 12.35s
01/28/2023 02:01:07 PM  [*] Sat Jan 28 14:01:07 2023: Train Epoch: 7 [12800/60900 (21%)]	Loss: 186.243271 | Elapsed: 12.33s
01/28/2023 02:01:20 PM  [*] Sat Jan 28 14:01:20 2023: Train Epoch: 7 [19200/60900 (32%)]	Loss: 179.845062 | Elapsed: 12.30s
01/28/2023 02:01:32 PM  [*] Sat Jan 28 14:01:32 2023: Train Epoch: 7 [25600/60900 (42%)]	Loss: 160.826431 | Elapsed: 12.64s
01/28/2023 02:01:45 PM  [*] Sat Jan 28 14:01:45 2023: Train Epoch: 7 [32000/60900 (53%)]	Loss: 153.916733 | Elapsed: 12.70s
01/28/2023 02:01:58 PM  [*] Sat Jan 28 14:01:58 2023: Train Epoch: 7 [38400/60900 (63%)]	Loss: 165.453888 | Elapsed: 12.73s
01/28/2023 02:02:10 PM  [*] Sat Jan 28 14:02:10 2023: Train Epoch: 7 [44800/60900 (74%)]	Loss: 193.382767 | Elapsed: 12.81s
01/28/2023 02:02:23 PM  [*] Sat Jan 28 14:02:23 2023: Train Epoch: 7 [51200/60900 (84%)]	Loss: 176.209671 | Elapsed: 12.80s
01/28/2023 02:02:36 PM  [*] Sat Jan 28 14:02:36 2023: Train Epoch: 7 [57600/60900 (95%)]	Loss: 161.712814 | Elapsed: 12.90s
01/28/2023 02:02:45 PM  [*] Sat Jan 28 14:02:45 2023:    7    | Tr.loss: 177.104803 | Elapsed:  122.64  s
01/28/2023 02:02:45 PM [!] Sat Jan 28 14:02:45 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_7_1674910965-model.torch
01/28/2023 02:02:46 PM  [*] Masking sequences: iteration 8...
01/28/2023 02:03:09 PM  [*] Started epoch: 8
01/28/2023 02:03:09 PM  [*] Sat Jan 28 14:03:09 2023: Train Epoch: 8 [  0  /60900 (0 %)]	Loss: 175.360275 | Elapsed: 0.39s
01/28/2023 02:03:22 PM  [*] Sat Jan 28 14:03:22 2023: Train Epoch: 8 [6400 /60900 (11%)]	Loss: 198.591461 | Elapsed: 12.68s
01/28/2023 02:03:35 PM  [*] Sat Jan 28 14:03:35 2023: Train Epoch: 8 [12800/60900 (21%)]	Loss: 188.683777 | Elapsed: 12.65s
01/28/2023 02:03:47 PM  [*] Sat Jan 28 14:03:47 2023: Train Epoch: 8 [19200/60900 (32%)]	Loss: 189.903900 | Elapsed: 12.63s
01/28/2023 02:04:00 PM  [*] Sat Jan 28 14:04:00 2023: Train Epoch: 8 [25600/60900 (42%)]	Loss: 169.963776 | Elapsed: 12.62s
01/28/2023 02:04:13 PM  [*] Sat Jan 28 14:04:13 2023: Train Epoch: 8 [32000/60900 (53%)]	Loss: 169.981873 | Elapsed: 12.80s
01/28/2023 02:04:25 PM  [*] Sat Jan 28 14:04:25 2023: Train Epoch: 8 [38400/60900 (63%)]	Loss: 173.837967 | Elapsed: 12.78s
01/28/2023 02:04:38 PM  [*] Sat Jan 28 14:04:38 2023: Train Epoch: 8 [44800/60900 (74%)]	Loss: 181.075958 | Elapsed: 12.81s
01/28/2023 02:04:51 PM  [*] Sat Jan 28 14:04:51 2023: Train Epoch: 8 [51200/60900 (84%)]	Loss: 186.846375 | Elapsed: 12.65s
01/28/2023 02:05:04 PM  [*] Sat Jan 28 14:05:04 2023: Train Epoch: 8 [57600/60900 (95%)]	Loss: 162.454681 | Elapsed: 12.82s
01/28/2023 02:05:12 PM  [*] Sat Jan 28 14:05:12 2023:    8    | Tr.loss: 177.132783 | Elapsed:  123.38  s
01/28/2023 02:05:13 PM [!] Sat Jan 28 14:05:13 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_8_1674911112-model.torch
01/28/2023 02:05:14 PM  [*] Masking sequences: iteration 9...
01/28/2023 02:05:36 PM  [*] Started epoch: 9
01/28/2023 02:05:36 PM  [*] Sat Jan 28 14:05:36 2023: Train Epoch: 9 [  0  /60900 (0 %)]	Loss: 168.640823 | Elapsed: 0.44s
01/28/2023 02:05:48 PM  [*] Sat Jan 28 14:05:48 2023: Train Epoch: 9 [6400 /60900 (11%)]	Loss: 169.432907 | Elapsed: 12.34s
01/28/2023 02:06:01 PM  [*] Sat Jan 28 14:06:01 2023: Train Epoch: 9 [12800/60900 (21%)]	Loss: 183.078644 | Elapsed: 12.33s
01/28/2023 02:06:13 PM  [*] Sat Jan 28 14:06:13 2023: Train Epoch: 9 [19200/60900 (32%)]	Loss: 174.524719 | Elapsed: 12.39s
01/28/2023 02:06:25 PM  [*] Sat Jan 28 14:06:25 2023: Train Epoch: 9 [25600/60900 (42%)]	Loss: 178.502258 | Elapsed: 12.37s
01/28/2023 02:06:38 PM  [*] Sat Jan 28 14:06:38 2023: Train Epoch: 9 [32000/60900 (53%)]	Loss: 177.369354 | Elapsed: 12.38s
01/28/2023 02:06:50 PM  [*] Sat Jan 28 14:06:50 2023: Train Epoch: 9 [38400/60900 (63%)]	Loss: 175.612396 | Elapsed: 12.42s
01/28/2023 02:07:03 PM  [*] Sat Jan 28 14:07:03 2023: Train Epoch: 9 [44800/60900 (74%)]	Loss: 172.050415 | Elapsed: 12.37s
01/28/2023 02:07:15 PM  [*] Sat Jan 28 14:07:15 2023: Train Epoch: 9 [51200/60900 (84%)]	Loss: 164.692047 | Elapsed: 12.41s
01/28/2023 02:07:28 PM  [*] Sat Jan 28 14:07:28 2023: Train Epoch: 9 [57600/60900 (95%)]	Loss: 178.561264 | Elapsed: 12.47s
01/28/2023 02:07:36 PM  [*] Sat Jan 28 14:07:36 2023:    9    | Tr.loss: 176.686063 | Elapsed:  120.22  s
01/28/2023 02:07:36 PM [!] Sat Jan 28 14:07:36 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_9_1674911256-model.torch
01/28/2023 02:07:37 PM  [*] Masking sequences: iteration 10...
01/28/2023 02:07:58 PM  [*] Started epoch: 10
01/28/2023 02:07:58 PM  [*] Sat Jan 28 14:07:58 2023: Train Epoch: 10 [  0  /60900 (0 %)]	Loss: 167.512619 | Elapsed: 0.34s
01/28/2023 02:08:11 PM  [*] Sat Jan 28 14:08:11 2023: Train Epoch: 10 [6400 /60900 (11%)]	Loss: 183.248291 | Elapsed: 12.35s
01/28/2023 02:08:23 PM  [*] Sat Jan 28 14:08:23 2023: Train Epoch: 10 [12800/60900 (21%)]	Loss: 165.457108 | Elapsed: 12.39s
01/28/2023 02:08:35 PM  [*] Sat Jan 28 14:08:35 2023: Train Epoch: 10 [19200/60900 (32%)]	Loss: 169.212326 | Elapsed: 12.35s
01/28/2023 02:08:48 PM  [*] Sat Jan 28 14:08:48 2023: Train Epoch: 10 [25600/60900 (42%)]	Loss: 169.150177 | Elapsed: 12.43s
01/28/2023 02:09:00 PM  [*] Sat Jan 28 14:09:00 2023: Train Epoch: 10 [32000/60900 (53%)]	Loss: 175.176819 | Elapsed: 12.34s
01/28/2023 02:09:12 PM  [*] Sat Jan 28 14:09:12 2023: Train Epoch: 10 [38400/60900 (63%)]	Loss: 206.335083 | Elapsed: 12.38s
01/28/2023 02:09:25 PM  [*] Sat Jan 28 14:09:25 2023: Train Epoch: 10 [44800/60900 (74%)]	Loss: 204.208466 | Elapsed: 12.43s
01/28/2023 02:09:37 PM  [*] Sat Jan 28 14:09:37 2023: Train Epoch: 10 [51200/60900 (84%)]	Loss: 173.595047 | Elapsed: 12.46s
01/28/2023 02:09:50 PM  [*] Sat Jan 28 14:09:50 2023: Train Epoch: 10 [57600/60900 (95%)]	Loss: 182.897217 | Elapsed: 12.42s
01/28/2023 02:09:58 PM  [*] Sat Jan 28 14:09:58 2023:   10    | Tr.loss: 176.530263 | Elapsed:  120.06  s
01/28/2023 02:09:58 PM [!] Sat Jan 28 14:09:58 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_10_1674911398-model.torch
01/28/2023 02:09:59 PM  [*] Masking sequences: iteration 11...
01/28/2023 02:10:20 PM  [*] Started epoch: 11
01/28/2023 02:10:20 PM  [*] Sat Jan 28 14:10:20 2023: Train Epoch: 11 [  0  /60900 (0 %)]	Loss: 176.811523 | Elapsed: 0.28s
01/28/2023 02:10:32 PM  [*] Sat Jan 28 14:10:32 2023: Train Epoch: 11 [6400 /60900 (11%)]	Loss: 177.496979 | Elapsed: 12.36s
01/28/2023 02:10:45 PM  [*] Sat Jan 28 14:10:45 2023: Train Epoch: 11 [12800/60900 (21%)]	Loss: 170.288086 | Elapsed: 12.34s
01/28/2023 02:10:57 PM  [*] Sat Jan 28 14:10:57 2023: Train Epoch: 11 [19200/60900 (32%)]	Loss: 188.550690 | Elapsed: 12.34s
01/28/2023 02:11:10 PM  [*] Sat Jan 28 14:11:10 2023: Train Epoch: 11 [25600/60900 (42%)]	Loss: 183.531998 | Elapsed: 12.49s
01/28/2023 02:11:22 PM  [*] Sat Jan 28 14:11:22 2023: Train Epoch: 11 [32000/60900 (53%)]	Loss: 170.012238 | Elapsed: 12.38s
01/28/2023 02:11:35 PM  [*] Sat Jan 28 14:11:35 2023: Train Epoch: 11 [38400/60900 (63%)]	Loss: 199.053787 | Elapsed: 12.53s
01/28/2023 02:11:47 PM  [*] Sat Jan 28 14:11:47 2023: Train Epoch: 11 [44800/60900 (74%)]	Loss: 196.141464 | Elapsed: 12.39s
01/28/2023 02:11:59 PM  [*] Sat Jan 28 14:11:59 2023: Train Epoch: 11 [51200/60900 (84%)]	Loss: 180.317627 | Elapsed: 12.41s
01/28/2023 02:12:12 PM  [*] Sat Jan 28 14:12:12 2023: Train Epoch: 11 [57600/60900 (95%)]	Loss: 200.213043 | Elapsed: 12.38s
01/28/2023 02:12:20 PM  [*] Sat Jan 28 14:12:20 2023:   11    | Tr.loss: 176.348063 | Elapsed:  120.05  s
01/28/2023 02:12:20 PM [!] Sat Jan 28 14:12:20 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_11_1674911540-model.torch
01/28/2023 02:12:21 PM  [*] Masking sequences: iteration 12...
01/28/2023 02:12:42 PM  [*] Started epoch: 12
01/28/2023 02:12:42 PM  [*] Sat Jan 28 14:12:42 2023: Train Epoch: 12 [  0  /60900 (0 %)]	Loss: 193.274628 | Elapsed: 0.40s
01/28/2023 02:12:55 PM  [*] Sat Jan 28 14:12:55 2023: Train Epoch: 12 [6400 /60900 (11%)]	Loss: 172.009644 | Elapsed: 12.45s
01/28/2023 02:13:07 PM  [*] Sat Jan 28 14:13:07 2023: Train Epoch: 12 [12800/60900 (21%)]	Loss: 162.059235 | Elapsed: 12.40s
01/28/2023 02:13:19 PM  [*] Sat Jan 28 14:13:19 2023: Train Epoch: 12 [19200/60900 (32%)]	Loss: 165.557480 | Elapsed: 12.40s
01/28/2023 02:13:32 PM  [*] Sat Jan 28 14:13:32 2023: Train Epoch: 12 [25600/60900 (42%)]	Loss: 176.297607 | Elapsed: 12.43s
01/28/2023 02:13:44 PM  [*] Sat Jan 28 14:13:44 2023: Train Epoch: 12 [32000/60900 (53%)]	Loss: 179.745636 | Elapsed: 12.39s
01/28/2023 02:13:57 PM  [*] Sat Jan 28 14:13:57 2023: Train Epoch: 12 [38400/60900 (63%)]	Loss: 173.835236 | Elapsed: 12.37s
01/28/2023 02:14:09 PM  [*] Sat Jan 28 14:14:09 2023: Train Epoch: 12 [44800/60900 (74%)]	Loss: 188.412628 | Elapsed: 12.54s
01/28/2023 02:14:22 PM  [*] Sat Jan 28 14:14:22 2023: Train Epoch: 12 [51200/60900 (84%)]	Loss: 189.924850 | Elapsed: 12.45s
01/28/2023 02:14:34 PM  [*] Sat Jan 28 14:14:34 2023: Train Epoch: 12 [57600/60900 (95%)]	Loss: 164.895996 | Elapsed: 12.43s
01/28/2023 02:14:42 PM  [*] Sat Jan 28 14:14:42 2023:   12    | Tr.loss: 176.256472 | Elapsed:  120.39  s
01/28/2023 02:14:43 PM [!] Sat Jan 28 14:14:43 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_12_1674911682-model.torch
01/28/2023 02:14:44 PM  [*] Masking sequences: iteration 13...
01/28/2023 02:15:04 PM  [*] Started epoch: 13
01/28/2023 02:15:05 PM  [*] Sat Jan 28 14:15:05 2023: Train Epoch: 13 [  0  /60900 (0 %)]	Loss: 164.930954 | Elapsed: 0.35s
01/28/2023 02:15:17 PM  [*] Sat Jan 28 14:15:17 2023: Train Epoch: 13 [6400 /60900 (11%)]	Loss: 168.787201 | Elapsed: 12.71s
01/28/2023 02:15:30 PM  [*] Sat Jan 28 14:15:30 2023: Train Epoch: 13 [12800/60900 (21%)]	Loss: 182.509094 | Elapsed: 12.85s
01/28/2023 02:15:43 PM  [*] Sat Jan 28 14:15:43 2023: Train Epoch: 13 [19200/60900 (32%)]	Loss: 195.917923 | Elapsed: 12.52s
01/28/2023 02:15:55 PM  [*] Sat Jan 28 14:15:55 2023: Train Epoch: 13 [25600/60900 (42%)]	Loss: 165.519196 | Elapsed: 12.53s
01/28/2023 02:16:08 PM  [*] Sat Jan 28 14:16:08 2023: Train Epoch: 13 [32000/60900 (53%)]	Loss: 180.733627 | Elapsed: 12.89s
01/28/2023 02:16:21 PM  [*] Sat Jan 28 14:16:21 2023: Train Epoch: 13 [38400/60900 (63%)]	Loss: 177.625565 | Elapsed: 12.89s
01/28/2023 02:16:34 PM  [*] Sat Jan 28 14:16:34 2023: Train Epoch: 13 [44800/60900 (74%)]	Loss: 155.386932 | Elapsed: 12.60s
01/28/2023 02:16:46 PM  [*] Sat Jan 28 14:16:46 2023: Train Epoch: 13 [51200/60900 (84%)]	Loss: 186.396957 | Elapsed: 12.73s
01/28/2023 02:16:59 PM  [*] Sat Jan 28 14:16:59 2023: Train Epoch: 13 [57600/60900 (95%)]	Loss: 176.898361 | Elapsed: 12.79s
01/28/2023 02:17:07 PM  [*] Sat Jan 28 14:17:07 2023:   13    | Tr.loss: 176.139283 | Elapsed:  123.28  s
01/28/2023 02:17:08 PM [!] Sat Jan 28 14:17:08 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_13_1674911827-model.torch
01/28/2023 02:17:09 PM  [*] Masking sequences: iteration 14...
01/28/2023 02:17:32 PM  [*] Started epoch: 14
01/28/2023 02:17:32 PM  [*] Sat Jan 28 14:17:32 2023: Train Epoch: 14 [  0  /60900 (0 %)]	Loss: 191.673096 | Elapsed: 0.32s
01/28/2023 02:17:45 PM  [*] Sat Jan 28 14:17:45 2023: Train Epoch: 14 [6400 /60900 (11%)]	Loss: 191.517761 | Elapsed: 12.78s
01/28/2023 02:17:57 PM  [*] Sat Jan 28 14:17:57 2023: Train Epoch: 14 [12800/60900 (21%)]	Loss: 161.323608 | Elapsed: 12.45s
01/28/2023 02:18:10 PM  [*] Sat Jan 28 14:18:10 2023: Train Epoch: 14 [19200/60900 (32%)]	Loss: 181.268478 | Elapsed: 12.49s
01/28/2023 02:18:22 PM  [*] Sat Jan 28 14:18:22 2023: Train Epoch: 14 [25600/60900 (42%)]	Loss: 173.586594 | Elapsed: 12.48s
01/28/2023 02:18:35 PM  [*] Sat Jan 28 14:18:35 2023: Train Epoch: 14 [32000/60900 (53%)]	Loss: 176.039764 | Elapsed: 12.55s
01/28/2023 02:18:49 PM  [*] Sat Jan 28 14:18:49 2023: Train Epoch: 14 [38400/60900 (63%)]	Loss: 179.186829 | Elapsed: 13.98s
01/28/2023 02:19:02 PM  [*] Sat Jan 28 14:19:02 2023: Train Epoch: 14 [44800/60900 (74%)]	Loss: 185.870850 | Elapsed: 13.82s
01/28/2023 02:19:16 PM  [*] Sat Jan 28 14:19:16 2023: Train Epoch: 14 [51200/60900 (84%)]	Loss: 168.352234 | Elapsed: 13.27s
01/28/2023 02:19:29 PM  [*] Sat Jan 28 14:19:29 2023: Train Epoch: 14 [57600/60900 (95%)]	Loss: 168.937653 | Elapsed: 13.22s
01/28/2023 02:19:38 PM  [*] Sat Jan 28 14:19:38 2023:   14    | Tr.loss: 176.260864 | Elapsed:  126.37  s
01/28/2023 02:19:38 PM [!] Sat Jan 28 14:19:38 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_14_1674911978-model.torch
01/28/2023 02:19:40 PM  [*] Masking sequences: iteration 15...
01/28/2023 02:20:02 PM  [*] Started epoch: 15
01/28/2023 02:20:03 PM  [*] Sat Jan 28 14:20:03 2023: Train Epoch: 15 [  0  /60900 (0 %)]	Loss: 177.525681 | Elapsed: 0.45s
01/28/2023 02:20:15 PM  [*] Sat Jan 28 14:20:15 2023: Train Epoch: 15 [6400 /60900 (11%)]	Loss: 162.964203 | Elapsed: 12.72s
01/28/2023 02:20:28 PM  [*] Sat Jan 28 14:20:28 2023: Train Epoch: 15 [12800/60900 (21%)]	Loss: 172.591812 | Elapsed: 12.68s
01/28/2023 02:20:41 PM  [*] Sat Jan 28 14:20:41 2023: Train Epoch: 15 [19200/60900 (32%)]	Loss: 178.247803 | Elapsed: 12.71s
01/28/2023 02:20:53 PM  [*] Sat Jan 28 14:20:53 2023: Train Epoch: 15 [25600/60900 (42%)]	Loss: 172.055542 | Elapsed: 12.71s
01/28/2023 02:21:06 PM  [*] Sat Jan 28 14:21:06 2023: Train Epoch: 15 [32000/60900 (53%)]	Loss: 195.520340 | Elapsed: 12.79s
01/28/2023 02:21:19 PM  [*] Sat Jan 28 14:21:19 2023: Train Epoch: 15 [38400/60900 (63%)]	Loss: 184.706970 | Elapsed: 12.77s
01/28/2023 02:21:32 PM  [*] Sat Jan 28 14:21:32 2023: Train Epoch: 15 [44800/60900 (74%)]	Loss: 189.031189 | Elapsed: 12.86s
01/28/2023 02:21:45 PM  [*] Sat Jan 28 14:21:45 2023: Train Epoch: 15 [51200/60900 (84%)]	Loss: 177.755615 | Elapsed: 12.93s
01/28/2023 02:21:58 PM  [*] Sat Jan 28 14:21:58 2023: Train Epoch: 15 [57600/60900 (95%)]	Loss: 194.755646 | Elapsed: 12.98s
01/28/2023 02:22:06 PM  [*] Sat Jan 28 14:22:06 2023:   15    | Tr.loss: 176.231771 | Elapsed:  124.01  s
01/28/2023 02:22:07 PM [!] Sat Jan 28 14:22:07 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_15_1674912126-model.torch
01/28/2023 02:22:08 PM  [*] Masking sequences: iteration 16...
01/28/2023 02:22:30 PM  [*] Started epoch: 16
01/28/2023 02:22:30 PM  [*] Sat Jan 28 14:22:30 2023: Train Epoch: 16 [  0  /60900 (0 %)]	Loss: 174.770050 | Elapsed: 0.30s
01/28/2023 02:22:43 PM  [*] Sat Jan 28 14:22:43 2023: Train Epoch: 16 [6400 /60900 (11%)]	Loss: 176.277435 | Elapsed: 12.71s
01/28/2023 02:22:56 PM  [*] Sat Jan 28 14:22:56 2023: Train Epoch: 16 [12800/60900 (21%)]	Loss: 181.866241 | Elapsed: 12.80s
01/28/2023 02:23:09 PM  [*] Sat Jan 28 14:23:09 2023: Train Epoch: 16 [19200/60900 (32%)]	Loss: 175.268463 | Elapsed: 12.99s
01/28/2023 02:23:22 PM  [*] Sat Jan 28 14:23:22 2023: Train Epoch: 16 [25600/60900 (42%)]	Loss: 167.343430 | Elapsed: 12.88s
01/28/2023 02:23:35 PM  [*] Sat Jan 28 14:23:35 2023: Train Epoch: 16 [32000/60900 (53%)]	Loss: 171.162796 | Elapsed: 13.14s
01/28/2023 02:23:48 PM  [*] Sat Jan 28 14:23:48 2023: Train Epoch: 16 [38400/60900 (63%)]	Loss: 185.666641 | Elapsed: 13.02s
01/28/2023 02:24:01 PM  [*] Sat Jan 28 14:24:01 2023: Train Epoch: 16 [44800/60900 (74%)]	Loss: 176.018692 | Elapsed: 12.89s
01/28/2023 02:24:14 PM  [*] Sat Jan 28 14:24:14 2023: Train Epoch: 16 [51200/60900 (84%)]	Loss: 167.781586 | Elapsed: 13.08s
01/28/2023 02:24:27 PM  [*] Sat Jan 28 14:24:27 2023: Train Epoch: 16 [57600/60900 (95%)]	Loss: 173.910797 | Elapsed: 13.24s
01/28/2023 02:24:36 PM  [*] Sat Jan 28 14:24:36 2023:   16    | Tr.loss: 176.248505 | Elapsed:  125.68  s
01/28/2023 02:24:36 PM [!] Sat Jan 28 14:24:36 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_16_1674912276-model.torch
01/28/2023 02:24:37 PM  [*] Masking sequences: iteration 17...
01/28/2023 02:24:59 PM  [*] Started epoch: 17
01/28/2023 02:24:59 PM  [*] Sat Jan 28 14:24:59 2023: Train Epoch: 17 [  0  /60900 (0 %)]	Loss: 159.802429 | Elapsed: 0.45s
01/28/2023 02:25:12 PM  [*] Sat Jan 28 14:25:12 2023: Train Epoch: 17 [6400 /60900 (11%)]	Loss: 159.761551 | Elapsed: 12.72s
01/28/2023 02:25:25 PM  [*] Sat Jan 28 14:25:25 2023: Train Epoch: 17 [12800/60900 (21%)]	Loss: 164.865265 | Elapsed: 12.83s
01/28/2023 02:25:38 PM  [*] Sat Jan 28 14:25:38 2023: Train Epoch: 17 [19200/60900 (32%)]	Loss: 176.953506 | Elapsed: 12.69s
01/28/2023 02:25:50 PM  [*] Sat Jan 28 14:25:50 2023: Train Epoch: 17 [25600/60900 (42%)]	Loss: 178.474197 | Elapsed: 12.86s
01/28/2023 02:26:03 PM  [*] Sat Jan 28 14:26:03 2023: Train Epoch: 17 [32000/60900 (53%)]	Loss: 182.334671 | Elapsed: 12.91s
01/28/2023 02:26:16 PM  [*] Sat Jan 28 14:26:16 2023: Train Epoch: 17 [38400/60900 (63%)]	Loss: 170.587448 | Elapsed: 12.86s
01/28/2023 02:26:29 PM  [*] Sat Jan 28 14:26:29 2023: Train Epoch: 17 [44800/60900 (74%)]	Loss: 163.924149 | Elapsed: 13.22s
01/28/2023 02:26:43 PM  [*] Sat Jan 28 14:26:43 2023: Train Epoch: 17 [51200/60900 (84%)]	Loss: 170.644653 | Elapsed: 13.17s
01/28/2023 02:26:56 PM  [*] Sat Jan 28 14:26:56 2023: Train Epoch: 17 [57600/60900 (95%)]	Loss: 193.877960 | Elapsed: 13.08s
01/28/2023 02:27:04 PM  [*] Sat Jan 28 14:27:04 2023:   17    | Tr.loss: 176.263512 | Elapsed:  125.37  s
01/28/2023 02:27:05 PM [!] Sat Jan 28 14:27:05 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_17_1674912424-model.torch
01/28/2023 02:27:06 PM  [*] Masking sequences: iteration 18...
01/28/2023 02:27:30 PM  [*] Started epoch: 18
01/28/2023 02:27:30 PM  [*] Sat Jan 28 14:27:30 2023: Train Epoch: 18 [  0  /60900 (0 %)]	Loss: 179.812531 | Elapsed: 0.37s
01/28/2023 02:27:43 PM  [*] Sat Jan 28 14:27:43 2023: Train Epoch: 18 [6400 /60900 (11%)]	Loss: 185.761063 | Elapsed: 12.85s
01/28/2023 02:27:56 PM  [*] Sat Jan 28 14:27:56 2023: Train Epoch: 18 [12800/60900 (21%)]	Loss: 199.731277 | Elapsed: 13.06s
01/28/2023 02:28:09 PM  [*] Sat Jan 28 14:28:09 2023: Train Epoch: 18 [19200/60900 (32%)]	Loss: 177.865143 | Elapsed: 12.74s
01/28/2023 02:28:22 PM  [*] Sat Jan 28 14:28:22 2023: Train Epoch: 18 [25600/60900 (42%)]	Loss: 173.956055 | Elapsed: 12.83s
01/28/2023 02:28:35 PM  [*] Sat Jan 28 14:28:35 2023: Train Epoch: 18 [32000/60900 (53%)]	Loss: 186.280991 | Elapsed: 13.02s
01/28/2023 02:28:48 PM  [*] Sat Jan 28 14:28:48 2023: Train Epoch: 18 [38400/60900 (63%)]	Loss: 183.157730 | Elapsed: 12.97s
01/28/2023 02:29:01 PM  [*] Sat Jan 28 14:29:01 2023: Train Epoch: 18 [44800/60900 (74%)]	Loss: 181.845184 | Elapsed: 12.91s
01/28/2023 02:29:14 PM  [*] Sat Jan 28 14:29:14 2023: Train Epoch: 18 [51200/60900 (84%)]	Loss: 180.406860 | Elapsed: 12.86s
01/28/2023 02:29:26 PM  [*] Sat Jan 28 14:29:26 2023: Train Epoch: 18 [57600/60900 (95%)]	Loss: 178.454437 | Elapsed: 12.63s
01/28/2023 02:29:35 PM  [*] Sat Jan 28 14:29:35 2023:   18    | Tr.loss: 176.368841 | Elapsed:  124.82  s
01/28/2023 02:29:35 PM [!] Sat Jan 28 14:29:35 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_18_1674912575-model.torch
01/28/2023 02:29:37 PM  [*] Masking sequences: iteration 19...
01/28/2023 02:29:59 PM  [*] Started epoch: 19
01/28/2023 02:29:59 PM  [*] Sat Jan 28 14:29:59 2023: Train Epoch: 19 [  0  /60900 (0 %)]	Loss: 177.348663 | Elapsed: 0.30s
01/28/2023 02:30:12 PM  [*] Sat Jan 28 14:30:12 2023: Train Epoch: 19 [6400 /60900 (11%)]	Loss: 186.377777 | Elapsed: 12.42s
01/28/2023 02:30:24 PM  [*] Sat Jan 28 14:30:24 2023: Train Epoch: 19 [12800/60900 (21%)]	Loss: 180.435577 | Elapsed: 12.54s
01/28/2023 02:30:37 PM  [*] Sat Jan 28 14:30:37 2023: Train Epoch: 19 [19200/60900 (32%)]	Loss: 179.297073 | Elapsed: 12.47s
01/28/2023 02:30:50 PM  [*] Sat Jan 28 14:30:50 2023: Train Epoch: 19 [25600/60900 (42%)]	Loss: 181.372772 | Elapsed: 12.76s
01/28/2023 02:31:02 PM  [*] Sat Jan 28 14:31:02 2023: Train Epoch: 19 [32000/60900 (53%)]	Loss: 195.589264 | Elapsed: 12.37s
01/28/2023 02:31:15 PM  [*] Sat Jan 28 14:31:15 2023: Train Epoch: 19 [38400/60900 (63%)]	Loss: 207.304901 | Elapsed: 12.52s
01/28/2023 02:31:27 PM  [*] Sat Jan 28 14:31:27 2023: Train Epoch: 19 [44800/60900 (74%)]	Loss: 169.658112 | Elapsed: 12.63s
01/28/2023 02:31:40 PM  [*] Sat Jan 28 14:31:40 2023: Train Epoch: 19 [51200/60900 (84%)]	Loss: 176.384888 | Elapsed: 12.77s
01/28/2023 02:31:53 PM  [*] Sat Jan 28 14:31:53 2023: Train Epoch: 19 [57600/60900 (95%)]	Loss: 183.354950 | Elapsed: 12.69s
01/28/2023 02:32:01 PM  [*] Sat Jan 28 14:32:01 2023:   19    | Tr.loss: 176.358865 | Elapsed:  121.64  s
01/28/2023 02:32:01 PM [!] Sat Jan 28 14:32:01 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_19_1674912721-model.torch
01/28/2023 02:32:02 PM  [*] Masking sequences: iteration 20...
01/28/2023 02:32:23 PM  [*] Started epoch: 20
01/28/2023 02:32:24 PM  [*] Sat Jan 28 14:32:24 2023: Train Epoch: 20 [  0  /60900 (0 %)]	Loss: 187.567596 | Elapsed: 0.83s
01/28/2023 02:32:36 PM  [*] Sat Jan 28 14:32:36 2023: Train Epoch: 20 [6400 /60900 (11%)]	Loss: 185.275391 | Elapsed: 12.41s
01/28/2023 02:32:49 PM  [*] Sat Jan 28 14:32:49 2023: Train Epoch: 20 [12800/60900 (21%)]	Loss: 185.844284 | Elapsed: 12.36s
01/28/2023 02:33:01 PM  [*] Sat Jan 28 14:33:01 2023: Train Epoch: 20 [19200/60900 (32%)]	Loss: 189.418488 | Elapsed: 12.48s
01/28/2023 02:33:13 PM  [*] Sat Jan 28 14:33:13 2023: Train Epoch: 20 [25600/60900 (42%)]	Loss: 185.808807 | Elapsed: 12.47s
01/28/2023 02:33:26 PM  [*] Sat Jan 28 14:33:26 2023: Train Epoch: 20 [32000/60900 (53%)]	Loss: 153.667389 | Elapsed: 12.53s
01/28/2023 02:33:39 PM  [*] Sat Jan 28 14:33:39 2023: Train Epoch: 20 [38400/60900 (63%)]	Loss: 188.887665 | Elapsed: 12.56s
01/28/2023 02:33:51 PM  [*] Sat Jan 28 14:33:51 2023: Train Epoch: 20 [44800/60900 (74%)]	Loss: 162.019302 | Elapsed: 12.65s
01/28/2023 02:34:04 PM  [*] Sat Jan 28 14:34:04 2023: Train Epoch: 20 [51200/60900 (84%)]	Loss: 165.769470 | Elapsed: 12.60s
01/28/2023 02:34:16 PM  [*] Sat Jan 28 14:34:16 2023: Train Epoch: 20 [57600/60900 (95%)]	Loss: 164.933456 | Elapsed: 12.60s
01/28/2023 02:34:25 PM  [*] Sat Jan 28 14:34:25 2023:   20    | Tr.loss: 176.507324 | Elapsed:  121.74  s
01/28/2023 02:34:25 PM [!] Sat Jan 28 14:34:25 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_20_1674912865-model.torch
01/28/2023 02:34:26 PM  [*] Masking sequences: iteration 21...
01/28/2023 02:34:47 PM  [*] Started epoch: 21
01/28/2023 02:34:47 PM  [*] Sat Jan 28 14:34:47 2023: Train Epoch: 21 [  0  /60900 (0 %)]	Loss: 158.184616 | Elapsed: 0.29s
01/28/2023 02:35:00 PM  [*] Sat Jan 28 14:35:00 2023: Train Epoch: 21 [6400 /60900 (11%)]	Loss: 168.574860 | Elapsed: 12.40s
01/28/2023 02:35:12 PM  [*] Sat Jan 28 14:35:12 2023: Train Epoch: 21 [12800/60900 (21%)]	Loss: 183.617249 | Elapsed: 12.41s
01/28/2023 02:35:25 PM  [*] Sat Jan 28 14:35:25 2023: Train Epoch: 21 [19200/60900 (32%)]	Loss: 168.592987 | Elapsed: 12.41s
01/28/2023 02:35:37 PM  [*] Sat Jan 28 14:35:37 2023: Train Epoch: 21 [25600/60900 (42%)]	Loss: 163.403946 | Elapsed: 12.47s
01/28/2023 02:35:50 PM  [*] Sat Jan 28 14:35:50 2023: Train Epoch: 21 [32000/60900 (53%)]	Loss: 169.031860 | Elapsed: 12.51s
01/28/2023 02:36:02 PM  [*] Sat Jan 28 14:36:02 2023: Train Epoch: 21 [38400/60900 (63%)]	Loss: 162.754608 | Elapsed: 12.53s
01/28/2023 02:36:15 PM  [*] Sat Jan 28 14:36:15 2023: Train Epoch: 21 [44800/60900 (74%)]	Loss: 170.240723 | Elapsed: 12.48s
01/28/2023 02:36:27 PM  [*] Sat Jan 28 14:36:27 2023: Train Epoch: 21 [51200/60900 (84%)]	Loss: 196.507187 | Elapsed: 12.53s
01/28/2023 02:36:40 PM  [*] Sat Jan 28 14:36:40 2023: Train Epoch: 21 [57600/60900 (95%)]	Loss: 171.340668 | Elapsed: 12.49s
01/28/2023 02:36:48 PM  [*] Sat Jan 28 14:36:48 2023:   21    | Tr.loss: 176.346341 | Elapsed:  120.78  s
01/28/2023 02:36:48 PM [!] Sat Jan 28 14:36:48 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_21_1674913008-model.torch
01/28/2023 02:36:49 PM  [*] Masking sequences: iteration 22...
01/28/2023 02:37:10 PM  [*] Started epoch: 22
01/28/2023 02:37:10 PM  [*] Sat Jan 28 14:37:10 2023: Train Epoch: 22 [  0  /60900 (0 %)]	Loss: 172.253326 | Elapsed: 0.44s
01/28/2023 02:37:23 PM  [*] Sat Jan 28 14:37:23 2023: Train Epoch: 22 [6400 /60900 (11%)]	Loss: 157.601685 | Elapsed: 12.46s
01/28/2023 02:37:35 PM  [*] Sat Jan 28 14:37:35 2023: Train Epoch: 22 [12800/60900 (21%)]	Loss: 179.216293 | Elapsed: 12.37s
01/28/2023 02:37:48 PM  [*] Sat Jan 28 14:37:48 2023: Train Epoch: 22 [19200/60900 (32%)]	Loss: 184.223541 | Elapsed: 12.39s
01/28/2023 02:38:00 PM  [*] Sat Jan 28 14:38:00 2023: Train Epoch: 22 [25600/60900 (42%)]	Loss: 164.628677 | Elapsed: 12.38s
01/28/2023 02:38:12 PM  [*] Sat Jan 28 14:38:12 2023: Train Epoch: 22 [32000/60900 (53%)]	Loss: 182.000076 | Elapsed: 12.45s
01/28/2023 02:38:25 PM  [*] Sat Jan 28 14:38:25 2023: Train Epoch: 22 [38400/60900 (63%)]	Loss: 187.254349 | Elapsed: 12.45s
01/28/2023 02:38:37 PM  [*] Sat Jan 28 14:38:37 2023: Train Epoch: 22 [44800/60900 (74%)]	Loss: 191.250687 | Elapsed: 12.55s
01/28/2023 02:38:50 PM  [*] Sat Jan 28 14:38:50 2023: Train Epoch: 22 [51200/60900 (84%)]	Loss: 163.894470 | Elapsed: 12.59s
01/28/2023 02:39:03 PM  [*] Sat Jan 28 14:39:03 2023: Train Epoch: 22 [57600/60900 (95%)]	Loss: 164.804504 | Elapsed: 12.76s
01/28/2023 02:39:11 PM  [*] Sat Jan 28 14:39:11 2023:   22    | Tr.loss: 175.993797 | Elapsed:  121.08  s
01/28/2023 02:39:12 PM [!] Sat Jan 28 14:39:12 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_22_1674913151-model.torch
01/28/2023 02:39:13 PM  [*] Masking sequences: iteration 23...
01/28/2023 02:39:33 PM  [*] Started epoch: 23
01/28/2023 02:39:34 PM  [*] Sat Jan 28 14:39:34 2023: Train Epoch: 23 [  0  /60900 (0 %)]	Loss: 170.258698 | Elapsed: 0.44s
01/28/2023 02:39:46 PM  [*] Sat Jan 28 14:39:46 2023: Train Epoch: 23 [6400 /60900 (11%)]	Loss: 176.701965 | Elapsed: 12.48s
01/28/2023 02:39:59 PM  [*] Sat Jan 28 14:39:59 2023: Train Epoch: 23 [12800/60900 (21%)]	Loss: 188.856644 | Elapsed: 12.40s
01/28/2023 02:40:11 PM  [*] Sat Jan 28 14:40:11 2023: Train Epoch: 23 [19200/60900 (32%)]	Loss: 185.255234 | Elapsed: 12.45s
01/28/2023 02:40:24 PM  [*] Sat Jan 28 14:40:24 2023: Train Epoch: 23 [25600/60900 (42%)]	Loss: 150.843140 | Elapsed: 12.54s
01/28/2023 02:40:36 PM  [*] Sat Jan 28 14:40:36 2023: Train Epoch: 23 [32000/60900 (53%)]	Loss: 159.176590 | Elapsed: 12.46s
01/28/2023 02:40:49 PM  [*] Sat Jan 28 14:40:49 2023: Train Epoch: 23 [38400/60900 (63%)]	Loss: 187.904449 | Elapsed: 12.52s
01/28/2023 02:41:01 PM  [*] Sat Jan 28 14:41:01 2023: Train Epoch: 23 [44800/60900 (74%)]	Loss: 161.596771 | Elapsed: 12.53s
01/28/2023 02:41:14 PM  [*] Sat Jan 28 14:41:14 2023: Train Epoch: 23 [51200/60900 (84%)]	Loss: 173.311401 | Elapsed: 12.58s
01/28/2023 02:41:26 PM  [*] Sat Jan 28 14:41:26 2023: Train Epoch: 23 [57600/60900 (95%)]	Loss: 209.118561 | Elapsed: 12.54s
01/28/2023 02:41:35 PM  [*] Sat Jan 28 14:41:35 2023:   23    | Tr.loss: 176.297005 | Elapsed:  121.24  s
01/28/2023 02:41:35 PM [!] Sat Jan 28 14:41:35 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_23_1674913295-model.torch
01/28/2023 02:41:36 PM  [*] Masking sequences: iteration 24...
01/28/2023 02:41:57 PM  [*] Started epoch: 24
01/28/2023 02:41:58 PM  [*] Sat Jan 28 14:41:58 2023: Train Epoch: 24 [  0  /60900 (0 %)]	Loss: 167.855927 | Elapsed: 0.38s
01/28/2023 02:42:10 PM  [*] Sat Jan 28 14:42:10 2023: Train Epoch: 24 [6400 /60900 (11%)]	Loss: 176.143509 | Elapsed: 12.47s
01/28/2023 02:42:23 PM  [*] Sat Jan 28 14:42:23 2023: Train Epoch: 24 [12800/60900 (21%)]	Loss: 165.854568 | Elapsed: 12.42s
01/28/2023 02:42:35 PM  [*] Sat Jan 28 14:42:35 2023: Train Epoch: 24 [19200/60900 (32%)]	Loss: 154.673340 | Elapsed: 12.52s
01/28/2023 02:42:48 PM  [*] Sat Jan 28 14:42:48 2023: Train Epoch: 24 [25600/60900 (42%)]	Loss: 163.916580 | Elapsed: 12.46s
01/28/2023 02:43:00 PM  [*] Sat Jan 28 14:43:00 2023: Train Epoch: 24 [32000/60900 (53%)]	Loss: 169.698639 | Elapsed: 12.46s
01/28/2023 02:43:13 PM  [*] Sat Jan 28 14:43:13 2023: Train Epoch: 24 [38400/60900 (63%)]	Loss: 173.301361 | Elapsed: 12.56s
01/28/2023 02:43:25 PM  [*] Sat Jan 28 14:43:25 2023: Train Epoch: 24 [44800/60900 (74%)]	Loss: 158.068634 | Elapsed: 12.52s
01/28/2023 02:43:38 PM  [*] Sat Jan 28 14:43:38 2023: Train Epoch: 24 [51200/60900 (84%)]	Loss: 173.904114 | Elapsed: 12.53s
01/28/2023 02:43:50 PM  [*] Sat Jan 28 14:43:50 2023: Train Epoch: 24 [57600/60900 (95%)]	Loss: 175.604340 | Elapsed: 12.60s
01/28/2023 02:43:58 PM  [*] Sat Jan 28 14:43:58 2023:   24    | Tr.loss: 176.135589 | Elapsed:  121.17  s
01/28/2023 02:43:59 PM [!] Sat Jan 28 14:43:59 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_24_1674913438-model.torch
01/28/2023 02:44:00 PM  [*] Masking sequences: iteration 25...
01/28/2023 02:44:21 PM  [*] Started epoch: 25
01/28/2023 02:44:21 PM  [*] Sat Jan 28 14:44:21 2023: Train Epoch: 25 [  0  /60900 (0 %)]	Loss: 177.761871 | Elapsed: 0.35s
01/28/2023 02:44:34 PM  [*] Sat Jan 28 14:44:34 2023: Train Epoch: 25 [6400 /60900 (11%)]	Loss: 185.873184 | Elapsed: 12.49s
01/28/2023 02:44:46 PM  [*] Sat Jan 28 14:44:46 2023: Train Epoch: 25 [12800/60900 (21%)]	Loss: 176.283020 | Elapsed: 12.44s
01/28/2023 02:44:59 PM  [*] Sat Jan 28 14:44:59 2023: Train Epoch: 25 [19200/60900 (32%)]	Loss: 179.567688 | Elapsed: 12.51s
01/28/2023 02:45:11 PM  [*] Sat Jan 28 14:45:11 2023: Train Epoch: 25 [25600/60900 (42%)]	Loss: 193.420654 | Elapsed: 12.42s
01/28/2023 02:45:24 PM  [*] Sat Jan 28 14:45:24 2023: Train Epoch: 25 [32000/60900 (53%)]	Loss: 171.325562 | Elapsed: 12.67s
01/28/2023 02:45:37 PM  [*] Sat Jan 28 14:45:37 2023: Train Epoch: 25 [38400/60900 (63%)]	Loss: 166.128540 | Elapsed: 12.48s
01/28/2023 02:45:49 PM  [*] Sat Jan 28 14:45:49 2023: Train Epoch: 25 [44800/60900 (74%)]	Loss: 158.869629 | Elapsed: 12.48s
01/28/2023 02:46:01 PM  [*] Sat Jan 28 14:46:01 2023: Train Epoch: 25 [51200/60900 (84%)]	Loss: 160.295456 | Elapsed: 12.50s
01/28/2023 02:46:14 PM  [*] Sat Jan 28 14:46:14 2023: Train Epoch: 25 [57600/60900 (95%)]	Loss: 168.284805 | Elapsed: 12.51s
01/28/2023 02:46:22 PM  [*] Sat Jan 28 14:46:22 2023:   25    | Tr.loss: 175.985704 | Elapsed:  121.03  s
01/28/2023 02:46:23 PM [!] Sat Jan 28 14:46:23 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_25_1674913582-model.torch
01/28/2023 02:46:24 PM  [*] Masking sequences: iteration 26...
01/28/2023 02:46:44 PM  [*] Started epoch: 26
01/28/2023 02:46:45 PM  [*] Sat Jan 28 14:46:45 2023: Train Epoch: 26 [  0  /60900 (0 %)]	Loss: 188.433472 | Elapsed: 0.37s
01/28/2023 02:46:57 PM  [*] Sat Jan 28 14:46:57 2023: Train Epoch: 26 [6400 /60900 (11%)]	Loss: 172.171234 | Elapsed: 12.34s
01/28/2023 02:47:09 PM  [*] Sat Jan 28 14:47:09 2023: Train Epoch: 26 [12800/60900 (21%)]	Loss: 155.678101 | Elapsed: 12.34s
01/28/2023 02:47:22 PM  [*] Sat Jan 28 14:47:22 2023: Train Epoch: 26 [19200/60900 (32%)]	Loss: 174.521210 | Elapsed: 12.40s
01/28/2023 02:47:34 PM  [*] Sat Jan 28 14:47:34 2023: Train Epoch: 26 [25600/60900 (42%)]	Loss: 177.379791 | Elapsed: 12.41s
01/28/2023 02:47:47 PM  [*] Sat Jan 28 14:47:47 2023: Train Epoch: 26 [32000/60900 (53%)]	Loss: 180.258682 | Elapsed: 12.39s
01/28/2023 02:47:59 PM  [*] Sat Jan 28 14:47:59 2023: Train Epoch: 26 [38400/60900 (63%)]	Loss: 187.410583 | Elapsed: 12.43s
01/28/2023 02:48:11 PM  [*] Sat Jan 28 14:48:11 2023: Train Epoch: 26 [44800/60900 (74%)]	Loss: 188.763550 | Elapsed: 12.53s
01/28/2023 02:48:24 PM  [*] Sat Jan 28 14:48:24 2023: Train Epoch: 26 [51200/60900 (84%)]	Loss: 177.749268 | Elapsed: 12.57s
01/28/2023 02:48:37 PM  [*] Sat Jan 28 14:48:37 2023: Train Epoch: 26 [57600/60900 (95%)]	Loss: 185.383224 | Elapsed: 12.50s
01/28/2023 02:48:45 PM  [*] Sat Jan 28 14:48:45 2023:   26    | Tr.loss: 176.142490 | Elapsed:  120.44  s
01/28/2023 02:48:45 PM [!] Sat Jan 28 14:48:45 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_26_1674913725-model.torch
01/28/2023 02:48:46 PM  [*] Masking sequences: iteration 27...
01/28/2023 02:49:07 PM  [*] Started epoch: 27
01/28/2023 02:49:07 PM  [*] Sat Jan 28 14:49:07 2023: Train Epoch: 27 [  0  /60900 (0 %)]	Loss: 178.465912 | Elapsed: 0.35s
01/28/2023 02:49:20 PM  [*] Sat Jan 28 14:49:20 2023: Train Epoch: 27 [6400 /60900 (11%)]	Loss: 179.389999 | Elapsed: 12.48s
01/28/2023 02:49:32 PM  [*] Sat Jan 28 14:49:32 2023: Train Epoch: 27 [12800/60900 (21%)]	Loss: 193.686829 | Elapsed: 12.44s
01/28/2023 02:49:45 PM  [*] Sat Jan 28 14:49:45 2023: Train Epoch: 27 [19200/60900 (32%)]	Loss: 191.869934 | Elapsed: 12.47s
01/28/2023 02:49:57 PM  [*] Sat Jan 28 14:49:57 2023: Train Epoch: 27 [25600/60900 (42%)]	Loss: 192.434204 | Elapsed: 12.45s
01/28/2023 02:50:10 PM  [*] Sat Jan 28 14:50:10 2023: Train Epoch: 27 [32000/60900 (53%)]	Loss: 164.837326 | Elapsed: 12.53s
01/28/2023 02:50:22 PM  [*] Sat Jan 28 14:50:22 2023: Train Epoch: 27 [38400/60900 (63%)]	Loss: 196.106735 | Elapsed: 12.48s
01/28/2023 02:50:35 PM  [*] Sat Jan 28 14:50:35 2023: Train Epoch: 27 [44800/60900 (74%)]	Loss: 164.644409 | Elapsed: 12.47s
01/28/2023 02:50:47 PM  [*] Sat Jan 28 14:50:47 2023: Train Epoch: 27 [51200/60900 (84%)]	Loss: 169.689270 | Elapsed: 12.48s
01/28/2023 02:51:00 PM  [*] Sat Jan 28 14:51:00 2023: Train Epoch: 27 [57600/60900 (95%)]	Loss: 171.063080 | Elapsed: 12.66s
01/28/2023 02:51:08 PM  [*] Sat Jan 28 14:51:08 2023:   27    | Tr.loss: 176.079217 | Elapsed:  120.95  s
01/28/2023 02:51:08 PM [!] Sat Jan 28 14:51:08 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_27_1674913868-model.torch
01/28/2023 02:51:09 PM  [*] Masking sequences: iteration 28...
01/28/2023 02:51:29 PM  [*] Started epoch: 28
01/28/2023 02:51:29 PM  [*] Sat Jan 28 14:51:29 2023: Train Epoch: 28 [  0  /60900 (0 %)]	Loss: 173.527588 | Elapsed: 0.30s
01/28/2023 02:51:42 PM  [*] Sat Jan 28 14:51:42 2023: Train Epoch: 28 [6400 /60900 (11%)]	Loss: 162.456604 | Elapsed: 12.32s
01/28/2023 02:51:54 PM  [*] Sat Jan 28 14:51:54 2023: Train Epoch: 28 [12800/60900 (21%)]	Loss: 149.269867 | Elapsed: 12.27s
01/28/2023 02:52:06 PM  [*] Sat Jan 28 14:52:06 2023: Train Epoch: 28 [19200/60900 (32%)]	Loss: 191.814224 | Elapsed: 12.38s
01/28/2023 02:52:19 PM  [*] Sat Jan 28 14:52:19 2023: Train Epoch: 28 [25600/60900 (42%)]	Loss: 163.268234 | Elapsed: 12.21s
01/28/2023 02:52:31 PM  [*] Sat Jan 28 14:52:31 2023: Train Epoch: 28 [32000/60900 (53%)]	Loss: 167.803070 | Elapsed: 12.29s
01/28/2023 02:52:43 PM  [*] Sat Jan 28 14:52:43 2023: Train Epoch: 28 [38400/60900 (63%)]	Loss: 169.286758 | Elapsed: 12.26s
01/28/2023 02:52:55 PM  [*] Sat Jan 28 14:52:55 2023: Train Epoch: 28 [44800/60900 (74%)]	Loss: 175.588577 | Elapsed: 12.29s
01/28/2023 02:53:08 PM  [*] Sat Jan 28 14:53:08 2023: Train Epoch: 28 [51200/60900 (84%)]	Loss: 162.815506 | Elapsed: 12.28s
01/28/2023 02:53:20 PM  [*] Sat Jan 28 14:53:20 2023: Train Epoch: 28 [57600/60900 (95%)]	Loss: 164.356598 | Elapsed: 12.43s
01/28/2023 02:53:28 PM  [*] Sat Jan 28 14:53:28 2023:   28    | Tr.loss: 176.077415 | Elapsed:  118.96  s
01/28/2023 02:53:28 PM [!] Sat Jan 28 14:53:28 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_28_1674914008-model.torch
01/28/2023 02:53:29 PM  [*] Masking sequences: iteration 29...
01/28/2023 02:53:49 PM  [*] Started epoch: 29
01/28/2023 02:53:49 PM  [*] Sat Jan 28 14:53:49 2023: Train Epoch: 29 [  0  /60900 (0 %)]	Loss: 177.727600 | Elapsed: 0.31s
01/28/2023 02:54:02 PM  [*] Sat Jan 28 14:54:02 2023: Train Epoch: 29 [6400 /60900 (11%)]	Loss: 181.185059 | Elapsed: 12.36s
01/28/2023 02:54:14 PM  [*] Sat Jan 28 14:54:14 2023: Train Epoch: 29 [12800/60900 (21%)]	Loss: 174.440704 | Elapsed: 12.19s
01/28/2023 02:54:26 PM  [*] Sat Jan 28 14:54:26 2023: Train Epoch: 29 [19200/60900 (32%)]	Loss: 163.360992 | Elapsed: 12.27s
01/28/2023 02:54:38 PM  [*] Sat Jan 28 14:54:38 2023: Train Epoch: 29 [25600/60900 (42%)]	Loss: 196.570435 | Elapsed: 12.22s
01/28/2023 02:54:51 PM  [*] Sat Jan 28 14:54:51 2023: Train Epoch: 29 [32000/60900 (53%)]	Loss: 162.308029 | Elapsed: 12.31s
01/28/2023 02:55:03 PM  [*] Sat Jan 28 14:55:03 2023: Train Epoch: 29 [38400/60900 (63%)]	Loss: 179.317917 | Elapsed: 12.24s
01/28/2023 02:55:16 PM  [*] Sat Jan 28 14:55:16 2023: Train Epoch: 29 [44800/60900 (74%)]	Loss: 187.273514 | Elapsed: 12.52s
01/28/2023 02:55:28 PM  [*] Sat Jan 28 14:55:28 2023: Train Epoch: 29 [51200/60900 (84%)]	Loss: 175.442184 | Elapsed: 12.44s
01/28/2023 02:55:40 PM  [*] Sat Jan 28 14:55:40 2023: Train Epoch: 29 [57600/60900 (95%)]	Loss: 160.549744 | Elapsed: 12.24s
01/28/2023 02:55:48 PM  [*] Sat Jan 28 14:55:48 2023:   29    | Tr.loss: 176.346388 | Elapsed:  119.05  s
01/28/2023 02:55:49 PM [!] Sat Jan 28 14:55:49 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_29_1674914148-model.torch
01/28/2023 02:55:50 PM  [*] Masking sequences: iteration 30...
01/28/2023 02:56:09 PM  [*] Started epoch: 30
01/28/2023 02:56:10 PM  [*] Sat Jan 28 14:56:10 2023: Train Epoch: 30 [  0  /60900 (0 %)]	Loss: 185.059479 | Elapsed: 0.45s
01/28/2023 02:56:22 PM  [*] Sat Jan 28 14:56:22 2023: Train Epoch: 30 [6400 /60900 (11%)]	Loss: 186.515717 | Elapsed: 12.27s
01/28/2023 02:56:34 PM  [*] Sat Jan 28 14:56:34 2023: Train Epoch: 30 [12800/60900 (21%)]	Loss: 169.218170 | Elapsed: 12.19s
01/28/2023 02:56:46 PM  [*] Sat Jan 28 14:56:46 2023: Train Epoch: 30 [19200/60900 (32%)]	Loss: 193.484711 | Elapsed: 12.21s
01/28/2023 02:56:59 PM  [*] Sat Jan 28 14:56:59 2023: Train Epoch: 30 [25600/60900 (42%)]	Loss: 177.231186 | Elapsed: 12.22s
01/28/2023 02:57:11 PM  [*] Sat Jan 28 14:57:11 2023: Train Epoch: 30 [32000/60900 (53%)]	Loss: 168.594574 | Elapsed: 12.20s
01/28/2023 02:57:23 PM  [*] Sat Jan 28 14:57:23 2023: Train Epoch: 30 [38400/60900 (63%)]	Loss: 160.564362 | Elapsed: 12.20s
01/28/2023 02:57:35 PM  [*] Sat Jan 28 14:57:35 2023: Train Epoch: 30 [44800/60900 (74%)]	Loss: 174.146957 | Elapsed: 12.29s
01/28/2023 02:57:47 PM  [*] Sat Jan 28 14:57:47 2023: Train Epoch: 30 [51200/60900 (84%)]	Loss: 184.336609 | Elapsed: 12.18s
01/28/2023 02:58:00 PM  [*] Sat Jan 28 14:58:00 2023: Train Epoch: 30 [57600/60900 (95%)]	Loss: 171.914581 | Elapsed: 12.27s
01/28/2023 02:58:08 PM  [*] Sat Jan 28 14:58:08 2023:   30    | Tr.loss: 176.306039 | Elapsed:  118.55  s
01/28/2023 02:58:08 PM [!] Sat Jan 28 14:58:08 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_30_1674914288-model.torch
01/28/2023 02:58:09 PM [!] Sat Jan 28 14:58:09 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\1674914288-model.torch
		train time: 1674914288-trainTime.npy
		train losses: 1674914288-trainLosses.npy
		train AUC: 1674914288-auc.npy
01/28/2023 02:58:11 PM  [!] Training pretrained model on downstream task...
01/28/2023 02:58:11 PM  [*] Started epoch: 1
01/28/2023 02:58:11 PM  [*] Sat Jan 28 14:58:11 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.215029 | Elapsed: 0.25s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2524
01/28/2023 02:58:20 PM  [*] Sat Jan 28 14:58:20 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.417766 | Elapsed: 9.11s | FPR 0.0003 -> TPR 0.3971 & F1 0.5684 | AUC 0.8892
01/28/2023 02:58:29 PM  [*] Sat Jan 28 14:58:29 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.372284 | Elapsed: 9.07s | FPR 0.0003 -> TPR 0.4583 & F1 0.6286 | AUC 0.8313
01/28/2023 02:58:33 PM  [*] Sat Jan 28 14:58:33 2023:    1    | Tr.loss: 0.488470 | Elapsed:   22.11  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8139
01/28/2023 02:58:33 PM  [*] Started epoch: 2
01/28/2023 02:58:33 PM  [*] Sat Jan 28 14:58:33 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.367717 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.7561 & F1 0.8611 | AUC 0.9083
01/28/2023 02:58:42 PM  [*] Sat Jan 28 14:58:42 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.358312 | Elapsed: 9.12s | FPR 0.0003 -> TPR 0.4559 & F1 0.6263 | AUC 0.8936
01/28/2023 02:58:51 PM  [*] Sat Jan 28 14:58:51 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.296488 | Elapsed: 9.07s | FPR 0.0003 -> TPR 0.5556 & F1 0.7143 | AUC 0.9435
01/28/2023 02:58:55 PM  [*] Sat Jan 28 14:58:55 2023:    2    | Tr.loss: 0.297327 | Elapsed:   21.91  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.37 | AUC: 0.9348
01/28/2023 02:58:55 PM  [*] Started epoch: 3
01/28/2023 02:58:55 PM  [*] Sat Jan 28 14:58:55 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.296675 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.8222 & F1 0.9024 | AUC 0.9532
01/28/2023 02:59:04 PM  [*] Sat Jan 28 14:59:04 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.164349 | Elapsed: 9.05s | FPR 0.0003 -> TPR 0.6957 & F1 0.8205 | AUC 0.9878
01/28/2023 02:59:13 PM  [*] Sat Jan 28 14:59:13 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.216731 | Elapsed: 9.06s | FPR 0.0003 -> TPR 0.5205 & F1 0.6847 | AUC 0.9285
01/28/2023 02:59:17 PM  [*] Sat Jan 28 14:59:17 2023:    3    | Tr.loss: 0.213213 | Elapsed:   21.93  s | FPR 0.0003 -> TPR: 0.36 & F1: 0.53 | AUC: 0.9688
01/28/2023 02:59:17 PM [!] Sat Jan 28 14:59:17 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\downstreamTask_pretrained\training_files\1674914357-model.torch
		train time: 1674914357-trainTime.npy
		train losses: 1674914357-trainLosses.npy
		train AUC: 1674914357-auc.npy
		train F1s : 1674914357-trainF1s.npy
		train TPRs: 1674914357-trainTPRs.npy
01/28/2023 02:59:17 PM  [!] Training non_pretrained model on downstream task...
01/28/2023 02:59:17 PM  [*] Started epoch: 1
01/28/2023 02:59:17 PM  [*] Sat Jan 28 14:59:17 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.913862 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.2326 & F1 0.3774 | AUC 0.4762
01/28/2023 02:59:24 PM  [*] Sat Jan 28 14:59:24 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.347082 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4247 & F1 0.5962 | AUC 0.8823
01/28/2023 02:59:30 PM  [*] Sat Jan 28 14:59:30 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.269603 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.3016 & F1 0.4634 | AUC 0.9211
01/28/2023 02:59:32 PM  [*] Sat Jan 28 14:59:32 2023:    1    | Tr.loss: 0.496807 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8218
01/28/2023 02:59:32 PM  [*] Started epoch: 2
01/28/2023 02:59:33 PM  [*] Sat Jan 28 14:59:33 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.308597 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5532 & F1 0.7123 | AUC 0.9299
01/28/2023 02:59:39 PM  [*] Sat Jan 28 14:59:39 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.354757 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5634 & F1 0.7207 | AUC 0.9485
01/28/2023 02:59:45 PM  [*] Sat Jan 28 14:59:45 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.286537 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8154 & F1 0.8983 | AUC 0.9701
01/28/2023 02:59:48 PM  [*] Sat Jan 28 14:59:48 2023:    2    | Tr.loss: 0.309452 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.32 | AUC: 0.9275
01/28/2023 02:59:48 PM  [*] Started epoch: 3
01/28/2023 02:59:48 PM  [*] Sat Jan 28 14:59:48 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.199976 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7234 & F1 0.8395 | AUC 0.9675
01/28/2023 02:59:54 PM  [*] Sat Jan 28 14:59:54 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.211705 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6071 & F1 0.7556 | AUC 0.9436
01/28/2023 03:00:00 PM  [*] Sat Jan 28 15:00:00 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.293121 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.7612 & F1 0.8644 | AUC 0.9688
01/28/2023 03:00:03 PM  [*] Sat Jan 28 15:00:03 2023:    3    | Tr.loss: 0.239280 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.20 & F1: 0.33 | AUC: 0.9593
01/28/2023 03:00:03 PM [!] Sat Jan 28 15:00:03 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\downstreamTask_non_pretrained\training_files\1674914403-model.torch
		train time: 1674914403-trainTime.npy
		train losses: 1674914403-trainLosses.npy
		train AUC: 1674914403-auc.npy
		train F1s : 1674914403-trainF1s.npy
		train TPRs: 1674914403-trainTPRs.npy
01/28/2023 03:00:03 PM  [!] Training full_data model on downstream task...
01/28/2023 03:00:04 PM  [*] Started epoch: 1
01/28/2023 03:00:04 PM  [*] Sat Jan 28 15:00:04 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 2.731800 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0250 & F1 0.0488 | AUC 0.4281
01/28/2023 03:00:10 PM  [*] Sat Jan 28 15:00:10 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.440572 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.2899 & F1 0.4494 | AUC 0.8322
01/28/2023 03:00:16 PM  [*] Sat Jan 28 15:00:16 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.299208 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.6197 & F1 0.7652 | AUC 0.9199
01/28/2023 03:00:22 PM  [*] Sat Jan 28 15:00:22 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.247208 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.4444 & F1 0.6154 | AUC 0.9271
01/28/2023 03:00:29 PM  [*] Sat Jan 28 15:00:29 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.174422 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.6230 & F1 0.7677 | AUC 0.9559
01/28/2023 03:00:35 PM  [*] Sat Jan 28 15:00:35 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.323984 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9238
01/28/2023 03:00:41 PM  [*] Sat Jan 28 15:00:41 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.222281 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7344 & F1 0.8468 | AUC 0.9505
01/28/2023 03:00:47 PM  [*] Sat Jan 28 15:00:47 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.217680 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8451 & F1 0.9160 | AUC 0.9645
01/28/2023 03:00:53 PM  [*] Sat Jan 28 15:00:53 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.183227 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.9016 & F1 0.9483 | AUC 0.9903
01/28/2023 03:01:00 PM  [*] Sat Jan 28 15:01:00 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.221347 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.7429 & F1 0.8525 | AUC 0.9714
01/28/2023 03:01:06 PM [!] Learning rate: 2.5e-05
01/28/2023 03:01:06 PM  [*] Sat Jan 28 15:01:06 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.382722 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6212 & F1 0.7664 | AUC 0.9439
01/28/2023 03:01:12 PM  [*] Sat Jan 28 15:01:12 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.161688 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.6757 & F1 0.8065 | AUC 0.9636
01/28/2023 03:01:19 PM  [*] Sat Jan 28 15:01:19 2023:    1    | Tr.loss: 0.304961 | Elapsed:   75.61  s | FPR 0.0003 -> TPR: 0.03 & F1: 0.06 | AUC: 0.9327
01/28/2023 03:01:19 PM  [*] Started epoch: 2
01/28/2023 03:01:19 PM  [*] Sat Jan 28 15:01:19 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.170141 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8889 & F1 0.9412 | AUC 0.9825
01/28/2023 03:01:25 PM  [*] Sat Jan 28 15:01:25 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.172840 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8209 & F1 0.9016 | AUC 0.9824
01/28/2023 03:01:32 PM  [*] Sat Jan 28 15:01:32 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.189221 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.6087 & F1 0.7568 | AUC 0.9705
01/28/2023 03:01:38 PM  [*] Sat Jan 28 15:01:38 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.169035 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.7576 & F1 0.8621 | AUC 0.9786
01/28/2023 03:01:44 PM  [*] Sat Jan 28 15:01:44 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.103531 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.9605 & F1 0.9799 | AUC 0.9956
01/28/2023 03:01:50 PM  [*] Sat Jan 28 15:01:50 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.173606 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7538 & F1 0.8596 | AUC 0.9723
01/28/2023 03:01:56 PM  [*] Sat Jan 28 15:01:56 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.178790 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.7436 & F1 0.8529 | AUC 0.9668
01/28/2023 03:02:03 PM  [*] Sat Jan 28 15:02:03 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.153932 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.5362 & F1 0.6981 | AUC 0.9818
01/28/2023 03:02:09 PM  [*] Sat Jan 28 15:02:09 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.173334 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.9437 & F1 0.9710 | AUC 0.9893
01/28/2023 03:02:09 PM [!] Learning rate: 2.5e-06
01/28/2023 03:02:15 PM  [*] Sat Jan 28 15:02:15 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.178101 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.6515 & F1 0.7890 | AUC 0.9795
01/28/2023 03:02:21 PM  [*] Sat Jan 28 15:02:21 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.213907 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8235 & F1 0.9032 | AUC 0.9816
01/28/2023 03:02:28 PM  [*] Sat Jan 28 15:02:28 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.222147 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8103 & F1 0.8952 | AUC 0.9823
01/28/2023 03:02:35 PM  [*] Sat Jan 28 15:02:35 2023:    2    | Tr.loss: 0.169565 | Elapsed:   75.61  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9800
01/28/2023 03:02:35 PM  [*] Started epoch: 3
01/28/2023 03:02:35 PM  [*] Sat Jan 28 15:02:35 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.272969 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5750 & F1 0.7302 | AUC 0.9573
01/28/2023 03:02:41 PM  [*] Sat Jan 28 15:02:41 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.149441 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8987 & F1 0.9467 | AUC 0.9849
01/28/2023 03:02:47 PM  [*] Sat Jan 28 15:02:47 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.176262 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.6111 & F1 0.7586 | AUC 0.9826
01/28/2023 03:02:53 PM  [*] Sat Jan 28 15:02:53 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.186791 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8308 & F1 0.9076 | AUC 0.9749
01/28/2023 03:03:00 PM  [*] Sat Jan 28 15:03:00 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.297729 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.6111 & F1 0.7586 | AUC 0.9549
01/28/2023 03:03:06 PM  [*] Sat Jan 28 15:03:06 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.115597 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8986 & F1 0.9466 | AUC 0.9911
01/28/2023 03:03:12 PM  [*] Sat Jan 28 15:03:12 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.197174 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8438 & F1 0.9153 | AUC 0.9844
01/28/2023 03:03:13 PM [!] Learning rate: 2.5000000000000004e-07
01/28/2023 03:03:18 PM  [*] Sat Jan 28 15:03:18 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.165043 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8514 & F1 0.9197 | AUC 0.9844
01/28/2023 03:03:24 PM  [*] Sat Jan 28 15:03:24 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.098131 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.9565 & F1 0.9778 | AUC 0.9967
01/28/2023 03:03:31 PM  [*] Sat Jan 28 15:03:31 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.079610 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.9242 & F1 0.9606 | AUC 0.9906
01/28/2023 03:03:37 PM  [*] Sat Jan 28 15:03:37 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.069598 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.9333 & F1 0.9655 | AUC 0.9936
01/28/2023 03:03:43 PM  [*] Sat Jan 28 15:03:43 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.179259 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7119 & F1 0.8317 | AUC 0.9822
01/28/2023 03:03:50 PM  [*] Sat Jan 28 15:03:50 2023:    3    | Tr.loss: 0.162992 | Elapsed:   75.45  s | FPR 0.0003 -> TPR: 0.45 & F1: 0.62 | AUC: 0.9816
01/28/2023 03:03:51 PM [!] Sat Jan 28 15:03:51 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\downstreamTask_full_data\training_files\1674914630-model.torch
		train time: 1674914630-trainTime.npy
		train losses: 1674914630-trainLosses.npy
		train AUC: 1674914630-auc.npy
		train F1s : 1674914630-trainF1s.npy
		train TPRs: 1674914630-trainTPRs.npy
01/28/2023 03:03:51 PM  [*] Evaluating pretrained model on test set...
01/28/2023 03:03:56 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0359 | F1: 0.0694
01/28/2023 03:03:56 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1230 | F1: 0.2191
01/28/2023 03:03:56 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3110 | F1: 0.4741
01/28/2023 03:03:56 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3503 | F1: 0.5179
01/28/2023 03:03:56 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4256 | F1: 0.5935
01/28/2023 03:03:56 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5609 | F1: 0.7072
01/28/2023 03:03:56 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8068 | F1: 0.8529
01/28/2023 03:03:56 PM  [*] Evaluating non_pretrained model on test set...
01/28/2023 03:04:01 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0182 | F1: 0.0357
01/28/2023 03:04:01 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1121 | F1: 0.2015
01/28/2023 03:04:01 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2419 | F1: 0.3894
01/28/2023 03:04:01 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.2927 | F1: 0.4519
01/28/2023 03:04:01 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.3600 | F1: 0.5262
01/28/2023 03:04:01 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.4823 | F1: 0.6397
01/28/2023 03:04:01 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7072 | F1: 0.7892
01/28/2023 03:04:01 PM  [*] Evaluating full_data model on test set...
01/28/2023 03:04:06 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.1059 | F1: 0.1916
01/28/2023 03:04:06 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2911 | F1: 0.4508
01/28/2023 03:04:06 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3027 | F1: 0.4644
01/28/2023 03:04:06 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3458 | F1: 0.5130
01/28/2023 03:04:06 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4007 | F1: 0.5687
01/28/2023 03:04:06 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5192 | F1: 0.6722
01/28/2023 03:04:06 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8402 | F1: 0.8729
01/28/2023 03:04:06 PM  [!] Running pre-training split 2/3
01/28/2023 03:04:09 PM  [!] Pre-training model...
01/28/2023 03:04:09 PM  [*] Masking sequences: iteration 1...
01/28/2023 03:04:27 PM  [*] Started epoch: 1
01/28/2023 03:04:28 PM  [*] Sat Jan 28 15:04:28 2023: Train Epoch: 1 [  0  /60900 (0 %)]	Loss: 478.433746 | Elapsed: 0.86s
01/28/2023 03:04:40 PM  [*] Sat Jan 28 15:04:40 2023: Train Epoch: 1 [6400 /60900 (11%)]	Loss: 232.552246 | Elapsed: 12.26s
01/28/2023 03:04:53 PM  [*] Sat Jan 28 15:04:53 2023: Train Epoch: 1 [12800/60900 (21%)]	Loss: 218.455872 | Elapsed: 12.23s
01/28/2023 03:05:05 PM  [*] Sat Jan 28 15:05:05 2023: Train Epoch: 1 [19200/60900 (32%)]	Loss: 194.169785 | Elapsed: 12.31s
01/28/2023 03:05:17 PM  [*] Sat Jan 28 15:05:17 2023: Train Epoch: 1 [25600/60900 (42%)]	Loss: 184.542053 | Elapsed: 12.23s
01/28/2023 03:05:29 PM  [*] Sat Jan 28 15:05:29 2023: Train Epoch: 1 [32000/60900 (53%)]	Loss: 205.024216 | Elapsed: 12.26s
01/28/2023 03:05:42 PM  [*] Sat Jan 28 15:05:42 2023: Train Epoch: 1 [38400/60900 (63%)]	Loss: 221.593323 | Elapsed: 12.31s
01/28/2023 03:05:54 PM  [*] Sat Jan 28 15:05:54 2023: Train Epoch: 1 [44800/60900 (74%)]	Loss: 171.387177 | Elapsed: 12.28s
01/28/2023 03:06:06 PM  [*] Sat Jan 28 15:06:06 2023: Train Epoch: 1 [51200/60900 (84%)]	Loss: 178.710480 | Elapsed: 12.38s
01/28/2023 03:06:19 PM  [*] Sat Jan 28 15:06:19 2023: Train Epoch: 1 [57600/60900 (95%)]	Loss: 192.679474 | Elapsed: 12.37s
01/28/2023 03:06:27 PM  [*] Sat Jan 28 15:06:27 2023:    1    | Tr.loss: 207.189197 | Elapsed:  119.52  s
01/28/2023 03:06:27 PM [!] Sat Jan 28 15:06:27 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_1_1674914787-model.torch
01/28/2023 03:06:28 PM  [*] Masking sequences: iteration 2...
01/28/2023 03:06:48 PM  [*] Started epoch: 2
01/28/2023 03:06:48 PM  [*] Sat Jan 28 15:06:48 2023: Train Epoch: 2 [  0  /60900 (0 %)]	Loss: 192.159515 | Elapsed: 0.45s
01/28/2023 03:07:00 PM  [*] Sat Jan 28 15:07:00 2023: Train Epoch: 2 [6400 /60900 (11%)]	Loss: 178.015289 | Elapsed: 12.24s
01/28/2023 03:07:13 PM  [*] Sat Jan 28 15:07:13 2023: Train Epoch: 2 [12800/60900 (21%)]	Loss: 184.427246 | Elapsed: 12.28s
01/28/2023 03:07:25 PM  [*] Sat Jan 28 15:07:25 2023: Train Epoch: 2 [19200/60900 (32%)]	Loss: 200.346252 | Elapsed: 12.18s
01/28/2023 03:07:37 PM  [*] Sat Jan 28 15:07:37 2023: Train Epoch: 2 [25600/60900 (42%)]	Loss: 200.342270 | Elapsed: 12.22s
01/28/2023 03:07:49 PM  [*] Sat Jan 28 15:07:49 2023: Train Epoch: 2 [32000/60900 (53%)]	Loss: 190.469940 | Elapsed: 12.25s
01/28/2023 03:08:02 PM  [*] Sat Jan 28 15:08:02 2023: Train Epoch: 2 [38400/60900 (63%)]	Loss: 188.399689 | Elapsed: 12.29s
01/28/2023 03:08:14 PM  [*] Sat Jan 28 15:08:14 2023: Train Epoch: 2 [44800/60900 (74%)]	Loss: 195.751251 | Elapsed: 12.27s
01/28/2023 03:08:26 PM  [*] Sat Jan 28 15:08:26 2023: Train Epoch: 2 [51200/60900 (84%)]	Loss: 190.004105 | Elapsed: 12.26s
01/28/2023 03:08:38 PM  [*] Sat Jan 28 15:08:38 2023: Train Epoch: 2 [57600/60900 (95%)]	Loss: 188.436142 | Elapsed: 12.20s
01/28/2023 03:08:46 PM  [*] Sat Jan 28 15:08:46 2023:    2    | Tr.loss: 186.761380 | Elapsed:  118.57  s
01/28/2023 03:08:47 PM [!] Sat Jan 28 15:08:47 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_2_1674914926-model.torch
01/28/2023 03:08:48 PM  [*] Masking sequences: iteration 3...
01/28/2023 03:09:07 PM  [*] Started epoch: 3
01/28/2023 03:09:07 PM  [*] Sat Jan 28 15:09:07 2023: Train Epoch: 3 [  0  /60900 (0 %)]	Loss: 167.429413 | Elapsed: 0.37s
01/28/2023 03:09:20 PM  [*] Sat Jan 28 15:09:20 2023: Train Epoch: 3 [6400 /60900 (11%)]	Loss: 169.051636 | Elapsed: 12.14s
01/28/2023 03:09:32 PM  [*] Sat Jan 28 15:09:32 2023: Train Epoch: 3 [12800/60900 (21%)]	Loss: 191.706512 | Elapsed: 12.20s
01/28/2023 03:09:44 PM  [*] Sat Jan 28 15:09:44 2023: Train Epoch: 3 [19200/60900 (32%)]	Loss: 180.417358 | Elapsed: 12.24s
01/28/2023 03:09:56 PM  [*] Sat Jan 28 15:09:56 2023: Train Epoch: 3 [25600/60900 (42%)]	Loss: 199.510437 | Elapsed: 12.26s
01/28/2023 03:10:09 PM  [*] Sat Jan 28 15:10:09 2023: Train Epoch: 3 [32000/60900 (53%)]	Loss: 181.248840 | Elapsed: 12.23s
01/28/2023 03:10:21 PM  [*] Sat Jan 28 15:10:21 2023: Train Epoch: 3 [38400/60900 (63%)]	Loss: 182.603271 | Elapsed: 12.19s
01/28/2023 03:10:33 PM  [*] Sat Jan 28 15:10:33 2023: Train Epoch: 3 [44800/60900 (74%)]	Loss: 177.330811 | Elapsed: 12.21s
01/28/2023 03:10:45 PM  [*] Sat Jan 28 15:10:45 2023: Train Epoch: 3 [51200/60900 (84%)]	Loss: 164.032867 | Elapsed: 12.20s
01/28/2023 03:10:57 PM  [*] Sat Jan 28 15:10:57 2023: Train Epoch: 3 [57600/60900 (95%)]	Loss: 194.244293 | Elapsed: 12.18s
01/28/2023 03:11:05 PM  [*] Sat Jan 28 15:11:05 2023:    3    | Tr.loss: 181.824200 | Elapsed:  118.18  s
01/28/2023 03:11:06 PM [!] Sat Jan 28 15:11:06 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_3_1674915065-model.torch
01/28/2023 03:11:07 PM  [*] Masking sequences: iteration 4...
01/28/2023 03:11:26 PM  [*] Started epoch: 4
01/28/2023 03:11:26 PM  [*] Sat Jan 28 15:11:26 2023: Train Epoch: 4 [  0  /60900 (0 %)]	Loss: 193.401459 | Elapsed: 0.34s
01/28/2023 03:11:38 PM  [*] Sat Jan 28 15:11:38 2023: Train Epoch: 4 [6400 /60900 (11%)]	Loss: 190.221939 | Elapsed: 12.26s
01/28/2023 03:11:51 PM  [*] Sat Jan 28 15:11:51 2023: Train Epoch: 4 [12800/60900 (21%)]	Loss: 185.812286 | Elapsed: 12.22s
01/28/2023 03:12:03 PM  [*] Sat Jan 28 15:12:03 2023: Train Epoch: 4 [19200/60900 (32%)]	Loss: 177.766052 | Elapsed: 12.27s
01/28/2023 03:12:15 PM  [*] Sat Jan 28 15:12:15 2023: Train Epoch: 4 [25600/60900 (42%)]	Loss: 179.764282 | Elapsed: 12.22s
01/28/2023 03:12:27 PM  [*] Sat Jan 28 15:12:27 2023: Train Epoch: 4 [32000/60900 (53%)]	Loss: 197.721375 | Elapsed: 12.20s
01/28/2023 03:12:40 PM  [*] Sat Jan 28 15:12:40 2023: Train Epoch: 4 [38400/60900 (63%)]	Loss: 154.200211 | Elapsed: 12.32s
01/28/2023 03:12:52 PM  [*] Sat Jan 28 15:12:52 2023: Train Epoch: 4 [44800/60900 (74%)]	Loss: 193.761841 | Elapsed: 12.26s
01/28/2023 03:13:04 PM  [*] Sat Jan 28 15:13:04 2023: Train Epoch: 4 [51200/60900 (84%)]	Loss: 174.508911 | Elapsed: 12.29s
01/28/2023 03:13:16 PM  [*] Sat Jan 28 15:13:16 2023: Train Epoch: 4 [57600/60900 (95%)]	Loss: 187.750305 | Elapsed: 12.26s
01/28/2023 03:13:24 PM  [*] Sat Jan 28 15:13:24 2023:    4    | Tr.loss: 178.816710 | Elapsed:  118.56  s
01/28/2023 03:13:25 PM [!] Sat Jan 28 15:13:25 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_4_1674915204-model.torch
01/28/2023 03:13:26 PM  [*] Masking sequences: iteration 5...
01/28/2023 03:13:45 PM  [*] Started epoch: 5
01/28/2023 03:13:45 PM  [*] Sat Jan 28 15:13:45 2023: Train Epoch: 5 [  0  /60900 (0 %)]	Loss: 193.714401 | Elapsed: 0.44s
01/28/2023 03:13:58 PM  [*] Sat Jan 28 15:13:58 2023: Train Epoch: 5 [6400 /60900 (11%)]	Loss: 187.565033 | Elapsed: 12.20s
01/28/2023 03:14:10 PM  [*] Sat Jan 28 15:14:10 2023: Train Epoch: 5 [12800/60900 (21%)]	Loss: 167.197418 | Elapsed: 12.19s
01/28/2023 03:14:22 PM  [*] Sat Jan 28 15:14:22 2023: Train Epoch: 5 [19200/60900 (32%)]	Loss: 161.312469 | Elapsed: 12.19s
01/28/2023 03:14:34 PM  [*] Sat Jan 28 15:14:34 2023: Train Epoch: 5 [25600/60900 (42%)]	Loss: 182.897247 | Elapsed: 12.21s
01/28/2023 03:14:46 PM  [*] Sat Jan 28 15:14:46 2023: Train Epoch: 5 [32000/60900 (53%)]	Loss: 173.767639 | Elapsed: 12.22s
01/28/2023 03:14:59 PM  [*] Sat Jan 28 15:14:59 2023: Train Epoch: 5 [38400/60900 (63%)]	Loss: 190.025192 | Elapsed: 12.21s
01/28/2023 03:15:11 PM  [*] Sat Jan 28 15:15:11 2023: Train Epoch: 5 [44800/60900 (74%)]	Loss: 171.528839 | Elapsed: 12.24s
01/28/2023 03:15:23 PM  [*] Sat Jan 28 15:15:23 2023: Train Epoch: 5 [51200/60900 (84%)]	Loss: 186.767212 | Elapsed: 12.21s
01/28/2023 03:15:35 PM  [*] Sat Jan 28 15:15:35 2023: Train Epoch: 5 [57600/60900 (95%)]	Loss: 158.072281 | Elapsed: 12.28s
01/28/2023 03:15:43 PM  [*] Sat Jan 28 15:15:43 2023:    5    | Tr.loss: 177.070952 | Elapsed:  118.30  s
01/28/2023 03:15:44 PM [!] Sat Jan 28 15:15:44 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_5_1674915343-model.torch
01/28/2023 03:15:45 PM  [*] Masking sequences: iteration 6...
01/28/2023 03:16:04 PM  [*] Started epoch: 6
01/28/2023 03:16:04 PM  [*] Sat Jan 28 15:16:04 2023: Train Epoch: 6 [  0  /60900 (0 %)]	Loss: 169.528259 | Elapsed: 0.45s
01/28/2023 03:16:17 PM  [*] Sat Jan 28 15:16:17 2023: Train Epoch: 6 [6400 /60900 (11%)]	Loss: 173.929474 | Elapsed: 12.19s
01/28/2023 03:16:29 PM  [*] Sat Jan 28 15:16:29 2023: Train Epoch: 6 [12800/60900 (21%)]	Loss: 165.573044 | Elapsed: 12.17s
01/28/2023 03:16:41 PM  [*] Sat Jan 28 15:16:41 2023: Train Epoch: 6 [19200/60900 (32%)]	Loss: 185.085495 | Elapsed: 12.26s
01/28/2023 03:16:53 PM  [*] Sat Jan 28 15:16:53 2023: Train Epoch: 6 [25600/60900 (42%)]	Loss: 174.707077 | Elapsed: 12.23s
01/28/2023 03:17:06 PM  [*] Sat Jan 28 15:17:06 2023: Train Epoch: 6 [32000/60900 (53%)]	Loss: 185.182098 | Elapsed: 12.28s
01/28/2023 03:17:18 PM  [*] Sat Jan 28 15:17:18 2023: Train Epoch: 6 [38400/60900 (63%)]	Loss: 191.388123 | Elapsed: 12.27s
01/28/2023 03:17:30 PM  [*] Sat Jan 28 15:17:30 2023: Train Epoch: 6 [44800/60900 (74%)]	Loss: 170.441467 | Elapsed: 12.22s
01/28/2023 03:17:42 PM  [*] Sat Jan 28 15:17:42 2023: Train Epoch: 6 [51200/60900 (84%)]	Loss: 162.405548 | Elapsed: 12.21s
01/28/2023 03:17:55 PM  [*] Sat Jan 28 15:17:55 2023: Train Epoch: 6 [57600/60900 (95%)]	Loss: 191.845184 | Elapsed: 12.25s
01/28/2023 03:18:02 PM  [*] Sat Jan 28 15:18:02 2023:    6    | Tr.loss: 176.123156 | Elapsed:  118.43  s
01/28/2023 03:18:03 PM [!] Sat Jan 28 15:18:03 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_6_1674915482-model.torch
01/28/2023 03:18:04 PM  [*] Masking sequences: iteration 7...
01/28/2023 03:18:23 PM  [*] Started epoch: 7
01/28/2023 03:18:24 PM  [*] Sat Jan 28 15:18:24 2023: Train Epoch: 7 [  0  /60900 (0 %)]	Loss: 188.995956 | Elapsed: 0.28s
01/28/2023 03:18:36 PM  [*] Sat Jan 28 15:18:36 2023: Train Epoch: 7 [6400 /60900 (11%)]	Loss: 187.231064 | Elapsed: 12.17s
01/28/2023 03:18:48 PM  [*] Sat Jan 28 15:18:48 2023: Train Epoch: 7 [12800/60900 (21%)]	Loss: 140.785095 | Elapsed: 12.17s
01/28/2023 03:19:00 PM  [*] Sat Jan 28 15:19:00 2023: Train Epoch: 7 [19200/60900 (32%)]	Loss: 164.830444 | Elapsed: 12.25s
01/28/2023 03:19:12 PM  [*] Sat Jan 28 15:19:12 2023: Train Epoch: 7 [25600/60900 (42%)]	Loss: 170.805786 | Elapsed: 12.19s
01/28/2023 03:19:25 PM  [*] Sat Jan 28 15:19:25 2023: Train Epoch: 7 [32000/60900 (53%)]	Loss: 194.687561 | Elapsed: 12.21s
01/28/2023 03:19:37 PM  [*] Sat Jan 28 15:19:37 2023: Train Epoch: 7 [38400/60900 (63%)]	Loss: 183.937408 | Elapsed: 12.17s
01/28/2023 03:19:49 PM  [*] Sat Jan 28 15:19:49 2023: Train Epoch: 7 [44800/60900 (74%)]	Loss: 181.092896 | Elapsed: 12.24s
01/28/2023 03:20:01 PM  [*] Sat Jan 28 15:20:01 2023: Train Epoch: 7 [51200/60900 (84%)]	Loss: 182.560059 | Elapsed: 12.20s
01/28/2023 03:20:13 PM  [*] Sat Jan 28 15:20:13 2023: Train Epoch: 7 [57600/60900 (95%)]	Loss: 178.695419 | Elapsed: 12.23s
01/28/2023 03:20:21 PM  [*] Sat Jan 28 15:20:21 2023:    7    | Tr.loss: 175.508680 | Elapsed:  118.01  s
01/28/2023 03:20:22 PM [!] Sat Jan 28 15:20:22 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_7_1674915621-model.torch
01/28/2023 03:20:23 PM  [*] Masking sequences: iteration 8...
01/28/2023 03:20:42 PM  [*] Started epoch: 8
01/28/2023 03:20:42 PM  [*] Sat Jan 28 15:20:42 2023: Train Epoch: 8 [  0  /60900 (0 %)]	Loss: 201.876923 | Elapsed: 0.36s
01/28/2023 03:20:54 PM  [*] Sat Jan 28 15:20:54 2023: Train Epoch: 8 [6400 /60900 (11%)]	Loss: 203.786804 | Elapsed: 12.15s
01/28/2023 03:21:07 PM  [*] Sat Jan 28 15:21:07 2023: Train Epoch: 8 [12800/60900 (21%)]	Loss: 169.917709 | Elapsed: 12.20s
01/28/2023 03:21:19 PM  [*] Sat Jan 28 15:21:19 2023: Train Epoch: 8 [19200/60900 (32%)]	Loss: 191.175415 | Elapsed: 12.20s
01/28/2023 03:21:31 PM  [*] Sat Jan 28 15:21:31 2023: Train Epoch: 8 [25600/60900 (42%)]	Loss: 184.676071 | Elapsed: 12.25s
01/28/2023 03:21:43 PM  [*] Sat Jan 28 15:21:43 2023: Train Epoch: 8 [32000/60900 (53%)]	Loss: 167.761505 | Elapsed: 12.21s
01/28/2023 03:21:56 PM  [*] Sat Jan 28 15:21:56 2023: Train Epoch: 8 [38400/60900 (63%)]	Loss: 171.614380 | Elapsed: 12.32s
01/28/2023 03:22:08 PM  [*] Sat Jan 28 15:22:08 2023: Train Epoch: 8 [44800/60900 (74%)]	Loss: 185.060760 | Elapsed: 12.23s
01/28/2023 03:22:20 PM  [*] Sat Jan 28 15:22:20 2023: Train Epoch: 8 [51200/60900 (84%)]	Loss: 177.285156 | Elapsed: 12.22s
01/28/2023 03:22:32 PM  [*] Sat Jan 28 15:22:32 2023: Train Epoch: 8 [57600/60900 (95%)]	Loss: 187.010406 | Elapsed: 12.19s
01/28/2023 03:22:40 PM  [*] Sat Jan 28 15:22:40 2023:    8    | Tr.loss: 175.346298 | Elapsed:  118.29  s
01/28/2023 03:22:41 PM [!] Sat Jan 28 15:22:41 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_8_1674915760-model.torch
01/28/2023 03:22:42 PM  [*] Masking sequences: iteration 9...
01/28/2023 03:23:01 PM  [*] Started epoch: 9
01/28/2023 03:23:01 PM  [*] Sat Jan 28 15:23:01 2023: Train Epoch: 9 [  0  /60900 (0 %)]	Loss: 187.426300 | Elapsed: 0.39s
01/28/2023 03:23:13 PM  [*] Sat Jan 28 15:23:13 2023: Train Epoch: 9 [6400 /60900 (11%)]	Loss: 188.644958 | Elapsed: 12.20s
01/28/2023 03:23:26 PM  [*] Sat Jan 28 15:23:26 2023: Train Epoch: 9 [12800/60900 (21%)]	Loss: 154.424896 | Elapsed: 12.21s
01/28/2023 03:23:38 PM  [*] Sat Jan 28 15:23:38 2023: Train Epoch: 9 [19200/60900 (32%)]	Loss: 175.647156 | Elapsed: 12.22s
01/28/2023 03:23:50 PM  [*] Sat Jan 28 15:23:50 2023: Train Epoch: 9 [25600/60900 (42%)]	Loss: 159.862381 | Elapsed: 12.23s
01/28/2023 03:24:02 PM  [*] Sat Jan 28 15:24:02 2023: Train Epoch: 9 [32000/60900 (53%)]	Loss: 173.429108 | Elapsed: 12.21s
01/28/2023 03:24:14 PM  [*] Sat Jan 28 15:24:14 2023: Train Epoch: 9 [38400/60900 (63%)]	Loss: 189.691513 | Elapsed: 12.21s
01/28/2023 03:24:27 PM  [*] Sat Jan 28 15:24:27 2023: Train Epoch: 9 [44800/60900 (74%)]	Loss: 168.048645 | Elapsed: 12.20s
01/28/2023 03:24:39 PM  [*] Sat Jan 28 15:24:39 2023: Train Epoch: 9 [51200/60900 (84%)]	Loss: 155.959366 | Elapsed: 12.28s
01/28/2023 03:24:51 PM  [*] Sat Jan 28 15:24:51 2023: Train Epoch: 9 [57600/60900 (95%)]	Loss: 156.897614 | Elapsed: 12.22s
01/28/2023 03:24:59 PM  [*] Sat Jan 28 15:24:59 2023:    9    | Tr.loss: 175.295969 | Elapsed:  118.23  s
01/28/2023 03:24:59 PM [!] Sat Jan 28 15:24:59 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_9_1674915899-model.torch
01/28/2023 03:25:00 PM  [*] Masking sequences: iteration 10...
01/28/2023 03:25:20 PM  [*] Started epoch: 10
01/28/2023 03:25:20 PM  [*] Sat Jan 28 15:25:20 2023: Train Epoch: 10 [  0  /60900 (0 %)]	Loss: 174.875763 | Elapsed: 0.69s
01/28/2023 03:25:32 PM  [*] Sat Jan 28 15:25:32 2023: Train Epoch: 10 [6400 /60900 (11%)]	Loss: 171.900635 | Elapsed: 12.16s
01/28/2023 03:25:45 PM  [*] Sat Jan 28 15:25:45 2023: Train Epoch: 10 [12800/60900 (21%)]	Loss: 170.231979 | Elapsed: 12.15s
01/28/2023 03:25:57 PM  [*] Sat Jan 28 15:25:57 2023: Train Epoch: 10 [19200/60900 (32%)]	Loss: 177.723907 | Elapsed: 12.20s
01/28/2023 03:26:09 PM  [*] Sat Jan 28 15:26:09 2023: Train Epoch: 10 [25600/60900 (42%)]	Loss: 174.392853 | Elapsed: 12.22s
01/28/2023 03:26:21 PM  [*] Sat Jan 28 15:26:21 2023: Train Epoch: 10 [32000/60900 (53%)]	Loss: 177.582794 | Elapsed: 12.21s
01/28/2023 03:26:33 PM  [*] Sat Jan 28 15:26:33 2023: Train Epoch: 10 [38400/60900 (63%)]	Loss: 175.734482 | Elapsed: 12.24s
01/28/2023 03:26:46 PM  [*] Sat Jan 28 15:26:46 2023: Train Epoch: 10 [44800/60900 (74%)]	Loss: 188.818878 | Elapsed: 12.18s
01/28/2023 03:26:58 PM  [*] Sat Jan 28 15:26:58 2023: Train Epoch: 10 [51200/60900 (84%)]	Loss: 188.031830 | Elapsed: 12.21s
01/28/2023 03:27:10 PM  [*] Sat Jan 28 15:27:10 2023: Train Epoch: 10 [57600/60900 (95%)]	Loss: 167.459366 | Elapsed: 12.23s
01/28/2023 03:27:18 PM  [*] Sat Jan 28 15:27:18 2023:   10    | Tr.loss: 174.926382 | Elapsed:  118.43  s
01/28/2023 03:27:18 PM [!] Sat Jan 28 15:27:18 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_10_1674916038-model.torch
01/28/2023 03:27:19 PM  [*] Masking sequences: iteration 11...
01/28/2023 03:27:38 PM  [*] Started epoch: 11
01/28/2023 03:27:39 PM  [*] Sat Jan 28 15:27:39 2023: Train Epoch: 11 [  0  /60900 (0 %)]	Loss: 182.983002 | Elapsed: 0.29s
01/28/2023 03:27:51 PM  [*] Sat Jan 28 15:27:51 2023: Train Epoch: 11 [6400 /60900 (11%)]	Loss: 163.239868 | Elapsed: 12.20s
01/28/2023 03:28:03 PM  [*] Sat Jan 28 15:28:03 2023: Train Epoch: 11 [12800/60900 (21%)]	Loss: 186.633560 | Elapsed: 12.16s
01/28/2023 03:28:15 PM  [*] Sat Jan 28 15:28:15 2023: Train Epoch: 11 [19200/60900 (32%)]	Loss: 195.308136 | Elapsed: 12.29s
01/28/2023 03:28:28 PM  [*] Sat Jan 28 15:28:28 2023: Train Epoch: 11 [25600/60900 (42%)]	Loss: 174.006882 | Elapsed: 12.18s
01/28/2023 03:28:40 PM  [*] Sat Jan 28 15:28:40 2023: Train Epoch: 11 [32000/60900 (53%)]	Loss: 151.468262 | Elapsed: 12.19s
01/28/2023 03:28:52 PM  [*] Sat Jan 28 15:28:52 2023: Train Epoch: 11 [38400/60900 (63%)]	Loss: 173.107529 | Elapsed: 12.32s
01/28/2023 03:29:04 PM  [*] Sat Jan 28 15:29:04 2023: Train Epoch: 11 [44800/60900 (74%)]	Loss: 182.484375 | Elapsed: 12.19s
01/28/2023 03:29:17 PM  [*] Sat Jan 28 15:29:17 2023: Train Epoch: 11 [51200/60900 (84%)]	Loss: 173.197205 | Elapsed: 12.23s
01/28/2023 03:29:29 PM  [*] Sat Jan 28 15:29:29 2023: Train Epoch: 11 [57600/60900 (95%)]	Loss: 170.178375 | Elapsed: 12.22s
01/28/2023 03:29:37 PM  [*] Sat Jan 28 15:29:37 2023:   11    | Tr.loss: 174.789127 | Elapsed:  118.17  s
01/28/2023 03:29:37 PM [!] Sat Jan 28 15:29:37 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_11_1674916177-model.torch
01/28/2023 03:29:38 PM  [*] Masking sequences: iteration 12...
01/28/2023 03:29:59 PM  [*] Started epoch: 12
01/28/2023 03:29:59 PM  [*] Sat Jan 28 15:29:59 2023: Train Epoch: 12 [  0  /60900 (0 %)]	Loss: 185.794907 | Elapsed: 0.40s
01/28/2023 03:30:12 PM  [*] Sat Jan 28 15:30:12 2023: Train Epoch: 12 [6400 /60900 (11%)]	Loss: 192.211700 | Elapsed: 12.56s
01/28/2023 03:30:24 PM  [*] Sat Jan 28 15:30:24 2023: Train Epoch: 12 [12800/60900 (21%)]	Loss: 168.332306 | Elapsed: 12.56s
01/28/2023 03:30:37 PM  [*] Sat Jan 28 15:30:37 2023: Train Epoch: 12 [19200/60900 (32%)]	Loss: 165.065552 | Elapsed: 12.38s
01/28/2023 03:30:49 PM  [*] Sat Jan 28 15:30:49 2023: Train Epoch: 12 [25600/60900 (42%)]	Loss: 169.873627 | Elapsed: 12.40s
01/28/2023 03:31:01 PM  [*] Sat Jan 28 15:31:01 2023: Train Epoch: 12 [32000/60900 (53%)]	Loss: 203.473404 | Elapsed: 12.26s
01/28/2023 03:31:13 PM  [*] Sat Jan 28 15:31:13 2023: Train Epoch: 12 [38400/60900 (63%)]	Loss: 185.697922 | Elapsed: 12.25s
01/28/2023 03:31:26 PM  [*] Sat Jan 28 15:31:26 2023: Train Epoch: 12 [44800/60900 (74%)]	Loss: 153.437347 | Elapsed: 12.26s
01/28/2023 03:31:38 PM  [*] Sat Jan 28 15:31:38 2023: Train Epoch: 12 [51200/60900 (84%)]	Loss: 186.420547 | Elapsed: 12.32s
01/28/2023 03:31:50 PM  [*] Sat Jan 28 15:31:50 2023: Train Epoch: 12 [57600/60900 (95%)]	Loss: 181.914749 | Elapsed: 12.27s
01/28/2023 03:31:58 PM  [*] Sat Jan 28 15:31:58 2023:   12    | Tr.loss: 174.846412 | Elapsed:  119.71  s
01/28/2023 03:31:59 PM [!] Sat Jan 28 15:31:59 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_12_1674916318-model.torch
01/28/2023 03:32:00 PM  [*] Masking sequences: iteration 13...
01/28/2023 03:32:19 PM  [*] Started epoch: 13
01/28/2023 03:32:20 PM  [*] Sat Jan 28 15:32:20 2023: Train Epoch: 13 [  0  /60900 (0 %)]	Loss: 167.732880 | Elapsed: 0.31s
01/28/2023 03:32:32 PM  [*] Sat Jan 28 15:32:32 2023: Train Epoch: 13 [6400 /60900 (11%)]	Loss: 166.273224 | Elapsed: 12.30s
01/28/2023 03:32:44 PM  [*] Sat Jan 28 15:32:44 2023: Train Epoch: 13 [12800/60900 (21%)]	Loss: 176.478180 | Elapsed: 12.24s
01/28/2023 03:32:56 PM  [*] Sat Jan 28 15:32:56 2023: Train Epoch: 13 [19200/60900 (32%)]	Loss: 177.002090 | Elapsed: 12.24s
01/28/2023 03:33:09 PM  [*] Sat Jan 28 15:33:09 2023: Train Epoch: 13 [25600/60900 (42%)]	Loss: 169.213013 | Elapsed: 12.26s
01/28/2023 03:33:21 PM  [*] Sat Jan 28 15:33:21 2023: Train Epoch: 13 [32000/60900 (53%)]	Loss: 178.062378 | Elapsed: 12.24s
01/28/2023 03:33:33 PM  [*] Sat Jan 28 15:33:33 2023: Train Epoch: 13 [38400/60900 (63%)]	Loss: 182.032349 | Elapsed: 12.30s
01/28/2023 03:33:46 PM  [*] Sat Jan 28 15:33:46 2023: Train Epoch: 13 [44800/60900 (74%)]	Loss: 164.960037 | Elapsed: 12.41s
01/28/2023 03:33:58 PM  [*] Sat Jan 28 15:33:58 2023: Train Epoch: 13 [51200/60900 (84%)]	Loss: 172.997406 | Elapsed: 12.25s
01/28/2023 03:34:10 PM  [*] Sat Jan 28 15:34:10 2023: Train Epoch: 13 [57600/60900 (95%)]	Loss: 158.938873 | Elapsed: 12.20s
01/28/2023 03:34:18 PM  [*] Sat Jan 28 15:34:18 2023:   13    | Tr.loss: 174.802652 | Elapsed:  118.70  s
01/28/2023 03:34:18 PM [!] Sat Jan 28 15:34:18 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_13_1674916458-model.torch
01/28/2023 03:34:19 PM  [*] Masking sequences: iteration 14...
01/28/2023 03:34:39 PM  [*] Started epoch: 14
01/28/2023 03:34:39 PM  [*] Sat Jan 28 15:34:39 2023: Train Epoch: 14 [  0  /60900 (0 %)]	Loss: 167.678223 | Elapsed: 0.35s
01/28/2023 03:34:51 PM  [*] Sat Jan 28 15:34:51 2023: Train Epoch: 14 [6400 /60900 (11%)]	Loss: 167.791748 | Elapsed: 12.15s
01/28/2023 03:35:03 PM  [*] Sat Jan 28 15:35:03 2023: Train Epoch: 14 [12800/60900 (21%)]	Loss: 190.120850 | Elapsed: 12.21s
01/28/2023 03:35:16 PM  [*] Sat Jan 28 15:35:16 2023: Train Epoch: 14 [19200/60900 (32%)]	Loss: 170.390717 | Elapsed: 12.23s
01/28/2023 03:35:28 PM  [*] Sat Jan 28 15:35:28 2023: Train Epoch: 14 [25600/60900 (42%)]	Loss: 157.896332 | Elapsed: 12.18s
01/28/2023 03:35:40 PM  [*] Sat Jan 28 15:35:40 2023: Train Epoch: 14 [32000/60900 (53%)]	Loss: 178.693878 | Elapsed: 12.23s
01/28/2023 03:35:52 PM  [*] Sat Jan 28 15:35:52 2023: Train Epoch: 14 [38400/60900 (63%)]	Loss: 175.740875 | Elapsed: 12.22s
01/28/2023 03:36:04 PM  [*] Sat Jan 28 15:36:04 2023: Train Epoch: 14 [44800/60900 (74%)]	Loss: 176.265808 | Elapsed: 12.18s
01/28/2023 03:36:17 PM  [*] Sat Jan 28 15:36:17 2023: Train Epoch: 14 [51200/60900 (84%)]	Loss: 172.298096 | Elapsed: 12.23s
01/28/2023 03:36:29 PM  [*] Sat Jan 28 15:36:29 2023: Train Epoch: 14 [57600/60900 (95%)]	Loss: 174.684464 | Elapsed: 12.21s
01/28/2023 03:36:37 PM  [*] Sat Jan 28 15:36:37 2023:   14    | Tr.loss: 174.746739 | Elapsed:  118.15  s
01/28/2023 03:36:37 PM [!] Sat Jan 28 15:36:37 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_14_1674916597-model.torch
01/28/2023 03:36:38 PM  [*] Masking sequences: iteration 15...
01/28/2023 03:36:58 PM  [*] Started epoch: 15
01/28/2023 03:36:58 PM  [*] Sat Jan 28 15:36:58 2023: Train Epoch: 15 [  0  /60900 (0 %)]	Loss: 180.923431 | Elapsed: 0.44s
01/28/2023 03:37:10 PM  [*] Sat Jan 28 15:37:10 2023: Train Epoch: 15 [6400 /60900 (11%)]	Loss: 170.367432 | Elapsed: 12.19s
01/28/2023 03:37:22 PM  [*] Sat Jan 28 15:37:22 2023: Train Epoch: 15 [12800/60900 (21%)]	Loss: 186.775970 | Elapsed: 12.20s
01/28/2023 03:37:35 PM  [*] Sat Jan 28 15:37:35 2023: Train Epoch: 15 [19200/60900 (32%)]	Loss: 159.735779 | Elapsed: 12.33s
01/28/2023 03:37:47 PM  [*] Sat Jan 28 15:37:47 2023: Train Epoch: 15 [25600/60900 (42%)]	Loss: 163.898438 | Elapsed: 12.29s
01/28/2023 03:37:59 PM  [*] Sat Jan 28 15:37:59 2023: Train Epoch: 15 [32000/60900 (53%)]	Loss: 193.220306 | Elapsed: 12.20s
01/28/2023 03:38:11 PM  [*] Sat Jan 28 15:38:11 2023: Train Epoch: 15 [38400/60900 (63%)]	Loss: 150.973297 | Elapsed: 12.20s
01/28/2023 03:38:24 PM  [*] Sat Jan 28 15:38:24 2023: Train Epoch: 15 [44800/60900 (74%)]	Loss: 174.950851 | Elapsed: 12.26s
01/28/2023 03:38:36 PM  [*] Sat Jan 28 15:38:36 2023: Train Epoch: 15 [51200/60900 (84%)]	Loss: 161.858521 | Elapsed: 12.21s
01/28/2023 03:38:48 PM  [*] Sat Jan 28 15:38:48 2023: Train Epoch: 15 [57600/60900 (95%)]	Loss: 166.649094 | Elapsed: 12.20s
01/28/2023 03:38:56 PM  [*] Sat Jan 28 15:38:56 2023:   15    | Tr.loss: 174.500059 | Elapsed:  118.44  s
01/28/2023 03:38:57 PM [!] Sat Jan 28 15:38:57 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_15_1674916736-model.torch
01/28/2023 03:38:57 PM  [*] Masking sequences: iteration 16...
01/28/2023 03:39:17 PM  [*] Started epoch: 16
01/28/2023 03:39:17 PM  [*] Sat Jan 28 15:39:17 2023: Train Epoch: 16 [  0  /60900 (0 %)]	Loss: 185.074188 | Elapsed: 0.33s
01/28/2023 03:39:29 PM  [*] Sat Jan 28 15:39:29 2023: Train Epoch: 16 [6400 /60900 (11%)]	Loss: 162.566162 | Elapsed: 12.20s
01/28/2023 03:39:41 PM  [*] Sat Jan 28 15:39:41 2023: Train Epoch: 16 [12800/60900 (21%)]	Loss: 192.925308 | Elapsed: 12.21s
01/28/2023 03:39:54 PM  [*] Sat Jan 28 15:39:54 2023: Train Epoch: 16 [19200/60900 (32%)]	Loss: 169.886673 | Elapsed: 12.23s
01/28/2023 03:40:06 PM  [*] Sat Jan 28 15:40:06 2023: Train Epoch: 16 [25600/60900 (42%)]	Loss: 155.971497 | Elapsed: 12.31s
01/28/2023 03:40:18 PM  [*] Sat Jan 28 15:40:18 2023: Train Epoch: 16 [32000/60900 (53%)]	Loss: 157.657593 | Elapsed: 12.31s
01/28/2023 03:40:30 PM  [*] Sat Jan 28 15:40:30 2023: Train Epoch: 16 [38400/60900 (63%)]	Loss: 181.750793 | Elapsed: 12.23s
01/28/2023 03:40:43 PM  [*] Sat Jan 28 15:40:43 2023: Train Epoch: 16 [44800/60900 (74%)]	Loss: 165.418289 | Elapsed: 12.25s
01/28/2023 03:40:55 PM  [*] Sat Jan 28 15:40:55 2023: Train Epoch: 16 [51200/60900 (84%)]	Loss: 183.414032 | Elapsed: 12.24s
01/28/2023 03:41:07 PM  [*] Sat Jan 28 15:41:07 2023: Train Epoch: 16 [57600/60900 (95%)]	Loss: 182.036285 | Elapsed: 12.20s
01/28/2023 03:41:15 PM  [*] Sat Jan 28 15:41:15 2023:   16    | Tr.loss: 174.608602 | Elapsed:  118.56  s
01/28/2023 03:41:16 PM [!] Sat Jan 28 15:41:16 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_16_1674916875-model.torch
01/28/2023 03:41:17 PM  [*] Masking sequences: iteration 17...
01/28/2023 03:41:36 PM  [*] Started epoch: 17
01/28/2023 03:41:36 PM  [*] Sat Jan 28 15:41:36 2023: Train Epoch: 17 [  0  /60900 (0 %)]	Loss: 183.729004 | Elapsed: 0.42s
01/28/2023 03:41:49 PM  [*] Sat Jan 28 15:41:49 2023: Train Epoch: 17 [6400 /60900 (11%)]	Loss: 168.967087 | Elapsed: 12.24s
01/28/2023 03:42:02 PM  [*] Sat Jan 28 15:42:02 2023: Train Epoch: 17 [12800/60900 (21%)]	Loss: 183.267838 | Elapsed: 13.03s
01/28/2023 03:42:14 PM  [*] Sat Jan 28 15:42:14 2023: Train Epoch: 17 [19200/60900 (32%)]	Loss: 171.956711 | Elapsed: 12.38s
01/28/2023 03:42:26 PM  [*] Sat Jan 28 15:42:26 2023: Train Epoch: 17 [25600/60900 (42%)]	Loss: 153.598358 | Elapsed: 12.21s
01/28/2023 03:42:39 PM  [*] Sat Jan 28 15:42:39 2023: Train Epoch: 17 [32000/60900 (53%)]	Loss: 184.451141 | Elapsed: 12.30s
01/28/2023 03:42:51 PM  [*] Sat Jan 28 15:42:51 2023: Train Epoch: 17 [38400/60900 (63%)]	Loss: 177.374634 | Elapsed: 12.63s
01/28/2023 03:43:04 PM  [*] Sat Jan 28 15:43:04 2023: Train Epoch: 17 [44800/60900 (74%)]	Loss: 166.238525 | Elapsed: 12.60s
01/28/2023 03:43:16 PM  [*] Sat Jan 28 15:43:16 2023: Train Epoch: 17 [51200/60900 (84%)]	Loss: 160.089386 | Elapsed: 12.58s
01/28/2023 03:43:29 PM  [*] Sat Jan 28 15:43:29 2023: Train Epoch: 17 [57600/60900 (95%)]	Loss: 185.916946 | Elapsed: 12.33s
01/28/2023 03:43:37 PM  [*] Sat Jan 28 15:43:37 2023:   17    | Tr.loss: 174.404139 | Elapsed:  120.64  s
01/28/2023 03:43:37 PM [!] Sat Jan 28 15:43:37 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_17_1674917017-model.torch
01/28/2023 03:43:38 PM  [*] Masking sequences: iteration 18...
01/28/2023 03:44:01 PM  [*] Started epoch: 18
01/28/2023 03:44:01 PM  [*] Sat Jan 28 15:44:01 2023: Train Epoch: 18 [  0  /60900 (0 %)]	Loss: 168.591217 | Elapsed: 0.36s
01/28/2023 03:44:13 PM  [*] Sat Jan 28 15:44:13 2023: Train Epoch: 18 [6400 /60900 (11%)]	Loss: 173.905609 | Elapsed: 12.40s
01/28/2023 03:44:26 PM  [*] Sat Jan 28 15:44:26 2023: Train Epoch: 18 [12800/60900 (21%)]	Loss: 178.858429 | Elapsed: 12.34s
01/28/2023 03:44:38 PM  [*] Sat Jan 28 15:44:38 2023: Train Epoch: 18 [19200/60900 (32%)]	Loss: 177.645081 | Elapsed: 12.35s
01/28/2023 03:44:50 PM  [*] Sat Jan 28 15:44:50 2023: Train Epoch: 18 [25600/60900 (42%)]	Loss: 163.835266 | Elapsed: 12.30s
01/28/2023 03:45:03 PM  [*] Sat Jan 28 15:45:03 2023: Train Epoch: 18 [32000/60900 (53%)]	Loss: 180.568726 | Elapsed: 12.29s
01/28/2023 03:45:15 PM  [*] Sat Jan 28 15:45:15 2023: Train Epoch: 18 [38400/60900 (63%)]	Loss: 180.532135 | Elapsed: 12.30s
01/28/2023 03:45:27 PM  [*] Sat Jan 28 15:45:27 2023: Train Epoch: 18 [44800/60900 (74%)]	Loss: 148.941071 | Elapsed: 12.30s
01/28/2023 03:45:39 PM  [*] Sat Jan 28 15:45:39 2023: Train Epoch: 18 [51200/60900 (84%)]	Loss: 163.033844 | Elapsed: 12.30s
01/28/2023 03:45:52 PM  [*] Sat Jan 28 15:45:52 2023: Train Epoch: 18 [57600/60900 (95%)]	Loss: 176.236725 | Elapsed: 12.35s
01/28/2023 03:46:00 PM  [*] Sat Jan 28 15:46:00 2023:   18    | Tr.loss: 174.676600 | Elapsed:  119.27  s
01/28/2023 03:46:00 PM [!] Sat Jan 28 15:46:00 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_18_1674917160-model.torch
01/28/2023 03:46:01 PM  [*] Masking sequences: iteration 19...
01/28/2023 03:46:21 PM  [*] Started epoch: 19
01/28/2023 03:46:21 PM  [*] Sat Jan 28 15:46:21 2023: Train Epoch: 19 [  0  /60900 (0 %)]	Loss: 181.179550 | Elapsed: 0.30s
01/28/2023 03:46:34 PM  [*] Sat Jan 28 15:46:34 2023: Train Epoch: 19 [6400 /60900 (11%)]	Loss: 153.138977 | Elapsed: 12.30s
01/28/2023 03:46:46 PM  [*] Sat Jan 28 15:46:46 2023: Train Epoch: 19 [12800/60900 (21%)]	Loss: 174.282928 | Elapsed: 12.21s
01/28/2023 03:46:58 PM  [*] Sat Jan 28 15:46:58 2023: Train Epoch: 19 [19200/60900 (32%)]	Loss: 172.913971 | Elapsed: 12.18s
01/28/2023 03:47:10 PM  [*] Sat Jan 28 15:47:10 2023: Train Epoch: 19 [25600/60900 (42%)]	Loss: 171.815384 | Elapsed: 12.22s
01/28/2023 03:47:23 PM  [*] Sat Jan 28 15:47:23 2023: Train Epoch: 19 [32000/60900 (53%)]	Loss: 165.274750 | Elapsed: 12.25s
01/28/2023 03:47:35 PM  [*] Sat Jan 28 15:47:35 2023: Train Epoch: 19 [38400/60900 (63%)]	Loss: 157.419296 | Elapsed: 12.20s
01/28/2023 03:47:47 PM  [*] Sat Jan 28 15:47:47 2023: Train Epoch: 19 [44800/60900 (74%)]	Loss: 173.664093 | Elapsed: 12.25s
01/28/2023 03:47:59 PM  [*] Sat Jan 28 15:47:59 2023: Train Epoch: 19 [51200/60900 (84%)]	Loss: 163.356384 | Elapsed: 12.28s
01/28/2023 03:48:12 PM  [*] Sat Jan 28 15:48:12 2023: Train Epoch: 19 [57600/60900 (95%)]	Loss: 169.468292 | Elapsed: 12.31s
01/28/2023 03:48:20 PM  [*] Sat Jan 28 15:48:20 2023:   19    | Tr.loss: 174.529186 | Elapsed:  118.55  s
01/28/2023 03:48:20 PM [!] Sat Jan 28 15:48:20 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_19_1674917300-model.torch
01/28/2023 03:48:21 PM  [*] Masking sequences: iteration 20...
01/28/2023 03:48:41 PM  [*] Started epoch: 20
01/28/2023 03:48:41 PM  [*] Sat Jan 28 15:48:41 2023: Train Epoch: 20 [  0  /60900 (0 %)]	Loss: 187.562775 | Elapsed: 0.43s
01/28/2023 03:48:53 PM  [*] Sat Jan 28 15:48:53 2023: Train Epoch: 20 [6400 /60900 (11%)]	Loss: 190.611115 | Elapsed: 12.33s
01/28/2023 03:49:06 PM  [*] Sat Jan 28 15:49:06 2023: Train Epoch: 20 [12800/60900 (21%)]	Loss: 185.173157 | Elapsed: 12.32s
01/28/2023 03:49:18 PM  [*] Sat Jan 28 15:49:18 2023: Train Epoch: 20 [19200/60900 (32%)]	Loss: 175.987320 | Elapsed: 12.29s
01/28/2023 03:49:30 PM  [*] Sat Jan 28 15:49:30 2023: Train Epoch: 20 [25600/60900 (42%)]	Loss: 194.855057 | Elapsed: 12.22s
01/28/2023 03:49:43 PM  [*] Sat Jan 28 15:49:43 2023: Train Epoch: 20 [32000/60900 (53%)]	Loss: 161.715179 | Elapsed: 12.26s
01/28/2023 03:49:55 PM  [*] Sat Jan 28 15:49:55 2023: Train Epoch: 20 [38400/60900 (63%)]	Loss: 153.766190 | Elapsed: 12.28s
01/28/2023 03:50:07 PM  [*] Sat Jan 28 15:50:07 2023: Train Epoch: 20 [44800/60900 (74%)]	Loss: 175.298874 | Elapsed: 12.33s
01/28/2023 03:50:20 PM  [*] Sat Jan 28 15:50:20 2023: Train Epoch: 20 [51200/60900 (84%)]	Loss: 176.194427 | Elapsed: 12.33s
01/28/2023 03:50:32 PM  [*] Sat Jan 28 15:50:32 2023: Train Epoch: 20 [57600/60900 (95%)]	Loss: 187.640503 | Elapsed: 12.24s
01/28/2023 03:50:40 PM  [*] Sat Jan 28 15:50:40 2023:   20    | Tr.loss: 174.712151 | Elapsed:  119.09  s
01/28/2023 03:50:40 PM [!] Sat Jan 28 15:50:40 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_20_1674917440-model.torch
01/28/2023 03:50:41 PM  [*] Masking sequences: iteration 21...
01/28/2023 03:51:00 PM  [*] Started epoch: 21
01/28/2023 03:51:01 PM  [*] Sat Jan 28 15:51:01 2023: Train Epoch: 21 [  0  /60900 (0 %)]	Loss: 164.222687 | Elapsed: 0.83s
01/28/2023 03:51:14 PM  [*] Sat Jan 28 15:51:14 2023: Train Epoch: 21 [6400 /60900 (11%)]	Loss: 175.740204 | Elapsed: 12.24s
01/28/2023 03:51:26 PM  [*] Sat Jan 28 15:51:26 2023: Train Epoch: 21 [12800/60900 (21%)]	Loss: 164.688568 | Elapsed: 12.31s
01/28/2023 03:51:38 PM  [*] Sat Jan 28 15:51:38 2023: Train Epoch: 21 [19200/60900 (32%)]	Loss: 162.908798 | Elapsed: 12.53s
01/28/2023 03:51:51 PM  [*] Sat Jan 28 15:51:51 2023: Train Epoch: 21 [25600/60900 (42%)]	Loss: 189.914536 | Elapsed: 12.54s
01/28/2023 03:52:04 PM  [*] Sat Jan 28 15:52:04 2023: Train Epoch: 21 [32000/60900 (53%)]	Loss: 173.352570 | Elapsed: 12.64s
01/28/2023 03:52:16 PM  [*] Sat Jan 28 15:52:16 2023: Train Epoch: 21 [38400/60900 (63%)]	Loss: 169.455719 | Elapsed: 12.65s
01/28/2023 03:52:29 PM  [*] Sat Jan 28 15:52:29 2023: Train Epoch: 21 [44800/60900 (74%)]	Loss: 187.903778 | Elapsed: 12.51s
01/28/2023 03:52:41 PM  [*] Sat Jan 28 15:52:41 2023: Train Epoch: 21 [51200/60900 (84%)]	Loss: 193.933746 | Elapsed: 12.57s
01/28/2023 03:52:54 PM  [*] Sat Jan 28 15:52:54 2023: Train Epoch: 21 [57600/60900 (95%)]	Loss: 172.793152 | Elapsed: 12.48s
01/28/2023 03:53:02 PM  [*] Sat Jan 28 15:53:02 2023:   21    | Tr.loss: 174.575046 | Elapsed:  121.47  s
01/28/2023 03:53:02 PM [!] Sat Jan 28 15:53:02 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_21_1674917582-model.torch
01/28/2023 03:53:04 PM  [*] Masking sequences: iteration 22...
01/28/2023 03:53:25 PM  [*] Started epoch: 22
01/28/2023 03:53:26 PM  [*] Sat Jan 28 15:53:26 2023: Train Epoch: 22 [  0  /60900 (0 %)]	Loss: 195.500549 | Elapsed: 0.30s
01/28/2023 03:53:38 PM  [*] Sat Jan 28 15:53:38 2023: Train Epoch: 22 [6400 /60900 (11%)]	Loss: 189.322403 | Elapsed: 12.55s
01/28/2023 03:53:51 PM  [*] Sat Jan 28 15:53:51 2023: Train Epoch: 22 [12800/60900 (21%)]	Loss: 177.345398 | Elapsed: 12.47s
01/28/2023 03:54:03 PM  [*] Sat Jan 28 15:54:03 2023: Train Epoch: 22 [19200/60900 (32%)]	Loss: 165.546265 | Elapsed: 12.46s
01/28/2023 03:54:16 PM  [*] Sat Jan 28 15:54:16 2023: Train Epoch: 22 [25600/60900 (42%)]	Loss: 175.775925 | Elapsed: 12.51s
01/28/2023 03:54:28 PM  [*] Sat Jan 28 15:54:28 2023: Train Epoch: 22 [32000/60900 (53%)]	Loss: 161.066193 | Elapsed: 12.45s
01/28/2023 03:54:41 PM  [*] Sat Jan 28 15:54:41 2023: Train Epoch: 22 [38400/60900 (63%)]	Loss: 178.283768 | Elapsed: 12.48s
01/28/2023 03:54:53 PM  [*] Sat Jan 28 15:54:53 2023: Train Epoch: 22 [44800/60900 (74%)]	Loss: 170.450562 | Elapsed: 12.83s
01/28/2023 03:55:06 PM  [*] Sat Jan 28 15:55:06 2023: Train Epoch: 22 [51200/60900 (84%)]	Loss: 191.946304 | Elapsed: 12.76s
01/28/2023 03:55:19 PM  [*] Sat Jan 28 15:55:19 2023: Train Epoch: 22 [57600/60900 (95%)]	Loss: 197.827347 | Elapsed: 12.51s
01/28/2023 03:55:27 PM  [*] Sat Jan 28 15:55:27 2023:   22    | Tr.loss: 174.536300 | Elapsed:  121.59  s
01/28/2023 03:55:27 PM [!] Sat Jan 28 15:55:27 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_22_1674917727-model.torch
01/28/2023 03:55:29 PM  [*] Masking sequences: iteration 23...
01/28/2023 03:55:49 PM  [*] Started epoch: 23
01/28/2023 03:55:50 PM  [*] Sat Jan 28 15:55:50 2023: Train Epoch: 23 [  0  /60900 (0 %)]	Loss: 159.753601 | Elapsed: 0.45s
01/28/2023 03:56:02 PM  [*] Sat Jan 28 15:56:02 2023: Train Epoch: 23 [6400 /60900 (11%)]	Loss: 188.196182 | Elapsed: 12.51s
01/28/2023 03:56:15 PM  [*] Sat Jan 28 15:56:15 2023: Train Epoch: 23 [12800/60900 (21%)]	Loss: 189.855072 | Elapsed: 12.41s
01/28/2023 03:56:27 PM  [*] Sat Jan 28 15:56:27 2023: Train Epoch: 23 [19200/60900 (32%)]	Loss: 172.066956 | Elapsed: 12.45s
01/28/2023 03:56:40 PM  [*] Sat Jan 28 15:56:40 2023: Train Epoch: 23 [25600/60900 (42%)]	Loss: 174.613037 | Elapsed: 12.45s
01/28/2023 03:56:52 PM  [*] Sat Jan 28 15:56:52 2023: Train Epoch: 23 [32000/60900 (53%)]	Loss: 170.292923 | Elapsed: 12.54s
01/28/2023 03:57:05 PM  [*] Sat Jan 28 15:57:05 2023: Train Epoch: 23 [38400/60900 (63%)]	Loss: 161.787140 | Elapsed: 12.53s
01/28/2023 03:57:17 PM  [*] Sat Jan 28 15:57:17 2023: Train Epoch: 23 [44800/60900 (74%)]	Loss: 171.078720 | Elapsed: 12.52s
01/28/2023 03:57:30 PM  [*] Sat Jan 28 15:57:30 2023: Train Epoch: 23 [51200/60900 (84%)]	Loss: 167.700348 | Elapsed: 12.58s
01/28/2023 03:57:42 PM  [*] Sat Jan 28 15:57:42 2023: Train Epoch: 23 [57600/60900 (95%)]	Loss: 188.104187 | Elapsed: 12.50s
01/28/2023 03:57:50 PM  [*] Sat Jan 28 15:57:50 2023:   23    | Tr.loss: 174.807032 | Elapsed:  121.17  s
01/28/2023 03:57:51 PM [!] Sat Jan 28 15:57:51 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_23_1674917870-model.torch
01/28/2023 03:57:52 PM  [*] Masking sequences: iteration 24...
01/28/2023 03:58:13 PM  [*] Started epoch: 24
01/28/2023 03:58:13 PM  [*] Sat Jan 28 15:58:13 2023: Train Epoch: 24 [  0  /60900 (0 %)]	Loss: 174.710587 | Elapsed: 0.36s
01/28/2023 03:58:25 PM  [*] Sat Jan 28 15:58:25 2023: Train Epoch: 24 [6400 /60900 (11%)]	Loss: 166.703171 | Elapsed: 12.51s
01/28/2023 03:58:38 PM  [*] Sat Jan 28 15:58:38 2023: Train Epoch: 24 [12800/60900 (21%)]	Loss: 163.376083 | Elapsed: 12.57s
01/28/2023 03:58:50 PM  [*] Sat Jan 28 15:58:50 2023: Train Epoch: 24 [19200/60900 (32%)]	Loss: 172.966522 | Elapsed: 12.44s
01/28/2023 03:59:03 PM  [*] Sat Jan 28 15:59:03 2023: Train Epoch: 24 [25600/60900 (42%)]	Loss: 155.895157 | Elapsed: 12.51s
01/28/2023 03:59:16 PM  [*] Sat Jan 28 15:59:16 2023: Train Epoch: 24 [32000/60900 (53%)]	Loss: 181.739105 | Elapsed: 12.53s
01/28/2023 03:59:28 PM  [*] Sat Jan 28 15:59:28 2023: Train Epoch: 24 [38400/60900 (63%)]	Loss: 188.447327 | Elapsed: 12.51s
01/28/2023 03:59:41 PM  [*] Sat Jan 28 15:59:41 2023: Train Epoch: 24 [44800/60900 (74%)]	Loss: 143.214600 | Elapsed: 12.55s
01/28/2023 03:59:53 PM  [*] Sat Jan 28 15:59:53 2023: Train Epoch: 24 [51200/60900 (84%)]	Loss: 177.553116 | Elapsed: 12.57s
01/28/2023 04:00:06 PM  [*] Sat Jan 28 16:00:06 2023: Train Epoch: 24 [57600/60900 (95%)]	Loss: 195.300156 | Elapsed: 12.60s
01/28/2023 04:00:14 PM  [*] Sat Jan 28 16:00:14 2023:   24    | Tr.loss: 174.696593 | Elapsed:  121.27  s
01/28/2023 04:00:14 PM [!] Sat Jan 28 16:00:14 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_24_1674918014-model.torch
01/28/2023 04:00:15 PM  [*] Masking sequences: iteration 25...
01/28/2023 04:00:36 PM  [*] Started epoch: 25
01/28/2023 04:00:37 PM  [*] Sat Jan 28 16:00:37 2023: Train Epoch: 25 [  0  /60900 (0 %)]	Loss: 184.016235 | Elapsed: 0.34s
01/28/2023 04:00:49 PM  [*] Sat Jan 28 16:00:49 2023: Train Epoch: 25 [6400 /60900 (11%)]	Loss: 177.687836 | Elapsed: 12.47s
01/28/2023 04:01:02 PM  [*] Sat Jan 28 16:01:02 2023: Train Epoch: 25 [12800/60900 (21%)]	Loss: 193.682465 | Elapsed: 12.42s
01/28/2023 04:01:14 PM  [*] Sat Jan 28 16:01:14 2023: Train Epoch: 25 [19200/60900 (32%)]	Loss: 176.179321 | Elapsed: 12.55s
01/28/2023 04:01:27 PM  [*] Sat Jan 28 16:01:27 2023: Train Epoch: 25 [25600/60900 (42%)]	Loss: 161.453461 | Elapsed: 12.50s
01/28/2023 04:01:39 PM  [*] Sat Jan 28 16:01:39 2023: Train Epoch: 25 [32000/60900 (53%)]	Loss: 164.971741 | Elapsed: 12.60s
01/28/2023 04:01:52 PM  [*] Sat Jan 28 16:01:52 2023: Train Epoch: 25 [38400/60900 (63%)]	Loss: 157.046600 | Elapsed: 12.94s
01/28/2023 04:02:05 PM  [*] Sat Jan 28 16:02:05 2023: Train Epoch: 25 [44800/60900 (74%)]	Loss: 179.993576 | Elapsed: 12.81s
01/28/2023 04:02:18 PM  [*] Sat Jan 28 16:02:18 2023: Train Epoch: 25 [51200/60900 (84%)]	Loss: 191.241959 | Elapsed: 12.68s
01/28/2023 04:02:30 PM  [*] Sat Jan 28 16:02:30 2023: Train Epoch: 25 [57600/60900 (95%)]	Loss: 177.442642 | Elapsed: 12.55s
01/28/2023 04:02:39 PM  [*] Sat Jan 28 16:02:39 2023:   25    | Tr.loss: 174.690956 | Elapsed:  122.10  s
01/28/2023 04:02:39 PM [!] Sat Jan 28 16:02:39 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_25_1674918159-model.torch
01/28/2023 04:02:40 PM  [*] Masking sequences: iteration 26...
01/28/2023 04:03:01 PM  [*] Started epoch: 26
01/28/2023 04:03:01 PM  [*] Sat Jan 28 16:03:01 2023: Train Epoch: 26 [  0  /60900 (0 %)]	Loss: 163.215225 | Elapsed: 0.33s
01/28/2023 04:03:14 PM  [*] Sat Jan 28 16:03:14 2023: Train Epoch: 26 [6400 /60900 (11%)]	Loss: 177.024628 | Elapsed: 12.53s
01/28/2023 04:03:26 PM  [*] Sat Jan 28 16:03:26 2023: Train Epoch: 26 [12800/60900 (21%)]	Loss: 187.180649 | Elapsed: 12.50s
01/28/2023 04:03:39 PM  [*] Sat Jan 28 16:03:39 2023: Train Epoch: 26 [19200/60900 (32%)]	Loss: 164.995346 | Elapsed: 12.46s
01/28/2023 04:03:51 PM  [*] Sat Jan 28 16:03:51 2023: Train Epoch: 26 [25600/60900 (42%)]	Loss: 161.163300 | Elapsed: 12.51s
01/28/2023 04:04:04 PM  [*] Sat Jan 28 16:04:04 2023: Train Epoch: 26 [32000/60900 (53%)]	Loss: 164.017654 | Elapsed: 12.49s
01/28/2023 04:04:16 PM  [*] Sat Jan 28 16:04:16 2023: Train Epoch: 26 [38400/60900 (63%)]	Loss: 187.064499 | Elapsed: 12.58s
01/28/2023 04:04:29 PM  [*] Sat Jan 28 16:04:29 2023: Train Epoch: 26 [44800/60900 (74%)]	Loss: 188.501236 | Elapsed: 12.58s
01/28/2023 04:04:41 PM  [*] Sat Jan 28 16:04:41 2023: Train Epoch: 26 [51200/60900 (84%)]	Loss: 187.797089 | Elapsed: 12.63s
01/28/2023 04:04:54 PM  [*] Sat Jan 28 16:04:54 2023: Train Epoch: 26 [57600/60900 (95%)]	Loss: 165.262787 | Elapsed: 12.48s
01/28/2023 04:05:02 PM  [*] Sat Jan 28 16:05:02 2023:   26    | Tr.loss: 174.663881 | Elapsed:  121.23  s
01/28/2023 04:05:02 PM [!] Sat Jan 28 16:05:02 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_26_1674918302-model.torch
01/28/2023 04:05:04 PM  [*] Masking sequences: iteration 27...
01/28/2023 04:05:24 PM  [*] Started epoch: 27
01/28/2023 04:05:25 PM  [*] Sat Jan 28 16:05:25 2023: Train Epoch: 27 [  0  /60900 (0 %)]	Loss: 161.024231 | Elapsed: 0.42s
01/28/2023 04:05:37 PM  [*] Sat Jan 28 16:05:37 2023: Train Epoch: 27 [6400 /60900 (11%)]	Loss: 160.779037 | Elapsed: 12.38s
01/28/2023 04:05:49 PM  [*] Sat Jan 28 16:05:49 2023: Train Epoch: 27 [12800/60900 (21%)]	Loss: 161.630981 | Elapsed: 12.37s
01/28/2023 04:06:02 PM  [*] Sat Jan 28 16:06:02 2023: Train Epoch: 27 [19200/60900 (32%)]	Loss: 153.352905 | Elapsed: 12.40s
01/28/2023 04:06:14 PM  [*] Sat Jan 28 16:06:14 2023: Train Epoch: 27 [25600/60900 (42%)]	Loss: 169.083084 | Elapsed: 12.34s
01/28/2023 04:06:26 PM  [*] Sat Jan 28 16:06:26 2023: Train Epoch: 27 [32000/60900 (53%)]	Loss: 164.569382 | Elapsed: 12.41s
01/28/2023 04:06:39 PM  [*] Sat Jan 28 16:06:39 2023: Train Epoch: 27 [38400/60900 (63%)]	Loss: 177.249695 | Elapsed: 12.45s
01/28/2023 04:06:51 PM  [*] Sat Jan 28 16:06:51 2023: Train Epoch: 27 [44800/60900 (74%)]	Loss: 166.950806 | Elapsed: 12.44s
01/28/2023 04:07:04 PM  [*] Sat Jan 28 16:07:04 2023: Train Epoch: 27 [51200/60900 (84%)]	Loss: 184.301590 | Elapsed: 12.47s
01/28/2023 04:07:16 PM  [*] Sat Jan 28 16:07:16 2023: Train Epoch: 27 [57600/60900 (95%)]	Loss: 170.321625 | Elapsed: 12.59s
01/28/2023 04:07:24 PM  [*] Sat Jan 28 16:07:24 2023:   27    | Tr.loss: 174.531204 | Elapsed:  120.36  s
01/28/2023 04:07:25 PM [!] Sat Jan 28 16:07:25 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_27_1674918444-model.torch
01/28/2023 04:07:26 PM  [*] Masking sequences: iteration 28...
01/28/2023 04:07:47 PM  [*] Started epoch: 28
01/28/2023 04:07:47 PM  [*] Sat Jan 28 16:07:47 2023: Train Epoch: 28 [  0  /60900 (0 %)]	Loss: 173.265106 | Elapsed: 0.37s
01/28/2023 04:08:00 PM  [*] Sat Jan 28 16:08:00 2023: Train Epoch: 28 [6400 /60900 (11%)]	Loss: 169.032990 | Elapsed: 13.19s
01/28/2023 04:08:13 PM  [*] Sat Jan 28 16:08:13 2023: Train Epoch: 28 [12800/60900 (21%)]	Loss: 169.471344 | Elapsed: 12.35s
01/28/2023 04:08:25 PM  [*] Sat Jan 28 16:08:25 2023: Train Epoch: 28 [19200/60900 (32%)]	Loss: 183.728577 | Elapsed: 12.81s
01/28/2023 04:08:38 PM  [*] Sat Jan 28 16:08:38 2023: Train Epoch: 28 [25600/60900 (42%)]	Loss: 169.226105 | Elapsed: 12.69s
01/28/2023 04:08:51 PM  [*] Sat Jan 28 16:08:51 2023: Train Epoch: 28 [32000/60900 (53%)]	Loss: 176.469772 | Elapsed: 12.75s
01/28/2023 04:09:04 PM  [*] Sat Jan 28 16:09:04 2023: Train Epoch: 28 [38400/60900 (63%)]	Loss: 167.427948 | Elapsed: 12.69s
01/28/2023 04:09:16 PM  [*] Sat Jan 28 16:09:16 2023: Train Epoch: 28 [44800/60900 (74%)]	Loss: 183.594330 | Elapsed: 12.57s
01/28/2023 04:09:29 PM  [*] Sat Jan 28 16:09:29 2023: Train Epoch: 28 [51200/60900 (84%)]	Loss: 159.205215 | Elapsed: 12.55s
01/28/2023 04:09:41 PM  [*] Sat Jan 28 16:09:41 2023: Train Epoch: 28 [57600/60900 (95%)]	Loss: 188.990448 | Elapsed: 12.63s
01/28/2023 04:09:50 PM  [*] Sat Jan 28 16:09:50 2023:   28    | Tr.loss: 174.670128 | Elapsed:  122.98  s
01/28/2023 04:09:50 PM [!] Sat Jan 28 16:09:50 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_28_1674918590-model.torch
01/28/2023 04:09:51 PM  [*] Masking sequences: iteration 29...
01/28/2023 04:10:13 PM  [*] Started epoch: 29
01/28/2023 04:10:13 PM  [*] Sat Jan 28 16:10:13 2023: Train Epoch: 29 [  0  /60900 (0 %)]	Loss: 165.370331 | Elapsed: 0.45s
01/28/2023 04:10:26 PM  [*] Sat Jan 28 16:10:26 2023: Train Epoch: 29 [6400 /60900 (11%)]	Loss: 183.497803 | Elapsed: 12.46s
01/28/2023 04:10:38 PM  [*] Sat Jan 28 16:10:38 2023: Train Epoch: 29 [12800/60900 (21%)]	Loss: 177.165802 | Elapsed: 12.26s
01/28/2023 04:10:51 PM  [*] Sat Jan 28 16:10:51 2023: Train Epoch: 29 [19200/60900 (32%)]	Loss: 183.272980 | Elapsed: 12.52s
01/28/2023 04:11:03 PM  [*] Sat Jan 28 16:11:03 2023: Train Epoch: 29 [25600/60900 (42%)]	Loss: 181.041153 | Elapsed: 12.75s
01/28/2023 04:11:16 PM  [*] Sat Jan 28 16:11:16 2023: Train Epoch: 29 [32000/60900 (53%)]	Loss: 163.001877 | Elapsed: 12.53s
01/28/2023 04:11:28 PM  [*] Sat Jan 28 16:11:28 2023: Train Epoch: 29 [38400/60900 (63%)]	Loss: 175.248260 | Elapsed: 12.36s
01/28/2023 04:11:41 PM  [*] Sat Jan 28 16:11:41 2023: Train Epoch: 29 [44800/60900 (74%)]	Loss: 158.996155 | Elapsed: 12.86s
01/28/2023 04:11:54 PM  [*] Sat Jan 28 16:11:54 2023: Train Epoch: 29 [51200/60900 (84%)]	Loss: 169.060730 | Elapsed: 12.37s
01/28/2023 04:12:07 PM  [*] Sat Jan 28 16:12:07 2023: Train Epoch: 29 [57600/60900 (95%)]	Loss: 159.720947 | Elapsed: 12.97s
01/28/2023 04:12:15 PM  [*] Sat Jan 28 16:12:15 2023:   29    | Tr.loss: 174.727940 | Elapsed:  122.11  s
01/28/2023 04:12:16 PM [!] Sat Jan 28 16:12:16 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_29_1674918735-model.torch
01/28/2023 04:12:17 PM  [*] Masking sequences: iteration 30...
01/28/2023 04:12:38 PM  [*] Started epoch: 30
01/28/2023 04:12:38 PM  [*] Sat Jan 28 16:12:38 2023: Train Epoch: 30 [  0  /60900 (0 %)]	Loss: 158.717346 | Elapsed: 0.44s
01/28/2023 04:12:51 PM  [*] Sat Jan 28 16:12:51 2023: Train Epoch: 30 [6400 /60900 (11%)]	Loss: 169.526352 | Elapsed: 12.70s
01/28/2023 04:13:04 PM  [*] Sat Jan 28 16:13:04 2023: Train Epoch: 30 [12800/60900 (21%)]	Loss: 169.121918 | Elapsed: 12.60s
01/28/2023 04:13:16 PM  [*] Sat Jan 28 16:13:16 2023: Train Epoch: 30 [19200/60900 (32%)]	Loss: 175.111450 | Elapsed: 12.60s
01/28/2023 04:13:29 PM  [*] Sat Jan 28 16:13:29 2023: Train Epoch: 30 [25600/60900 (42%)]	Loss: 187.108765 | Elapsed: 12.55s
01/28/2023 04:13:42 PM  [*] Sat Jan 28 16:13:42 2023: Train Epoch: 30 [32000/60900 (53%)]	Loss: 196.735031 | Elapsed: 12.75s
01/28/2023 04:13:54 PM  [*] Sat Jan 28 16:13:54 2023: Train Epoch: 30 [38400/60900 (63%)]	Loss: 167.597580 | Elapsed: 12.47s
01/28/2023 04:14:07 PM  [*] Sat Jan 28 16:14:07 2023: Train Epoch: 30 [44800/60900 (74%)]	Loss: 181.130402 | Elapsed: 12.63s
01/28/2023 04:14:19 PM  [*] Sat Jan 28 16:14:19 2023: Train Epoch: 30 [51200/60900 (84%)]	Loss: 186.707275 | Elapsed: 12.61s
01/28/2023 04:14:32 PM  [*] Sat Jan 28 16:14:32 2023: Train Epoch: 30 [57600/60900 (95%)]	Loss: 162.617432 | Elapsed: 12.65s
01/28/2023 04:14:40 PM  [*] Sat Jan 28 16:14:40 2023:   30    | Tr.loss: 174.802576 | Elapsed:  122.38  s
01/28/2023 04:14:41 PM [!] Sat Jan 28 16:14:41 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_30_1674918880-model.torch
01/28/2023 04:14:41 PM [!] Sat Jan 28 16:14:41 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\1674918881-model.torch
		train time: 1674918881-trainTime.npy
		train losses: 1674918881-trainLosses.npy
		train AUC: 1674918881-auc.npy
01/28/2023 04:14:44 PM  [!] Training pretrained model on downstream task...
01/28/2023 04:14:44 PM  [*] Started epoch: 1
01/28/2023 04:14:44 PM  [*] Sat Jan 28 16:14:44 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.299557 | Elapsed: 0.29s | FPR 0.0003 -> TPR 0.1163 & F1 0.2083 | AUC 0.7475
01/28/2023 04:14:53 PM  [*] Sat Jan 28 16:14:53 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.393430 | Elapsed: 9.12s | FPR 0.0003 -> TPR 0.4918 & F1 0.6593 | AUC 0.8798
01/28/2023 04:15:02 PM  [*] Sat Jan 28 16:15:02 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.399319 | Elapsed: 9.12s | FPR 0.0003 -> TPR 0.4769 & F1 0.6458 | AUC 0.9075
01/28/2023 04:15:06 PM  [*] Sat Jan 28 16:15:06 2023:    1    | Tr.loss: 0.464952 | Elapsed:   22.29  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8437
01/28/2023 04:15:06 PM  [*] Started epoch: 2
01/28/2023 04:15:06 PM  [*] Sat Jan 28 16:15:06 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.326020 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.7209 & F1 0.8378 | AUC 0.9158
01/28/2023 04:15:15 PM  [*] Sat Jan 28 16:15:15 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.276025 | Elapsed: 9.11s | FPR 0.0003 -> TPR 0.4828 & F1 0.6512 | AUC 0.9253
01/28/2023 04:15:24 PM  [*] Sat Jan 28 16:15:24 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.216612 | Elapsed: 9.24s | FPR 0.0003 -> TPR 0.8824 & F1 0.9375 | AUC 0.9724
01/28/2023 04:15:28 PM  [*] Sat Jan 28 16:15:28 2023:    2    | Tr.loss: 0.306504 | Elapsed:   22.16  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.39 | AUC: 0.9286
01/28/2023 04:15:28 PM  [*] Started epoch: 3
01/28/2023 04:15:28 PM  [*] Sat Jan 28 16:15:28 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.311032 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.8158 & F1 0.8986 | AUC 0.9383
01/28/2023 04:15:37 PM  [*] Sat Jan 28 16:15:37 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.161770 | Elapsed: 9.17s | FPR 0.0003 -> TPR 0.9028 & F1 0.9489 | AUC 0.9802
01/28/2023 04:15:46 PM  [*] Sat Jan 28 16:15:46 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.233048 | Elapsed: 9.15s | FPR 0.0003 -> TPR 0.7703 & F1 0.8702 | AUC 0.9725
01/28/2023 04:15:50 PM  [*] Sat Jan 28 16:15:50 2023:    3    | Tr.loss: 0.227829 | Elapsed:   22.19  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9623
01/28/2023 04:15:51 PM [!] Sat Jan 28 16:15:51 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\downstreamTask_pretrained\training_files\1674918950-model.torch
		train time: 1674918950-trainTime.npy
		train losses: 1674918950-trainLosses.npy
		train AUC: 1674918950-auc.npy
		train F1s : 1674918950-trainF1s.npy
		train TPRs: 1674918950-trainTPRs.npy
01/28/2023 04:15:51 PM  [!] Training non_pretrained model on downstream task...
01/28/2023 04:15:51 PM  [*] Started epoch: 1
01/28/2023 04:15:51 PM  [*] Sat Jan 28 16:15:51 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.257583 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0233 & F1 0.0455 | AUC 0.3677
01/28/2023 04:15:57 PM  [*] Sat Jan 28 16:15:57 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.428573 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.2714 & F1 0.4270 | AUC 0.8433
01/28/2023 04:16:04 PM  [*] Sat Jan 28 16:16:04 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.296631 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.6471 & F1 0.7857 | AUC 0.9141
01/28/2023 04:16:07 PM  [*] Sat Jan 28 16:16:07 2023:    1    | Tr.loss: 0.470284 | Elapsed:   15.39  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8289
01/28/2023 04:16:07 PM  [*] Started epoch: 2
01/28/2023 04:16:07 PM  [*] Sat Jan 28 16:16:07 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.352462 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6818 & F1 0.8108 | AUC 0.9295
01/28/2023 04:16:13 PM  [*] Sat Jan 28 16:16:13 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.331292 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.3284 & F1 0.4944 | AUC 0.9294
01/28/2023 04:16:19 PM  [*] Sat Jan 28 16:16:19 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.235044 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.6027 & F1 0.7521 | AUC 0.9447
01/28/2023 04:16:22 PM  [*] Sat Jan 28 16:16:22 2023:    2    | Tr.loss: 0.302952 | Elapsed:   15.28  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.38 | AUC: 0.9309
01/28/2023 04:16:22 PM  [*] Started epoch: 3
01/28/2023 04:16:22 PM  [*] Sat Jan 28 16:16:22 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.422965 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5128 & F1 0.6780 | AUC 0.8903
01/28/2023 04:16:28 PM  [*] Sat Jan 28 16:16:28 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.183095 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.6923 & F1 0.8182 | AUC 0.9639
01/28/2023 04:16:35 PM  [*] Sat Jan 28 16:16:35 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.183346 | Elapsed: 6.38s | FPR 0.0003 -> TPR 0.9697 & F1 0.9846 | AUC 0.9915
01/28/2023 04:16:37 PM  [*] Sat Jan 28 16:16:37 2023:    3    | Tr.loss: 0.238769 | Elapsed:   15.47  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.39 | AUC: 0.9591
01/28/2023 04:16:38 PM [!] Sat Jan 28 16:16:38 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\downstreamTask_non_pretrained\training_files\1674918997-model.torch
		train time: 1674918997-trainTime.npy
		train losses: 1674918997-trainLosses.npy
		train AUC: 1674918997-auc.npy
		train F1s : 1674918997-trainF1s.npy
		train TPRs: 1674918997-trainTPRs.npy
01/28/2023 04:16:38 PM  [!] Training full_data model on downstream task...
01/28/2023 04:16:38 PM  [*] Started epoch: 1
01/28/2023 04:16:38 PM  [*] Sat Jan 28 16:16:38 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 1.666725 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0976 & F1 0.1778 | AUC 0.5058
01/28/2023 04:16:45 PM  [*] Sat Jan 28 16:16:45 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.372075 | Elapsed: 6.32s | FPR 0.0003 -> TPR 0.2329 & F1 0.3778 | AUC 0.8153
01/28/2023 04:16:51 PM  [*] Sat Jan 28 16:16:51 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.277840 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.2778 & F1 0.4348 | AUC 0.8760
01/28/2023 04:16:57 PM  [*] Sat Jan 28 16:16:57 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.266848 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.4923 & F1 0.6598 | AUC 0.9310
01/28/2023 04:17:04 PM  [*] Sat Jan 28 16:17:04 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.243175 | Elapsed: 6.45s | FPR 0.0003 -> TPR 0.5493 & F1 0.7091 | AUC 0.9407
01/28/2023 04:17:10 PM  [*] Sat Jan 28 16:17:10 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.448573 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.3968 & F1 0.5682 | AUC 0.9185
01/28/2023 04:17:17 PM  [*] Sat Jan 28 16:17:17 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.240487 | Elapsed: 6.38s | FPR 0.0003 -> TPR 0.6232 & F1 0.7679 | AUC 0.9570
01/28/2023 04:17:23 PM  [*] Sat Jan 28 16:17:23 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.247228 | Elapsed: 6.33s | FPR 0.0003 -> TPR 0.7971 & F1 0.8871 | AUC 0.9635
01/28/2023 04:17:29 PM  [*] Sat Jan 28 16:17:29 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.210787 | Elapsed: 6.32s | FPR 0.0003 -> TPR 0.6761 & F1 0.8067 | AUC 0.9655
01/28/2023 04:17:36 PM  [*] Sat Jan 28 16:17:36 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.193365 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.8676 & F1 0.9291 | AUC 0.9706
01/28/2023 04:17:42 PM [!] Learning rate: 2.5e-05
01/28/2023 04:17:42 PM  [*] Sat Jan 28 16:17:42 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.163904 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.7018 & F1 0.8247 | AUC 0.9759
01/28/2023 04:17:48 PM  [*] Sat Jan 28 16:17:48 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.255681 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.6212 & F1 0.7664 | AUC 0.9621
01/28/2023 04:17:56 PM  [*] Sat Jan 28 16:17:56 2023:    1    | Tr.loss: 0.295646 | Elapsed:   77.36  s | FPR 0.0003 -> TPR: 0.10 & F1: 0.17 | AUC: 0.9361
01/28/2023 04:17:56 PM  [*] Started epoch: 2
01/28/2023 04:17:56 PM  [*] Sat Jan 28 16:17:56 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.169839 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9286 & F1 0.9630 | AUC 0.9848
01/28/2023 04:18:02 PM  [*] Sat Jan 28 16:18:02 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.192890 | Elapsed: 6.35s | FPR 0.0003 -> TPR 0.8571 & F1 0.9231 | AUC 0.9769
01/28/2023 04:18:08 PM  [*] Sat Jan 28 16:18:08 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.153214 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.8551 & F1 0.9219 | AUC 0.9799
01/28/2023 04:18:15 PM  [*] Sat Jan 28 16:18:15 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.204069 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.8947 & F1 0.9444 | AUC 0.9759
01/28/2023 04:18:21 PM  [*] Sat Jan 28 16:18:21 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.261268 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.7838 & F1 0.8788 | AUC 0.9678
01/28/2023 04:18:27 PM  [*] Sat Jan 28 16:18:27 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.155383 | Elapsed: 6.35s | FPR 0.0003 -> TPR 0.8000 & F1 0.8889 | AUC 0.9838
01/28/2023 04:18:34 PM  [*] Sat Jan 28 16:18:34 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.124339 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.8095 & F1 0.8947 | AUC 0.9923
01/28/2023 04:18:40 PM  [*] Sat Jan 28 16:18:40 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.099596 | Elapsed: 6.33s | FPR 0.0003 -> TPR 0.9481 & F1 0.9733 | AUC 0.9944
01/28/2023 04:18:46 PM  [*] Sat Jan 28 16:18:46 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.130342 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.9286 & F1 0.9630 | AUC 0.9943
01/28/2023 04:18:47 PM [!] Learning rate: 2.5e-06
01/28/2023 04:18:53 PM  [*] Sat Jan 28 16:18:53 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.201045 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.8676 & F1 0.9291 | AUC 0.9793
01/28/2023 04:18:59 PM  [*] Sat Jan 28 16:18:59 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.197783 | Elapsed: 6.35s | FPR 0.0003 -> TPR 0.5588 & F1 0.7170 | AUC 0.9632
01/28/2023 04:19:05 PM  [*] Sat Jan 28 16:19:05 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.119793 | Elapsed: 6.33s | FPR 0.0003 -> TPR 0.9000 & F1 0.9474 | AUC 0.9929
01/28/2023 04:19:13 PM  [*] Sat Jan 28 16:19:13 2023:    2    | Tr.loss: 0.164615 | Elapsed:   77.29  s | FPR 0.0003 -> TPR: 0.45 & F1: 0.62 | AUC: 0.9811
01/28/2023 04:19:13 PM  [*] Started epoch: 3
01/28/2023 04:19:13 PM  [*] Sat Jan 28 16:19:13 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.159760 | Elapsed: 0.14s | FPR 0.0003 -> TPR 0.9348 & F1 0.9663 | AUC 0.9891
01/28/2023 04:19:19 PM  [*] Sat Jan 28 16:19:19 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.148013 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.9333 & F1 0.9655 | AUC 0.9888
01/28/2023 04:19:26 PM  [*] Sat Jan 28 16:19:26 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.217616 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.8649 & F1 0.9275 | AUC 0.9766
01/28/2023 04:19:32 PM  [*] Sat Jan 28 16:19:32 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.318985 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.6418 & F1 0.7818 | AUC 0.9629
01/28/2023 04:19:38 PM  [*] Sat Jan 28 16:19:38 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.171995 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.8406 & F1 0.9134 | AUC 0.9827
01/28/2023 04:19:45 PM  [*] Sat Jan 28 16:19:45 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.101004 | Elapsed: 6.32s | FPR 0.0003 -> TPR 0.8657 & F1 0.9280 | AUC 0.9923
01/28/2023 04:19:51 PM  [*] Sat Jan 28 16:19:51 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.280513 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.6164 & F1 0.7627 | AUC 0.9477
01/28/2023 04:19:52 PM [!] Learning rate: 2.5000000000000004e-07
01/28/2023 04:19:57 PM  [*] Sat Jan 28 16:19:57 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.155821 | Elapsed: 6.39s | FPR 0.0003 -> TPR 0.8308 & F1 0.9076 | AUC 0.9868
01/28/2023 04:20:04 PM  [*] Sat Jan 28 16:20:04 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.077768 | Elapsed: 6.51s | FPR 0.0003 -> TPR 0.9167 & F1 0.9565 | AUC 0.9940
01/28/2023 04:20:10 PM  [*] Sat Jan 28 16:20:10 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.155541 | Elapsed: 6.32s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091 | AUC 0.9841
01/28/2023 04:20:17 PM  [*] Sat Jan 28 16:20:17 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.113813 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.9016 & F1 0.9483 | AUC 0.9950
01/28/2023 04:20:23 PM  [*] Sat Jan 28 16:20:23 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.136973 | Elapsed: 6.41s | FPR 0.0003 -> TPR 0.7937 & F1 0.8850 | AUC 0.9876
01/28/2023 04:20:31 PM  [*] Sat Jan 28 16:20:31 2023:    3    | Tr.loss: 0.156321 | Elapsed:   77.64  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9830
01/28/2023 04:20:31 PM [!] Sat Jan 28 16:20:31 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\downstreamTask_full_data\training_files\1674919231-model.torch
		train time: 1674919231-trainTime.npy
		train losses: 1674919231-trainLosses.npy
		train AUC: 1674919231-auc.npy
		train F1s : 1674919231-trainF1s.npy
		train TPRs: 1674919231-trainTPRs.npy
01/28/2023 04:20:31 PM  [*] Evaluating pretrained model on test set...
01/28/2023 04:20:36 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0710 | F1: 0.1327
01/28/2023 04:20:36 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1283 | F1: 0.2273
01/28/2023 04:20:36 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2558 | F1: 0.4071
01/28/2023 04:20:36 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3862 | F1: 0.5561
01/28/2023 04:20:36 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4323 | F1: 0.6001
01/28/2023 04:20:36 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5036 | F1: 0.6586
01/28/2023 04:20:36 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7962 | F1: 0.8465
01/28/2023 04:20:36 PM  [*] Evaluating non_pretrained model on test set...
01/28/2023 04:20:41 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0706 | F1: 0.1319
01/28/2023 04:20:41 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1568 | F1: 0.2710
01/28/2023 04:20:41 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2601 | F1: 0.4126
01/28/2023 04:20:41 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.2957 | F1: 0.4555
01/28/2023 04:20:41 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.3498 | F1: 0.5150
01/28/2023 04:20:41 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.4639 | F1: 0.6230
01/28/2023 04:20:41 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7667 | F1: 0.8280
01/28/2023 04:20:41 PM  [*] Evaluating full_data model on test set...
01/28/2023 04:20:46 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0437 | F1: 0.0838
01/28/2023 04:20:46 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2329 | F1: 0.3777
01/28/2023 04:20:46 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3048 | F1: 0.4669
01/28/2023 04:20:46 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.4042 | F1: 0.5747
01/28/2023 04:20:46 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4506 | F1: 0.6176
01/28/2023 04:20:46 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5566 | F1: 0.7036
01/28/2023 04:20:46 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8009 | F1: 0.8494
01/28/2023 04:20:46 PM  [!] Running pre-training split 3/3
01/28/2023 04:20:49 PM  [!] Pre-training model...
01/28/2023 04:20:50 PM  [*] Masking sequences: iteration 1...
01/28/2023 04:21:09 PM  [*] Started epoch: 1
01/28/2023 04:21:10 PM  [*] Sat Jan 28 16:21:10 2023: Train Epoch: 1 [  0  /60900 (0 %)]	Loss: 453.740204 | Elapsed: 0.42s
01/28/2023 04:21:22 PM  [*] Sat Jan 28 16:21:22 2023: Train Epoch: 1 [6400 /60900 (11%)]	Loss: 246.683853 | Elapsed: 12.51s
01/28/2023 04:21:35 PM  [*] Sat Jan 28 16:21:35 2023: Train Epoch: 1 [12800/60900 (21%)]	Loss: 204.222626 | Elapsed: 12.45s
01/28/2023 04:21:47 PM  [*] Sat Jan 28 16:21:47 2023: Train Epoch: 1 [19200/60900 (32%)]	Loss: 208.355286 | Elapsed: 12.70s
01/28/2023 04:22:00 PM  [*] Sat Jan 28 16:22:00 2023: Train Epoch: 1 [25600/60900 (42%)]	Loss: 202.085220 | Elapsed: 12.62s
01/28/2023 04:22:12 PM  [*] Sat Jan 28 16:22:12 2023: Train Epoch: 1 [32000/60900 (53%)]	Loss: 183.338104 | Elapsed: 12.28s
01/28/2023 04:22:25 PM  [*] Sat Jan 28 16:22:25 2023: Train Epoch: 1 [38400/60900 (63%)]	Loss: 173.136169 | Elapsed: 12.37s
01/28/2023 04:22:37 PM  [*] Sat Jan 28 16:22:37 2023: Train Epoch: 1 [44800/60900 (74%)]	Loss: 207.227188 | Elapsed: 12.70s
01/28/2023 04:22:50 PM  [*] Sat Jan 28 16:22:50 2023: Train Epoch: 1 [51200/60900 (84%)]	Loss: 183.960358 | Elapsed: 12.66s
01/28/2023 04:23:03 PM  [*] Sat Jan 28 16:23:03 2023: Train Epoch: 1 [57600/60900 (95%)]	Loss: 188.642532 | Elapsed: 12.77s
01/28/2023 04:23:11 PM  [*] Sat Jan 28 16:23:11 2023:    1    | Tr.loss: 207.121613 | Elapsed:  121.82  s
01/28/2023 04:23:11 PM [!] Sat Jan 28 16:23:11 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_1_1674919391-model.torch
01/28/2023 04:23:13 PM  [*] Masking sequences: iteration 2...
01/28/2023 04:23:34 PM  [*] Started epoch: 2
01/28/2023 04:23:35 PM  [*] Sat Jan 28 16:23:35 2023: Train Epoch: 2 [  0  /60900 (0 %)]	Loss: 208.721802 | Elapsed: 0.83s
01/28/2023 04:23:47 PM  [*] Sat Jan 28 16:23:47 2023: Train Epoch: 2 [6400 /60900 (11%)]	Loss: 205.021805 | Elapsed: 12.53s
01/28/2023 04:24:00 PM  [*] Sat Jan 28 16:24:00 2023: Train Epoch: 2 [12800/60900 (21%)]	Loss: 194.121307 | Elapsed: 12.52s
01/28/2023 04:24:12 PM  [*] Sat Jan 28 16:24:12 2023: Train Epoch: 2 [19200/60900 (32%)]	Loss: 180.196899 | Elapsed: 12.43s
01/28/2023 04:24:25 PM  [*] Sat Jan 28 16:24:25 2023: Train Epoch: 2 [25600/60900 (42%)]	Loss: 193.923676 | Elapsed: 12.46s
01/28/2023 04:24:37 PM  [*] Sat Jan 28 16:24:37 2023: Train Epoch: 2 [32000/60900 (53%)]	Loss: 192.121445 | Elapsed: 12.54s
01/28/2023 04:24:50 PM  [*] Sat Jan 28 16:24:50 2023: Train Epoch: 2 [38400/60900 (63%)]	Loss: 193.967484 | Elapsed: 12.66s
01/28/2023 04:25:03 PM  [*] Sat Jan 28 16:25:03 2023: Train Epoch: 2 [44800/60900 (74%)]	Loss: 198.531723 | Elapsed: 12.52s
01/28/2023 04:25:15 PM  [*] Sat Jan 28 16:25:15 2023: Train Epoch: 2 [51200/60900 (84%)]	Loss: 164.693634 | Elapsed: 12.63s
01/28/2023 04:25:28 PM  [*] Sat Jan 28 16:25:28 2023: Train Epoch: 2 [57600/60900 (95%)]	Loss: 185.996841 | Elapsed: 12.52s
01/28/2023 04:25:36 PM  [*] Sat Jan 28 16:25:36 2023:    2    | Tr.loss: 185.207522 | Elapsed:  121.78  s
01/28/2023 04:25:36 PM [!] Sat Jan 28 16:25:36 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_2_1674919536-model.torch
01/28/2023 04:25:37 PM  [*] Masking sequences: iteration 3...
01/28/2023 04:25:58 PM  [*] Started epoch: 3
01/28/2023 04:25:58 PM  [*] Sat Jan 28 16:25:58 2023: Train Epoch: 3 [  0  /60900 (0 %)]	Loss: 168.472214 | Elapsed: 0.44s
01/28/2023 04:26:11 PM  [*] Sat Jan 28 16:26:11 2023: Train Epoch: 3 [6400 /60900 (11%)]	Loss: 197.003082 | Elapsed: 12.48s
01/28/2023 04:26:23 PM  [*] Sat Jan 28 16:26:23 2023: Train Epoch: 3 [12800/60900 (21%)]	Loss: 173.865204 | Elapsed: 12.40s
01/28/2023 04:26:36 PM  [*] Sat Jan 28 16:26:36 2023: Train Epoch: 3 [19200/60900 (32%)]	Loss: 155.658478 | Elapsed: 12.42s
01/28/2023 04:26:48 PM  [*] Sat Jan 28 16:26:48 2023: Train Epoch: 3 [25600/60900 (42%)]	Loss: 172.620758 | Elapsed: 12.58s
01/28/2023 04:27:01 PM  [*] Sat Jan 28 16:27:01 2023: Train Epoch: 3 [32000/60900 (53%)]	Loss: 181.539856 | Elapsed: 12.50s
01/28/2023 04:27:13 PM  [*] Sat Jan 28 16:27:13 2023: Train Epoch: 3 [38400/60900 (63%)]	Loss: 164.810684 | Elapsed: 12.50s
01/28/2023 04:27:26 PM  [*] Sat Jan 28 16:27:26 2023: Train Epoch: 3 [44800/60900 (74%)]	Loss: 184.877777 | Elapsed: 12.49s
01/28/2023 04:27:38 PM  [*] Sat Jan 28 16:27:38 2023: Train Epoch: 3 [51200/60900 (84%)]	Loss: 181.340836 | Elapsed: 12.50s
01/28/2023 04:27:51 PM  [*] Sat Jan 28 16:27:51 2023: Train Epoch: 3 [57600/60900 (95%)]	Loss: 186.136993 | Elapsed: 12.53s
01/28/2023 04:27:59 PM  [*] Sat Jan 28 16:27:59 2023:    3    | Tr.loss: 180.096349 | Elapsed:  120.99  s
01/28/2023 04:27:59 PM [!] Sat Jan 28 16:27:59 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_3_1674919679-model.torch
01/28/2023 04:28:01 PM  [*] Masking sequences: iteration 4...
01/28/2023 04:28:21 PM  [*] Started epoch: 4
01/28/2023 04:28:22 PM  [*] Sat Jan 28 16:28:22 2023: Train Epoch: 4 [  0  /60900 (0 %)]	Loss: 170.985489 | Elapsed: 0.44s
01/28/2023 04:28:34 PM  [*] Sat Jan 28 16:28:34 2023: Train Epoch: 4 [6400 /60900 (11%)]	Loss: 184.803101 | Elapsed: 12.49s
01/28/2023 04:28:47 PM  [*] Sat Jan 28 16:28:47 2023: Train Epoch: 4 [12800/60900 (21%)]	Loss: 186.504990 | Elapsed: 12.44s
01/28/2023 04:28:59 PM  [*] Sat Jan 28 16:28:59 2023: Train Epoch: 4 [19200/60900 (32%)]	Loss: 169.357239 | Elapsed: 12.48s
01/28/2023 04:29:12 PM  [*] Sat Jan 28 16:29:12 2023: Train Epoch: 4 [25600/60900 (42%)]	Loss: 165.330627 | Elapsed: 12.48s
01/28/2023 04:29:24 PM  [*] Sat Jan 28 16:29:24 2023: Train Epoch: 4 [32000/60900 (53%)]	Loss: 186.848053 | Elapsed: 12.48s
01/28/2023 04:29:37 PM  [*] Sat Jan 28 16:29:37 2023: Train Epoch: 4 [38400/60900 (63%)]	Loss: 185.827271 | Elapsed: 12.65s
01/28/2023 04:29:50 PM  [*] Sat Jan 28 16:29:50 2023: Train Epoch: 4 [44800/60900 (74%)]	Loss: 178.471832 | Elapsed: 12.62s
01/28/2023 04:30:02 PM  [*] Sat Jan 28 16:30:02 2023: Train Epoch: 4 [51200/60900 (84%)]	Loss: 183.962677 | Elapsed: 12.57s
01/28/2023 04:30:15 PM  [*] Sat Jan 28 16:30:15 2023: Train Epoch: 4 [57600/60900 (95%)]	Loss: 162.158340 | Elapsed: 12.51s
01/28/2023 04:30:23 PM  [*] Sat Jan 28 16:30:23 2023:    4    | Tr.loss: 177.342528 | Elapsed:  121.45  s
01/28/2023 04:30:23 PM [!] Sat Jan 28 16:30:23 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_4_1674919823-model.torch
01/28/2023 04:30:25 PM  [*] Masking sequences: iteration 5...
01/28/2023 04:30:46 PM  [*] Started epoch: 5
01/28/2023 04:30:46 PM  [*] Sat Jan 28 16:30:46 2023: Train Epoch: 5 [  0  /60900 (0 %)]	Loss: 167.861343 | Elapsed: 0.37s
01/28/2023 04:30:59 PM  [*] Sat Jan 28 16:30:59 2023: Train Epoch: 5 [6400 /60900 (11%)]	Loss: 166.126328 | Elapsed: 12.47s
01/28/2023 04:31:11 PM  [*] Sat Jan 28 16:31:11 2023: Train Epoch: 5 [12800/60900 (21%)]	Loss: 175.586304 | Elapsed: 12.47s
01/28/2023 04:31:24 PM  [*] Sat Jan 28 16:31:24 2023: Train Epoch: 5 [19200/60900 (32%)]	Loss: 146.898041 | Elapsed: 12.48s
01/28/2023 04:31:36 PM  [*] Sat Jan 28 16:31:36 2023: Train Epoch: 5 [25600/60900 (42%)]	Loss: 187.574219 | Elapsed: 12.54s
01/28/2023 04:31:49 PM  [*] Sat Jan 28 16:31:49 2023: Train Epoch: 5 [32000/60900 (53%)]	Loss: 174.325378 | Elapsed: 12.53s
01/28/2023 04:32:01 PM  [*] Sat Jan 28 16:32:01 2023: Train Epoch: 5 [38400/60900 (63%)]	Loss: 158.462128 | Elapsed: 12.47s
01/28/2023 04:32:14 PM  [*] Sat Jan 28 16:32:14 2023: Train Epoch: 5 [44800/60900 (74%)]	Loss: 175.618683 | Elapsed: 12.55s
01/28/2023 04:32:26 PM  [*] Sat Jan 28 16:32:26 2023: Train Epoch: 5 [51200/60900 (84%)]	Loss: 186.275955 | Elapsed: 12.44s
01/28/2023 04:32:39 PM  [*] Sat Jan 28 16:32:39 2023: Train Epoch: 5 [57600/60900 (95%)]	Loss: 190.485596 | Elapsed: 12.55s
01/28/2023 04:32:47 PM  [*] Sat Jan 28 16:32:47 2023:    5    | Tr.loss: 175.526309 | Elapsed:  121.08  s
01/28/2023 04:32:47 PM [!] Sat Jan 28 16:32:47 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_5_1674919967-model.torch
01/28/2023 04:32:48 PM  [*] Masking sequences: iteration 6...
01/28/2023 04:33:09 PM  [*] Started epoch: 6
01/28/2023 04:33:10 PM  [*] Sat Jan 28 16:33:10 2023: Train Epoch: 6 [  0  /60900 (0 %)]	Loss: 186.164093 | Elapsed: 0.44s
01/28/2023 04:33:22 PM  [*] Sat Jan 28 16:33:22 2023: Train Epoch: 6 [6400 /60900 (11%)]	Loss: 186.989548 | Elapsed: 12.46s
01/28/2023 04:33:35 PM  [*] Sat Jan 28 16:33:35 2023: Train Epoch: 6 [12800/60900 (21%)]	Loss: 172.595520 | Elapsed: 12.47s
01/28/2023 04:33:47 PM  [*] Sat Jan 28 16:33:47 2023: Train Epoch: 6 [19200/60900 (32%)]	Loss: 154.470825 | Elapsed: 12.44s
01/28/2023 04:34:00 PM  [*] Sat Jan 28 16:34:00 2023: Train Epoch: 6 [25600/60900 (42%)]	Loss: 184.092316 | Elapsed: 12.47s
01/28/2023 04:34:12 PM  [*] Sat Jan 28 16:34:12 2023: Train Epoch: 6 [32000/60900 (53%)]	Loss: 174.060974 | Elapsed: 12.47s
01/28/2023 04:34:24 PM  [*] Sat Jan 28 16:34:24 2023: Train Epoch: 6 [38400/60900 (63%)]	Loss: 167.975815 | Elapsed: 12.48s
01/28/2023 04:34:37 PM  [*] Sat Jan 28 16:34:37 2023: Train Epoch: 6 [44800/60900 (74%)]	Loss: 179.794388 | Elapsed: 12.40s
01/28/2023 04:34:49 PM  [*] Sat Jan 28 16:34:49 2023: Train Epoch: 6 [51200/60900 (84%)]	Loss: 174.378296 | Elapsed: 12.59s
01/28/2023 04:35:02 PM  [*] Sat Jan 28 16:35:02 2023: Train Epoch: 6 [57600/60900 (95%)]	Loss: 165.843811 | Elapsed: 12.48s
01/28/2023 04:35:10 PM  [*] Sat Jan 28 16:35:10 2023:    6    | Tr.loss: 174.454092 | Elapsed:  120.84  s
01/28/2023 04:35:11 PM [!] Sat Jan 28 16:35:11 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_6_1674920110-model.torch
01/28/2023 04:35:12 PM  [*] Masking sequences: iteration 7...
01/28/2023 04:35:33 PM  [*] Started epoch: 7
01/28/2023 04:35:33 PM  [*] Sat Jan 28 16:35:33 2023: Train Epoch: 7 [  0  /60900 (0 %)]	Loss: 152.199402 | Elapsed: 0.45s
01/28/2023 04:35:46 PM  [*] Sat Jan 28 16:35:46 2023: Train Epoch: 7 [6400 /60900 (11%)]	Loss: 177.586029 | Elapsed: 12.47s
01/28/2023 04:35:58 PM  [*] Sat Jan 28 16:35:58 2023: Train Epoch: 7 [12800/60900 (21%)]	Loss: 159.212158 | Elapsed: 12.43s
01/28/2023 04:36:11 PM  [*] Sat Jan 28 16:36:11 2023: Train Epoch: 7 [19200/60900 (32%)]	Loss: 181.403412 | Elapsed: 12.50s
01/28/2023 04:36:23 PM  [*] Sat Jan 28 16:36:23 2023: Train Epoch: 7 [25600/60900 (42%)]	Loss: 185.376755 | Elapsed: 12.46s
01/28/2023 04:36:36 PM  [*] Sat Jan 28 16:36:36 2023: Train Epoch: 7 [32000/60900 (53%)]	Loss: 185.882843 | Elapsed: 12.41s
01/28/2023 04:36:48 PM  [*] Sat Jan 28 16:36:48 2023: Train Epoch: 7 [38400/60900 (63%)]	Loss: 166.748199 | Elapsed: 12.56s
01/28/2023 04:37:01 PM  [*] Sat Jan 28 16:37:01 2023: Train Epoch: 7 [44800/60900 (74%)]	Loss: 173.627518 | Elapsed: 12.53s
01/28/2023 04:37:13 PM  [*] Sat Jan 28 16:37:13 2023: Train Epoch: 7 [51200/60900 (84%)]	Loss: 190.727631 | Elapsed: 12.51s
01/28/2023 04:37:26 PM  [*] Sat Jan 28 16:37:26 2023: Train Epoch: 7 [57600/60900 (95%)]	Loss: 183.969208 | Elapsed: 12.48s
01/28/2023 04:37:34 PM  [*] Sat Jan 28 16:37:34 2023:    7    | Tr.loss: 174.047747 | Elapsed:  120.93  s
01/28/2023 04:37:34 PM [!] Sat Jan 28 16:37:34 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_7_1674920254-model.torch
01/28/2023 04:37:35 PM  [*] Masking sequences: iteration 8...
01/28/2023 04:37:56 PM  [*] Started epoch: 8
01/28/2023 04:37:57 PM  [*] Sat Jan 28 16:37:57 2023: Train Epoch: 8 [  0  /60900 (0 %)]	Loss: 165.949188 | Elapsed: 0.40s
01/28/2023 04:38:09 PM  [*] Sat Jan 28 16:38:09 2023: Train Epoch: 8 [6400 /60900 (11%)]	Loss: 159.019897 | Elapsed: 12.49s
01/28/2023 04:38:22 PM  [*] Sat Jan 28 16:38:22 2023: Train Epoch: 8 [12800/60900 (21%)]	Loss: 186.605530 | Elapsed: 12.43s
01/28/2023 04:38:34 PM  [*] Sat Jan 28 16:38:34 2023: Train Epoch: 8 [19200/60900 (32%)]	Loss: 184.072601 | Elapsed: 12.49s
01/28/2023 04:38:47 PM  [*] Sat Jan 28 16:38:47 2023: Train Epoch: 8 [25600/60900 (42%)]	Loss: 198.084656 | Elapsed: 12.50s
01/28/2023 04:38:59 PM  [*] Sat Jan 28 16:38:59 2023: Train Epoch: 8 [32000/60900 (53%)]	Loss: 160.153503 | Elapsed: 12.47s
01/28/2023 04:39:12 PM  [*] Sat Jan 28 16:39:12 2023: Train Epoch: 8 [38400/60900 (63%)]	Loss: 172.761627 | Elapsed: 12.57s
01/28/2023 04:39:24 PM  [*] Sat Jan 28 16:39:24 2023: Train Epoch: 8 [44800/60900 (74%)]	Loss: 201.350342 | Elapsed: 12.61s
01/28/2023 04:39:37 PM  [*] Sat Jan 28 16:39:37 2023: Train Epoch: 8 [51200/60900 (84%)]	Loss: 186.239044 | Elapsed: 12.50s
01/28/2023 04:39:49 PM  [*] Sat Jan 28 16:39:49 2023: Train Epoch: 8 [57600/60900 (95%)]	Loss: 177.482391 | Elapsed: 12.53s
01/28/2023 04:39:58 PM  [*] Sat Jan 28 16:39:58 2023:    8    | Tr.loss: 173.708164 | Elapsed:  121.14  s
01/28/2023 04:39:58 PM [!] Sat Jan 28 16:39:58 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_8_1674920398-model.torch
01/28/2023 04:39:59 PM  [*] Masking sequences: iteration 9...
01/28/2023 04:40:20 PM  [*] Started epoch: 9
01/28/2023 04:40:21 PM  [*] Sat Jan 28 16:40:21 2023: Train Epoch: 9 [  0  /60900 (0 %)]	Loss: 166.675323 | Elapsed: 0.35s
01/28/2023 04:40:33 PM  [*] Sat Jan 28 16:40:33 2023: Train Epoch: 9 [6400 /60900 (11%)]	Loss: 174.850403 | Elapsed: 12.46s
01/28/2023 04:40:45 PM  [*] Sat Jan 28 16:40:45 2023: Train Epoch: 9 [12800/60900 (21%)]	Loss: 187.640900 | Elapsed: 12.41s
01/28/2023 04:40:58 PM  [*] Sat Jan 28 16:40:58 2023: Train Epoch: 9 [19200/60900 (32%)]	Loss: 161.215118 | Elapsed: 12.53s
01/28/2023 04:41:10 PM  [*] Sat Jan 28 16:41:10 2023: Train Epoch: 9 [25600/60900 (42%)]	Loss: 148.508331 | Elapsed: 12.44s
01/28/2023 04:41:23 PM  [*] Sat Jan 28 16:41:23 2023: Train Epoch: 9 [32000/60900 (53%)]	Loss: 198.872635 | Elapsed: 12.51s
01/28/2023 04:41:35 PM  [*] Sat Jan 28 16:41:35 2023: Train Epoch: 9 [38400/60900 (63%)]	Loss: 173.638367 | Elapsed: 12.53s
01/28/2023 04:41:48 PM  [*] Sat Jan 28 16:41:48 2023: Train Epoch: 9 [44800/60900 (74%)]	Loss: 154.698700 | Elapsed: 12.51s
01/28/2023 04:42:01 PM  [*] Sat Jan 28 16:42:01 2023: Train Epoch: 9 [51200/60900 (84%)]	Loss: 149.611786 | Elapsed: 12.60s
01/28/2023 04:42:13 PM  [*] Sat Jan 28 16:42:13 2023: Train Epoch: 9 [57600/60900 (95%)]	Loss: 178.011627 | Elapsed: 12.52s
01/28/2023 04:42:21 PM  [*] Sat Jan 28 16:42:21 2023:    9    | Tr.loss: 173.308424 | Elapsed:  121.06  s
01/28/2023 04:42:22 PM [!] Sat Jan 28 16:42:22 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_9_1674920541-model.torch
01/28/2023 04:42:23 PM  [*] Masking sequences: iteration 10...
01/28/2023 04:42:44 PM  [*] Started epoch: 10
01/28/2023 04:42:44 PM  [*] Sat Jan 28 16:42:44 2023: Train Epoch: 10 [  0  /60900 (0 %)]	Loss: 170.236221 | Elapsed: 0.34s
01/28/2023 04:42:57 PM  [*] Sat Jan 28 16:42:57 2023: Train Epoch: 10 [6400 /60900 (11%)]	Loss: 207.065857 | Elapsed: 12.49s
01/28/2023 04:43:09 PM  [*] Sat Jan 28 16:43:09 2023: Train Epoch: 10 [12800/60900 (21%)]	Loss: 171.486755 | Elapsed: 12.40s
01/28/2023 04:43:21 PM  [*] Sat Jan 28 16:43:21 2023: Train Epoch: 10 [19200/60900 (32%)]	Loss: 158.528000 | Elapsed: 12.44s
01/28/2023 04:43:34 PM  [*] Sat Jan 28 16:43:34 2023: Train Epoch: 10 [25600/60900 (42%)]	Loss: 170.710297 | Elapsed: 12.48s
01/28/2023 04:43:46 PM  [*] Sat Jan 28 16:43:46 2023: Train Epoch: 10 [32000/60900 (53%)]	Loss: 157.345032 | Elapsed: 12.52s
01/28/2023 04:43:59 PM  [*] Sat Jan 28 16:43:59 2023: Train Epoch: 10 [38400/60900 (63%)]	Loss: 178.323120 | Elapsed: 12.52s
01/28/2023 04:44:11 PM  [*] Sat Jan 28 16:44:11 2023: Train Epoch: 10 [44800/60900 (74%)]	Loss: 171.579788 | Elapsed: 12.48s
01/28/2023 04:44:24 PM  [*] Sat Jan 28 16:44:24 2023: Train Epoch: 10 [51200/60900 (84%)]	Loss: 183.184052 | Elapsed: 12.55s
01/28/2023 04:44:37 PM  [*] Sat Jan 28 16:44:37 2023: Train Epoch: 10 [57600/60900 (95%)]	Loss: 174.565857 | Elapsed: 12.54s
01/28/2023 04:44:45 PM  [*] Sat Jan 28 16:44:45 2023:   10    | Tr.loss: 173.439527 | Elapsed:  120.96  s
01/28/2023 04:44:45 PM [!] Sat Jan 28 16:44:45 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_10_1674920685-model.torch
01/28/2023 04:44:46 PM  [*] Masking sequences: iteration 11...
01/28/2023 04:45:08 PM  [*] Started epoch: 11
01/28/2023 04:45:08 PM  [*] Sat Jan 28 16:45:08 2023: Train Epoch: 11 [  0  /60900 (0 %)]	Loss: 174.964218 | Elapsed: 0.30s
01/28/2023 04:45:20 PM  [*] Sat Jan 28 16:45:20 2023: Train Epoch: 11 [6400 /60900 (11%)]	Loss: 172.350342 | Elapsed: 12.42s
01/28/2023 04:45:33 PM  [*] Sat Jan 28 16:45:33 2023: Train Epoch: 11 [12800/60900 (21%)]	Loss: 148.819550 | Elapsed: 12.39s
01/28/2023 04:45:45 PM  [*] Sat Jan 28 16:45:45 2023: Train Epoch: 11 [19200/60900 (32%)]	Loss: 193.345551 | Elapsed: 12.57s
01/28/2023 04:45:58 PM  [*] Sat Jan 28 16:45:58 2023: Train Epoch: 11 [25600/60900 (42%)]	Loss: 162.788818 | Elapsed: 12.48s
01/28/2023 04:46:10 PM  [*] Sat Jan 28 16:46:10 2023: Train Epoch: 11 [32000/60900 (53%)]	Loss: 187.096710 | Elapsed: 12.50s
01/28/2023 04:46:23 PM  [*] Sat Jan 28 16:46:23 2023: Train Epoch: 11 [38400/60900 (63%)]	Loss: 167.141541 | Elapsed: 12.53s
01/28/2023 04:46:35 PM  [*] Sat Jan 28 16:46:35 2023: Train Epoch: 11 [44800/60900 (74%)]	Loss: 192.986664 | Elapsed: 12.47s
01/28/2023 04:46:48 PM  [*] Sat Jan 28 16:46:48 2023: Train Epoch: 11 [51200/60900 (84%)]	Loss: 184.611221 | Elapsed: 12.54s
01/28/2023 04:47:00 PM  [*] Sat Jan 28 16:47:00 2023: Train Epoch: 11 [57600/60900 (95%)]	Loss: 189.710663 | Elapsed: 12.50s
01/28/2023 04:47:08 PM  [*] Sat Jan 28 16:47:08 2023:   11    | Tr.loss: 173.067688 | Elapsed:  120.87  s
01/28/2023 04:47:09 PM [!] Sat Jan 28 16:47:09 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_11_1674920828-model.torch
01/28/2023 04:47:10 PM  [*] Masking sequences: iteration 12...
01/28/2023 04:47:31 PM  [*] Started epoch: 12
01/28/2023 04:47:31 PM  [*] Sat Jan 28 16:47:31 2023: Train Epoch: 12 [  0  /60900 (0 %)]	Loss: 177.655716 | Elapsed: 0.35s
01/28/2023 04:47:44 PM  [*] Sat Jan 28 16:47:44 2023: Train Epoch: 12 [6400 /60900 (11%)]	Loss: 181.834076 | Elapsed: 12.47s
01/28/2023 04:47:56 PM  [*] Sat Jan 28 16:47:56 2023: Train Epoch: 12 [12800/60900 (21%)]	Loss: 167.920502 | Elapsed: 12.48s
01/28/2023 04:48:09 PM  [*] Sat Jan 28 16:48:09 2023: Train Epoch: 12 [19200/60900 (32%)]	Loss: 178.959290 | Elapsed: 12.45s
01/28/2023 04:48:21 PM  [*] Sat Jan 28 16:48:21 2023: Train Epoch: 12 [25600/60900 (42%)]	Loss: 167.510590 | Elapsed: 12.51s
01/28/2023 04:48:34 PM  [*] Sat Jan 28 16:48:34 2023: Train Epoch: 12 [32000/60900 (53%)]	Loss: 166.775314 | Elapsed: 12.56s
01/28/2023 04:48:46 PM  [*] Sat Jan 28 16:48:46 2023: Train Epoch: 12 [38400/60900 (63%)]	Loss: 177.092072 | Elapsed: 12.47s
01/28/2023 04:48:59 PM  [*] Sat Jan 28 16:48:59 2023: Train Epoch: 12 [44800/60900 (74%)]	Loss: 181.559143 | Elapsed: 12.55s
01/28/2023 04:49:11 PM  [*] Sat Jan 28 16:49:11 2023: Train Epoch: 12 [51200/60900 (84%)]	Loss: 173.805267 | Elapsed: 12.48s
01/28/2023 04:49:24 PM  [*] Sat Jan 28 16:49:24 2023: Train Epoch: 12 [57600/60900 (95%)]	Loss: 171.442352 | Elapsed: 12.48s
01/28/2023 04:49:32 PM  [*] Sat Jan 28 16:49:32 2023:   12    | Tr.loss: 173.228024 | Elapsed:  120.89  s
01/28/2023 04:49:32 PM [!] Sat Jan 28 16:49:32 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_12_1674920972-model.torch
01/28/2023 04:49:34 PM  [*] Masking sequences: iteration 13...
01/28/2023 04:49:54 PM  [*] Started epoch: 13
01/28/2023 04:49:55 PM  [*] Sat Jan 28 16:49:55 2023: Train Epoch: 13 [  0  /60900 (0 %)]	Loss: 173.717407 | Elapsed: 0.45s
01/28/2023 04:50:07 PM  [*] Sat Jan 28 16:50:07 2023: Train Epoch: 13 [6400 /60900 (11%)]	Loss: 172.201813 | Elapsed: 12.50s
01/28/2023 04:50:20 PM  [*] Sat Jan 28 16:50:20 2023: Train Epoch: 13 [12800/60900 (21%)]	Loss: 186.777557 | Elapsed: 12.43s
01/28/2023 04:50:32 PM  [*] Sat Jan 28 16:50:32 2023: Train Epoch: 13 [19200/60900 (32%)]	Loss: 162.267319 | Elapsed: 12.47s
01/28/2023 04:50:45 PM  [*] Sat Jan 28 16:50:45 2023: Train Epoch: 13 [25600/60900 (42%)]	Loss: 156.622147 | Elapsed: 12.43s
01/28/2023 04:50:57 PM  [*] Sat Jan 28 16:50:57 2023: Train Epoch: 13 [32000/60900 (53%)]	Loss: 182.907562 | Elapsed: 12.51s
01/28/2023 04:51:10 PM  [*] Sat Jan 28 16:51:10 2023: Train Epoch: 13 [38400/60900 (63%)]	Loss: 177.994507 | Elapsed: 12.58s
01/28/2023 04:51:22 PM  [*] Sat Jan 28 16:51:22 2023: Train Epoch: 13 [44800/60900 (74%)]	Loss: 180.928268 | Elapsed: 12.61s
01/28/2023 04:51:35 PM  [*] Sat Jan 28 16:51:35 2023: Train Epoch: 13 [51200/60900 (84%)]	Loss: 157.840271 | Elapsed: 12.60s
01/28/2023 04:51:47 PM  [*] Sat Jan 28 16:51:47 2023: Train Epoch: 13 [57600/60900 (95%)]	Loss: 181.994354 | Elapsed: 12.48s
01/28/2023 04:51:56 PM  [*] Sat Jan 28 16:51:56 2023:   13    | Tr.loss: 173.051971 | Elapsed:  121.28  s
01/28/2023 04:51:56 PM [!] Sat Jan 28 16:51:56 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_13_1674921116-model.torch
01/28/2023 04:51:57 PM  [*] Masking sequences: iteration 14...
01/28/2023 04:52:18 PM  [*] Started epoch: 14
01/28/2023 04:52:19 PM  [*] Sat Jan 28 16:52:19 2023: Train Epoch: 14 [  0  /60900 (0 %)]	Loss: 181.392303 | Elapsed: 0.41s
01/28/2023 04:52:31 PM  [*] Sat Jan 28 16:52:31 2023: Train Epoch: 14 [6400 /60900 (11%)]	Loss: 179.284027 | Elapsed: 12.53s
01/28/2023 04:52:44 PM  [*] Sat Jan 28 16:52:44 2023: Train Epoch: 14 [12800/60900 (21%)]	Loss: 170.516602 | Elapsed: 12.43s
01/28/2023 04:52:56 PM  [*] Sat Jan 28 16:52:56 2023: Train Epoch: 14 [19200/60900 (32%)]	Loss: 157.474792 | Elapsed: 12.47s
01/28/2023 04:53:09 PM  [*] Sat Jan 28 16:53:09 2023: Train Epoch: 14 [25600/60900 (42%)]	Loss: 177.478683 | Elapsed: 12.42s
01/28/2023 04:53:21 PM  [*] Sat Jan 28 16:53:21 2023: Train Epoch: 14 [32000/60900 (53%)]	Loss: 170.119934 | Elapsed: 12.55s
01/28/2023 04:53:34 PM  [*] Sat Jan 28 16:53:34 2023: Train Epoch: 14 [38400/60900 (63%)]	Loss: 178.878510 | Elapsed: 12.49s
01/28/2023 04:53:46 PM  [*] Sat Jan 28 16:53:46 2023: Train Epoch: 14 [44800/60900 (74%)]	Loss: 163.408173 | Elapsed: 12.50s
01/28/2023 04:53:59 PM  [*] Sat Jan 28 16:53:59 2023: Train Epoch: 14 [51200/60900 (84%)]	Loss: 173.411438 | Elapsed: 12.54s
01/28/2023 04:54:11 PM  [*] Sat Jan 28 16:54:11 2023: Train Epoch: 14 [57600/60900 (95%)]	Loss: 188.047211 | Elapsed: 12.55s
01/28/2023 04:54:20 PM  [*] Sat Jan 28 16:54:20 2023:   14    | Tr.loss: 173.008080 | Elapsed:  121.11  s
01/28/2023 04:54:20 PM [!] Sat Jan 28 16:54:20 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_14_1674921260-model.torch
01/28/2023 04:54:21 PM  [*] Masking sequences: iteration 15...
01/28/2023 04:54:42 PM  [*] Started epoch: 15
01/28/2023 04:54:43 PM  [*] Sat Jan 28 16:54:43 2023: Train Epoch: 15 [  0  /60900 (0 %)]	Loss: 144.833832 | Elapsed: 0.42s
01/28/2023 04:54:55 PM  [*] Sat Jan 28 16:54:55 2023: Train Epoch: 15 [6400 /60900 (11%)]	Loss: 174.872406 | Elapsed: 12.51s
01/28/2023 04:55:08 PM  [*] Sat Jan 28 16:55:08 2023: Train Epoch: 15 [12800/60900 (21%)]	Loss: 198.124466 | Elapsed: 12.57s
01/28/2023 04:55:20 PM  [*] Sat Jan 28 16:55:20 2023: Train Epoch: 15 [19200/60900 (32%)]	Loss: 160.894852 | Elapsed: 12.73s
01/28/2023 04:55:33 PM  [*] Sat Jan 28 16:55:33 2023: Train Epoch: 15 [25600/60900 (42%)]	Loss: 160.520172 | Elapsed: 12.56s
01/28/2023 04:55:45 PM  [*] Sat Jan 28 16:55:45 2023: Train Epoch: 15 [32000/60900 (53%)]	Loss: 150.386612 | Elapsed: 12.44s
01/28/2023 04:55:58 PM  [*] Sat Jan 28 16:55:58 2023: Train Epoch: 15 [38400/60900 (63%)]	Loss: 175.813477 | Elapsed: 12.55s
01/28/2023 04:56:10 PM  [*] Sat Jan 28 16:56:10 2023: Train Epoch: 15 [44800/60900 (74%)]	Loss: 154.587982 | Elapsed: 12.43s
01/28/2023 04:56:23 PM  [*] Sat Jan 28 16:56:23 2023: Train Epoch: 15 [51200/60900 (84%)]	Loss: 181.747269 | Elapsed: 12.58s
01/28/2023 04:56:35 PM  [*] Sat Jan 28 16:56:35 2023: Train Epoch: 15 [57600/60900 (95%)]	Loss: 186.356140 | Elapsed: 12.50s
01/28/2023 04:56:44 PM  [*] Sat Jan 28 16:56:44 2023:   15    | Tr.loss: 172.927858 | Elapsed:  121.40  s
01/28/2023 04:56:44 PM [!] Sat Jan 28 16:56:44 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_15_1674921404-model.torch
01/28/2023 04:56:45 PM  [*] Masking sequences: iteration 16...
01/28/2023 04:57:07 PM  [*] Started epoch: 16
01/28/2023 04:57:07 PM  [*] Sat Jan 28 16:57:07 2023: Train Epoch: 16 [  0  /60900 (0 %)]	Loss: 183.841675 | Elapsed: 0.45s
01/28/2023 04:57:20 PM  [*] Sat Jan 28 16:57:20 2023: Train Epoch: 16 [6400 /60900 (11%)]	Loss: 181.156326 | Elapsed: 12.50s
01/28/2023 04:57:32 PM  [*] Sat Jan 28 16:57:32 2023: Train Epoch: 16 [12800/60900 (21%)]	Loss: 190.174820 | Elapsed: 12.53s
01/28/2023 04:57:45 PM  [*] Sat Jan 28 16:57:45 2023: Train Epoch: 16 [19200/60900 (32%)]	Loss: 157.556824 | Elapsed: 12.51s
01/28/2023 04:57:57 PM  [*] Sat Jan 28 16:57:57 2023: Train Epoch: 16 [25600/60900 (42%)]	Loss: 178.717133 | Elapsed: 12.59s
01/28/2023 04:58:10 PM  [*] Sat Jan 28 16:58:10 2023: Train Epoch: 16 [32000/60900 (53%)]	Loss: 159.638855 | Elapsed: 12.58s
01/28/2023 04:58:22 PM  [*] Sat Jan 28 16:58:22 2023: Train Epoch: 16 [38400/60900 (63%)]	Loss: 167.937500 | Elapsed: 12.56s
01/28/2023 04:58:35 PM  [*] Sat Jan 28 16:58:35 2023: Train Epoch: 16 [44800/60900 (74%)]	Loss: 187.272461 | Elapsed: 12.55s
01/28/2023 04:58:48 PM  [*] Sat Jan 28 16:58:48 2023: Train Epoch: 16 [51200/60900 (84%)]	Loss: 150.297333 | Elapsed: 12.70s
01/28/2023 04:59:00 PM  [*] Sat Jan 28 16:59:00 2023: Train Epoch: 16 [57600/60900 (95%)]	Loss: 152.489838 | Elapsed: 12.65s
01/28/2023 04:59:08 PM  [*] Sat Jan 28 16:59:08 2023:   16    | Tr.loss: 173.055322 | Elapsed:  121.81  s
01/28/2023 04:59:09 PM [!] Sat Jan 28 16:59:09 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_16_1674921548-model.torch
01/28/2023 04:59:10 PM  [*] Masking sequences: iteration 17...
01/28/2023 04:59:31 PM  [*] Started epoch: 17
01/28/2023 04:59:31 PM  [*] Sat Jan 28 16:59:31 2023: Train Epoch: 17 [  0  /60900 (0 %)]	Loss: 179.757721 | Elapsed: 0.35s
01/28/2023 04:59:44 PM  [*] Sat Jan 28 16:59:44 2023: Train Epoch: 17 [6400 /60900 (11%)]	Loss: 146.515411 | Elapsed: 12.45s
01/28/2023 04:59:56 PM  [*] Sat Jan 28 16:59:56 2023: Train Epoch: 17 [12800/60900 (21%)]	Loss: 169.293091 | Elapsed: 12.45s
01/28/2023 05:00:09 PM  [*] Sat Jan 28 17:00:09 2023: Train Epoch: 17 [19200/60900 (32%)]	Loss: 172.016052 | Elapsed: 12.53s
01/28/2023 05:00:21 PM  [*] Sat Jan 28 17:00:21 2023: Train Epoch: 17 [25600/60900 (42%)]	Loss: 174.721237 | Elapsed: 12.47s
01/28/2023 05:00:34 PM  [*] Sat Jan 28 17:00:34 2023: Train Epoch: 17 [32000/60900 (53%)]	Loss: 170.494492 | Elapsed: 12.47s
01/28/2023 05:00:46 PM  [*] Sat Jan 28 17:00:46 2023: Train Epoch: 17 [38400/60900 (63%)]	Loss: 152.801941 | Elapsed: 12.44s
01/28/2023 05:00:59 PM  [*] Sat Jan 28 17:00:59 2023: Train Epoch: 17 [44800/60900 (74%)]	Loss: 180.113342 | Elapsed: 12.62s
01/28/2023 05:01:11 PM  [*] Sat Jan 28 17:01:11 2023: Train Epoch: 17 [51200/60900 (84%)]	Loss: 180.770294 | Elapsed: 12.45s
01/28/2023 05:01:24 PM  [*] Sat Jan 28 17:01:24 2023: Train Epoch: 17 [57600/60900 (95%)]	Loss: 178.450928 | Elapsed: 12.53s
01/28/2023 05:01:32 PM  [*] Sat Jan 28 17:01:32 2023:   17    | Tr.loss: 172.996119 | Elapsed:  121.03  s
01/28/2023 05:01:33 PM [!] Sat Jan 28 17:01:33 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_17_1674921692-model.torch
01/28/2023 05:01:34 PM  [*] Masking sequences: iteration 18...
01/28/2023 05:01:55 PM  [*] Started epoch: 18
01/28/2023 05:01:55 PM  [*] Sat Jan 28 17:01:55 2023: Train Epoch: 18 [  0  /60900 (0 %)]	Loss: 171.480499 | Elapsed: 0.82s
01/28/2023 05:02:08 PM  [*] Sat Jan 28 17:02:08 2023: Train Epoch: 18 [6400 /60900 (11%)]	Loss: 178.400940 | Elapsed: 12.48s
01/28/2023 05:02:20 PM  [*] Sat Jan 28 17:02:20 2023: Train Epoch: 18 [12800/60900 (21%)]	Loss: 169.895111 | Elapsed: 12.41s
01/28/2023 05:02:33 PM  [*] Sat Jan 28 17:02:33 2023: Train Epoch: 18 [19200/60900 (32%)]	Loss: 180.234589 | Elapsed: 12.47s
01/28/2023 05:02:45 PM  [*] Sat Jan 28 17:02:45 2023: Train Epoch: 18 [25600/60900 (42%)]	Loss: 193.719086 | Elapsed: 12.50s
01/28/2023 05:02:58 PM  [*] Sat Jan 28 17:02:58 2023: Train Epoch: 18 [32000/60900 (53%)]	Loss: 165.477112 | Elapsed: 12.51s
01/28/2023 05:03:10 PM  [*] Sat Jan 28 17:03:10 2023: Train Epoch: 18 [38400/60900 (63%)]	Loss: 176.992889 | Elapsed: 12.45s
01/28/2023 05:03:23 PM  [*] Sat Jan 28 17:03:23 2023: Train Epoch: 18 [44800/60900 (74%)]	Loss: 174.810913 | Elapsed: 12.52s
01/28/2023 05:03:35 PM  [*] Sat Jan 28 17:03:35 2023: Train Epoch: 18 [51200/60900 (84%)]	Loss: 198.246826 | Elapsed: 12.50s
01/28/2023 05:03:48 PM  [*] Sat Jan 28 17:03:48 2023: Train Epoch: 18 [57600/60900 (95%)]	Loss: 166.172012 | Elapsed: 12.51s
01/28/2023 05:03:56 PM  [*] Sat Jan 28 17:03:56 2023:   18    | Tr.loss: 173.105918 | Elapsed:  121.24  s
01/28/2023 05:03:56 PM [!] Sat Jan 28 17:03:56 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_18_1674921836-model.torch
01/28/2023 05:03:57 PM  [*] Masking sequences: iteration 19...
01/28/2023 05:04:19 PM  [*] Started epoch: 19
01/28/2023 05:04:19 PM  [*] Sat Jan 28 17:04:19 2023: Train Epoch: 19 [  0  /60900 (0 %)]	Loss: 162.121262 | Elapsed: 0.38s
01/28/2023 05:04:31 PM  [*] Sat Jan 28 17:04:31 2023: Train Epoch: 19 [6400 /60900 (11%)]	Loss: 177.071625 | Elapsed: 12.57s
01/28/2023 05:04:44 PM  [*] Sat Jan 28 17:04:44 2023: Train Epoch: 19 [12800/60900 (21%)]	Loss: 199.369766 | Elapsed: 12.46s
01/28/2023 05:04:56 PM  [*] Sat Jan 28 17:04:56 2023: Train Epoch: 19 [19200/60900 (32%)]	Loss: 156.944458 | Elapsed: 12.39s
01/28/2023 05:05:09 PM  [*] Sat Jan 28 17:05:09 2023: Train Epoch: 19 [25600/60900 (42%)]	Loss: 174.860016 | Elapsed: 12.45s
01/28/2023 05:05:21 PM  [*] Sat Jan 28 17:05:21 2023: Train Epoch: 19 [32000/60900 (53%)]	Loss: 158.922577 | Elapsed: 12.44s
01/28/2023 05:05:34 PM  [*] Sat Jan 28 17:05:34 2023: Train Epoch: 19 [38400/60900 (63%)]	Loss: 173.777008 | Elapsed: 12.50s
01/28/2023 05:05:46 PM  [*] Sat Jan 28 17:05:46 2023: Train Epoch: 19 [44800/60900 (74%)]	Loss: 173.852112 | Elapsed: 12.46s
01/28/2023 05:05:59 PM  [*] Sat Jan 28 17:05:59 2023: Train Epoch: 19 [51200/60900 (84%)]	Loss: 177.857697 | Elapsed: 12.59s
01/28/2023 05:06:11 PM  [*] Sat Jan 28 17:06:11 2023: Train Epoch: 19 [57600/60900 (95%)]	Loss: 165.958038 | Elapsed: 12.62s
01/28/2023 05:06:20 PM  [*] Sat Jan 28 17:06:20 2023:   19    | Tr.loss: 172.999902 | Elapsed:  121.00  s
01/28/2023 05:06:20 PM [!] Sat Jan 28 17:06:20 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_19_1674921980-model.torch
01/28/2023 05:06:21 PM  [*] Masking sequences: iteration 20...
01/28/2023 05:06:42 PM  [*] Started epoch: 20
01/28/2023 05:06:43 PM  [*] Sat Jan 28 17:06:43 2023: Train Epoch: 20 [  0  /60900 (0 %)]	Loss: 172.857101 | Elapsed: 0.84s
01/28/2023 05:06:56 PM  [*] Sat Jan 28 17:06:56 2023: Train Epoch: 20 [6400 /60900 (11%)]	Loss: 165.279266 | Elapsed: 12.58s
01/28/2023 05:07:08 PM  [*] Sat Jan 28 17:07:08 2023: Train Epoch: 20 [12800/60900 (21%)]	Loss: 155.329712 | Elapsed: 12.51s
01/28/2023 05:07:21 PM  [*] Sat Jan 28 17:07:21 2023: Train Epoch: 20 [19200/60900 (32%)]	Loss: 185.816452 | Elapsed: 12.55s
01/28/2023 05:07:33 PM  [*] Sat Jan 28 17:07:33 2023: Train Epoch: 20 [25600/60900 (42%)]	Loss: 164.811554 | Elapsed: 12.50s
01/28/2023 05:07:46 PM  [*] Sat Jan 28 17:07:46 2023: Train Epoch: 20 [32000/60900 (53%)]	Loss: 183.218445 | Elapsed: 12.50s
01/28/2023 05:07:58 PM  [*] Sat Jan 28 17:07:58 2023: Train Epoch: 20 [38400/60900 (63%)]	Loss: 170.914154 | Elapsed: 12.56s
01/28/2023 05:08:11 PM  [*] Sat Jan 28 17:08:11 2023: Train Epoch: 20 [44800/60900 (74%)]	Loss: 180.952255 | Elapsed: 12.54s
01/28/2023 05:08:23 PM  [*] Sat Jan 28 17:08:23 2023: Train Epoch: 20 [51200/60900 (84%)]	Loss: 171.854141 | Elapsed: 12.49s
01/28/2023 05:08:36 PM  [*] Sat Jan 28 17:08:36 2023: Train Epoch: 20 [57600/60900 (95%)]	Loss: 172.379486 | Elapsed: 12.51s
01/28/2023 05:08:44 PM  [*] Sat Jan 28 17:08:44 2023:   20    | Tr.loss: 173.029437 | Elapsed:  121.72  s
01/28/2023 05:08:44 PM [!] Sat Jan 28 17:08:44 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_20_1674922124-model.torch
01/28/2023 05:08:45 PM  [*] Masking sequences: iteration 21...
01/28/2023 05:09:07 PM  [*] Started epoch: 21
01/28/2023 05:09:07 PM  [*] Sat Jan 28 17:09:07 2023: Train Epoch: 21 [  0  /60900 (0 %)]	Loss: 163.622742 | Elapsed: 0.43s
01/28/2023 05:09:20 PM  [*] Sat Jan 28 17:09:20 2023: Train Epoch: 21 [6400 /60900 (11%)]	Loss: 166.369736 | Elapsed: 12.70s
01/28/2023 05:09:33 PM  [*] Sat Jan 28 17:09:33 2023: Train Epoch: 21 [12800/60900 (21%)]	Loss: 196.029282 | Elapsed: 12.73s
01/28/2023 05:09:45 PM  [*] Sat Jan 28 17:09:45 2023: Train Epoch: 21 [19200/60900 (32%)]	Loss: 155.989075 | Elapsed: 12.80s
01/28/2023 05:09:58 PM  [*] Sat Jan 28 17:09:58 2023: Train Epoch: 21 [25600/60900 (42%)]	Loss: 214.929657 | Elapsed: 12.68s
01/28/2023 05:10:11 PM  [*] Sat Jan 28 17:10:11 2023: Train Epoch: 21 [32000/60900 (53%)]	Loss: 175.667206 | Elapsed: 12.85s
01/28/2023 05:10:24 PM  [*] Sat Jan 28 17:10:24 2023: Train Epoch: 21 [38400/60900 (63%)]	Loss: 174.311157 | Elapsed: 12.90s
01/28/2023 05:10:37 PM  [*] Sat Jan 28 17:10:37 2023: Train Epoch: 21 [44800/60900 (74%)]	Loss: 160.872894 | Elapsed: 12.79s
01/28/2023 05:10:50 PM  [*] Sat Jan 28 17:10:50 2023: Train Epoch: 21 [51200/60900 (84%)]	Loss: 188.530731 | Elapsed: 12.82s
01/28/2023 05:11:02 PM  [*] Sat Jan 28 17:11:02 2023: Train Epoch: 21 [57600/60900 (95%)]	Loss: 193.606461 | Elapsed: 12.68s
01/28/2023 05:11:11 PM  [*] Sat Jan 28 17:11:11 2023:   21    | Tr.loss: 173.037824 | Elapsed:  123.75  s
01/28/2023 05:11:11 PM [!] Sat Jan 28 17:11:11 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_21_1674922271-model.torch
01/28/2023 05:11:12 PM  [*] Masking sequences: iteration 22...
01/28/2023 05:11:36 PM  [*] Started epoch: 22
01/28/2023 05:11:36 PM  [*] Sat Jan 28 17:11:36 2023: Train Epoch: 22 [  0  /60900 (0 %)]	Loss: 160.763748 | Elapsed: 0.43s
01/28/2023 05:11:49 PM  [*] Sat Jan 28 17:11:49 2023: Train Epoch: 22 [6400 /60900 (11%)]	Loss: 161.870270 | Elapsed: 12.65s
01/28/2023 05:12:02 PM  [*] Sat Jan 28 17:12:02 2023: Train Epoch: 22 [12800/60900 (21%)]	Loss: 175.829437 | Elapsed: 12.77s
01/28/2023 05:12:14 PM  [*] Sat Jan 28 17:12:14 2023: Train Epoch: 22 [19200/60900 (32%)]	Loss: 166.277893 | Elapsed: 12.85s
01/28/2023 05:12:27 PM  [*] Sat Jan 28 17:12:27 2023: Train Epoch: 22 [25600/60900 (42%)]	Loss: 171.070374 | Elapsed: 12.95s
01/28/2023 05:12:40 PM  [*] Sat Jan 28 17:12:40 2023: Train Epoch: 22 [32000/60900 (53%)]	Loss: 164.134796 | Elapsed: 12.76s
01/28/2023 05:12:53 PM  [*] Sat Jan 28 17:12:53 2023: Train Epoch: 22 [38400/60900 (63%)]	Loss: 171.874390 | Elapsed: 12.53s
01/28/2023 05:13:05 PM  [*] Sat Jan 28 17:13:05 2023: Train Epoch: 22 [44800/60900 (74%)]	Loss: 168.680603 | Elapsed: 12.53s
01/28/2023 05:13:18 PM  [*] Sat Jan 28 17:13:18 2023: Train Epoch: 22 [51200/60900 (84%)]	Loss: 173.055832 | Elapsed: 12.64s
01/28/2023 05:13:30 PM  [*] Sat Jan 28 17:13:30 2023: Train Epoch: 22 [57600/60900 (95%)]	Loss: 185.630661 | Elapsed: 12.55s
01/28/2023 05:13:39 PM  [*] Sat Jan 28 17:13:39 2023:   22    | Tr.loss: 173.037405 | Elapsed:  122.83  s
01/28/2023 05:13:39 PM [!] Sat Jan 28 17:13:39 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_22_1674922419-model.torch
01/28/2023 05:13:40 PM  [*] Masking sequences: iteration 23...
01/28/2023 05:14:01 PM  [*] Started epoch: 23
01/28/2023 05:14:01 PM  [*] Sat Jan 28 17:14:01 2023: Train Epoch: 23 [  0  /60900 (0 %)]	Loss: 180.470978 | Elapsed: 0.36s
01/28/2023 05:14:14 PM  [*] Sat Jan 28 17:14:14 2023: Train Epoch: 23 [6400 /60900 (11%)]	Loss: 167.787170 | Elapsed: 12.51s
01/28/2023 05:14:26 PM  [*] Sat Jan 28 17:14:26 2023: Train Epoch: 23 [12800/60900 (21%)]	Loss: 150.969757 | Elapsed: 12.44s
01/28/2023 05:14:39 PM  [*] Sat Jan 28 17:14:39 2023: Train Epoch: 23 [19200/60900 (32%)]	Loss: 172.676910 | Elapsed: 12.46s
01/28/2023 05:14:51 PM  [*] Sat Jan 28 17:14:51 2023: Train Epoch: 23 [25600/60900 (42%)]	Loss: 176.145874 | Elapsed: 12.57s
01/28/2023 05:15:04 PM  [*] Sat Jan 28 17:15:04 2023: Train Epoch: 23 [32000/60900 (53%)]	Loss: 170.030197 | Elapsed: 12.45s
01/28/2023 05:15:16 PM  [*] Sat Jan 28 17:15:16 2023: Train Epoch: 23 [38400/60900 (63%)]	Loss: 169.837189 | Elapsed: 12.46s
01/28/2023 05:15:29 PM  [*] Sat Jan 28 17:15:29 2023: Train Epoch: 23 [44800/60900 (74%)]	Loss: 169.876038 | Elapsed: 12.51s
01/28/2023 05:15:41 PM  [*] Sat Jan 28 17:15:41 2023: Train Epoch: 23 [51200/60900 (84%)]	Loss: 174.033295 | Elapsed: 12.59s
01/28/2023 05:15:54 PM  [*] Sat Jan 28 17:15:54 2023: Train Epoch: 23 [57600/60900 (95%)]	Loss: 173.406464 | Elapsed: 12.45s
01/28/2023 05:16:02 PM  [*] Sat Jan 28 17:16:02 2023:   23    | Tr.loss: 173.174325 | Elapsed:  120.98  s
01/28/2023 05:16:02 PM [!] Sat Jan 28 17:16:02 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_23_1674922562-model.torch
01/28/2023 05:16:04 PM  [*] Masking sequences: iteration 24...
01/28/2023 05:16:24 PM  [*] Started epoch: 24
01/28/2023 05:16:25 PM  [*] Sat Jan 28 17:16:25 2023: Train Epoch: 24 [  0  /60900 (0 %)]	Loss: 168.900177 | Elapsed: 0.40s
01/28/2023 05:16:37 PM  [*] Sat Jan 28 17:16:37 2023: Train Epoch: 24 [6400 /60900 (11%)]	Loss: 175.656372 | Elapsed: 12.51s
01/28/2023 05:16:50 PM  [*] Sat Jan 28 17:16:50 2023: Train Epoch: 24 [12800/60900 (21%)]	Loss: 163.258179 | Elapsed: 12.49s
01/28/2023 05:17:02 PM  [*] Sat Jan 28 17:17:02 2023: Train Epoch: 24 [19200/60900 (32%)]	Loss: 169.145477 | Elapsed: 12.50s
01/28/2023 05:17:15 PM  [*] Sat Jan 28 17:17:15 2023: Train Epoch: 24 [25600/60900 (42%)]	Loss: 173.880951 | Elapsed: 12.49s
01/28/2023 05:17:27 PM  [*] Sat Jan 28 17:17:27 2023: Train Epoch: 24 [32000/60900 (53%)]	Loss: 178.841064 | Elapsed: 12.42s
01/28/2023 05:17:40 PM  [*] Sat Jan 28 17:17:40 2023: Train Epoch: 24 [38400/60900 (63%)]	Loss: 185.006058 | Elapsed: 12.48s
01/28/2023 05:17:52 PM  [*] Sat Jan 28 17:17:52 2023: Train Epoch: 24 [44800/60900 (74%)]	Loss: 167.556854 | Elapsed: 12.50s
01/28/2023 05:18:05 PM  [*] Sat Jan 28 17:18:05 2023: Train Epoch: 24 [51200/60900 (84%)]	Loss: 181.504211 | Elapsed: 12.60s
01/28/2023 05:18:17 PM  [*] Sat Jan 28 17:18:17 2023: Train Epoch: 24 [57600/60900 (95%)]	Loss: 161.924469 | Elapsed: 12.56s
01/28/2023 05:18:26 PM  [*] Sat Jan 28 17:18:26 2023:   24    | Tr.loss: 173.004775 | Elapsed:  121.09  s
01/28/2023 05:18:26 PM [!] Sat Jan 28 17:18:26 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_24_1674922706-model.torch
01/28/2023 05:18:27 PM  [*] Masking sequences: iteration 25...
01/28/2023 05:18:48 PM  [*] Started epoch: 25
01/28/2023 05:18:48 PM  [*] Sat Jan 28 17:18:48 2023: Train Epoch: 25 [  0  /60900 (0 %)]	Loss: 191.014175 | Elapsed: 0.43s
01/28/2023 05:19:01 PM  [*] Sat Jan 28 17:19:01 2023: Train Epoch: 25 [6400 /60900 (11%)]	Loss: 173.966629 | Elapsed: 12.48s
01/28/2023 05:19:13 PM  [*] Sat Jan 28 17:19:13 2023: Train Epoch: 25 [12800/60900 (21%)]	Loss: 173.323090 | Elapsed: 12.43s
01/28/2023 05:19:26 PM  [*] Sat Jan 28 17:19:26 2023: Train Epoch: 25 [19200/60900 (32%)]	Loss: 192.684906 | Elapsed: 12.50s
01/28/2023 05:19:38 PM  [*] Sat Jan 28 17:19:38 2023: Train Epoch: 25 [25600/60900 (42%)]	Loss: 162.330582 | Elapsed: 12.46s
01/28/2023 05:19:51 PM  [*] Sat Jan 28 17:19:51 2023: Train Epoch: 25 [32000/60900 (53%)]	Loss: 185.203049 | Elapsed: 12.53s
01/28/2023 05:20:03 PM  [*] Sat Jan 28 17:20:03 2023: Train Epoch: 25 [38400/60900 (63%)]	Loss: 187.355896 | Elapsed: 12.49s
01/28/2023 05:20:16 PM  [*] Sat Jan 28 17:20:16 2023: Train Epoch: 25 [44800/60900 (74%)]	Loss: 174.212982 | Elapsed: 12.52s
01/28/2023 05:20:28 PM  [*] Sat Jan 28 17:20:28 2023: Train Epoch: 25 [51200/60900 (84%)]	Loss: 162.327927 | Elapsed: 12.46s
01/28/2023 05:20:40 PM  [*] Sat Jan 28 17:20:40 2023: Train Epoch: 25 [57600/60900 (95%)]	Loss: 170.317383 | Elapsed: 12.49s
01/28/2023 05:20:49 PM  [*] Sat Jan 28 17:20:49 2023:   25    | Tr.loss: 172.927116 | Elapsed:  120.98  s
01/28/2023 05:20:49 PM [!] Sat Jan 28 17:20:49 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_25_1674922849-model.torch
01/28/2023 05:20:50 PM  [*] Masking sequences: iteration 26...
01/28/2023 05:21:11 PM  [*] Started epoch: 26
01/28/2023 05:21:12 PM  [*] Sat Jan 28 17:21:12 2023: Train Epoch: 26 [  0  /60900 (0 %)]	Loss: 166.466370 | Elapsed: 0.35s
01/28/2023 05:21:24 PM  [*] Sat Jan 28 17:21:24 2023: Train Epoch: 26 [6400 /60900 (11%)]	Loss: 178.765686 | Elapsed: 12.55s
01/28/2023 05:21:37 PM  [*] Sat Jan 28 17:21:37 2023: Train Epoch: 26 [12800/60900 (21%)]	Loss: 176.180298 | Elapsed: 12.56s
01/28/2023 05:21:49 PM  [*] Sat Jan 28 17:21:49 2023: Train Epoch: 26 [19200/60900 (32%)]	Loss: 182.886597 | Elapsed: 12.49s
01/28/2023 05:22:02 PM  [*] Sat Jan 28 17:22:02 2023: Train Epoch: 26 [25600/60900 (42%)]	Loss: 180.564911 | Elapsed: 12.48s
01/28/2023 05:22:14 PM  [*] Sat Jan 28 17:22:14 2023: Train Epoch: 26 [32000/60900 (53%)]	Loss: 170.537567 | Elapsed: 12.57s
01/28/2023 05:22:27 PM  [*] Sat Jan 28 17:22:27 2023: Train Epoch: 26 [38400/60900 (63%)]	Loss: 170.539490 | Elapsed: 12.53s
01/28/2023 05:22:39 PM  [*] Sat Jan 28 17:22:39 2023: Train Epoch: 26 [44800/60900 (74%)]	Loss: 165.564758 | Elapsed: 12.34s
01/28/2023 05:22:51 PM  [*] Sat Jan 28 17:22:51 2023: Train Epoch: 26 [51200/60900 (84%)]	Loss: 178.965332 | Elapsed: 12.29s
01/28/2023 05:23:04 PM  [*] Sat Jan 28 17:23:04 2023: Train Epoch: 26 [57600/60900 (95%)]	Loss: 167.667847 | Elapsed: 12.35s
01/28/2023 05:23:12 PM  [*] Sat Jan 28 17:23:12 2023:   26    | Tr.loss: 173.138846 | Elapsed:  120.46  s
01/28/2023 05:23:12 PM [!] Sat Jan 28 17:23:12 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_26_1674922992-model.torch
01/28/2023 05:23:13 PM  [*] Masking sequences: iteration 27...
01/28/2023 05:23:33 PM  [*] Started epoch: 27
01/28/2023 05:23:34 PM  [*] Sat Jan 28 17:23:34 2023: Train Epoch: 27 [  0  /60900 (0 %)]	Loss: 163.389832 | Elapsed: 0.28s
01/28/2023 05:23:46 PM  [*] Sat Jan 28 17:23:46 2023: Train Epoch: 27 [6400 /60900 (11%)]	Loss: 186.471069 | Elapsed: 12.37s
01/28/2023 05:23:59 PM  [*] Sat Jan 28 17:23:59 2023: Train Epoch: 27 [12800/60900 (21%)]	Loss: 172.957932 | Elapsed: 12.74s
01/28/2023 05:24:11 PM  [*] Sat Jan 28 17:24:11 2023: Train Epoch: 27 [19200/60900 (32%)]	Loss: 168.854248 | Elapsed: 12.85s
01/28/2023 05:24:24 PM  [*] Sat Jan 28 17:24:24 2023: Train Epoch: 27 [25600/60900 (42%)]	Loss: 181.680634 | Elapsed: 12.28s
01/28/2023 05:24:36 PM  [*] Sat Jan 28 17:24:36 2023: Train Epoch: 27 [32000/60900 (53%)]	Loss: 156.796478 | Elapsed: 12.29s
01/28/2023 05:24:48 PM  [*] Sat Jan 28 17:24:48 2023: Train Epoch: 27 [38400/60900 (63%)]	Loss: 177.519836 | Elapsed: 12.36s
01/28/2023 05:25:01 PM  [*] Sat Jan 28 17:25:01 2023: Train Epoch: 27 [44800/60900 (74%)]	Loss: 164.100098 | Elapsed: 12.33s
01/28/2023 05:25:13 PM  [*] Sat Jan 28 17:25:13 2023: Train Epoch: 27 [51200/60900 (84%)]	Loss: 189.041107 | Elapsed: 12.36s
01/28/2023 05:25:26 PM  [*] Sat Jan 28 17:25:26 2023: Train Epoch: 27 [57600/60900 (95%)]	Loss: 162.482346 | Elapsed: 12.39s
01/28/2023 05:25:34 PM  [*] Sat Jan 28 17:25:34 2023:   27    | Tr.loss: 173.078459 | Elapsed:  120.28  s
01/28/2023 05:25:34 PM [!] Sat Jan 28 17:25:34 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_27_1674923134-model.torch
01/28/2023 05:25:35 PM  [*] Masking sequences: iteration 28...
01/28/2023 05:25:55 PM  [*] Started epoch: 28
01/28/2023 05:25:55 PM  [*] Sat Jan 28 17:25:55 2023: Train Epoch: 28 [  0  /60900 (0 %)]	Loss: 159.617004 | Elapsed: 0.42s
01/28/2023 05:26:08 PM  [*] Sat Jan 28 17:26:08 2023: Train Epoch: 28 [6400 /60900 (11%)]	Loss: 169.722504 | Elapsed: 12.27s
01/28/2023 05:26:20 PM  [*] Sat Jan 28 17:26:20 2023: Train Epoch: 28 [12800/60900 (21%)]	Loss: 160.981003 | Elapsed: 12.27s
01/28/2023 05:26:32 PM  [*] Sat Jan 28 17:26:32 2023: Train Epoch: 28 [19200/60900 (32%)]	Loss: 153.436844 | Elapsed: 12.31s
01/28/2023 05:26:44 PM  [*] Sat Jan 28 17:26:44 2023: Train Epoch: 28 [25600/60900 (42%)]	Loss: 175.504181 | Elapsed: 12.34s
01/28/2023 05:26:57 PM  [*] Sat Jan 28 17:26:57 2023: Train Epoch: 28 [32000/60900 (53%)]	Loss: 181.099426 | Elapsed: 12.31s
01/28/2023 05:27:09 PM  [*] Sat Jan 28 17:27:09 2023: Train Epoch: 28 [38400/60900 (63%)]	Loss: 177.298798 | Elapsed: 12.29s
01/28/2023 05:27:21 PM  [*] Sat Jan 28 17:27:21 2023: Train Epoch: 28 [44800/60900 (74%)]	Loss: 156.768799 | Elapsed: 12.35s
01/28/2023 05:27:34 PM  [*] Sat Jan 28 17:27:34 2023: Train Epoch: 28 [51200/60900 (84%)]	Loss: 175.687622 | Elapsed: 12.25s
01/28/2023 05:27:46 PM  [*] Sat Jan 28 17:27:46 2023: Train Epoch: 28 [57600/60900 (95%)]	Loss: 195.724823 | Elapsed: 12.40s
01/28/2023 05:27:54 PM  [*] Sat Jan 28 17:27:54 2023:   28    | Tr.loss: 172.850203 | Elapsed:  119.20  s
01/28/2023 05:27:54 PM [!] Sat Jan 28 17:27:54 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_28_1674923274-model.torch
01/28/2023 05:27:56 PM  [*] Masking sequences: iteration 29...
01/28/2023 05:28:15 PM  [*] Started epoch: 29
01/28/2023 05:28:15 PM  [*] Sat Jan 28 17:28:15 2023: Train Epoch: 29 [  0  /60900 (0 %)]	Loss: 184.691864 | Elapsed: 0.49s
01/28/2023 05:28:28 PM  [*] Sat Jan 28 17:28:28 2023: Train Epoch: 29 [6400 /60900 (11%)]	Loss: 171.441498 | Elapsed: 12.27s
01/28/2023 05:28:40 PM  [*] Sat Jan 28 17:28:40 2023: Train Epoch: 29 [12800/60900 (21%)]	Loss: 175.016724 | Elapsed: 12.26s
01/28/2023 05:28:52 PM  [*] Sat Jan 28 17:28:52 2023: Train Epoch: 29 [19200/60900 (32%)]	Loss: 184.234512 | Elapsed: 12.37s
01/28/2023 05:29:05 PM  [*] Sat Jan 28 17:29:05 2023: Train Epoch: 29 [25600/60900 (42%)]	Loss: 173.242371 | Elapsed: 12.32s
01/28/2023 05:29:17 PM  [*] Sat Jan 28 17:29:17 2023: Train Epoch: 29 [32000/60900 (53%)]	Loss: 167.638000 | Elapsed: 12.28s
01/28/2023 05:29:29 PM  [*] Sat Jan 28 17:29:29 2023: Train Epoch: 29 [38400/60900 (63%)]	Loss: 175.965958 | Elapsed: 12.28s
01/28/2023 05:29:42 PM  [*] Sat Jan 28 17:29:42 2023: Train Epoch: 29 [44800/60900 (74%)]	Loss: 187.952026 | Elapsed: 12.26s
01/28/2023 05:29:54 PM  [*] Sat Jan 28 17:29:54 2023: Train Epoch: 29 [51200/60900 (84%)]	Loss: 184.903870 | Elapsed: 12.31s
01/28/2023 05:30:06 PM  [*] Sat Jan 28 17:30:06 2023: Train Epoch: 29 [57600/60900 (95%)]	Loss: 171.739929 | Elapsed: 12.25s
01/28/2023 05:30:14 PM  [*] Sat Jan 28 17:30:14 2023:   29    | Tr.loss: 173.173833 | Elapsed:  119.13  s
01/28/2023 05:30:15 PM [!] Sat Jan 28 17:30:15 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_29_1674923414-model.torch
01/28/2023 05:30:16 PM  [*] Masking sequences: iteration 30...
01/28/2023 05:30:35 PM  [*] Started epoch: 30
01/28/2023 05:30:36 PM  [*] Sat Jan 28 17:30:36 2023: Train Epoch: 30 [  0  /60900 (0 %)]	Loss: 154.470215 | Elapsed: 0.78s
01/28/2023 05:30:48 PM  [*] Sat Jan 28 17:30:48 2023: Train Epoch: 30 [6400 /60900 (11%)]	Loss: 155.739868 | Elapsed: 12.28s
01/28/2023 05:31:00 PM  [*] Sat Jan 28 17:31:00 2023: Train Epoch: 30 [12800/60900 (21%)]	Loss: 159.601608 | Elapsed: 12.26s
01/28/2023 05:31:13 PM  [*] Sat Jan 28 17:31:13 2023: Train Epoch: 30 [19200/60900 (32%)]	Loss: 187.359879 | Elapsed: 12.27s
01/28/2023 05:31:25 PM  [*] Sat Jan 28 17:31:25 2023: Train Epoch: 30 [25600/60900 (42%)]	Loss: 170.438293 | Elapsed: 12.24s
01/28/2023 05:31:37 PM  [*] Sat Jan 28 17:31:37 2023: Train Epoch: 30 [32000/60900 (53%)]	Loss: 162.594955 | Elapsed: 12.42s
01/28/2023 05:31:50 PM  [*] Sat Jan 28 17:31:50 2023: Train Epoch: 30 [38400/60900 (63%)]	Loss: 152.596390 | Elapsed: 12.31s
01/28/2023 05:32:02 PM  [*] Sat Jan 28 17:32:02 2023: Train Epoch: 30 [44800/60900 (74%)]	Loss: 172.513092 | Elapsed: 12.28s
01/28/2023 05:32:14 PM  [*] Sat Jan 28 17:32:14 2023: Train Epoch: 30 [51200/60900 (84%)]	Loss: 178.837952 | Elapsed: 12.30s
01/28/2023 05:32:27 PM  [*] Sat Jan 28 17:32:27 2023: Train Epoch: 30 [57600/60900 (95%)]	Loss: 174.592865 | Elapsed: 12.34s
01/28/2023 05:32:35 PM  [*] Sat Jan 28 17:32:35 2023:   30    | Tr.loss: 172.897527 | Elapsed:  119.42  s
01/28/2023 05:32:35 PM [!] Sat Jan 28 17:32:35 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\epoch_30_1674923555-model.torch
01/28/2023 05:32:35 PM [!] Sat Jan 28 17:32:35 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\preTraining\training_files\1674923555-model.torch
		train time: 1674923555-trainTime.npy
		train losses: 1674923555-trainLosses.npy
		train AUC: 1674923555-auc.npy
01/28/2023 05:32:37 PM  [!] Training pretrained model on downstream task...
01/28/2023 05:32:37 PM  [*] Started epoch: 1
01/28/2023 05:32:38 PM  [*] Sat Jan 28 17:32:38 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.098554 | Elapsed: 0.29s | FPR 0.0003 -> TPR 0.0930 & F1 0.1702 | AUC 0.3765
01/28/2023 05:32:47 PM  [*] Sat Jan 28 17:32:47 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.414683 | Elapsed: 9.10s | FPR 0.0003 -> TPR 0.4348 & F1 0.6061 | AUC 0.8752
01/28/2023 05:32:56 PM  [*] Sat Jan 28 17:32:56 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.398045 | Elapsed: 9.05s | FPR 0.0003 -> TPR 0.5735 & F1 0.7290 | AUC 0.8911
01/28/2023 05:33:00 PM  [*] Sat Jan 28 17:33:00 2023:    1    | Tr.loss: 0.479504 | Elapsed:   22.18  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8322
01/28/2023 05:33:00 PM  [*] Started epoch: 2
01/28/2023 05:33:00 PM  [*] Sat Jan 28 17:33:00 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.194459 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.8182 & F1 0.9000 | AUC 0.9864
01/28/2023 05:33:09 PM  [*] Sat Jan 28 17:33:09 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.179229 | Elapsed: 9.04s | FPR 0.0003 -> TPR 0.5217 & F1 0.6857 | AUC 0.9612
01/28/2023 05:33:18 PM  [*] Sat Jan 28 17:33:18 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.237509 | Elapsed: 9.07s | FPR 0.0003 -> TPR 0.6406 & F1 0.7810 | AUC 0.9714
01/28/2023 05:33:21 PM  [*] Sat Jan 28 17:33:21 2023:    2    | Tr.loss: 0.263094 | Elapsed:   21.86  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.38 | AUC: 0.9497
01/28/2023 05:33:21 PM  [*] Started epoch: 3
01/28/2023 05:33:21 PM  [*] Sat Jan 28 17:33:21 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.197912 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.8140 & F1 0.8974 | AUC 0.9745
01/28/2023 05:33:31 PM  [*] Sat Jan 28 17:33:31 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.222769 | Elapsed: 9.04s | FPR 0.0003 -> TPR 0.9143 & F1 0.9552 | AUC 0.9824
01/28/2023 05:33:40 PM  [*] Sat Jan 28 17:33:40 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.312467 | Elapsed: 9.05s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091 | AUC 0.9568
01/28/2023 05:33:43 PM  [*] Sat Jan 28 17:33:43 2023:    3    | Tr.loss: 0.191356 | Elapsed:   21.86  s | FPR 0.0003 -> TPR: 0.41 & F1: 0.58 | AUC: 0.9745
01/28/2023 05:33:44 PM [!] Sat Jan 28 17:33:44 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\downstreamTask_pretrained\training_files\1674923623-model.torch
		train time: 1674923623-trainTime.npy
		train losses: 1674923623-trainLosses.npy
		train AUC: 1674923623-auc.npy
		train F1s : 1674923623-trainF1s.npy
		train TPRs: 1674923623-trainTPRs.npy
01/28/2023 05:33:44 PM  [!] Training non_pretrained model on downstream task...
01/28/2023 05:33:44 PM  [*] Started epoch: 1
01/28/2023 05:33:44 PM  [*] Sat Jan 28 17:33:44 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.931144 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0250 & F1 0.0488 | AUC 0.4458
01/28/2023 05:33:50 PM  [*] Sat Jan 28 17:33:50 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.471910 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3944 & F1 0.5657 | AUC 0.8271
01/28/2023 05:33:57 PM  [*] Sat Jan 28 17:33:57 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.376909 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3226 & F1 0.4878 | AUC 0.9024
01/28/2023 05:33:59 PM  [*] Sat Jan 28 17:33:59 2023:    1    | Tr.loss: 0.481070 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8290
01/28/2023 05:33:59 PM  [*] Started epoch: 2
01/28/2023 05:33:59 PM  [*] Sat Jan 28 17:33:59 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.261114 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7083 & F1 0.8293 | AUC 0.9661
01/28/2023 05:34:05 PM  [*] Sat Jan 28 17:34:05 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.224057 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6714 & F1 0.8034 | AUC 0.9376
01/28/2023 05:34:12 PM  [*] Sat Jan 28 17:34:12 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.411396 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7460 & F1 0.8545 | AUC 0.9125
01/28/2023 05:34:14 PM  [*] Sat Jan 28 17:34:14 2023:    2    | Tr.loss: 0.302630 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.30 & F1: 0.46 | AUC: 0.9314
01/28/2023 05:34:14 PM  [*] Started epoch: 3
01/28/2023 05:34:14 PM  [*] Sat Jan 28 17:34:14 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.305747 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7179 & F1 0.8358 | AUC 0.9467
01/28/2023 05:34:20 PM  [*] Sat Jan 28 17:34:20 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.241672 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6429 & F1 0.7826 | AUC 0.9733
01/28/2023 05:34:27 PM  [*] Sat Jan 28 17:34:27 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.221938 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.7746 & F1 0.8730 | AUC 0.9529
01/28/2023 05:34:29 PM  [*] Sat Jan 28 17:34:29 2023:    3    | Tr.loss: 0.236969 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.35 & F1: 0.52 | AUC: 0.9604
01/28/2023 05:34:30 PM [!] Sat Jan 28 17:34:30 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\downstreamTask_non_pretrained\training_files\1674923669-model.torch
		train time: 1674923669-trainTime.npy
		train losses: 1674923669-trainLosses.npy
		train AUC: 1674923669-auc.npy
		train F1s : 1674923669-trainF1s.npy
		train TPRs: 1674923669-trainTPRs.npy
01/28/2023 05:34:30 PM  [!] Training full_data model on downstream task...
01/28/2023 05:34:30 PM  [*] Started epoch: 1
01/28/2023 05:34:30 PM  [*] Sat Jan 28 17:34:30 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 3.050142 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.1818 & F1 0.3077 | AUC 0.5114
01/28/2023 05:34:36 PM  [*] Sat Jan 28 17:34:36 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.406594 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5672 & F1 0.7238 | AUC 0.8806
01/28/2023 05:34:43 PM  [*] Sat Jan 28 17:34:43 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.382399 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9202
01/28/2023 05:34:49 PM  [*] Sat Jan 28 17:34:49 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.329035 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4032 & F1 0.5747 | AUC 0.9274
01/28/2023 05:34:55 PM  [*] Sat Jan 28 17:34:55 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.324160 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.5672 & F1 0.7238 | AUC 0.9362
01/28/2023 05:35:01 PM  [*] Sat Jan 28 17:35:01 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.249373 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4627 & F1 0.6327 | AUC 0.9331
01/28/2023 05:35:07 PM  [*] Sat Jan 28 17:35:07 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.168557 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7778 & F1 0.8750 | AUC 0.9658
01/28/2023 05:35:13 PM  [*] Sat Jan 28 17:35:13 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.184503 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.9306 & F1 0.9640 | AUC 0.9881
01/28/2023 05:35:20 PM  [*] Sat Jan 28 17:35:20 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.159426 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8400 & F1 0.9130 | AUC 0.9728
01/28/2023 05:35:26 PM  [*] Sat Jan 28 17:35:26 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.226025 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.6923 & F1 0.8182 | AUC 0.9820
01/28/2023 05:35:32 PM [!] Learning rate: 2.5e-05
01/28/2023 05:35:32 PM  [*] Sat Jan 28 17:35:32 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.315370 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5303 & F1 0.6931 | AUC 0.9528
01/28/2023 05:35:38 PM  [*] Sat Jan 28 17:35:38 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.125761 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.9231 & F1 0.9600 | AUC 0.9903
01/28/2023 05:35:45 PM  [*] Sat Jan 28 17:35:45 2023:    1    | Tr.loss: 0.297397 | Elapsed:   75.31  s | FPR 0.0003 -> TPR: 0.08 & F1: 0.15 | AUC: 0.9358
01/28/2023 05:35:45 PM  [*] Started epoch: 2
01/28/2023 05:35:45 PM  [*] Sat Jan 28 17:35:45 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.146099 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.9091 & F1 0.9524 | AUC 0.9875
01/28/2023 05:35:52 PM  [*] Sat Jan 28 17:35:52 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.143082 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.9385 & F1 0.9683 | AUC 0.9895
01/28/2023 05:35:58 PM  [*] Sat Jan 28 17:35:58 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.240101 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5522 & F1 0.7115 | AUC 0.9774
01/28/2023 05:36:04 PM  [*] Sat Jan 28 17:36:04 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.112759 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8060 & F1 0.8926 | AUC 0.9783
01/28/2023 05:36:10 PM  [*] Sat Jan 28 17:36:10 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.105699 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.9508 & F1 0.9748 | AUC 0.9954
01/28/2023 05:36:16 PM  [*] Sat Jan 28 17:36:16 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.170613 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.8529 & F1 0.9206 | AUC 0.9835
01/28/2023 05:36:23 PM  [*] Sat Jan 28 17:36:23 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.094575 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7286 & F1 0.8430 | AUC 0.9852
01/28/2023 05:36:29 PM  [*] Sat Jan 28 17:36:29 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.192058 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7794 & F1 0.8760 | AUC 0.9710
01/28/2023 05:36:35 PM  [*] Sat Jan 28 17:36:35 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.172456 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8413 & F1 0.9138 | AUC 0.9957
01/28/2023 05:36:36 PM [!] Learning rate: 2.5e-06
01/28/2023 05:36:41 PM  [*] Sat Jan 28 17:36:41 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.150345 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7937 & F1 0.8850 | AUC 0.9863
01/28/2023 05:36:47 PM  [*] Sat Jan 28 17:36:47 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.170245 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8784 & F1 0.9353 | AUC 0.9875
01/28/2023 05:36:53 PM  [*] Sat Jan 28 17:36:53 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.186118 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6712 & F1 0.8033 | AUC 0.9721
01/28/2023 05:37:01 PM  [*] Sat Jan 28 17:37:01 2023:    2    | Tr.loss: 0.167199 | Elapsed:   75.22  s | FPR 0.0003 -> TPR: 0.46 & F1: 0.63 | AUC: 0.9806
01/28/2023 05:37:01 PM  [*] Started epoch: 3
01/28/2023 05:37:01 PM  [*] Sat Jan 28 17:37:01 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.100524 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.9250 & F1 0.9610 | AUC 0.9958
01/28/2023 05:37:07 PM  [*] Sat Jan 28 17:37:07 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.094743 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.9324 & F1 0.9650 | AUC 0.9932
01/28/2023 05:37:13 PM  [*] Sat Jan 28 17:37:13 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.146460 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8406 & F1 0.9134 | AUC 0.9846
01/28/2023 05:37:19 PM  [*] Sat Jan 28 17:37:19 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.116401 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.9677 & F1 0.9836 | AUC 0.9970
01/28/2023 05:37:25 PM  [*] Sat Jan 28 17:37:25 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.180459 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8800 & F1 0.9362 | AUC 0.9685
01/28/2023 05:37:32 PM  [*] Sat Jan 28 17:37:32 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.134253 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7826 & F1 0.8780 | AUC 0.9827
01/28/2023 05:37:38 PM  [*] Sat Jan 28 17:37:38 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.088376 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8816 & F1 0.9371 | AUC 0.9808
01/28/2023 05:37:39 PM [!] Learning rate: 2.5000000000000004e-07
01/28/2023 05:37:44 PM  [*] Sat Jan 28 17:37:44 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.151928 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8060 & F1 0.8926 | AUC 0.9810
01/28/2023 05:37:50 PM  [*] Sat Jan 28 17:37:50 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.173422 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8889 & F1 0.9412 | AUC 0.9881
01/28/2023 05:37:56 PM  [*] Sat Jan 28 17:37:56 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.122480 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8551 & F1 0.9219 | AUC 0.9939
01/28/2023 05:38:03 PM  [*] Sat Jan 28 17:38:03 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.152576 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.5735 & F1 0.7290 | AUC 0.9830
01/28/2023 05:38:09 PM  [*] Sat Jan 28 17:38:09 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.134639 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.8182 & F1 0.9000 | AUC 0.9750
01/28/2023 05:38:16 PM  [*] Sat Jan 28 17:38:16 2023:    3    | Tr.loss: 0.159202 | Elapsed:   75.45  s | FPR 0.0003 -> TPR: 0.49 & F1: 0.66 | AUC: 0.9825
01/28/2023 05:38:17 PM [!] Sat Jan 28 17:38:17 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969\downstreamTask_full_data\training_files\1674923896-model.torch
		train time: 1674923896-trainTime.npy
		train losses: 1674923896-trainLosses.npy
		train AUC: 1674923896-auc.npy
		train F1s : 1674923896-trainF1s.npy
		train TPRs: 1674923896-trainTPRs.npy
01/28/2023 05:38:17 PM  [*] Evaluating pretrained model on test set...
01/28/2023 05:38:22 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0324 | F1: 0.0628
01/28/2023 05:38:22 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1988 | F1: 0.3316
01/28/2023 05:38:22 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3336 | F1: 0.5000
01/28/2023 05:38:22 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.4306 | F1: 0.6010
01/28/2023 05:38:22 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.5019 | F1: 0.6646
01/28/2023 05:38:22 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.6165 | F1: 0.7509
01/28/2023 05:38:22 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7701 | F1: 0.8303
01/28/2023 05:38:22 PM  [*] Evaluating non_pretrained model on test set...
01/28/2023 05:38:27 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0965 | F1: 0.1759
01/28/2023 05:38:27 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1488 | F1: 0.2590
01/28/2023 05:38:27 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2273 | F1: 0.3701
01/28/2023 05:38:27 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.2606 | F1: 0.4126
01/28/2023 05:38:27 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.2970 | F1: 0.4550
01/28/2023 05:38:27 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.4386 | F1: 0.5991
01/28/2023 05:38:27 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.6755 | F1: 0.7674
01/28/2023 05:38:27 PM  [*] Evaluating full_data model on test set...
01/28/2023 05:38:32 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0171 | F1: 0.0337
01/28/2023 05:38:32 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2694 | F1: 0.4244
01/28/2023 05:38:32 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3372 | F1: 0.5041
01/28/2023 05:38:32 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3867 | F1: 0.5567
01/28/2023 05:38:32 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4374 | F1: 0.6050
01/28/2023 05:38:32 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5388 | F1: 0.6888
01/28/2023 05:38:32 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8102 | F1: 0.8549
01/28/2023 05:38:32 PM  [!] Finished pre-training evaluation over 3 splits! Saved metrics to:
	C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\pretrain_epoch_analysis_mask_every_epoch_1674909969/results_full.json
01/28/2023 05:38:32 PM  [*] Evaluating pre-training length utility -- looping over per-epoch models...
01/28/2023 05:38:32 PM  [!] Evaluating model from split: 0 | epoch: 1
01/28/2023 05:38:32 PM  [*] Started epoch: 1
01/28/2023 05:38:33 PM  [*] Sat Jan 28 17:38:33 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.832578 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2874
01/28/2023 05:38:39 PM  [*] Sat Jan 28 17:38:39 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.481119 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.0556 & F1 0.1053 | AUC 0.8080
01/28/2023 05:38:45 PM  [*] Sat Jan 28 17:38:45 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.511088 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.2000 & F1 0.3333 | AUC 0.8013
01/28/2023 05:38:47 PM  [*] Sat Jan 28 17:38:47 2023:    1    | Tr.loss: 0.491200 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8172
01/28/2023 05:38:47 PM  [*] Started epoch: 2
01/28/2023 05:38:48 PM  [*] Sat Jan 28 17:38:48 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.312761 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6000 & F1 0.7500 | AUC 0.9281
01/28/2023 05:38:54 PM  [*] Sat Jan 28 17:38:54 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.347100 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4521 & F1 0.6226 | AUC 0.9209
01/28/2023 05:39:00 PM  [*] Sat Jan 28 17:39:00 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.212987 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4242 & F1 0.5957 | AUC 0.9639
01/28/2023 05:39:03 PM  [*] Sat Jan 28 17:39:03 2023:    2    | Tr.loss: 0.311468 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.21 & F1: 0.35 | AUC: 0.9270
01/28/2023 05:39:03 PM  [*] Started epoch: 3
01/28/2023 05:39:03 PM  [*] Sat Jan 28 17:39:03 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.244729 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8293 & F1 0.9067 | AUC 0.9650
01/28/2023 05:39:09 PM  [*] Sat Jan 28 17:39:09 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.279000 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6562 & F1 0.7925 | AUC 0.9646
01/28/2023 05:39:15 PM  [*] Sat Jan 28 17:39:15 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.166737 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8806 & F1 0.9365 | AUC 0.9860
01/28/2023 05:39:18 PM  [*] Sat Jan 28 17:39:18 2023:    3    | Tr.loss: 0.219395 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.33 & F1: 0.49 | AUC: 0.9669
01/28/2023 05:39:23 PM  [!] Evaluating model from split: 0 | epoch: 2
01/28/2023 05:39:23 PM  [*] Started epoch: 1
01/28/2023 05:39:23 PM  [*] Sat Jan 28 17:39:23 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.434061 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2229
01/28/2023 05:39:30 PM  [*] Sat Jan 28 17:39:30 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.440110 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.1449 & F1 0.2532 | AUC 0.8331
01/28/2023 05:39:36 PM  [*] Sat Jan 28 17:39:36 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.422105 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5970 & F1 0.7477 | AUC 0.9068
01/28/2023 05:39:38 PM  [*] Sat Jan 28 17:39:38 2023:    1    | Tr.loss: 0.493237 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8238
01/28/2023 05:39:38 PM  [*] Started epoch: 2
01/28/2023 05:39:38 PM  [*] Sat Jan 28 17:39:38 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.395680 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.3404 & F1 0.5079 | AUC 0.8511
01/28/2023 05:39:45 PM  [*] Sat Jan 28 17:39:45 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.295277 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5942 & F1 0.7455 | AUC 0.9490
01/28/2023 05:39:51 PM  [*] Sat Jan 28 17:39:51 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.246092 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4328 & F1 0.6042 | AUC 0.9435
01/28/2023 05:39:53 PM  [*] Sat Jan 28 17:39:53 2023:    2    | Tr.loss: 0.297745 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.23 & F1: 0.37 | AUC: 0.9363
01/28/2023 05:39:53 PM  [*] Started epoch: 3
01/28/2023 05:39:53 PM  [*] Sat Jan 28 17:39:53 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.219576 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8095 & F1 0.8947 | AUC 0.9621
01/28/2023 05:40:00 PM  [*] Sat Jan 28 17:40:00 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.248973 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4627 & F1 0.6327 | AUC 0.9371
01/28/2023 05:40:06 PM  [*] Sat Jan 28 17:40:06 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.158932 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7639 & F1 0.8661 | AUC 0.9732
01/28/2023 05:40:08 PM  [*] Sat Jan 28 17:40:08 2023:    3    | Tr.loss: 0.213251 | Elapsed:   15.15  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.35 | AUC: 0.9690
01/28/2023 05:40:13 PM  [!] Evaluating model from split: 0 | epoch: 3
01/28/2023 05:40:14 PM  [*] Started epoch: 1
01/28/2023 05:40:14 PM  [*] Sat Jan 28 17:40:14 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.597039 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2203
01/28/2023 05:40:20 PM  [*] Sat Jan 28 17:40:20 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.434333 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.2676 & F1 0.4222 | AUC 0.8460
01/28/2023 05:40:27 PM  [*] Sat Jan 28 17:40:27 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.399488 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.2836 & F1 0.4419 | AUC 0.8915
01/28/2023 05:40:29 PM  [*] Sat Jan 28 17:40:29 2023:    1    | Tr.loss: 0.471354 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8344
01/28/2023 05:40:29 PM  [*] Started epoch: 2
01/28/2023 05:40:29 PM  [*] Sat Jan 28 17:40:29 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.364617 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.3250 & F1 0.4906 | AUC 0.9313
01/28/2023 05:40:35 PM  [*] Sat Jan 28 17:40:35 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.161826 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8592 & F1 0.9242 | AUC 0.9820
01/28/2023 05:40:42 PM  [*] Sat Jan 28 17:40:42 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.212160 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7042 & F1 0.8264 | AUC 0.9677
01/28/2023 05:40:44 PM  [*] Sat Jan 28 17:40:44 2023:    2    | Tr.loss: 0.294740 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.23 & F1: 0.37 | AUC: 0.9372
01/28/2023 05:40:44 PM  [*] Started epoch: 3
01/28/2023 05:40:44 PM  [*] Sat Jan 28 17:40:44 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.328688 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.6591 & F1 0.7945 | AUC 0.9261
01/28/2023 05:40:51 PM  [*] Sat Jan 28 17:40:51 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.141153 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8806 & F1 0.9365 | AUC 0.9916
01/28/2023 05:40:57 PM  [*] Sat Jan 28 17:40:57 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.285315 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9542
01/28/2023 05:40:59 PM  [*] Sat Jan 28 17:40:59 2023:    3    | Tr.loss: 0.216463 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.51 | AUC: 0.9672
01/28/2023 05:41:04 PM  [!] Evaluating model from split: 0 | epoch: 4
01/28/2023 05:41:05 PM  [*] Started epoch: 1
01/28/2023 05:41:05 PM  [*] Sat Jan 28 17:41:05 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.805678 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0250 & F1 0.0488 | AUC 0.4062
01/28/2023 05:41:11 PM  [*] Sat Jan 28 17:41:11 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.507490 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.1067 & F1 0.1928 | AUC 0.7872
01/28/2023 05:41:18 PM  [*] Sat Jan 28 17:41:18 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.326464 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.1690 & F1 0.2892 | AUC 0.9320
01/28/2023 05:41:20 PM  [*] Sat Jan 28 17:41:20 2023:    1    | Tr.loss: 0.475758 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8185
01/28/2023 05:41:20 PM  [*] Started epoch: 2
01/28/2023 05:41:20 PM  [*] Sat Jan 28 17:41:20 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.431458 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.1860 & F1 0.3137 | AUC 0.8317
01/28/2023 05:41:26 PM  [*] Sat Jan 28 17:41:26 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.371742 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.2656 & F1 0.4198 | AUC 0.8958
01/28/2023 05:41:33 PM  [*] Sat Jan 28 17:41:33 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.246399 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7407 & F1 0.8511 | AUC 0.9533
01/28/2023 05:41:35 PM  [*] Sat Jan 28 17:41:35 2023:    2    | Tr.loss: 0.305873 | Elapsed:   15.10  s | FPR 0.0003 -> TPR: 0.21 & F1: 0.34 | AUC: 0.9304
01/28/2023 05:41:35 PM  [*] Started epoch: 3
01/28/2023 05:41:35 PM  [*] Sat Jan 28 17:41:35 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.241194 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5870 & F1 0.7397 | AUC 0.9734
01/28/2023 05:41:41 PM  [*] Sat Jan 28 17:41:41 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.213876 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8000 & F1 0.8889 | AUC 0.9811
01/28/2023 05:41:48 PM  [*] Sat Jan 28 17:41:48 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.177462 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7812 & F1 0.8772 | AUC 0.9809
01/28/2023 05:41:50 PM  [*] Sat Jan 28 17:41:50 2023:    3    | Tr.loss: 0.214115 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.35 & F1: 0.52 | AUC: 0.9686
01/28/2023 05:41:55 PM  [!] Evaluating model from split: 0 | epoch: 5
01/28/2023 05:41:56 PM  [*] Started epoch: 1
01/28/2023 05:41:56 PM  [*] Sat Jan 28 17:41:56 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.619720 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2035
01/28/2023 05:42:02 PM  [*] Sat Jan 28 17:42:02 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.434077 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.3803 & F1 0.5510 | AUC 0.8781
01/28/2023 05:42:08 PM  [*] Sat Jan 28 17:42:08 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.500627 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.3485 & F1 0.5169 | AUC 0.8819
01/28/2023 05:42:11 PM  [*] Sat Jan 28 17:42:11 2023:    1    | Tr.loss: 0.476725 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8262
01/28/2023 05:42:11 PM  [*] Started epoch: 2
01/28/2023 05:42:11 PM  [*] Sat Jan 28 17:42:11 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.375839 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.4524 & F1 0.6230 | AUC 0.8696
01/28/2023 05:42:17 PM  [*] Sat Jan 28 17:42:17 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.235708 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.5758 & F1 0.7308 | AUC 0.9325
01/28/2023 05:42:23 PM  [*] Sat Jan 28 17:42:23 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.272113 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.6094 & F1 0.7573 | AUC 0.9575
01/28/2023 05:42:26 PM  [*] Sat Jan 28 17:42:26 2023:    2    | Tr.loss: 0.306587 | Elapsed:   15.12  s | FPR 0.0003 -> TPR: 0.23 & F1: 0.38 | AUC: 0.9302
01/28/2023 05:42:26 PM  [*] Started epoch: 3
01/28/2023 05:42:26 PM  [*] Sat Jan 28 17:42:26 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.299280 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.3333 & F1 0.5000 | AUC 0.9246
01/28/2023 05:42:32 PM  [*] Sat Jan 28 17:42:32 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.260786 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5882 & F1 0.7407 | AUC 0.9648
01/28/2023 05:42:39 PM  [*] Sat Jan 28 17:42:39 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.226722 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6875 & F1 0.8148 | AUC 0.9586
01/28/2023 05:42:41 PM  [*] Sat Jan 28 17:42:41 2023:    3    | Tr.loss: 0.222712 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9657
01/28/2023 05:42:46 PM  [!] Evaluating model from split: 0 | epoch: 6
01/28/2023 05:42:47 PM  [*] Started epoch: 1
01/28/2023 05:42:47 PM  [*] Sat Jan 28 17:42:47 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.468332 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2516
01/28/2023 05:42:53 PM  [*] Sat Jan 28 17:42:53 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.413035 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4853 & F1 0.6535 | AUC 0.8658
01/28/2023 05:42:59 PM  [*] Sat Jan 28 17:42:59 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.428768 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.2985 & F1 0.4598 | AUC 0.9123
01/28/2023 05:43:02 PM  [*] Sat Jan 28 17:43:02 2023:    1    | Tr.loss: 0.466594 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8392
01/28/2023 05:43:02 PM  [*] Started epoch: 2
01/28/2023 05:43:02 PM  [*] Sat Jan 28 17:43:02 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.436630 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.1860 & F1 0.3137 | AUC 0.8749
01/28/2023 05:43:08 PM  [*] Sat Jan 28 17:43:08 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.310670 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6000 & F1 0.7500 | AUC 0.9284
01/28/2023 05:43:14 PM  [*] Sat Jan 28 17:43:14 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.114589 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.9385 & F1 0.9683 | AUC 0.9890
01/28/2023 05:43:17 PM  [*] Sat Jan 28 17:43:17 2023:    2    | Tr.loss: 0.287030 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.11 & F1: 0.20 | AUC: 0.9410
01/28/2023 05:43:17 PM  [*] Started epoch: 3
01/28/2023 05:43:17 PM  [*] Sat Jan 28 17:43:17 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.213814 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7872 & F1 0.8810 | AUC 0.9612
01/28/2023 05:43:23 PM  [*] Sat Jan 28 17:43:23 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.256605 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8551 & F1 0.9219 | AUC 0.9710
01/28/2023 05:43:29 PM  [*] Sat Jan 28 17:43:29 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.285063 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8026 & F1 0.8905 | AUC 0.9397
01/28/2023 05:43:32 PM  [*] Sat Jan 28 17:43:32 2023:    3    | Tr.loss: 0.212926 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.28 & F1: 0.44 | AUC: 0.9684
01/28/2023 05:43:37 PM  [!] Evaluating model from split: 0 | epoch: 7
01/28/2023 05:43:38 PM  [*] Started epoch: 1
01/28/2023 05:43:38 PM  [*] Sat Jan 28 17:43:38 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.023798 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3006
01/28/2023 05:43:44 PM  [*] Sat Jan 28 17:43:44 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.410595 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.3016 & F1 0.4634 | AUC 0.8318
01/28/2023 05:43:50 PM  [*] Sat Jan 28 17:43:50 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.290240 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6133 & F1 0.7603 | AUC 0.9216
01/28/2023 05:43:53 PM  [*] Sat Jan 28 17:43:53 2023:    1    | Tr.loss: 0.501401 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8101
01/28/2023 05:43:53 PM  [*] Started epoch: 2
01/28/2023 05:43:53 PM  [*] Sat Jan 28 17:43:53 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.367392 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.4146 & F1 0.5862 | AUC 0.9162
01/28/2023 05:43:59 PM  [*] Sat Jan 28 17:43:59 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.369626 | Elapsed: 6.53s | FPR 0.0003 -> TPR 0.4583 & F1 0.6286 | AUC 0.9147
01/28/2023 05:44:06 PM  [*] Sat Jan 28 17:44:06 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.294421 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.3235 & F1 0.4889 | AUC 0.9384
01/28/2023 05:44:14 PM  [*] Sat Jan 28 17:44:14 2023:    2    | Tr.loss: 0.315615 | Elapsed:   21.45  s | FPR 0.0003 -> TPR: 0.12 & F1: 0.21 | AUC: 0.9251
01/28/2023 05:44:14 PM  [*] Started epoch: 3
01/28/2023 05:44:14 PM  [*] Sat Jan 28 17:44:14 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.244844 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6977 & F1 0.8219 | AUC 0.9657
01/28/2023 05:44:20 PM  [*] Sat Jan 28 17:44:20 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.261719 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4545 & F1 0.6250 | AUC 0.9385
01/28/2023 05:44:27 PM  [*] Sat Jan 28 17:44:27 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.138999 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.8281 & F1 0.9060 | AUC 0.9878
01/28/2023 05:44:29 PM  [*] Sat Jan 28 17:44:29 2023:    3    | Tr.loss: 0.220709 | Elapsed:   15.23  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.45 | AUC: 0.9664
01/28/2023 05:44:34 PM  [!] Evaluating model from split: 0 | epoch: 8
01/28/2023 05:44:35 PM  [*] Started epoch: 1
01/28/2023 05:44:35 PM  [*] Sat Jan 28 17:44:35 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.828320 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3052
01/28/2023 05:44:42 PM  [*] Sat Jan 28 17:44:42 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.356561 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.2836 & F1 0.4419 | AUC 0.8566
01/28/2023 05:44:48 PM  [*] Sat Jan 28 17:44:48 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.214882 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.1733 & F1 0.2955 | AUC 0.9067
01/28/2023 05:44:50 PM  [*] Sat Jan 28 17:44:50 2023:    1    | Tr.loss: 0.476774 | Elapsed:   15.22  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8296
01/28/2023 05:44:50 PM  [*] Started epoch: 2
01/28/2023 05:44:50 PM  [*] Sat Jan 28 17:44:50 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.344851 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6250 & F1 0.7692 | AUC 0.8724
01/28/2023 05:44:57 PM  [*] Sat Jan 28 17:44:57 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.270890 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.8310 & F1 0.9077 | AUC 0.9645
01/28/2023 05:45:03 PM  [*] Sat Jan 28 17:45:03 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.303210 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.2794 & F1 0.4368 | AUC 0.9311
01/28/2023 05:45:06 PM  [*] Sat Jan 28 17:45:06 2023:    2    | Tr.loss: 0.294711 | Elapsed:   15.39  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.36 | AUC: 0.9373
01/28/2023 05:45:06 PM  [*] Started epoch: 3
01/28/2023 05:45:06 PM  [*] Sat Jan 28 17:45:06 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.206106 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8222 & F1 0.9024 | AUC 0.9743
01/28/2023 05:45:12 PM  [*] Sat Jan 28 17:45:12 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.238388 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.5821 & F1 0.7358 | AUC 0.9688
01/28/2023 05:45:18 PM  [*] Sat Jan 28 17:45:18 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.198926 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.6812 & F1 0.8103 | AUC 0.9715
01/28/2023 05:45:21 PM  [*] Sat Jan 28 17:45:21 2023:    3    | Tr.loss: 0.219897 | Elapsed:   15.19  s | FPR 0.0003 -> TPR: 0.33 & F1: 0.49 | AUC: 0.9667
01/28/2023 05:45:26 PM  [!] Evaluating model from split: 0 | epoch: 9
01/28/2023 05:45:27 PM  [*] Started epoch: 1
01/28/2023 05:45:27 PM  [*] Sat Jan 28 17:45:27 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.726030 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2742
01/28/2023 05:45:33 PM  [*] Sat Jan 28 17:45:33 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.447343 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.1571 & F1 0.2716 | AUC 0.8600
01/28/2023 05:45:39 PM  [*] Sat Jan 28 17:45:39 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.296123 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3611 & F1 0.5306 | AUC 0.9110
01/28/2023 05:45:42 PM  [*] Sat Jan 28 17:45:42 2023:    1    | Tr.loss: 0.483700 | Elapsed:   14.98  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8212
01/28/2023 05:45:42 PM  [*] Started epoch: 2
01/28/2023 05:45:42 PM  [*] Sat Jan 28 17:45:42 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.352943 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7600 & F1 0.8636 | AUC 0.9043
01/28/2023 05:45:48 PM  [*] Sat Jan 28 17:45:48 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.336042 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.5075 & F1 0.6733 | AUC 0.9439
01/28/2023 05:45:54 PM  [*] Sat Jan 28 17:45:54 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.268994 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9398
01/28/2023 05:45:57 PM  [*] Sat Jan 28 17:45:57 2023:    2    | Tr.loss: 0.306189 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.15 & F1: 0.26 | AUC: 0.9304
01/28/2023 05:45:57 PM  [*] Started epoch: 3
01/28/2023 05:45:57 PM  [*] Sat Jan 28 17:45:57 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.272142 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5946 & F1 0.7458 | AUC 0.9650
01/28/2023 05:46:03 PM  [*] Sat Jan 28 17:46:03 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.319881 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.3056 & F1 0.4681 | AUC 0.9449
01/28/2023 05:46:09 PM  [*] Sat Jan 28 17:46:09 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.248809 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6230 & F1 0.7677 | AUC 0.9685
01/28/2023 05:46:12 PM  [*] Sat Jan 28 17:46:12 2023:    3    | Tr.loss: 0.221287 | Elapsed:   15.11  s | FPR 0.0003 -> TPR: 0.33 & F1: 0.50 | AUC: 0.9660
01/28/2023 05:46:17 PM  [!] Evaluating model from split: 0 | epoch: 10
01/28/2023 05:46:17 PM  [*] Started epoch: 1
01/28/2023 05:46:17 PM  [*] Sat Jan 28 17:46:17 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.261184 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2280
01/28/2023 05:46:24 PM  [*] Sat Jan 28 17:46:24 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.412179 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.0435 & F1 0.0833 | AUC 0.8163
01/28/2023 05:46:30 PM  [*] Sat Jan 28 17:46:30 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.405046 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.2286 & F1 0.3721 | AUC 0.8633
01/28/2023 05:46:32 PM  [*] Sat Jan 28 17:46:32 2023:    1    | Tr.loss: 0.487180 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8221
01/28/2023 05:46:32 PM  [*] Started epoch: 2
01/28/2023 05:46:33 PM  [*] Sat Jan 28 17:46:33 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.277876 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.7234 & F1 0.8395 | AUC 0.9462
01/28/2023 05:46:39 PM  [*] Sat Jan 28 17:46:39 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.349969 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5873 & F1 0.7400 | AUC 0.9459
01/28/2023 05:46:45 PM  [*] Sat Jan 28 17:46:45 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.279810 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.5429 & F1 0.7037 | AUC 0.9557
01/28/2023 05:46:47 PM  [*] Sat Jan 28 17:46:47 2023:    2    | Tr.loss: 0.306546 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.36 | AUC: 0.9297
01/28/2023 05:46:47 PM  [*] Started epoch: 3
01/28/2023 05:46:48 PM  [*] Sat Jan 28 17:46:48 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.335766 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.5814 & F1 0.7353 | AUC 0.9291
01/28/2023 05:46:54 PM  [*] Sat Jan 28 17:46:54 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.247550 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7206 & F1 0.8376 | AUC 0.9637
01/28/2023 05:47:00 PM  [*] Sat Jan 28 17:47:00 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.223908 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7015 & F1 0.8246 | AUC 0.9774
01/28/2023 05:47:03 PM  [*] Sat Jan 28 17:47:03 2023:    3    | Tr.loss: 0.218613 | Elapsed:   15.11  s | FPR 0.0003 -> TPR: 0.17 & F1: 0.29 | AUC: 0.9672
01/28/2023 05:47:08 PM  [!] Evaluating model from split: 0 | epoch: 11
01/28/2023 05:47:08 PM  [*] Started epoch: 1
01/28/2023 05:47:08 PM  [*] Sat Jan 28 17:47:08 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 4.191028 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2158
01/28/2023 05:47:15 PM  [*] Sat Jan 28 17:47:15 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.368057 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.4062 & F1 0.5778 | AUC 0.8633
01/28/2023 05:47:21 PM  [*] Sat Jan 28 17:47:21 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.344931 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.3382 & F1 0.5055 | AUC 0.8653
01/28/2023 05:47:23 PM  [*] Sat Jan 28 17:47:23 2023:    1    | Tr.loss: 0.499920 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8119
01/28/2023 05:47:23 PM  [*] Started epoch: 2
01/28/2023 05:47:23 PM  [*] Sat Jan 28 17:47:23 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.293649 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7600 & F1 0.8636 | AUC 0.9464
01/28/2023 05:47:30 PM  [*] Sat Jan 28 17:47:30 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.390208 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7385 & F1 0.8496 | AUC 0.9429
01/28/2023 05:47:36 PM  [*] Sat Jan 28 17:47:36 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.281463 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4783 & F1 0.6471 | AUC 0.9355
01/28/2023 05:47:38 PM  [*] Sat Jan 28 17:47:38 2023:    2    | Tr.loss: 0.305416 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.18 & F1: 0.31 | AUC: 0.9310
01/28/2023 05:47:38 PM  [*] Started epoch: 3
01/28/2023 05:47:39 PM  [*] Sat Jan 28 17:47:39 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.262482 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6444 & F1 0.7838 | AUC 0.9491
01/28/2023 05:47:45 PM  [*] Sat Jan 28 17:47:45 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.203692 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6462 & F1 0.7850 | AUC 0.9582
01/28/2023 05:47:51 PM  [*] Sat Jan 28 17:47:51 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.196336 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7595 & F1 0.8633 | AUC 0.9831
01/28/2023 05:47:53 PM  [*] Sat Jan 28 17:47:53 2023:    3    | Tr.loss: 0.207668 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9707
01/28/2023 05:47:58 PM  [!] Evaluating model from split: 0 | epoch: 12
01/28/2023 05:47:59 PM  [*] Started epoch: 1
01/28/2023 05:47:59 PM  [*] Sat Jan 28 17:47:59 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.799993 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3065
01/28/2023 05:48:05 PM  [*] Sat Jan 28 17:48:05 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.479992 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.2836 & F1 0.4419 | AUC 0.8091
01/28/2023 05:48:12 PM  [*] Sat Jan 28 17:48:12 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.281765 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6897 & F1 0.8163 | AUC 0.9577
01/28/2023 05:48:14 PM  [*] Sat Jan 28 17:48:14 2023:    1    | Tr.loss: 0.481823 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8268
01/28/2023 05:48:14 PM  [*] Started epoch: 2
01/28/2023 05:48:14 PM  [*] Sat Jan 28 17:48:14 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.363899 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.3864 & F1 0.5574 | AUC 0.9068
01/28/2023 05:48:20 PM  [*] Sat Jan 28 17:48:20 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.251558 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6061 & F1 0.7547 | AUC 0.9311
01/28/2023 05:48:27 PM  [*] Sat Jan 28 17:48:27 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.156397 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7778 & F1 0.8750 | AUC 0.9821
01/28/2023 05:48:29 PM  [*] Sat Jan 28 17:48:29 2023:    2    | Tr.loss: 0.285562 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.20 & F1: 0.34 | AUC: 0.9423
01/28/2023 05:48:29 PM  [*] Started epoch: 3
01/28/2023 05:48:29 PM  [*] Sat Jan 28 17:48:29 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.271946 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6829 & F1 0.8116 | AUC 0.9523
01/28/2023 05:48:36 PM  [*] Sat Jan 28 17:48:36 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.287264 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3333 & F1 0.5000 | AUC 0.9369
01/28/2023 05:48:42 PM  [*] Sat Jan 28 17:48:42 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.130814 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8571 & F1 0.9231 | AUC 0.9788
01/28/2023 05:48:44 PM  [*] Sat Jan 28 17:48:44 2023:    3    | Tr.loss: 0.215429 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.35 & F1: 0.52 | AUC: 0.9681
01/28/2023 05:48:49 PM  [!] Evaluating model from split: 0 | epoch: 13
01/28/2023 05:48:50 PM  [*] Started epoch: 1
01/28/2023 05:48:50 PM  [*] Sat Jan 28 17:48:50 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.042588 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2667
01/28/2023 05:48:56 PM  [*] Sat Jan 28 17:48:56 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.557734 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.1538 & F1 0.2667 | AUC 0.7793
01/28/2023 05:49:02 PM  [*] Sat Jan 28 17:49:02 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.431394 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.1857 & F1 0.3133 | AUC 0.8624
01/28/2023 05:49:05 PM  [*] Sat Jan 28 17:49:05 2023:    1    | Tr.loss: 0.501693 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8123
01/28/2023 05:49:05 PM  [*] Started epoch: 2
01/28/2023 05:49:05 PM  [*] Sat Jan 28 17:49:05 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.527009 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.1333 & F1 0.2353 | AUC 0.7614
01/28/2023 05:49:11 PM  [*] Sat Jan 28 17:49:11 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.384823 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6308 & F1 0.7736 | AUC 0.9169
01/28/2023 05:49:17 PM  [*] Sat Jan 28 17:49:17 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.221839 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8308 & F1 0.9076 | AUC 0.9793
01/28/2023 05:49:20 PM  [*] Sat Jan 28 17:49:20 2023:    2    | Tr.loss: 0.317027 | Elapsed:   15.12  s | FPR 0.0003 -> TPR: 0.10 & F1: 0.19 | AUC: 0.9248
01/28/2023 05:49:20 PM  [*] Started epoch: 3
01/28/2023 05:49:20 PM  [*] Sat Jan 28 17:49:20 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.289923 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.6364 & F1 0.7778 | AUC 0.9330
01/28/2023 05:49:26 PM  [*] Sat Jan 28 17:49:26 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.189617 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8592 & F1 0.9242 | AUC 0.9806
01/28/2023 05:49:33 PM  [*] Sat Jan 28 17:49:33 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.144270 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6812 & F1 0.8103 | AUC 0.9703
01/28/2023 05:49:35 PM  [*] Sat Jan 28 17:49:35 2023:    3    | Tr.loss: 0.221766 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.23 & F1: 0.38 | AUC: 0.9664
01/28/2023 05:49:40 PM  [!] Evaluating model from split: 0 | epoch: 14
01/28/2023 05:49:41 PM  [*] Started epoch: 1
01/28/2023 05:49:41 PM  [*] Sat Jan 28 17:49:41 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.759799 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3392
01/28/2023 05:49:47 PM  [*] Sat Jan 28 17:49:47 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.440240 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.1905 & F1 0.3200 | AUC 0.8039
01/28/2023 05:49:53 PM  [*] Sat Jan 28 17:49:53 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.286303 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3485 & F1 0.5169 | AUC 0.9403
01/28/2023 05:49:56 PM  [*] Sat Jan 28 17:49:56 2023:    1    | Tr.loss: 0.495107 | Elapsed:   14.98  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8106
01/28/2023 05:49:56 PM  [*] Started epoch: 2
01/28/2023 05:49:56 PM  [*] Sat Jan 28 17:49:56 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.334103 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5200 & F1 0.6842 | AUC 0.8736
01/28/2023 05:50:02 PM  [*] Sat Jan 28 17:50:02 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.327531 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.6111 & F1 0.7586 | AUC 0.9345
01/28/2023 05:50:08 PM  [*] Sat Jan 28 17:50:08 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.207807 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.5970 & F1 0.7477 | AUC 0.9263
01/28/2023 05:50:11 PM  [*] Sat Jan 28 17:50:11 2023:    2    | Tr.loss: 0.307712 | Elapsed:   15.11  s | FPR 0.0003 -> TPR: 0.17 & F1: 0.28 | AUC: 0.9294
01/28/2023 05:50:11 PM  [*] Started epoch: 3
01/28/2023 05:50:11 PM  [*] Sat Jan 28 17:50:11 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.350685 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6905 & F1 0.8169 | AUC 0.9069
01/28/2023 05:50:17 PM  [*] Sat Jan 28 17:50:17 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.179607 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8939 & F1 0.9440 | AUC 0.9697
01/28/2023 05:50:23 PM  [*] Sat Jan 28 17:50:23 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.323353 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.4265 & F1 0.5979 | AUC 0.9545
01/28/2023 05:50:26 PM  [*] Sat Jan 28 17:50:26 2023:    3    | Tr.loss: 0.220533 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.38 | AUC: 0.9663
01/28/2023 05:50:31 PM  [!] Evaluating model from split: 0 | epoch: 15
01/28/2023 05:50:32 PM  [*] Started epoch: 1
01/28/2023 05:50:32 PM  [*] Sat Jan 28 17:50:32 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.430086 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.1304
01/28/2023 05:50:38 PM  [*] Sat Jan 28 17:50:38 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.470397 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.2656 & F1 0.4198 | AUC 0.8715
01/28/2023 05:50:44 PM  [*] Sat Jan 28 17:50:44 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.340537 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.4286 & F1 0.6000 | AUC 0.9052
01/28/2023 05:50:47 PM  [*] Sat Jan 28 17:50:47 2023:    1    | Tr.loss: 0.493314 | Elapsed:   14.96  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8225
01/28/2023 05:50:47 PM  [*] Started epoch: 2
01/28/2023 05:50:47 PM  [*] Sat Jan 28 17:50:47 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.403876 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5946 & F1 0.7458 | AUC 0.9289
01/28/2023 05:50:53 PM  [*] Sat Jan 28 17:50:53 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.233275 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8438 & F1 0.9153 | AUC 0.9722
01/28/2023 05:50:59 PM  [*] Sat Jan 28 17:50:59 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.314416 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6061 & F1 0.7547 | AUC 0.9461
01/28/2023 05:51:02 PM  [*] Sat Jan 28 17:51:02 2023:    2    | Tr.loss: 0.298834 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.23 & F1: 0.38 | AUC: 0.9350
01/28/2023 05:51:02 PM  [*] Started epoch: 3
01/28/2023 05:51:02 PM  [*] Sat Jan 28 17:51:02 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.240518 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.7209 & F1 0.8378 | AUC 0.9723
01/28/2023 05:51:08 PM  [*] Sat Jan 28 17:51:08 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.244644 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8235 & F1 0.9032 | AUC 0.9775
01/28/2023 05:51:14 PM  [*] Sat Jan 28 17:51:14 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.192751 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6471 & F1 0.7857 | AUC 0.9508
01/28/2023 05:51:17 PM  [*] Sat Jan 28 17:51:17 2023:    3    | Tr.loss: 0.217315 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.51 | AUC: 0.9672
01/28/2023 05:51:22 PM  [!] Evaluating model from split: 0 | epoch: 16
01/28/2023 05:51:22 PM  [*] Started epoch: 1
01/28/2023 05:51:23 PM  [*] Sat Jan 28 17:51:23 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.907020 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2198
01/28/2023 05:51:29 PM  [*] Sat Jan 28 17:51:29 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.480156 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.1167 & F1 0.2090 | AUC 0.7958
01/28/2023 05:51:35 PM  [*] Sat Jan 28 17:51:35 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.396982 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.4590 & F1 0.6292 | AUC 0.9149
01/28/2023 05:51:37 PM  [*] Sat Jan 28 17:51:37 2023:    1    | Tr.loss: 0.497767 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8129
01/28/2023 05:51:37 PM  [*] Started epoch: 2
01/28/2023 05:51:38 PM  [*] Sat Jan 28 17:51:38 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.366564 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.5238 & F1 0.6875 | AUC 0.8766
01/28/2023 05:51:44 PM  [*] Sat Jan 28 17:51:44 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.353757 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4000 & F1 0.5714 | AUC 0.9071
01/28/2023 05:51:50 PM  [*] Sat Jan 28 17:51:50 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.307275 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.4605 & F1 0.6306 | AUC 0.9137
01/28/2023 05:51:53 PM  [*] Sat Jan 28 17:51:53 2023:    2    | Tr.loss: 0.305789 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.31 | AUC: 0.9316
01/28/2023 05:51:53 PM  [*] Started epoch: 3
01/28/2023 05:51:53 PM  [*] Sat Jan 28 17:51:53 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.254024 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7895 & F1 0.8824 | AUC 0.9565
01/28/2023 05:51:59 PM  [*] Sat Jan 28 17:51:59 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.232615 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8182 & F1 0.9000 | AUC 0.9802
01/28/2023 05:52:05 PM  [*] Sat Jan 28 17:52:05 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.152666 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8873 & F1 0.9403 | AUC 0.9869
01/28/2023 05:52:08 PM  [*] Sat Jan 28 17:52:08 2023:    3    | Tr.loss: 0.214816 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.45 | AUC: 0.9683
01/28/2023 05:52:13 PM  [!] Evaluating model from split: 0 | epoch: 17
01/28/2023 05:52:13 PM  [*] Started epoch: 1
01/28/2023 05:52:13 PM  [*] Sat Jan 28 17:52:13 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.499211 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3784
01/28/2023 05:52:19 PM  [*] Sat Jan 28 17:52:19 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.421125 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.1739 & F1 0.2963 | AUC 0.8233
01/28/2023 05:52:26 PM  [*] Sat Jan 28 17:52:26 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.363524 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.2286 & F1 0.3721 | AUC 0.9005
01/28/2023 05:52:28 PM  [*] Sat Jan 28 17:52:28 2023:    1    | Tr.loss: 0.487940 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8193
01/28/2023 05:52:28 PM  [*] Started epoch: 2
01/28/2023 05:52:28 PM  [*] Sat Jan 28 17:52:28 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.358207 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9068
01/28/2023 05:52:34 PM  [*] Sat Jan 28 17:52:34 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.252886 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7586 & F1 0.8627 | AUC 0.9715
01/28/2023 05:52:41 PM  [*] Sat Jan 28 17:52:41 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.304383 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7353 & F1 0.8475 | AUC 0.9586
01/28/2023 05:52:43 PM  [*] Sat Jan 28 17:52:43 2023:    2    | Tr.loss: 0.300055 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.31 | AUC: 0.9345
01/28/2023 05:52:43 PM  [*] Started epoch: 3
01/28/2023 05:52:43 PM  [*] Sat Jan 28 17:52:43 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.256021 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7273 & F1 0.8421 | AUC 0.9636
01/28/2023 05:52:50 PM  [*] Sat Jan 28 17:52:50 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.228198 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7463 & F1 0.8547 | AUC 0.9751
01/28/2023 05:52:56 PM  [*] Sat Jan 28 17:52:56 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.271248 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8254 & F1 0.9043 | AUC 0.9661
01/28/2023 05:52:58 PM  [*] Sat Jan 28 17:52:58 2023:    3    | Tr.loss: 0.222395 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.32 & F1: 0.48 | AUC: 0.9665
01/28/2023 05:53:03 PM  [!] Evaluating model from split: 0 | epoch: 18
01/28/2023 05:53:04 PM  [*] Started epoch: 1
01/28/2023 05:53:04 PM  [*] Sat Jan 28 17:53:04 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.460366 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3333
01/28/2023 05:53:10 PM  [*] Sat Jan 28 17:53:10 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.405836 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3380 & F1 0.5053 | AUC 0.8504
01/28/2023 05:53:17 PM  [*] Sat Jan 28 17:53:17 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.337328 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.2857 & F1 0.4444 | AUC 0.8645
01/28/2023 05:53:19 PM  [*] Sat Jan 28 17:53:19 2023:    1    | Tr.loss: 0.499204 | Elapsed:   15.11  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8139
01/28/2023 05:53:19 PM  [*] Started epoch: 2
01/28/2023 05:53:19 PM  [*] Sat Jan 28 17:53:19 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.346696 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.4884 & F1 0.6562 | AUC 0.9192
01/28/2023 05:53:25 PM  [*] Sat Jan 28 17:53:25 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.230020 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.6197 & F1 0.7652 | AUC 0.9441
01/28/2023 05:53:32 PM  [*] Sat Jan 28 17:53:32 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.198840 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.8169 & F1 0.8992 | AUC 0.9573
01/28/2023 05:53:34 PM  [*] Sat Jan 28 17:53:34 2023:    2    | Tr.loss: 0.308863 | Elapsed:   15.15  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.36 | AUC: 0.9305
01/28/2023 05:53:34 PM  [*] Started epoch: 3
01/28/2023 05:53:34 PM  [*] Sat Jan 28 17:53:34 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.289771 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6512 & F1 0.7887 | AUC 0.9424
01/28/2023 05:53:41 PM  [*] Sat Jan 28 17:53:41 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.292121 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7432 & F1 0.8527 | AUC 0.9667
01/28/2023 05:53:47 PM  [*] Sat Jan 28 17:53:47 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.225103 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5570 & F1 0.7154 | AUC 0.9506
01/28/2023 05:53:49 PM  [*] Sat Jan 28 17:53:49 2023:    3    | Tr.loss: 0.223040 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.41 & F1: 0.58 | AUC: 0.9653
01/28/2023 05:53:54 PM  [!] Evaluating model from split: 0 | epoch: 19
01/28/2023 05:53:55 PM  [*] Started epoch: 1
01/28/2023 05:53:55 PM  [*] Sat Jan 28 17:53:55 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.030394 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2390
01/28/2023 05:54:01 PM  [*] Sat Jan 28 17:54:01 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.510956 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.2000 & F1 0.3333 | AUC 0.7843
01/28/2023 05:54:08 PM  [*] Sat Jan 28 17:54:08 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.337279 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.4706 & F1 0.6400 | AUC 0.8819
01/28/2023 05:54:10 PM  [*] Sat Jan 28 17:54:10 2023:    1    | Tr.loss: 0.506825 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8050
01/28/2023 05:54:10 PM  [*] Started epoch: 2
01/28/2023 05:54:10 PM  [*] Sat Jan 28 17:54:10 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.402792 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5952 & F1 0.7463 | AUC 0.9080
01/28/2023 05:54:16 PM  [*] Sat Jan 28 17:54:16 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.238908 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7778 & F1 0.8750 | AUC 0.9691
01/28/2023 05:54:23 PM  [*] Sat Jan 28 17:54:23 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.307429 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.6377 & F1 0.7788 | AUC 0.9402
01/28/2023 05:54:25 PM  [*] Sat Jan 28 17:54:25 2023:    2    | Tr.loss: 0.320099 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.15 & F1: 0.26 | AUC: 0.9218
01/28/2023 05:54:25 PM  [*] Started epoch: 3
01/28/2023 05:54:25 PM  [*] Sat Jan 28 17:54:25 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.292068 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.3784 & F1 0.5490 | AUC 0.9409
01/28/2023 05:54:32 PM  [*] Sat Jan 28 17:54:32 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.262441 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.2535 & F1 0.4045 | AUC 0.9543
01/28/2023 05:54:38 PM  [*] Sat Jan 28 17:54:38 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.179316 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7606 & F1 0.8640 | AUC 0.9752
01/28/2023 05:54:40 PM  [*] Sat Jan 28 17:54:40 2023:    3    | Tr.loss: 0.222455 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.38 | AUC: 0.9657
01/28/2023 05:54:45 PM  [!] Evaluating model from split: 0 | epoch: 20
01/28/2023 05:54:46 PM  [*] Started epoch: 1
01/28/2023 05:54:46 PM  [*] Sat Jan 28 17:54:46 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.378015 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3673
01/28/2023 05:54:52 PM  [*] Sat Jan 28 17:54:52 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.304603 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.1000 & F1 0.1818 | AUC 0.9029
01/28/2023 05:54:58 PM  [*] Sat Jan 28 17:54:58 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.496009 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.1944 & F1 0.3256 | AUC 0.8162
01/28/2023 05:55:01 PM  [*] Sat Jan 28 17:55:01 2023:    1    | Tr.loss: 0.501163 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8131
01/28/2023 05:55:01 PM  [*] Started epoch: 2
01/28/2023 05:55:01 PM  [*] Sat Jan 28 17:55:01 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.315937 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5745 & F1 0.7297 | AUC 0.9061
01/28/2023 05:55:07 PM  [*] Sat Jan 28 17:55:07 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.242265 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4058 & F1 0.5773 | AUC 0.9448
01/28/2023 05:55:13 PM  [*] Sat Jan 28 17:55:13 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.234142 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4848 & F1 0.6531 | AUC 0.9376
01/28/2023 05:55:16 PM  [*] Sat Jan 28 17:55:16 2023:    2    | Tr.loss: 0.310254 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.12 & F1: 0.21 | AUC: 0.9298
01/28/2023 05:55:16 PM  [*] Started epoch: 3
01/28/2023 05:55:16 PM  [*] Sat Jan 28 17:55:16 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.182331 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8462 & F1 0.9167 | AUC 0.9872
01/28/2023 05:55:22 PM  [*] Sat Jan 28 17:55:22 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.166861 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7973 & F1 0.8872 | AUC 0.9704
01/28/2023 05:55:28 PM  [*] Sat Jan 28 17:55:28 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.192996 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8235 & F1 0.9032 | AUC 0.9793
01/28/2023 05:55:31 PM  [*] Sat Jan 28 17:55:31 2023:    3    | Tr.loss: 0.222388 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9661
01/28/2023 05:55:36 PM  [!] Evaluating model from split: 0 | epoch: 21
01/28/2023 05:55:37 PM  [*] Started epoch: 1
01/28/2023 05:55:37 PM  [*] Sat Jan 28 17:55:37 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.874813 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3647
01/28/2023 05:55:43 PM  [*] Sat Jan 28 17:55:43 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.508788 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.3871 & F1 0.5581 | AUC 0.8115
01/28/2023 05:55:49 PM  [*] Sat Jan 28 17:55:49 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.464929 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.0312 & F1 0.0606 | AUC 0.8073
01/28/2023 05:55:52 PM  [*] Sat Jan 28 17:55:52 2023:    1    | Tr.loss: 0.499106 | Elapsed:   15.21  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8122
01/28/2023 05:55:52 PM  [*] Started epoch: 2
01/28/2023 05:55:52 PM  [*] Sat Jan 28 17:55:52 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.359771 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.3590 & F1 0.5283 | AUC 0.8908
01/28/2023 05:55:58 PM  [*] Sat Jan 28 17:55:58 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.209569 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.6338 & F1 0.7759 | AUC 0.9543
01/28/2023 05:56:04 PM  [*] Sat Jan 28 17:56:04 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.313007 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.7246 & F1 0.8403 | AUC 0.9532
01/28/2023 05:56:07 PM  [*] Sat Jan 28 17:56:07 2023:    2    | Tr.loss: 0.309818 | Elapsed:   15.10  s | FPR 0.0003 -> TPR: 0.20 & F1: 0.34 | AUC: 0.9279
01/28/2023 05:56:07 PM  [*] Started epoch: 3
01/28/2023 05:56:07 PM  [*] Sat Jan 28 17:56:07 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.202129 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.9149 & F1 0.9556 | AUC 0.9825
01/28/2023 05:56:13 PM  [*] Sat Jan 28 17:56:13 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.178034 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8413 & F1 0.9138 | AUC 0.9764
01/28/2023 05:56:19 PM  [*] Sat Jan 28 17:56:19 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.172960 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8289 & F1 0.9065 | AUC 0.9649
01/28/2023 05:56:22 PM  [*] Sat Jan 28 17:56:22 2023:    3    | Tr.loss: 0.213879 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.36 | AUC: 0.9683
01/28/2023 05:56:27 PM  [!] Evaluating model from split: 0 | epoch: 22
01/28/2023 05:56:28 PM  [*] Started epoch: 1
01/28/2023 05:56:28 PM  [*] Sat Jan 28 17:56:28 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.386078 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.4316
01/28/2023 05:56:34 PM  [*] Sat Jan 28 17:56:34 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.376203 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.3043 & F1 0.4667 | AUC 0.8906
01/28/2023 05:56:40 PM  [*] Sat Jan 28 17:56:40 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.321213 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.0685 & F1 0.1282 | AUC 0.9143
01/28/2023 05:56:43 PM  [*] Sat Jan 28 17:56:43 2023:    1    | Tr.loss: 0.491029 | Elapsed:   14.95  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8205
01/28/2023 05:56:43 PM  [*] Started epoch: 2
01/28/2023 05:56:43 PM  [*] Sat Jan 28 17:56:43 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.376305 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.3846 & F1 0.5556 | AUC 0.8974
01/28/2023 05:56:49 PM  [*] Sat Jan 28 17:56:49 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.323709 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3824 & F1 0.5532 | AUC 0.9331
01/28/2023 05:56:55 PM  [*] Sat Jan 28 17:56:55 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.184599 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8267 & F1 0.9051 | AUC 0.9744
01/28/2023 05:56:58 PM  [*] Sat Jan 28 17:56:58 2023:    2    | Tr.loss: 0.302774 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.17 & F1: 0.29 | AUC: 0.9332
01/28/2023 05:56:58 PM  [*] Started epoch: 3
01/28/2023 05:56:58 PM  [*] Sat Jan 28 17:56:58 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.248748 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.7857 & F1 0.8800 | AUC 0.9573
01/28/2023 05:57:04 PM  [*] Sat Jan 28 17:57:04 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.170941 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8871 & F1 0.9402 | AUC 0.9864
01/28/2023 05:57:10 PM  [*] Sat Jan 28 17:57:10 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.373060 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5417 & F1 0.7027 | AUC 0.9152
01/28/2023 05:57:13 PM  [*] Sat Jan 28 17:57:13 2023:    3    | Tr.loss: 0.222626 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.51 | AUC: 0.9662
01/28/2023 05:57:18 PM  [!] Evaluating model from split: 0 | epoch: 23
01/28/2023 05:57:19 PM  [*] Started epoch: 1
01/28/2023 05:57:19 PM  [*] Sat Jan 28 17:57:19 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.997005 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3036
01/28/2023 05:57:25 PM  [*] Sat Jan 28 17:57:25 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.518428 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.0635 & F1 0.1194 | AUC 0.8415
01/28/2023 05:57:31 PM  [*] Sat Jan 28 17:57:31 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.345827 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6324 & F1 0.7748 | AUC 0.9060
01/28/2023 05:57:34 PM  [*] Sat Jan 28 17:57:34 2023:    1    | Tr.loss: 0.488755 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8205
01/28/2023 05:57:34 PM  [*] Started epoch: 2
01/28/2023 05:57:34 PM  [*] Sat Jan 28 17:57:34 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.441628 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.2432 & F1 0.3913 | AUC 0.8679
01/28/2023 05:57:40 PM  [*] Sat Jan 28 17:57:40 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.263741 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.7714 & F1 0.8710 | AUC 0.9324
01/28/2023 05:57:46 PM  [*] Sat Jan 28 17:57:46 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.218028 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4848 & F1 0.6531 | AUC 0.9541
01/28/2023 05:57:49 PM  [*] Sat Jan 28 17:57:49 2023:    2    | Tr.loss: 0.314737 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.17 & F1: 0.29 | AUC: 0.9249
01/28/2023 05:57:49 PM  [*] Started epoch: 3
01/28/2023 05:57:49 PM  [*] Sat Jan 28 17:57:49 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.341548 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.2973 & F1 0.4583 | AUC 0.9399
01/28/2023 05:57:55 PM  [*] Sat Jan 28 17:57:55 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.252423 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8919 & F1 0.9429 | AUC 0.9662
01/28/2023 05:58:01 PM  [*] Sat Jan 28 17:58:01 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.238422 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.7031 & F1 0.8257 | AUC 0.9768
01/28/2023 05:58:04 PM  [*] Sat Jan 28 17:58:04 2023:    3    | Tr.loss: 0.223608 | Elapsed:   15.18  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.38 | AUC: 0.9655
01/28/2023 05:58:09 PM  [!] Evaluating model from split: 0 | epoch: 24
01/28/2023 05:58:09 PM  [*] Started epoch: 1
01/28/2023 05:58:10 PM  [*] Sat Jan 28 17:58:10 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.913954 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2926
01/28/2023 05:58:16 PM  [*] Sat Jan 28 17:58:16 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.460545 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.3667 & F1 0.5366 | AUC 0.8408
01/28/2023 05:58:22 PM  [*] Sat Jan 28 17:58:22 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.360969 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.2712 & F1 0.4267 | AUC 0.9119
01/28/2023 05:58:25 PM  [*] Sat Jan 28 17:58:25 2023:    1    | Tr.loss: 0.482008 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8285
01/28/2023 05:58:25 PM  [*] Started epoch: 2
01/28/2023 05:58:25 PM  [*] Sat Jan 28 17:58:25 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.268384 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8261 & F1 0.9048 | AUC 0.9686
01/28/2023 05:58:31 PM  [*] Sat Jan 28 17:58:31 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.269630 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4028 & F1 0.5743 | AUC 0.9360
01/28/2023 05:58:37 PM  [*] Sat Jan 28 17:58:37 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.437042 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3385 & F1 0.5057 | AUC 0.9226
01/28/2023 05:58:40 PM  [*] Sat Jan 28 17:58:40 2023:    2    | Tr.loss: 0.292135 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.36 | AUC: 0.9380
01/28/2023 05:58:40 PM  [*] Started epoch: 3
01/28/2023 05:58:40 PM  [*] Sat Jan 28 17:58:40 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.345953 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5556 & F1 0.7143 | AUC 0.9146
01/28/2023 05:58:46 PM  [*] Sat Jan 28 17:58:46 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.198264 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.5286 & F1 0.6916 | AUC 0.9700
01/28/2023 05:58:52 PM  [*] Sat Jan 28 17:58:52 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.144204 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.9254 & F1 0.9612 | AUC 0.9914
01/28/2023 05:58:55 PM  [*] Sat Jan 28 17:58:55 2023:    3    | Tr.loss: 0.212684 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.33 & F1: 0.50 | AUC: 0.9689
01/28/2023 05:59:00 PM  [!] Evaluating model from split: 0 | epoch: 25
01/28/2023 05:59:00 PM  [*] Started epoch: 1
01/28/2023 05:59:00 PM  [*] Sat Jan 28 17:59:00 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.985227 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.4284
01/28/2023 05:59:07 PM  [*] Sat Jan 28 17:59:07 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.352571 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.2273 & F1 0.3704 | AUC 0.8826
01/28/2023 05:59:13 PM  [*] Sat Jan 28 17:59:13 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.369798 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.2576 & F1 0.4096 | AUC 0.8781
01/28/2023 05:59:15 PM  [*] Sat Jan 28 17:59:15 2023:    1    | Tr.loss: 0.488406 | Elapsed:   15.10  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8192
01/28/2023 05:59:15 PM  [*] Started epoch: 2
01/28/2023 05:59:15 PM  [*] Sat Jan 28 17:59:15 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.385358 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.4545 & F1 0.6250 | AUC 0.8864
01/28/2023 05:59:22 PM  [*] Sat Jan 28 17:59:22 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.305078 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.3421 & F1 0.5098 | AUC 0.9454
01/28/2023 05:59:28 PM  [*] Sat Jan 28 17:59:28 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.256703 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.5156 & F1 0.6804 | AUC 0.9605
01/28/2023 05:59:30 PM  [*] Sat Jan 28 17:59:30 2023:    2    | Tr.loss: 0.307408 | Elapsed:   15.10  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.33 | AUC: 0.9309
01/28/2023 05:59:30 PM  [*] Started epoch: 3
01/28/2023 05:59:31 PM  [*] Sat Jan 28 17:59:31 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.126045 | Elapsed: 0.07s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
01/28/2023 05:59:37 PM  [*] Sat Jan 28 17:59:37 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.305735 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8182 & F1 0.9000 | AUC 0.9532
01/28/2023 05:59:43 PM  [*] Sat Jan 28 17:59:43 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.201236 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8919 & F1 0.9429 | AUC 0.9751
01/28/2023 05:59:45 PM  [*] Sat Jan 28 17:59:45 2023:    3    | Tr.loss: 0.219480 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.39 | AUC: 0.9668
01/28/2023 05:59:50 PM  [!] Evaluating model from split: 0 | epoch: 26
01/28/2023 05:59:51 PM  [*] Started epoch: 1
01/28/2023 05:59:51 PM  [*] Sat Jan 28 17:59:51 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.502475 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3125
01/28/2023 05:59:57 PM  [*] Sat Jan 28 17:59:57 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.447226 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.2727 & F1 0.4286 | AUC 0.8645
01/28/2023 06:00:04 PM  [*] Sat Jan 28 18:00:04 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.385579 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.2857 & F1 0.4444 | AUC 0.8498
01/28/2023 06:00:06 PM  [*] Sat Jan 28 18:00:06 2023:    1    | Tr.loss: 0.484823 | Elapsed:   14.96  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8207
01/28/2023 06:00:06 PM  [*] Started epoch: 2
01/28/2023 06:00:06 PM  [*] Sat Jan 28 18:00:06 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.303365 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6585 & F1 0.7941 | AUC 0.9480
01/28/2023 06:00:12 PM  [*] Sat Jan 28 18:00:12 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.282368 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9474
01/28/2023 06:00:19 PM  [*] Sat Jan 28 18:00:19 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.287838 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4868 & F1 0.6549 | AUC 0.9331
01/28/2023 06:00:21 PM  [*] Sat Jan 28 18:00:21 2023:    2    | Tr.loss: 0.309573 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.18 & F1: 0.31 | AUC: 0.9288
01/28/2023 06:00:21 PM  [*] Started epoch: 3
01/28/2023 06:00:21 PM  [*] Sat Jan 28 18:00:21 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.281292 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.7073 & F1 0.8286 | AUC 0.9544
01/28/2023 06:00:27 PM  [*] Sat Jan 28 18:00:27 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.358107 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6571 & F1 0.7931 | AUC 0.9305
01/28/2023 06:00:34 PM  [*] Sat Jan 28 18:00:34 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.279122 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6769 & F1 0.8073 | AUC 0.9532
01/28/2023 06:00:36 PM  [*] Sat Jan 28 18:00:36 2023:    3    | Tr.loss: 0.218854 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.20 & F1: 0.33 | AUC: 0.9674
01/28/2023 06:00:41 PM  [!] Evaluating model from split: 0 | epoch: 27
01/28/2023 06:00:42 PM  [*] Started epoch: 1
01/28/2023 06:00:42 PM  [*] Sat Jan 28 18:00:42 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.014774 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2802
01/28/2023 06:00:48 PM  [*] Sat Jan 28 18:00:48 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.418524 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.1940 & F1 0.3250 | AUC 0.8435
01/28/2023 06:00:54 PM  [*] Sat Jan 28 18:00:54 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.355260 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.0597 & F1 0.1127 | AUC 0.8869
01/28/2023 06:00:57 PM  [*] Sat Jan 28 18:00:57 2023:    1    | Tr.loss: 0.476700 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8307
01/28/2023 06:00:57 PM  [*] Started epoch: 2
01/28/2023 06:00:57 PM  [*] Sat Jan 28 18:00:57 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.393151 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7143 & F1 0.8333 | AUC 0.8983
01/28/2023 06:01:03 PM  [*] Sat Jan 28 18:01:03 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.358136 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6957 & F1 0.8205 | AUC 0.9360
01/28/2023 06:01:09 PM  [*] Sat Jan 28 18:01:09 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.300926 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5753 & F1 0.7304 | AUC 0.9351
01/28/2023 06:01:12 PM  [*] Sat Jan 28 18:01:12 2023:    2    | Tr.loss: 0.293456 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.12 & F1: 0.22 | AUC: 0.9380
01/28/2023 06:01:12 PM  [*] Started epoch: 3
01/28/2023 06:01:12 PM  [*] Sat Jan 28 18:01:12 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.213403 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8298 & F1 0.9070 | AUC 0.9612
01/28/2023 06:01:18 PM  [*] Sat Jan 28 18:01:18 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.233336 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6957 & F1 0.8205 | AUC 0.9561
01/28/2023 06:01:24 PM  [*] Sat Jan 28 18:01:24 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.225414 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8077 & F1 0.8936 | AUC 0.9639
01/28/2023 06:01:27 PM  [*] Sat Jan 28 18:01:27 2023:    3    | Tr.loss: 0.211404 | Elapsed:   15.11  s | FPR 0.0003 -> TPR: 0.37 & F1: 0.54 | AUC: 0.9689
01/28/2023 06:01:32 PM  [!] Evaluating model from split: 0 | epoch: 28
01/28/2023 06:01:33 PM  [*] Started epoch: 1
01/28/2023 06:01:33 PM  [*] Sat Jan 28 18:01:33 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.980940 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3302
01/28/2023 06:01:39 PM  [*] Sat Jan 28 18:01:39 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.567409 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.1333 & F1 0.2353 | AUC 0.8450
01/28/2023 06:01:45 PM  [*] Sat Jan 28 18:01:45 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.376152 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.2188 & F1 0.3590 | AUC 0.9062
01/28/2023 06:01:48 PM  [*] Sat Jan 28 18:01:48 2023:    1    | Tr.loss: 0.496186 | Elapsed:   15.21  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8150
01/28/2023 06:01:48 PM  [*] Started epoch: 2
01/28/2023 06:01:48 PM  [*] Sat Jan 28 18:01:48 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.389103 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5714 & F1 0.7273 | AUC 0.8961
01/28/2023 06:01:54 PM  [*] Sat Jan 28 18:01:54 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.354075 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9164
01/28/2023 06:02:01 PM  [*] Sat Jan 28 18:02:01 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.275123 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7083 & F1 0.8293 | AUC 0.9236
01/28/2023 06:02:03 PM  [*] Sat Jan 28 18:02:03 2023:    2    | Tr.loss: 0.317508 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.17 & F1: 0.29 | AUC: 0.9244
01/28/2023 06:02:03 PM  [*] Started epoch: 3
01/28/2023 06:02:03 PM  [*] Sat Jan 28 18:02:03 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.328257 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.3111 & F1 0.4746 | AUC 0.9135
01/28/2023 06:02:09 PM  [*] Sat Jan 28 18:02:09 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.192621 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7714 & F1 0.8710 | AUC 0.9733
01/28/2023 06:02:16 PM  [*] Sat Jan 28 18:02:16 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.190131 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8235 & F1 0.9032 | AUC 0.9821
01/28/2023 06:02:18 PM  [*] Sat Jan 28 18:02:18 2023:    3    | Tr.loss: 0.218231 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.51 | AUC: 0.9670
01/28/2023 06:02:23 PM  [!] Evaluating model from split: 0 | epoch: 29
01/28/2023 06:02:24 PM  [*] Started epoch: 1
01/28/2023 06:02:24 PM  [*] Sat Jan 28 18:02:24 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.405799 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2743
01/28/2023 06:02:30 PM  [*] Sat Jan 28 18:02:30 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.486724 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.0694 & F1 0.1299 | AUC 0.6739
01/28/2023 06:02:36 PM  [*] Sat Jan 28 18:02:36 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.368645 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.5238 & F1 0.6875 | AUC 0.9058
01/28/2023 06:02:39 PM  [*] Sat Jan 28 18:02:39 2023:    1    | Tr.loss: 0.488504 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8180
01/28/2023 06:02:39 PM  [*] Started epoch: 2
01/28/2023 06:02:39 PM  [*] Sat Jan 28 18:02:39 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.297953 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5476 & F1 0.7077 | AUC 0.9340
01/28/2023 06:02:45 PM  [*] Sat Jan 28 18:02:45 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.243862 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9528
01/28/2023 06:02:51 PM  [*] Sat Jan 28 18:02:51 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.319032 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.4559 & F1 0.6263 | AUC 0.9382
01/28/2023 06:02:54 PM  [*] Sat Jan 28 18:02:54 2023:    2    | Tr.loss: 0.304575 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.32 | AUC: 0.9331
01/28/2023 06:02:54 PM  [*] Started epoch: 3
01/28/2023 06:02:54 PM  [*] Sat Jan 28 18:02:54 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.232500 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.6087 & F1 0.7568 | AUC 0.9589
01/28/2023 06:03:00 PM  [*] Sat Jan 28 18:03:00 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.201098 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9777
01/28/2023 06:03:06 PM  [*] Sat Jan 28 18:03:06 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.209999 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091 | AUC 0.9650
01/28/2023 06:03:09 PM  [*] Sat Jan 28 18:03:09 2023:    3    | Tr.loss: 0.220522 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.21 & F1: 0.34 | AUC: 0.9666
01/28/2023 06:03:14 PM  [!] Evaluating model from split: 0 | epoch: 30
01/28/2023 06:03:15 PM  [*] Started epoch: 1
01/28/2023 06:03:15 PM  [*] Sat Jan 28 18:03:15 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.002834 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3011
01/28/2023 06:03:21 PM  [*] Sat Jan 28 18:03:21 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.403580 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.2319 & F1 0.3765 | AUC 0.8808
01/28/2023 06:03:27 PM  [*] Sat Jan 28 18:03:27 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.355041 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.3151 & F1 0.4792 | AUC 0.8808
01/28/2023 06:03:30 PM  [*] Sat Jan 28 18:03:30 2023:    1    | Tr.loss: 0.496976 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8149
01/28/2023 06:03:30 PM  [*] Started epoch: 2
01/28/2023 06:03:30 PM  [*] Sat Jan 28 18:03:30 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.447692 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.2955 & F1 0.4561 | AUC 0.8330
01/28/2023 06:03:36 PM  [*] Sat Jan 28 18:03:36 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.248697 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5469 & F1 0.7071 | AUC 0.9581
01/28/2023 06:03:42 PM  [*] Sat Jan 28 18:03:42 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.207753 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.9091 & F1 0.9524 | AUC 0.9808
01/28/2023 06:03:45 PM  [*] Sat Jan 28 18:03:45 2023:    2    | Tr.loss: 0.303938 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.15 & F1: 0.26 | AUC: 0.9325
01/28/2023 06:03:45 PM  [*] Started epoch: 3
01/28/2023 06:03:45 PM  [*] Sat Jan 28 18:03:45 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.236743 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7209 & F1 0.8378 | AUC 0.9657
01/28/2023 06:03:51 PM  [*] Sat Jan 28 18:03:51 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.232654 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.9167 & F1 0.9565 | AUC 0.9777
01/28/2023 06:03:57 PM  [*] Sat Jan 28 18:03:57 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.084446 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6579 & F1 0.7937 | AUC 0.9698
01/28/2023 06:04:00 PM  [*] Sat Jan 28 18:04:00 2023:    3    | Tr.loss: 0.217094 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.30 & F1: 0.46 | AUC: 0.9674
01/28/2023 06:04:05 PM  [!] Evaluating model from split: 1 | epoch: 1
01/28/2023 06:04:06 PM  [*] Started epoch: 1
01/28/2023 06:04:06 PM  [*] Sat Jan 28 18:04:06 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.695216 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.2000 & F1 0.3333 | AUC 0.7977
01/28/2023 06:04:12 PM  [*] Sat Jan 28 18:04:12 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.383263 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3143 & F1 0.4783 | AUC 0.9043
01/28/2023 06:04:18 PM  [*] Sat Jan 28 18:04:18 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.390745 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6029 & F1 0.7523 | AUC 0.8982
01/28/2023 06:04:20 PM  [*] Sat Jan 28 18:04:20 2023:    1    | Tr.loss: 0.465372 | Elapsed:   14.96  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8469
01/28/2023 06:04:20 PM  [*] Started epoch: 2
01/28/2023 06:04:21 PM  [*] Sat Jan 28 18:04:21 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.307658 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9297
01/28/2023 06:04:27 PM  [*] Sat Jan 28 18:04:27 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.325325 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6957 & F1 0.8205 | AUC 0.9163
01/28/2023 06:04:33 PM  [*] Sat Jan 28 18:04:33 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.270797 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7385 & F1 0.8496 | AUC 0.9675
01/28/2023 06:04:36 PM  [*] Sat Jan 28 18:04:36 2023:    2    | Tr.loss: 0.302278 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.18 & F1: 0.31 | AUC: 0.9306
01/28/2023 06:04:36 PM  [*] Started epoch: 3
01/28/2023 06:04:36 PM  [*] Sat Jan 28 18:04:36 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.301277 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.6905 & F1 0.8169 | AUC 0.9329
01/28/2023 06:04:42 PM  [*] Sat Jan 28 18:04:42 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.228071 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7231 & F1 0.8393 | AUC 0.9756
01/28/2023 06:04:48 PM  [*] Sat Jan 28 18:04:48 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.187992 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.9041 & F1 0.9496 | AUC 0.9792
01/28/2023 06:04:51 PM  [*] Sat Jan 28 18:04:51 2023:    3    | Tr.loss: 0.230398 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.32 & F1: 0.49 | AUC: 0.9616
01/28/2023 06:04:56 PM  [!] Evaluating model from split: 1 | epoch: 2
01/28/2023 06:04:56 PM  [*] Started epoch: 1
01/28/2023 06:04:56 PM  [*] Sat Jan 28 18:04:56 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.567677 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.1714 & F1 0.2927 | AUC 0.7990
01/28/2023 06:05:03 PM  [*] Sat Jan 28 18:05:03 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.432839 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.1618 & F1 0.2785 | AUC 0.8681
01/28/2023 06:05:09 PM  [*] Sat Jan 28 18:05:09 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.307006 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7000 & F1 0.8235 | AUC 0.9424
01/28/2023 06:05:11 PM  [*] Sat Jan 28 18:05:11 2023:    1    | Tr.loss: 0.464082 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8486
01/28/2023 06:05:11 PM  [*] Started epoch: 2
01/28/2023 06:05:11 PM  [*] Sat Jan 28 18:05:11 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.345915 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5652 & F1 0.7222 | AUC 0.9034
01/28/2023 06:05:18 PM  [*] Sat Jan 28 18:05:18 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.236773 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4444 & F1 0.6154 | AUC 0.9370
01/28/2023 06:05:24 PM  [*] Sat Jan 28 18:05:24 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.338150 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4559 & F1 0.6263 | AUC 0.9136
01/28/2023 06:05:26 PM  [*] Sat Jan 28 18:05:26 2023:    2    | Tr.loss: 0.297129 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.39 | AUC: 0.9330
01/28/2023 06:05:26 PM  [*] Started epoch: 3
01/28/2023 06:05:26 PM  [*] Sat Jan 28 18:05:26 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.237983 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.8571 & F1 0.9231 | AUC 0.9805
01/28/2023 06:05:33 PM  [*] Sat Jan 28 18:05:33 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.201281 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.4000 & F1 0.5714 | AUC 0.9435
01/28/2023 06:05:39 PM  [*] Sat Jan 28 18:05:39 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.186500 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7353 & F1 0.8475 | AUC 0.9802
01/28/2023 06:05:41 PM  [*] Sat Jan 28 18:05:41 2023:    3    | Tr.loss: 0.236287 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9594
01/28/2023 06:05:46 PM  [!] Evaluating model from split: 1 | epoch: 3
01/28/2023 06:05:47 PM  [*] Started epoch: 1
01/28/2023 06:05:47 PM  [*] Sat Jan 28 18:05:47 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.179604 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.1333 & F1 0.2353 | AUC 0.7801
01/28/2023 06:05:53 PM  [*] Sat Jan 28 18:05:53 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.356326 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.2429 & F1 0.3908 | AUC 0.9067
01/28/2023 06:06:00 PM  [*] Sat Jan 28 18:06:00 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.289089 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5857 & F1 0.7387 | AUC 0.9612
01/28/2023 06:06:02 PM  [*] Sat Jan 28 18:06:02 2023:    1    | Tr.loss: 0.468920 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8496
01/28/2023 06:06:02 PM  [*] Started epoch: 2
01/28/2023 06:06:02 PM  [*] Sat Jan 28 18:06:02 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.350902 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5455 & F1 0.7059 | AUC 0.9433
01/28/2023 06:06:08 PM  [*] Sat Jan 28 18:06:08 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.317032 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4776 & F1 0.6465 | AUC 0.8872
01/28/2023 06:06:15 PM  [*] Sat Jan 28 18:06:15 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.205410 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.6618 & F1 0.7965 | AUC 0.9766
01/28/2023 06:06:17 PM  [*] Sat Jan 28 18:06:17 2023:    2    | Tr.loss: 0.303501 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.30 & F1: 0.46 | AUC: 0.9278
01/28/2023 06:06:17 PM  [*] Started epoch: 3
01/28/2023 06:06:17 PM  [*] Sat Jan 28 18:06:17 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.182691 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091 | AUC 0.9800
01/28/2023 06:06:23 PM  [*] Sat Jan 28 18:06:23 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.251707 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.5135 & F1 0.6786 | AUC 0.9608
01/28/2023 06:06:30 PM  [*] Sat Jan 28 18:06:30 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.260448 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8788 & F1 0.9355 | AUC 0.9744
01/28/2023 06:06:32 PM  [*] Sat Jan 28 18:06:32 2023:    3    | Tr.loss: 0.239888 | Elapsed:   15.11  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9575
01/28/2023 06:06:37 PM  [!] Evaluating model from split: 1 | epoch: 4
01/28/2023 06:06:38 PM  [*] Started epoch: 1
01/28/2023 06:06:38 PM  [*] Sat Jan 28 18:06:38 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.700866 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0698 & F1 0.1304 | AUC 0.7608
01/28/2023 06:06:44 PM  [*] Sat Jan 28 18:06:44 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.508683 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.3115 & F1 0.4750 | AUC 0.8684
01/28/2023 06:06:50 PM  [*] Sat Jan 28 18:06:50 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.263095 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.2714 & F1 0.4270 | AUC 0.8757
01/28/2023 06:06:53 PM  [*] Sat Jan 28 18:06:53 2023:    1    | Tr.loss: 0.431301 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8647
01/28/2023 06:06:53 PM  [*] Started epoch: 2
01/28/2023 06:06:53 PM  [*] Sat Jan 28 18:06:53 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.235139 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7143 & F1 0.8333 | AUC 0.9551
01/28/2023 06:06:59 PM  [*] Sat Jan 28 18:06:59 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.225677 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7162 & F1 0.8346 | AUC 0.9558
01/28/2023 06:07:05 PM  [*] Sat Jan 28 18:07:05 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.327231 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4627 & F1 0.6327 | AUC 0.9326
01/28/2023 06:07:08 PM  [*] Sat Jan 28 18:07:08 2023:    2    | Tr.loss: 0.284780 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.36 | AUC: 0.9411
01/28/2023 06:07:08 PM  [*] Started epoch: 3
01/28/2023 06:07:08 PM  [*] Sat Jan 28 18:07:08 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.396909 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5854 & F1 0.7385 | AUC 0.8950
01/28/2023 06:07:14 PM  [*] Sat Jan 28 18:07:14 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.257708 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.6418 & F1 0.7818 | AUC 0.9417
01/28/2023 06:07:20 PM  [*] Sat Jan 28 18:07:20 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.071234 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.9718 & F1 0.9857 | AUC 0.9985
01/28/2023 06:07:23 PM  [*] Sat Jan 28 18:07:23 2023:    3    | Tr.loss: 0.208694 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.39 | AUC: 0.9696
01/28/2023 06:07:28 PM  [!] Evaluating model from split: 1 | epoch: 5
01/28/2023 06:07:29 PM  [*] Started epoch: 1
01/28/2023 06:07:29 PM  [*] Sat Jan 28 18:07:29 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.801813 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.2000 & F1 0.3333 | AUC 0.7322
01/28/2023 06:07:35 PM  [*] Sat Jan 28 18:07:35 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.525981 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.2941 & F1 0.4545 | AUC 0.7971
01/28/2023 06:07:41 PM  [*] Sat Jan 28 18:07:41 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.554021 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.3750 & F1 0.5455 | AUC 0.8356
01/28/2023 06:07:44 PM  [*] Sat Jan 28 18:07:44 2023:    1    | Tr.loss: 0.448396 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8576
01/28/2023 06:07:44 PM  [*] Started epoch: 2
01/28/2023 06:07:44 PM  [*] Sat Jan 28 18:07:44 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.317326 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.6512 & F1 0.7887 | AUC 0.9358
01/28/2023 06:07:50 PM  [*] Sat Jan 28 18:07:50 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.264281 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4848 & F1 0.6531 | AUC 0.9258
01/28/2023 06:07:56 PM  [*] Sat Jan 28 18:07:56 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.332321 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6119 & F1 0.7593 | AUC 0.9059
01/28/2023 06:07:59 PM  [*] Sat Jan 28 18:07:59 2023:    2    | Tr.loss: 0.300034 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.23 & F1: 0.37 | AUC: 0.9307
01/28/2023 06:07:59 PM  [*] Started epoch: 3
01/28/2023 06:07:59 PM  [*] Sat Jan 28 18:07:59 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.257059 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7674 & F1 0.8684 | AUC 0.9480
01/28/2023 06:08:05 PM  [*] Sat Jan 28 18:08:05 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.271518 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.6522 & F1 0.7895 | AUC 0.9645
01/28/2023 06:08:11 PM  [*] Sat Jan 28 18:08:11 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.202548 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8358 & F1 0.9106 | AUC 0.9801
01/28/2023 06:08:14 PM  [*] Sat Jan 28 18:08:14 2023:    3    | Tr.loss: 0.227567 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.40 & F1: 0.57 | AUC: 0.9623
01/28/2023 06:08:19 PM  [!] Evaluating model from split: 1 | epoch: 6
01/28/2023 06:08:20 PM  [*] Started epoch: 1
01/28/2023 06:08:20 PM  [*] Sat Jan 28 18:08:20 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.162862 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.2553 & F1 0.4068 | AUC 0.6483
01/28/2023 06:08:26 PM  [*] Sat Jan 28 18:08:26 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.332481 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.6176 & F1 0.7636 | AUC 0.9306
01/28/2023 06:08:32 PM  [*] Sat Jan 28 18:08:32 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.356050 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4677 & F1 0.6374 | AUC 0.9022
01/28/2023 06:08:35 PM  [*] Sat Jan 28 18:08:35 2023:    1    | Tr.loss: 0.468078 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8445
01/28/2023 06:08:35 PM  [*] Started epoch: 2
01/28/2023 06:08:35 PM  [*] Sat Jan 28 18:08:35 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.302458 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.5106 & F1 0.6761 | AUC 0.9243
01/28/2023 06:08:41 PM  [*] Sat Jan 28 18:08:41 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.352082 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3944 & F1 0.5657 | AUC 0.8759
01/28/2023 06:08:47 PM  [*] Sat Jan 28 18:08:47 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.178851 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7273 & F1 0.8421 | AUC 0.9673
01/28/2023 06:08:50 PM  [*] Sat Jan 28 18:08:50 2023:    2    | Tr.loss: 0.302946 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.18 & F1: 0.30 | AUC: 0.9294
01/28/2023 06:08:50 PM  [*] Started epoch: 3
01/28/2023 06:08:50 PM  [*] Sat Jan 28 18:08:50 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.258077 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.5714 & F1 0.7273 | AUC 0.9524
01/28/2023 06:08:56 PM  [*] Sat Jan 28 18:08:56 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.275150 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.7273 & F1 0.8421 | AUC 0.9592
01/28/2023 06:09:02 PM  [*] Sat Jan 28 18:09:02 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.240229 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091 | AUC 0.9623
01/28/2023 06:09:05 PM  [*] Sat Jan 28 18:09:05 2023:    3    | Tr.loss: 0.242322 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9560
01/28/2023 06:09:10 PM  [!] Evaluating model from split: 1 | epoch: 7
01/28/2023 06:09:10 PM  [*] Started epoch: 1
01/28/2023 06:09:10 PM  [*] Sat Jan 28 18:09:10 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.594151 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.2264 & F1 0.3692 | AUC 0.5729
01/28/2023 06:09:17 PM  [*] Sat Jan 28 18:09:17 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.415860 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.1304 & F1 0.2308 | AUC 0.8396
01/28/2023 06:09:23 PM  [*] Sat Jan 28 18:09:23 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.378885 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4848 & F1 0.6531 | AUC 0.8837
01/28/2023 06:09:25 PM  [*] Sat Jan 28 18:09:25 2023:    1    | Tr.loss: 0.453345 | Elapsed:   14.96  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8541
01/28/2023 06:09:25 PM  [*] Started epoch: 2
01/28/2023 06:09:25 PM  [*] Sat Jan 28 18:09:25 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.296380 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.6522 & F1 0.7895 | AUC 0.9408
01/28/2023 06:09:32 PM  [*] Sat Jan 28 18:09:32 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.461431 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4167 & F1 0.5882 | AUC 0.9117
01/28/2023 06:09:38 PM  [*] Sat Jan 28 18:09:38 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.333870 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6462 & F1 0.7850 | AUC 0.9453
01/28/2023 06:09:40 PM  [*] Sat Jan 28 18:09:40 2023:    2    | Tr.loss: 0.299803 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.20 & F1: 0.33 | AUC: 0.9303
01/28/2023 06:09:40 PM  [*] Started epoch: 3
01/28/2023 06:09:40 PM  [*] Sat Jan 28 18:09:40 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.229050 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7907 & F1 0.8831 | AUC 0.9535
01/28/2023 06:09:47 PM  [*] Sat Jan 28 18:09:47 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.223086 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5211 & F1 0.6852 | AUC 0.9369
01/28/2023 06:09:53 PM  [*] Sat Jan 28 18:09:53 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.204713 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7879 & F1 0.8814 | AUC 0.9675
01/28/2023 06:09:55 PM  [*] Sat Jan 28 18:09:55 2023:    3    | Tr.loss: 0.236609 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.46 & F1: 0.63 | AUC: 0.9594
01/28/2023 06:10:00 PM  [!] Evaluating model from split: 1 | epoch: 8
01/28/2023 06:10:01 PM  [*] Started epoch: 1
01/28/2023 06:10:01 PM  [*] Sat Jan 28 18:10:01 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.520223 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0476 & F1 0.0909 | AUC 0.6667
01/28/2023 06:10:07 PM  [*] Sat Jan 28 18:10:07 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.339413 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.2154 & F1 0.3544 | AUC 0.8892
01/28/2023 06:10:13 PM  [*] Sat Jan 28 18:10:13 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.278819 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5172 & F1 0.6818 | AUC 0.9158
01/28/2023 06:10:16 PM  [*] Sat Jan 28 18:10:16 2023:    1    | Tr.loss: 0.458766 | Elapsed:   14.97  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.8547
01/28/2023 06:10:16 PM  [*] Started epoch: 2
01/28/2023 06:10:16 PM  [*] Sat Jan 28 18:10:16 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.233319 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8478 & F1 0.9176 | AUC 0.9692
01/28/2023 06:10:22 PM  [*] Sat Jan 28 18:10:22 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.269563 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3973 & F1 0.5686 | AUC 0.9650
01/28/2023 06:10:28 PM  [*] Sat Jan 28 18:10:28 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.206142 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7917 & F1 0.8837 | AUC 0.9576
01/28/2023 06:10:31 PM  [*] Sat Jan 28 18:10:31 2023:    2    | Tr.loss: 0.310142 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.21 & F1: 0.34 | AUC: 0.9271
01/28/2023 06:10:31 PM  [*] Started epoch: 3
01/28/2023 06:10:31 PM  [*] Sat Jan 28 18:10:31 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.241063 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.7234 & F1 0.8395 | AUC 0.9512
01/28/2023 06:10:37 PM  [*] Sat Jan 28 18:10:37 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.204853 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7945 & F1 0.8855 | AUC 0.9729
01/28/2023 06:10:43 PM  [*] Sat Jan 28 18:10:43 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.241146 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7971 & F1 0.8871 | AUC 0.9628
01/28/2023 06:10:46 PM  [*] Sat Jan 28 18:10:46 2023:    3    | Tr.loss: 0.243538 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.45 | AUC: 0.9560
01/28/2023 06:10:51 PM  [!] Evaluating model from split: 1 | epoch: 9
01/28/2023 06:10:52 PM  [*] Started epoch: 1
01/28/2023 06:10:52 PM  [*] Sat Jan 28 18:10:52 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.138041 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.2791 & F1 0.4364 | AUC 0.7453
01/28/2023 06:10:58 PM  [*] Sat Jan 28 18:10:58 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.459109 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3036 & F1 0.4658 | AUC 0.8188
01/28/2023 06:11:04 PM  [*] Sat Jan 28 18:11:04 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.416549 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6429 & F1 0.7826 | AUC 0.9171
01/28/2023 06:11:07 PM  [*] Sat Jan 28 18:11:07 2023:    1    | Tr.loss: 0.445107 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8595
01/28/2023 06:11:07 PM  [*] Started epoch: 2
01/28/2023 06:11:07 PM  [*] Sat Jan 28 18:11:07 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.318987 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6047 & F1 0.7536 | AUC 0.9214
01/28/2023 06:11:13 PM  [*] Sat Jan 28 18:11:13 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.187898 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7808 & F1 0.8769 | AUC 0.9777
01/28/2023 06:11:19 PM  [*] Sat Jan 28 18:11:19 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.331255 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6875 & F1 0.8148 | AUC 0.9436
01/28/2023 06:11:22 PM  [*] Sat Jan 28 18:11:22 2023:    2    | Tr.loss: 0.278114 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.33 & F1: 0.49 | AUC: 0.9433
01/28/2023 06:11:22 PM  [*] Started epoch: 3
01/28/2023 06:11:22 PM  [*] Sat Jan 28 18:11:22 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.256098 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7381 & F1 0.8493 | AUC 0.9481
01/28/2023 06:11:28 PM  [*] Sat Jan 28 18:11:28 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.100349 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8462 & F1 0.9167 | AUC 0.9899
01/28/2023 06:11:34 PM  [*] Sat Jan 28 18:11:34 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.203172 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8413 & F1 0.9138 | AUC 0.9777
01/28/2023 06:11:37 PM  [*] Sat Jan 28 18:11:37 2023:    3    | Tr.loss: 0.222540 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.45 | AUC: 0.9650
01/28/2023 06:11:42 PM  [!] Evaluating model from split: 1 | epoch: 10
01/28/2023 06:11:43 PM  [*] Started epoch: 1
01/28/2023 06:11:43 PM  [*] Sat Jan 28 18:11:43 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.051658 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.3542 & F1 0.5231 | AUC 0.7904
01/28/2023 06:11:49 PM  [*] Sat Jan 28 18:11:49 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.489687 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.2698 & F1 0.4250 | AUC 0.8074
01/28/2023 06:11:55 PM  [*] Sat Jan 28 18:11:55 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.320293 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.4194 & F1 0.5909 | AUC 0.9308
01/28/2023 06:11:57 PM  [*] Sat Jan 28 18:11:57 2023:    1    | Tr.loss: 0.456847 | Elapsed:   14.94  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8579
01/28/2023 06:11:57 PM  [*] Started epoch: 2
01/28/2023 06:11:58 PM  [*] Sat Jan 28 18:11:58 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.264234 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.8537 & F1 0.9211 | AUC 0.9714
01/28/2023 06:12:04 PM  [*] Sat Jan 28 18:12:04 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.235229 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.5231 & F1 0.6869 | AUC 0.9484
01/28/2023 06:12:10 PM  [*] Sat Jan 28 18:12:10 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.241677 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9491
01/28/2023 06:12:12 PM  [*] Sat Jan 28 18:12:12 2023:    2    | Tr.loss: 0.289276 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.50 | AUC: 0.9396
01/28/2023 06:12:12 PM  [*] Started epoch: 3
01/28/2023 06:12:13 PM  [*] Sat Jan 28 18:12:13 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.222551 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5102 & F1 0.6757 | AUC 0.9585
01/28/2023 06:12:19 PM  [*] Sat Jan 28 18:12:19 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.224717 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7188 & F1 0.8364 | AUC 0.9551
01/28/2023 06:12:25 PM  [*] Sat Jan 28 18:12:25 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.244217 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6269 & F1 0.7706 | AUC 0.9656
01/28/2023 06:12:28 PM  [*] Sat Jan 28 18:12:28 2023:    3    | Tr.loss: 0.224566 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9642
01/28/2023 06:12:33 PM  [!] Evaluating model from split: 1 | epoch: 11
01/28/2023 06:12:33 PM  [*] Started epoch: 1
01/28/2023 06:12:33 PM  [*] Sat Jan 28 18:12:33 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.229938 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.2500 & F1 0.4000 | AUC 0.7083
01/28/2023 06:12:40 PM  [*] Sat Jan 28 18:12:40 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.412875 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.3651 & F1 0.5349 | AUC 0.8625
01/28/2023 06:12:46 PM  [*] Sat Jan 28 18:12:46 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.389394 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4500 & F1 0.6207 | AUC 0.8915
01/28/2023 06:12:48 PM  [*] Sat Jan 28 18:12:48 2023:    1    | Tr.loss: 0.469124 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8531
01/28/2023 06:12:48 PM  [*] Started epoch: 2
01/28/2023 06:12:48 PM  [*] Sat Jan 28 18:12:48 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.324481 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5946 & F1 0.7458 | AUC 0.9389
01/28/2023 06:12:55 PM  [*] Sat Jan 28 18:12:55 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.274169 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4194 & F1 0.5909 | AUC 0.9285
01/28/2023 06:13:01 PM  [*] Sat Jan 28 18:13:01 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.286145 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6833 & F1 0.8119 | AUC 0.9581
01/28/2023 06:13:03 PM  [*] Sat Jan 28 18:13:03 2023:    2    | Tr.loss: 0.287337 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.32 | AUC: 0.9390
01/28/2023 06:13:03 PM  [*] Started epoch: 3
01/28/2023 06:13:03 PM  [*] Sat Jan 28 18:13:03 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.161600 | Elapsed: 0.07s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
01/28/2023 06:13:10 PM  [*] Sat Jan 28 18:13:10 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.197586 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7763 & F1 0.8741 | AUC 0.9764
01/28/2023 06:13:16 PM  [*] Sat Jan 28 18:13:16 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.291517 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7164 & F1 0.8348 | AUC 0.9629
01/28/2023 06:13:18 PM  [*] Sat Jan 28 18:13:18 2023:    3    | Tr.loss: 0.220262 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9656
01/28/2023 06:13:23 PM  [!] Evaluating model from split: 1 | epoch: 12
01/28/2023 06:13:24 PM  [*] Started epoch: 1
01/28/2023 06:13:24 PM  [*] Sat Jan 28 18:13:24 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.464686 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.4048 & F1 0.5763 | AUC 0.8831
01/28/2023 06:13:30 PM  [*] Sat Jan 28 18:13:30 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.386394 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.4394 & F1 0.6105 | AUC 0.9004
01/28/2023 06:13:36 PM  [*] Sat Jan 28 18:13:36 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.451533 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.3065 & F1 0.4691 | AUC 0.8684
01/28/2023 06:13:39 PM  [*] Sat Jan 28 18:13:39 2023:    1    | Tr.loss: 0.461068 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8561
01/28/2023 06:13:39 PM  [*] Started epoch: 2
01/28/2023 06:13:39 PM  [*] Sat Jan 28 18:13:39 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.388791 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.4634 & F1 0.6333 | AUC 0.8940
01/28/2023 06:13:45 PM  [*] Sat Jan 28 18:13:45 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.325060 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4412 & F1 0.6122 | AUC 0.8911
01/28/2023 06:13:51 PM  [*] Sat Jan 28 18:13:51 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.387190 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7571 & F1 0.8618 | AUC 0.9421
01/28/2023 06:13:54 PM  [*] Sat Jan 28 18:13:54 2023:    2    | Tr.loss: 0.301017 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.50 | AUC: 0.9309
01/28/2023 06:13:54 PM  [*] Started epoch: 3
01/28/2023 06:13:54 PM  [*] Sat Jan 28 18:13:54 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.265529 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9553
01/28/2023 06:14:00 PM  [*] Sat Jan 28 18:14:00 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.146029 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7143 & F1 0.8333 | AUC 0.9811
01/28/2023 06:14:07 PM  [*] Sat Jan 28 18:14:07 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.208297 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.7571 & F1 0.8618 | AUC 0.9805
01/28/2023 06:14:09 PM  [*] Sat Jan 28 18:14:09 2023:    3    | Tr.loss: 0.229743 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.43 & F1: 0.60 | AUC: 0.9625
01/28/2023 06:14:14 PM  [!] Evaluating model from split: 1 | epoch: 13
01/28/2023 06:14:15 PM  [*] Started epoch: 1
01/28/2023 06:14:15 PM  [*] Sat Jan 28 18:14:15 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.804335 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0750 & F1 0.1395 | AUC 0.7604
01/28/2023 06:14:21 PM  [*] Sat Jan 28 18:14:21 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.485145 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.0588 & F1 0.1111 | AUC 0.8587
01/28/2023 06:14:27 PM  [*] Sat Jan 28 18:14:27 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.326405 | Elapsed: 6.12s | FPR 0.0003 -> TPR 0.5161 & F1 0.6809 | AUC 0.9015
01/28/2023 06:14:30 PM  [*] Sat Jan 28 18:14:30 2023:    1    | Tr.loss: 0.464201 | Elapsed:   14.93  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8504
01/28/2023 06:14:30 PM  [*] Started epoch: 2
01/28/2023 06:14:30 PM  [*] Sat Jan 28 18:14:30 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.420106 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.3721 & F1 0.5424 | AUC 0.8915
01/28/2023 06:14:36 PM  [*] Sat Jan 28 18:14:36 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.207534 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7662 & F1 0.8676 | AUC 0.9605
01/28/2023 06:14:42 PM  [*] Sat Jan 28 18:14:42 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.320437 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.4754 & F1 0.6444 | AUC 0.9374
01/28/2023 06:14:45 PM  [*] Sat Jan 28 18:14:45 2023:    2    | Tr.loss: 0.303221 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.28 & F1: 0.44 | AUC: 0.9297
01/28/2023 06:14:45 PM  [*] Started epoch: 3
01/28/2023 06:14:45 PM  [*] Sat Jan 28 18:14:45 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.380698 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.4250 & F1 0.5965 | AUC 0.9198
01/28/2023 06:14:51 PM  [*] Sat Jan 28 18:14:51 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.233510 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7538 & F1 0.8596 | AUC 0.9464
01/28/2023 06:14:57 PM  [*] Sat Jan 28 18:14:57 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.235057 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7188 & F1 0.8364 | AUC 0.9648
01/28/2023 06:15:00 PM  [*] Sat Jan 28 18:15:00 2023:    3    | Tr.loss: 0.237565 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.40 & F1: 0.57 | AUC: 0.9577
01/28/2023 06:15:05 PM  [!] Evaluating model from split: 1 | epoch: 14
01/28/2023 06:15:06 PM  [*] Started epoch: 1
01/28/2023 06:15:06 PM  [*] Sat Jan 28 18:15:06 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.664895 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0889 & F1 0.1633 | AUC 0.6632
01/28/2023 06:15:12 PM  [*] Sat Jan 28 18:15:12 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.412262 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.3289 & F1 0.4950 | AUC 0.8059
01/28/2023 06:15:18 PM  [*] Sat Jan 28 18:15:18 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.342630 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6000 & F1 0.7500 | AUC 0.9290
01/28/2023 06:15:21 PM  [*] Sat Jan 28 18:15:21 2023:    1    | Tr.loss: 0.454385 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8524
01/28/2023 06:15:21 PM  [*] Started epoch: 2
01/28/2023 06:15:21 PM  [*] Sat Jan 28 18:15:21 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.289917 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7381 & F1 0.8493 | AUC 0.9481
01/28/2023 06:15:27 PM  [*] Sat Jan 28 18:15:27 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.264612 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5606 & F1 0.7184 | AUC 0.9078
01/28/2023 06:15:33 PM  [*] Sat Jan 28 18:15:33 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.248636 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.4265 & F1 0.5979 | AUC 0.9531
01/28/2023 06:15:36 PM  [*] Sat Jan 28 18:15:36 2023:    2    | Tr.loss: 0.303065 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.38 | AUC: 0.9295
01/28/2023 06:15:36 PM  [*] Started epoch: 3
01/28/2023 06:15:36 PM  [*] Sat Jan 28 18:15:36 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.315258 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6042 & F1 0.7532 | AUC 0.9401
01/28/2023 06:15:42 PM  [*] Sat Jan 28 18:15:42 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.187636 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9633
01/28/2023 06:15:48 PM  [*] Sat Jan 28 18:15:48 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.332452 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4857 & F1 0.6538 | AUC 0.9400
01/28/2023 06:15:51 PM  [*] Sat Jan 28 18:15:51 2023:    3    | Tr.loss: 0.234476 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.23 & F1: 0.38 | AUC: 0.9603
01/28/2023 06:15:56 PM  [!] Evaluating model from split: 1 | epoch: 15
01/28/2023 06:15:56 PM  [*] Started epoch: 1
01/28/2023 06:15:56 PM  [*] Sat Jan 28 18:15:56 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.820642 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.2955 & F1 0.4561 | AUC 0.8068
01/28/2023 06:16:03 PM  [*] Sat Jan 28 18:16:03 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.371609 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.1096 & F1 0.1975 | AUC 0.8917
01/28/2023 06:16:09 PM  [*] Sat Jan 28 18:16:09 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.310302 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5156 & F1 0.6804 | AUC 0.9427
01/28/2023 06:16:11 PM  [*] Sat Jan 28 18:16:11 2023:    1    | Tr.loss: 0.464388 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8502
01/28/2023 06:16:11 PM  [*] Started epoch: 2
01/28/2023 06:16:11 PM  [*] Sat Jan 28 18:16:11 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.378822 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.4872 & F1 0.6552 | AUC 0.9123
01/28/2023 06:16:18 PM  [*] Sat Jan 28 18:16:18 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.297435 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6714 & F1 0.8034 | AUC 0.9438
01/28/2023 06:16:24 PM  [*] Sat Jan 28 18:16:24 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.266053 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6825 & F1 0.8113 | AUC 0.9642
01/28/2023 06:16:26 PM  [*] Sat Jan 28 18:16:26 2023:    2    | Tr.loss: 0.307987 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.31 & F1: 0.47 | AUC: 0.9277
01/28/2023 06:16:26 PM  [*] Started epoch: 3
01/28/2023 06:16:26 PM  [*] Sat Jan 28 18:16:26 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.202184 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7750 & F1 0.8732 | AUC 0.9844
01/28/2023 06:16:33 PM  [*] Sat Jan 28 18:16:33 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.374183 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8889 & F1 0.9412 | AUC 0.9526
01/28/2023 06:16:39 PM  [*] Sat Jan 28 18:16:39 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.233759 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091 | AUC 0.9730
01/28/2023 06:16:42 PM  [*] Sat Jan 28 18:16:42 2023:    3    | Tr.loss: 0.247981 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.45 | AUC: 0.9544
01/28/2023 06:16:46 PM  [!] Evaluating model from split: 1 | epoch: 16
01/28/2023 06:16:47 PM  [*] Started epoch: 1
01/28/2023 06:16:47 PM  [*] Sat Jan 28 18:16:47 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.896445 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.1064 & F1 0.1923 | AUC 0.7372
01/28/2023 06:16:53 PM  [*] Sat Jan 28 18:16:53 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.351845 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.0625 & F1 0.1176 | AUC 0.8685
01/28/2023 06:16:59 PM  [*] Sat Jan 28 18:16:59 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.359351 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.2985 & F1 0.4598 | AUC 0.8976
01/28/2023 06:17:02 PM  [*] Sat Jan 28 18:17:02 2023:    1    | Tr.loss: 0.463661 | Elapsed:   14.98  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8504
01/28/2023 06:17:02 PM  [*] Started epoch: 2
01/28/2023 06:17:02 PM  [*] Sat Jan 28 18:17:02 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.266921 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6750 & F1 0.8060 | AUC 0.9625
01/28/2023 06:17:08 PM  [*] Sat Jan 28 18:17:08 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.384219 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5278 & F1 0.6909 | AUC 0.8807
01/28/2023 06:17:14 PM  [*] Sat Jan 28 18:17:14 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.219622 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.6613 & F1 0.7961 | AUC 0.9612
01/28/2023 06:17:17 PM  [*] Sat Jan 28 18:17:17 2023:    2    | Tr.loss: 0.309037 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.14 & F1: 0.24 | AUC: 0.9278
01/28/2023 06:17:17 PM  [*] Started epoch: 3
01/28/2023 06:17:17 PM  [*] Sat Jan 28 18:17:17 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.330829 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.5227 & F1 0.6866 | AUC 0.9159
01/28/2023 06:17:23 PM  [*] Sat Jan 28 18:17:23 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.327977 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6032 & F1 0.7525 | AUC 0.9279
01/28/2023 06:17:29 PM  [*] Sat Jan 28 18:17:29 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.166122 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8406 & F1 0.9134 | AUC 0.9811
01/28/2023 06:17:32 PM  [*] Sat Jan 28 18:17:32 2023:    3    | Tr.loss: 0.229964 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.32 & F1: 0.49 | AUC: 0.9628
01/28/2023 06:17:37 PM  [!] Evaluating model from split: 1 | epoch: 17
01/28/2023 06:17:38 PM  [*] Started epoch: 1
01/28/2023 06:17:38 PM  [*] Sat Jan 28 18:17:38 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.208090 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.1944 & F1 0.3256 | AUC 0.6875
01/28/2023 06:17:44 PM  [*] Sat Jan 28 18:17:44 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.370677 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.1692 & F1 0.2895 | AUC 0.8299
01/28/2023 06:17:50 PM  [*] Sat Jan 28 18:17:50 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.377332 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5211 & F1 0.6852 | AUC 0.8961
01/28/2023 06:17:53 PM  [*] Sat Jan 28 18:17:53 2023:    1    | Tr.loss: 0.470266 | Elapsed:   14.98  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8466
01/28/2023 06:17:53 PM  [*] Started epoch: 2
01/28/2023 06:17:53 PM  [*] Sat Jan 28 18:17:53 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.332635 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5682 & F1 0.7246 | AUC 0.9227
01/28/2023 06:17:59 PM  [*] Sat Jan 28 18:17:59 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.372479 | Elapsed: 6.12s | FPR 0.0003 -> TPR 0.2817 & F1 0.4396 | AUC 0.8640
01/28/2023 06:18:05 PM  [*] Sat Jan 28 18:18:05 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.299874 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.5606 & F1 0.7184 | AUC 0.9434
01/28/2023 06:18:08 PM  [*] Sat Jan 28 18:18:08 2023:    2    | Tr.loss: 0.307424 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.38 | AUC: 0.9290
01/28/2023 06:18:08 PM  [*] Started epoch: 3
01/28/2023 06:18:08 PM  [*] Sat Jan 28 18:18:08 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.268524 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7857 & F1 0.8800 | AUC 0.9567
01/28/2023 06:18:14 PM  [*] Sat Jan 28 18:18:14 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.163972 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8676 & F1 0.9291 | AUC 0.9802
01/28/2023 06:18:20 PM  [*] Sat Jan 28 18:18:20 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.231445 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.7969 & F1 0.8870 | AUC 0.9707
01/28/2023 06:18:23 PM  [*] Sat Jan 28 18:18:23 2023:    3    | Tr.loss: 0.226750 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.32 & F1: 0.48 | AUC: 0.9632
01/28/2023 06:18:28 PM  [!] Evaluating model from split: 1 | epoch: 18
01/28/2023 06:18:28 PM  [*] Started epoch: 1
01/28/2023 06:18:29 PM  [*] Sat Jan 28 18:18:29 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.870880 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0682 & F1 0.1277 | AUC 0.6489
01/28/2023 06:18:35 PM  [*] Sat Jan 28 18:18:35 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.294263 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.4714 & F1 0.6408 | AUC 0.9060
01/28/2023 06:18:41 PM  [*] Sat Jan 28 18:18:41 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.365074 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5254 & F1 0.6889 | AUC 0.9006
01/28/2023 06:18:44 PM  [*] Sat Jan 28 18:18:44 2023:    1    | Tr.loss: 0.471036 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8479
01/28/2023 06:18:44 PM  [*] Started epoch: 2
01/28/2023 06:18:44 PM  [*] Sat Jan 28 18:18:44 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.320118 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5897 & F1 0.7419 | AUC 0.9395
01/28/2023 06:18:50 PM  [*] Sat Jan 28 18:18:50 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.256273 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6087 & F1 0.7568 | AUC 0.9446
01/28/2023 06:18:56 PM  [*] Sat Jan 28 18:18:56 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.362100 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5439 & F1 0.7045 | AUC 0.9221
01/28/2023 06:18:59 PM  [*] Sat Jan 28 18:18:59 2023:    2    | Tr.loss: 0.294880 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.37 & F1: 0.53 | AUC: 0.9335
01/28/2023 06:18:59 PM  [*] Started epoch: 3
01/28/2023 06:18:59 PM  [*] Sat Jan 28 18:18:59 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.183058 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.9091 & F1 0.9524 | AUC 0.9830
01/28/2023 06:19:05 PM  [*] Sat Jan 28 18:19:05 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.236145 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7460 & F1 0.8545 | AUC 0.9753
01/28/2023 06:19:11 PM  [*] Sat Jan 28 18:19:11 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.165097 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.6301 & F1 0.7731 | AUC 0.9498
01/28/2023 06:19:14 PM  [*] Sat Jan 28 18:19:14 2023:    3    | Tr.loss: 0.226051 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9631
01/28/2023 06:19:19 PM  [!] Evaluating model from split: 1 | epoch: 19
01/28/2023 06:19:19 PM  [*] Started epoch: 1
01/28/2023 06:19:19 PM  [*] Sat Jan 28 18:19:19 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.758610 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.2889 & F1 0.4483 | AUC 0.8398
01/28/2023 06:19:26 PM  [*] Sat Jan 28 18:19:26 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.392901 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.1127 & F1 0.2025 | AUC 0.8878
01/28/2023 06:19:32 PM  [*] Sat Jan 28 18:19:32 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.417363 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5075 & F1 0.6733 | AUC 0.8673
01/28/2023 06:19:34 PM  [*] Sat Jan 28 18:19:34 2023:    1    | Tr.loss: 0.458359 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.8496
01/28/2023 06:19:34 PM  [*] Started epoch: 2
01/28/2023 06:19:34 PM  [*] Sat Jan 28 18:19:34 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.311607 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.5833 & F1 0.7368 | AUC 0.9010
01/28/2023 06:19:41 PM  [*] Sat Jan 28 18:19:41 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.371934 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6250 & F1 0.7692 | AUC 0.9348
01/28/2023 06:19:47 PM  [*] Sat Jan 28 18:19:47 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.201710 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7313 & F1 0.8448 | AUC 0.9550
01/28/2023 06:19:49 PM  [*] Sat Jan 28 18:19:49 2023:    2    | Tr.loss: 0.310669 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.18 & F1: 0.30 | AUC: 0.9281
01/28/2023 06:19:49 PM  [*] Started epoch: 3
01/28/2023 06:19:49 PM  [*] Sat Jan 28 18:19:49 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.278901 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7609 & F1 0.8642 | AUC 0.9529
01/28/2023 06:19:56 PM  [*] Sat Jan 28 18:19:56 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.399640 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.1094 & F1 0.1972 | AUC 0.9295
01/28/2023 06:20:02 PM  [*] Sat Jan 28 18:20:02 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.151822 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6364 & F1 0.7778 | AUC 0.9748
01/28/2023 06:20:04 PM  [*] Sat Jan 28 18:20:04 2023:    3    | Tr.loss: 0.236734 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.39 | AUC: 0.9599
01/28/2023 06:20:09 PM  [!] Evaluating model from split: 1 | epoch: 20
01/28/2023 06:20:10 PM  [*] Started epoch: 1
01/28/2023 06:20:10 PM  [*] Sat Jan 28 18:20:10 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.670350 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0286 & F1 0.0556 | AUC 0.6246
01/28/2023 06:20:16 PM  [*] Sat Jan 28 18:20:16 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.433346 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.2537 & F1 0.4048 | AUC 0.8684
01/28/2023 06:20:22 PM  [*] Sat Jan 28 18:20:22 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.481259 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.3636 & F1 0.5333 | AUC 0.8447
01/28/2023 06:20:25 PM  [*] Sat Jan 28 18:20:25 2023:    1    | Tr.loss: 0.478474 | Elapsed:   14.97  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8380
01/28/2023 06:20:25 PM  [*] Started epoch: 2
01/28/2023 06:20:25 PM  [*] Sat Jan 28 18:20:25 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.320987 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6364 & F1 0.7778 | AUC 0.9159
01/28/2023 06:20:31 PM  [*] Sat Jan 28 18:20:31 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.355459 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.6324 & F1 0.7748 | AUC 0.9085
01/28/2023 06:20:37 PM  [*] Sat Jan 28 18:20:37 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.284879 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.8551 & F1 0.9219 | AUC 0.9425
01/28/2023 06:20:40 PM  [*] Sat Jan 28 18:20:40 2023:    2    | Tr.loss: 0.307230 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.26 & F1: 0.41 | AUC: 0.9273
01/28/2023 06:20:40 PM  [*] Started epoch: 3
01/28/2023 06:20:40 PM  [*] Sat Jan 28 18:20:40 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.305002 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.7805 & F1 0.8767 | AUC 0.9343
01/28/2023 06:20:46 PM  [*] Sat Jan 28 18:20:46 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.258552 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.7101 & F1 0.8305 | AUC 0.9696
01/28/2023 06:20:52 PM  [*] Sat Jan 28 18:20:52 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.148943 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8788 & F1 0.9355 | AUC 0.9906
01/28/2023 06:20:55 PM  [*] Sat Jan 28 18:20:55 2023:    3    | Tr.loss: 0.226159 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9630
01/28/2023 06:21:00 PM  [!] Evaluating model from split: 1 | epoch: 21
01/28/2023 06:21:01 PM  [*] Started epoch: 1
01/28/2023 06:21:01 PM  [*] Sat Jan 28 18:21:01 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.428013 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0488 & F1 0.0930 | AUC 0.6511
01/28/2023 06:21:07 PM  [*] Sat Jan 28 18:21:07 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.363619 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.0952 & F1 0.1739 | AUC 0.8488
01/28/2023 06:21:13 PM  [*] Sat Jan 28 18:21:13 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.257404 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6111 & F1 0.7586 | AUC 0.9263
01/28/2023 06:21:16 PM  [*] Sat Jan 28 18:21:16 2023:    1    | Tr.loss: 0.456703 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8544
01/28/2023 06:21:16 PM  [*] Started epoch: 2
01/28/2023 06:21:16 PM  [*] Sat Jan 28 18:21:16 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.355443 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6000 & F1 0.7500 | AUC 0.9094
01/28/2023 06:21:22 PM  [*] Sat Jan 28 18:21:22 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.244923 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5763 & F1 0.7312 | AUC 0.9554
01/28/2023 06:21:28 PM  [*] Sat Jan 28 18:21:28 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.173809 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.6377 & F1 0.7788 | AUC 0.9696
01/28/2023 06:21:31 PM  [*] Sat Jan 28 18:21:31 2023:    2    | Tr.loss: 0.305597 | Elapsed:   15.10  s | FPR 0.0003 -> TPR: 0.28 & F1: 0.44 | AUC: 0.9287
01/28/2023 06:21:31 PM  [*] Started epoch: 3
01/28/2023 06:21:31 PM  [*] Sat Jan 28 18:21:31 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.362218 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.3256 & F1 0.4912 | AUC 0.9053
01/28/2023 06:21:37 PM  [*] Sat Jan 28 18:21:37 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.176114 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6833 & F1 0.8119 | AUC 0.9600
01/28/2023 06:21:43 PM  [*] Sat Jan 28 18:21:43 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.150975 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.5873 & F1 0.7400 | AUC 0.9768
01/28/2023 06:21:46 PM  [*] Sat Jan 28 18:21:46 2023:    3    | Tr.loss: 0.232320 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.57 | AUC: 0.9609
01/28/2023 06:21:51 PM  [!] Evaluating model from split: 1 | epoch: 22
01/28/2023 06:21:51 PM  [*] Started epoch: 1
01/28/2023 06:21:52 PM  [*] Sat Jan 28 18:21:52 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.749059 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.1778 & F1 0.3019 | AUC 0.7801
01/28/2023 06:21:58 PM  [*] Sat Jan 28 18:21:58 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.415884 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.0984 & F1 0.1791 | AUC 0.8472
01/28/2023 06:22:04 PM  [*] Sat Jan 28 18:22:04 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.380148 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4348 & F1 0.6061 | AUC 0.9140
01/28/2023 06:22:06 PM  [*] Sat Jan 28 18:22:06 2023:    1    | Tr.loss: 0.455241 | Elapsed:   14.97  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8554
01/28/2023 06:22:06 PM  [*] Started epoch: 2
01/28/2023 06:22:06 PM  [*] Sat Jan 28 18:22:06 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.339036 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.3415 & F1 0.5091 | AUC 0.9141
01/28/2023 06:22:13 PM  [*] Sat Jan 28 18:22:13 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.327238 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6167 & F1 0.7629 | AUC 0.9521
01/28/2023 06:22:19 PM  [*] Sat Jan 28 18:22:19 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.141468 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5286 & F1 0.6916 | AUC 0.9405
01/28/2023 06:22:21 PM  [*] Sat Jan 28 18:22:21 2023:    2    | Tr.loss: 0.305029 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.23 & F1: 0.37 | AUC: 0.9287
01/28/2023 06:22:21 PM  [*] Started epoch: 3
01/28/2023 06:22:22 PM  [*] Sat Jan 28 18:22:22 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.273486 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5217 & F1 0.6857 | AUC 0.9366
01/28/2023 06:22:28 PM  [*] Sat Jan 28 18:22:28 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.286680 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6515 & F1 0.7890 | AUC 0.9516
01/28/2023 06:22:34 PM  [*] Sat Jan 28 18:22:34 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.283735 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7538 & F1 0.8596 | AUC 0.9587
01/28/2023 06:22:37 PM  [*] Sat Jan 28 18:22:37 2023:    3    | Tr.loss: 0.236909 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.36 & F1: 0.53 | AUC: 0.9578
01/28/2023 06:22:42 PM  [!] Evaluating model from split: 1 | epoch: 23
01/28/2023 06:22:42 PM  [*] Started epoch: 1
01/28/2023 06:22:42 PM  [*] Sat Jan 28 18:22:42 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.158276 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.1892 & F1 0.3182 | AUC 0.7487
01/28/2023 06:22:49 PM  [*] Sat Jan 28 18:22:49 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.402719 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4062 & F1 0.5778 | AUC 0.9084
01/28/2023 06:22:55 PM  [*] Sat Jan 28 18:22:55 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.327120 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.4714 & F1 0.6408 | AUC 0.9121
01/28/2023 06:22:57 PM  [*] Sat Jan 28 18:22:57 2023:    1    | Tr.loss: 0.461770 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8498
01/28/2023 06:22:57 PM  [*] Started epoch: 2
01/28/2023 06:22:57 PM  [*] Sat Jan 28 18:22:57 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.291623 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7143 & F1 0.8333 | AUC 0.9177
01/28/2023 06:23:03 PM  [*] Sat Jan 28 18:23:03 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.385926 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3521 & F1 0.5208 | AUC 0.8752
01/28/2023 06:23:10 PM  [*] Sat Jan 28 18:23:10 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.337483 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4783 & F1 0.6471 | AUC 0.9336
01/28/2023 06:23:12 PM  [*] Sat Jan 28 18:23:12 2023:    2    | Tr.loss: 0.300974 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.17 & F1: 0.29 | AUC: 0.9317
01/28/2023 06:23:12 PM  [*] Started epoch: 3
01/28/2023 06:23:12 PM  [*] Sat Jan 28 18:23:12 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.272367 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7750 & F1 0.8732 | AUC 0.9521
01/28/2023 06:23:19 PM  [*] Sat Jan 28 18:23:19 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.262959 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.7846 & F1 0.8793 | AUC 0.9508
01/28/2023 06:23:25 PM  [*] Sat Jan 28 18:23:25 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.138354 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.9286 & F1 0.9630 | AUC 0.9867
01/28/2023 06:23:27 PM  [*] Sat Jan 28 18:23:27 2023:    3    | Tr.loss: 0.230818 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.26 & F1: 0.42 | AUC: 0.9618
01/28/2023 06:23:32 PM  [!] Evaluating model from split: 1 | epoch: 24
01/28/2023 06:23:33 PM  [*] Started epoch: 1
01/28/2023 06:23:33 PM  [*] Sat Jan 28 18:23:33 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.405529 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.1556 & F1 0.2692 | AUC 0.7509
01/28/2023 06:23:39 PM  [*] Sat Jan 28 18:23:39 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.499077 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3662 & F1 0.5361 | AUC 0.8514
01/28/2023 06:23:45 PM  [*] Sat Jan 28 18:23:45 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.311918 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5645 & F1 0.7216 | AUC 0.9264
01/28/2023 06:23:48 PM  [*] Sat Jan 28 18:23:48 2023:    1    | Tr.loss: 0.466732 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8495
01/28/2023 06:23:48 PM  [*] Started epoch: 2
01/28/2023 06:23:48 PM  [*] Sat Jan 28 18:23:48 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.315582 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7619 & F1 0.8649 | AUC 0.9161
01/28/2023 06:23:54 PM  [*] Sat Jan 28 18:23:54 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.350171 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6212 & F1 0.7664 | AUC 0.8870
01/28/2023 06:24:00 PM  [*] Sat Jan 28 18:24:00 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.320435 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5224 & F1 0.6863 | AUC 0.9267
01/28/2023 06:24:03 PM  [*] Sat Jan 28 18:24:03 2023:    2    | Tr.loss: 0.310135 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.32 | AUC: 0.9280
01/28/2023 06:24:03 PM  [*] Started epoch: 3
01/28/2023 06:24:03 PM  [*] Sat Jan 28 18:24:03 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.290467 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.6389 & F1 0.7797 | AUC 0.9554
01/28/2023 06:24:09 PM  [*] Sat Jan 28 18:24:09 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.262776 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7344 & F1 0.8468 | AUC 0.9488
01/28/2023 06:24:16 PM  [*] Sat Jan 28 18:24:16 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.315193 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8784 & F1 0.9353 | AUC 0.9683
01/28/2023 06:24:18 PM  [*] Sat Jan 28 18:24:18 2023:    3    | Tr.loss: 0.246057 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9548
01/28/2023 06:24:23 PM  [!] Evaluating model from split: 1 | epoch: 25
01/28/2023 06:24:24 PM  [*] Started epoch: 1
01/28/2023 06:24:24 PM  [*] Sat Jan 28 18:24:24 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.231257 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.6224
01/28/2023 06:24:30 PM  [*] Sat Jan 28 18:24:30 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.447846 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.1692 & F1 0.2895 | AUC 0.8295
01/28/2023 06:24:36 PM  [*] Sat Jan 28 18:24:36 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.391034 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4103 & F1 0.5818 | AUC 0.9079
01/28/2023 06:24:39 PM  [*] Sat Jan 28 18:24:39 2023:    1    | Tr.loss: 0.473181 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8435
01/28/2023 06:24:39 PM  [*] Started epoch: 2
01/28/2023 06:24:39 PM  [*] Sat Jan 28 18:24:39 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.290615 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6739 & F1 0.8052 | AUC 0.9275
01/28/2023 06:24:45 PM  [*] Sat Jan 28 18:24:45 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.341918 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.5672 & F1 0.7238 | AUC 0.9261
01/28/2023 06:24:51 PM  [*] Sat Jan 28 18:24:51 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.173234 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7273 & F1 0.8421 | AUC 0.9677
01/28/2023 06:24:54 PM  [*] Sat Jan 28 18:24:54 2023:    2    | Tr.loss: 0.302272 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9294
01/28/2023 06:24:54 PM  [*] Started epoch: 3
01/28/2023 06:24:54 PM  [*] Sat Jan 28 18:24:54 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.143944 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.9348 & F1 0.9663 | AUC 0.9885
01/28/2023 06:25:00 PM  [*] Sat Jan 28 18:25:00 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.239263 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.6557 & F1 0.7921 | AUC 0.9594
01/28/2023 06:25:06 PM  [*] Sat Jan 28 18:25:06 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.192886 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8592 & F1 0.9242 | AUC 0.9738
01/28/2023 06:25:09 PM  [*] Sat Jan 28 18:25:09 2023:    3    | Tr.loss: 0.239755 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.30 & F1: 0.46 | AUC: 0.9574
01/28/2023 06:25:14 PM  [!] Evaluating model from split: 1 | epoch: 26
01/28/2023 06:25:15 PM  [*] Started epoch: 1
01/28/2023 06:25:15 PM  [*] Sat Jan 28 18:25:15 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.535542 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0698 & F1 0.1304 | AUC 0.6910
01/28/2023 06:25:21 PM  [*] Sat Jan 28 18:25:21 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.505672 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.1571 & F1 0.2716 | AUC 0.7843
01/28/2023 06:25:27 PM  [*] Sat Jan 28 18:25:27 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.377462 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4028 & F1 0.5743 | AUC 0.9154
01/28/2023 06:25:30 PM  [*] Sat Jan 28 18:25:30 2023:    1    | Tr.loss: 0.472524 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8487
01/28/2023 06:25:30 PM  [*] Started epoch: 2
01/28/2023 06:25:30 PM  [*] Sat Jan 28 18:25:30 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.299593 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9436
01/28/2023 06:25:36 PM  [*] Sat Jan 28 18:25:36 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.311043 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4286 & F1 0.6000 | AUC 0.9311
01/28/2023 06:25:42 PM  [*] Sat Jan 28 18:25:42 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.297940 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9586
01/28/2023 06:25:45 PM  [*] Sat Jan 28 18:25:45 2023:    2    | Tr.loss: 0.314341 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.27 & F1: 0.43 | AUC: 0.9237
01/28/2023 06:25:45 PM  [*] Started epoch: 3
01/28/2023 06:25:45 PM  [*] Sat Jan 28 18:25:45 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.255212 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7692 & F1 0.8696 | AUC 0.9431
01/28/2023 06:25:51 PM  [*] Sat Jan 28 18:25:51 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.196269 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.6866 & F1 0.8142 | AUC 0.9462
01/28/2023 06:25:57 PM  [*] Sat Jan 28 18:25:57 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.288343 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6053 & F1 0.7541 | AUC 0.9435
01/28/2023 06:26:00 PM  [*] Sat Jan 28 18:26:00 2023:    3    | Tr.loss: 0.245764 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.33 & F1: 0.49 | AUC: 0.9567
01/28/2023 06:26:05 PM  [!] Evaluating model from split: 1 | epoch: 27
01/28/2023 06:26:05 PM  [*] Started epoch: 1
01/28/2023 06:26:05 PM  [*] Sat Jan 28 18:26:05 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.370596 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.2500 & F1 0.4000 | AUC 0.7534
01/28/2023 06:26:12 PM  [*] Sat Jan 28 18:26:12 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.399918 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.3971 & F1 0.5684 | AUC 0.8706
01/28/2023 06:26:18 PM  [*] Sat Jan 28 18:26:18 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.459481 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.5672 & F1 0.7238 | AUC 0.8867
01/28/2023 06:26:20 PM  [*] Sat Jan 28 18:26:20 2023:    1    | Tr.loss: 0.456412 | Elapsed:   14.93  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8555
01/28/2023 06:26:20 PM  [*] Started epoch: 2
01/28/2023 06:26:20 PM  [*] Sat Jan 28 18:26:20 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.211846 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.8947 & F1 0.9444 | AUC 0.9838
01/28/2023 06:26:26 PM  [*] Sat Jan 28 18:26:26 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.252002 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.7000 & F1 0.8235 | AUC 0.9563
01/28/2023 06:26:33 PM  [*] Sat Jan 28 18:26:33 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.314170 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7463 & F1 0.8547 | AUC 0.9376
01/28/2023 06:26:35 PM  [*] Sat Jan 28 18:26:35 2023:    2    | Tr.loss: 0.305039 | Elapsed:   15.10  s | FPR 0.0003 -> TPR: 0.17 & F1: 0.28 | AUC: 0.9292
01/28/2023 06:26:35 PM  [*] Started epoch: 3
01/28/2023 06:26:35 PM  [*] Sat Jan 28 18:26:35 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.270746 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7619 & F1 0.8649 | AUC 0.9470
01/28/2023 06:26:42 PM  [*] Sat Jan 28 18:26:42 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.208411 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7237 & F1 0.8397 | AUC 0.9482
01/28/2023 06:26:48 PM  [*] Sat Jan 28 18:26:48 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.179227 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8750 & F1 0.9333 | AUC 0.9593
01/28/2023 06:26:50 PM  [*] Sat Jan 28 18:26:50 2023:    3    | Tr.loss: 0.233299 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.37 & F1: 0.54 | AUC: 0.9605
01/28/2023 06:26:55 PM  [!] Evaluating model from split: 1 | epoch: 28
01/28/2023 06:26:56 PM  [*] Started epoch: 1
01/28/2023 06:26:56 PM  [*] Sat Jan 28 18:26:56 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.571943 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.1667 & F1 0.2857 | AUC 0.8009
01/28/2023 06:27:02 PM  [*] Sat Jan 28 18:27:02 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.376661 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.2899 & F1 0.4494 | AUC 0.8509
01/28/2023 06:27:09 PM  [*] Sat Jan 28 18:27:09 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.276505 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.5238 & F1 0.6875 | AUC 0.9183
01/28/2023 06:27:11 PM  [*] Sat Jan 28 18:27:11 2023:    1    | Tr.loss: 0.457133 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8548
01/28/2023 06:27:11 PM  [*] Started epoch: 2
01/28/2023 06:27:11 PM  [*] Sat Jan 28 18:27:11 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.294683 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5556 & F1 0.7143 | AUC 0.9228
01/28/2023 06:27:17 PM  [*] Sat Jan 28 18:27:17 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.234271 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7273 & F1 0.8421 | AUC 0.9505
01/28/2023 06:27:24 PM  [*] Sat Jan 28 18:27:24 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.363499 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4833 & F1 0.6517 | AUC 0.9165
01/28/2023 06:27:26 PM  [*] Sat Jan 28 18:27:26 2023:    2    | Tr.loss: 0.306881 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.23 & F1: 0.37 | AUC: 0.9285
01/28/2023 06:27:26 PM  [*] Started epoch: 3
01/28/2023 06:27:26 PM  [*] Sat Jan 28 18:27:26 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.279548 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6905 & F1 0.8169 | AUC 0.9491
01/28/2023 06:27:32 PM  [*] Sat Jan 28 18:27:32 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.334056 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.6712 & F1 0.8033 | AUC 0.8975
01/28/2023 06:27:39 PM  [*] Sat Jan 28 18:27:39 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.186800 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7049 & F1 0.8269 | AUC 0.9756
01/28/2023 06:27:41 PM  [*] Sat Jan 28 18:27:41 2023:    3    | Tr.loss: 0.237754 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.32 & F1: 0.48 | AUC: 0.9592
01/28/2023 06:27:46 PM  [!] Evaluating model from split: 1 | epoch: 29
01/28/2023 06:27:47 PM  [*] Started epoch: 1
01/28/2023 06:27:47 PM  [*] Sat Jan 28 18:27:47 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.730699 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.1220 & F1 0.2174 | AUC 0.6776
01/28/2023 06:27:53 PM  [*] Sat Jan 28 18:27:53 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.411816 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.3906 & F1 0.5618 | AUC 0.8147
01/28/2023 06:27:59 PM  [*] Sat Jan 28 18:27:59 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.354511 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5676 & F1 0.7241 | AUC 0.8903
01/28/2023 06:28:02 PM  [*] Sat Jan 28 18:28:02 2023:    1    | Tr.loss: 0.453750 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8535
01/28/2023 06:28:02 PM  [*] Started epoch: 2
01/28/2023 06:28:02 PM  [*] Sat Jan 28 18:28:02 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.277777 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.6829 & F1 0.8116 | AUC 0.9602
01/28/2023 06:28:08 PM  [*] Sat Jan 28 18:28:08 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.255958 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6765 & F1 0.8070 | AUC 0.9283
01/28/2023 06:28:14 PM  [*] Sat Jan 28 18:28:14 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.333313 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6615 & F1 0.7963 | AUC 0.9253
01/28/2023 06:28:17 PM  [*] Sat Jan 28 18:28:17 2023:    2    | Tr.loss: 0.302336 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9308
01/28/2023 06:28:17 PM  [*] Started epoch: 3
01/28/2023 06:28:17 PM  [*] Sat Jan 28 18:28:17 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.334625 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6154 & F1 0.7619 | AUC 0.9323
01/28/2023 06:28:23 PM  [*] Sat Jan 28 18:28:23 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.186935 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.8143 & F1 0.8976 | AUC 0.9671
01/28/2023 06:28:29 PM  [*] Sat Jan 28 18:28:29 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.188033 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8116 & F1 0.8960 | AUC 0.9642
01/28/2023 06:28:32 PM  [*] Sat Jan 28 18:28:32 2023:    3    | Tr.loss: 0.231846 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.45 & F1: 0.62 | AUC: 0.9614
01/28/2023 06:28:37 PM  [!] Evaluating model from split: 1 | epoch: 30
01/28/2023 06:28:38 PM  [*] Started epoch: 1
01/28/2023 06:28:38 PM  [*] Sat Jan 28 18:28:38 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.095039 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0682 & F1 0.1277 | AUC 0.7136
01/28/2023 06:28:44 PM  [*] Sat Jan 28 18:28:44 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.348192 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4030 & F1 0.5745 | AUC 0.9014
01/28/2023 06:28:50 PM  [*] Sat Jan 28 18:28:50 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.382320 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4179 & F1 0.5895 | AUC 0.8835
01/28/2023 06:28:53 PM  [*] Sat Jan 28 18:28:53 2023:    1    | Tr.loss: 0.475490 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8452
01/28/2023 06:28:53 PM  [*] Started epoch: 2
01/28/2023 06:28:53 PM  [*] Sat Jan 28 18:28:53 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.352637 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.3830 & F1 0.5538 | AUC 0.8711
01/28/2023 06:28:59 PM  [*] Sat Jan 28 18:28:59 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.374524 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4769 & F1 0.6458 | AUC 0.9275
01/28/2023 06:29:05 PM  [*] Sat Jan 28 18:29:05 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.362762 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4559 & F1 0.6263 | AUC 0.9010
01/28/2023 06:29:08 PM  [*] Sat Jan 28 18:29:08 2023:    2    | Tr.loss: 0.300786 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.27 & F1: 0.42 | AUC: 0.9319
01/28/2023 06:29:08 PM  [*] Started epoch: 3
01/28/2023 06:29:08 PM  [*] Sat Jan 28 18:29:08 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.268114 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6410 & F1 0.7813 | AUC 0.9569
01/28/2023 06:29:14 PM  [*] Sat Jan 28 18:29:14 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.216708 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7536 & F1 0.8595 | AUC 0.9670
01/28/2023 06:29:20 PM  [*] Sat Jan 28 18:29:20 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.254611 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.7778 & F1 0.8750 | AUC 0.9676
01/28/2023 06:29:23 PM  [*] Sat Jan 28 18:29:23 2023:    3    | Tr.loss: 0.226480 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.28 & F1: 0.44 | AUC: 0.9634
01/28/2023 06:29:28 PM  [!] Evaluating model from split: 2 | epoch: 1
01/28/2023 06:29:28 PM  [*] Started epoch: 1
01/28/2023 06:29:29 PM  [*] Sat Jan 28 18:29:29 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.981838 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.2041 & F1 0.3390 | AUC 0.4789
01/28/2023 06:29:35 PM  [*] Sat Jan 28 18:29:35 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.508805 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.2698 & F1 0.4250 | AUC 0.8426
01/28/2023 06:29:41 PM  [*] Sat Jan 28 18:29:41 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.285712 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6857 & F1 0.8136 | AUC 0.9586
01/28/2023 06:29:43 PM  [*] Sat Jan 28 18:29:43 2023:    1    | Tr.loss: 0.494680 | Elapsed:   14.98  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8273
01/28/2023 06:29:43 PM  [*] Started epoch: 2
01/28/2023 06:29:43 PM  [*] Sat Jan 28 18:29:43 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.297863 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6444 & F1 0.7838 | AUC 0.9544
01/28/2023 06:29:50 PM  [*] Sat Jan 28 18:29:50 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.338252 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.1045 & F1 0.1892 | AUC 0.9245
01/28/2023 06:29:56 PM  [*] Sat Jan 28 18:29:56 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.251351 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5479 & F1 0.7080 | AUC 0.9685
01/28/2023 06:29:58 PM  [*] Sat Jan 28 18:29:58 2023:    2    | Tr.loss: 0.270769 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.21 & F1: 0.35 | AUC: 0.9474
01/28/2023 06:29:58 PM  [*] Started epoch: 3
01/28/2023 06:29:59 PM  [*] Sat Jan 28 18:29:59 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.268016 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.8095 & F1 0.8947 | AUC 0.9578
01/28/2023 06:30:05 PM  [*] Sat Jan 28 18:30:05 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.181647 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8378 & F1 0.9118 | AUC 0.9766
01/28/2023 06:30:11 PM  [*] Sat Jan 28 18:30:11 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.188312 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8235 & F1 0.9032 | AUC 0.9710
01/28/2023 06:30:14 PM  [*] Sat Jan 28 18:30:14 2023:    3    | Tr.loss: 0.195284 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.35 & F1: 0.52 | AUC: 0.9736
01/28/2023 06:30:18 PM  [!] Evaluating model from split: 2 | epoch: 2
01/28/2023 06:30:19 PM  [*] Started epoch: 1
01/28/2023 06:30:19 PM  [*] Sat Jan 28 18:30:19 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.130571 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.1163 & F1 0.2083 | AUC 0.3156
01/28/2023 06:30:25 PM  [*] Sat Jan 28 18:30:25 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.580308 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.1364 & F1 0.2400 | AUC 0.8177
01/28/2023 06:30:32 PM  [*] Sat Jan 28 18:30:32 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.390984 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5147 & F1 0.6796 | AUC 0.9233
01/28/2023 06:30:34 PM  [*] Sat Jan 28 18:30:34 2023:    1    | Tr.loss: 0.479497 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8368
01/28/2023 06:30:34 PM  [*] Started epoch: 2
01/28/2023 06:30:34 PM  [*] Sat Jan 28 18:30:34 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.337507 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.4186 & F1 0.5902 | AUC 0.9336
01/28/2023 06:30:40 PM  [*] Sat Jan 28 18:30:40 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.278499 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.4516 & F1 0.6222 | AUC 0.9537
01/28/2023 06:30:47 PM  [*] Sat Jan 28 18:30:47 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.339692 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.5072 & F1 0.6731 | AUC 0.9481
01/28/2023 06:30:49 PM  [*] Sat Jan 28 18:30:49 2023:    2    | Tr.loss: 0.260781 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.26 & F1: 0.41 | AUC: 0.9522
01/28/2023 06:30:49 PM  [*] Started epoch: 3
01/28/2023 06:30:49 PM  [*] Sat Jan 28 18:30:49 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.336025 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.6222 & F1 0.7671 | AUC 0.9099
01/28/2023 06:30:55 PM  [*] Sat Jan 28 18:30:55 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.212828 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.7846 & F1 0.8793 | AUC 0.9771
01/28/2023 06:31:02 PM  [*] Sat Jan 28 18:31:02 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.150744 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8971 & F1 0.9457 | AUC 0.9839
01/28/2023 06:31:04 PM  [*] Sat Jan 28 18:31:04 2023:    3    | Tr.loss: 0.187936 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.38 & F1: 0.55 | AUC: 0.9758
01/28/2023 06:31:09 PM  [!] Evaluating model from split: 2 | epoch: 3
01/28/2023 06:31:10 PM  [*] Started epoch: 1
01/28/2023 06:31:10 PM  [*] Sat Jan 28 18:31:10 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 4.347279 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0227 & F1 0.0444 | AUC 0.2182
01/28/2023 06:31:16 PM  [*] Sat Jan 28 18:31:16 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.452306 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.2424 & F1 0.3902 | AUC 0.8271
01/28/2023 06:31:22 PM  [*] Sat Jan 28 18:31:22 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.370976 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3175 & F1 0.4819 | AUC 0.9095
01/28/2023 06:31:25 PM  [*] Sat Jan 28 18:31:25 2023:    1    | Tr.loss: 0.485833 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8286
01/28/2023 06:31:25 PM  [*] Started epoch: 2
01/28/2023 06:31:25 PM  [*] Sat Jan 28 18:31:25 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.238633 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7955 & F1 0.8861 | AUC 0.9670
01/28/2023 06:31:31 PM  [*] Sat Jan 28 18:31:31 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.268243 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.7733 & F1 0.8722 | AUC 0.9747
01/28/2023 06:31:37 PM  [*] Sat Jan 28 18:31:37 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.300396 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.4833 & F1 0.6517 | AUC 0.9446
01/28/2023 06:31:40 PM  [*] Sat Jan 28 18:31:40 2023:    2    | Tr.loss: 0.263109 | Elapsed:   15.11  s | FPR 0.0003 -> TPR: 0.18 & F1: 0.31 | AUC: 0.9504
01/28/2023 06:31:40 PM  [*] Started epoch: 3
01/28/2023 06:31:40 PM  [*] Sat Jan 28 18:31:40 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.161237 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.8667 & F1 0.9286 | AUC 0.9848
01/28/2023 06:31:46 PM  [*] Sat Jan 28 18:31:46 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.206504 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7143 & F1 0.8333 | AUC 0.9819
01/28/2023 06:31:52 PM  [*] Sat Jan 28 18:31:52 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.109313 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.9571 & F1 0.9781 | AUC 0.9943
01/28/2023 06:31:55 PM  [*] Sat Jan 28 18:31:55 2023:    3    | Tr.loss: 0.193381 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.43 & F1: 0.60 | AUC: 0.9740
01/28/2023 06:32:00 PM  [!] Evaluating model from split: 2 | epoch: 4
01/28/2023 06:32:00 PM  [*] Started epoch: 1
01/28/2023 06:32:01 PM  [*] Sat Jan 28 18:32:01 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.668869 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0513 & F1 0.0976 | AUC 0.3272
01/28/2023 06:32:07 PM  [*] Sat Jan 28 18:32:07 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.397649 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3636 & F1 0.5333 | AUC 0.8356
01/28/2023 06:32:13 PM  [*] Sat Jan 28 18:32:13 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.339481 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.5634 & F1 0.7207 | AUC 0.8985
01/28/2023 06:32:15 PM  [*] Sat Jan 28 18:32:15 2023:    1    | Tr.loss: 0.492462 | Elapsed:   14.97  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.8129
01/28/2023 06:32:15 PM  [*] Started epoch: 2
01/28/2023 06:32:16 PM  [*] Sat Jan 28 18:32:16 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.386858 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.4681 & F1 0.6377 | AUC 0.8648
01/28/2023 06:32:22 PM  [*] Sat Jan 28 18:32:22 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.263981 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6029 & F1 0.7523 | AUC 0.9665
01/28/2023 06:32:28 PM  [*] Sat Jan 28 18:32:28 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.223967 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8281 & F1 0.9060 | AUC 0.9688
01/28/2023 06:32:31 PM  [*] Sat Jan 28 18:32:31 2023:    2    | Tr.loss: 0.284462 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.12 & F1: 0.21 | AUC: 0.9416
01/28/2023 06:32:31 PM  [*] Started epoch: 3
01/28/2023 06:32:31 PM  [*] Sat Jan 28 18:32:31 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.275353 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7179 & F1 0.8358 | AUC 0.9579
01/28/2023 06:32:37 PM  [*] Sat Jan 28 18:32:37 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.183674 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8507 & F1 0.9194 | AUC 0.9733
01/28/2023 06:32:43 PM  [*] Sat Jan 28 18:32:43 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.174118 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8769 & F1 0.9344 | AUC 0.9807
01/28/2023 06:32:46 PM  [*] Sat Jan 28 18:32:46 2023:    3    | Tr.loss: 0.200436 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9724
01/28/2023 06:32:51 PM  [!] Evaluating model from split: 2 | epoch: 5
01/28/2023 06:32:51 PM  [*] Started epoch: 1
01/28/2023 06:32:51 PM  [*] Sat Jan 28 18:32:51 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.221216 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3887
01/28/2023 06:32:57 PM  [*] Sat Jan 28 18:32:57 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.479506 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3538 & F1 0.5227 | AUC 0.8659
01/28/2023 06:33:04 PM  [*] Sat Jan 28 18:33:04 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.396916 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.5224 & F1 0.6863 | AUC 0.9104
01/28/2023 06:33:06 PM  [*] Sat Jan 28 18:33:06 2023:    1    | Tr.loss: 0.487350 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8254
01/28/2023 06:33:06 PM  [*] Started epoch: 2
01/28/2023 06:33:06 PM  [*] Sat Jan 28 18:33:06 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.272320 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.8409 & F1 0.9136 | AUC 0.9602
01/28/2023 06:33:12 PM  [*] Sat Jan 28 18:33:12 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.432370 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.1579 & F1 0.2727 | AUC 0.9123
01/28/2023 06:33:19 PM  [*] Sat Jan 28 18:33:19 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.159163 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7667 & F1 0.8679 | AUC 0.9696
01/28/2023 06:33:21 PM  [*] Sat Jan 28 18:33:21 2023:    2    | Tr.loss: 0.268723 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.14 & F1: 0.24 | AUC: 0.9486
01/28/2023 06:33:21 PM  [*] Started epoch: 3
01/28/2023 06:33:21 PM  [*] Sat Jan 28 18:33:21 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.259192 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7209 & F1 0.8378 | AUC 0.9513
01/28/2023 06:33:28 PM  [*] Sat Jan 28 18:33:28 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.219672 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7188 & F1 0.8364 | AUC 0.9661
01/28/2023 06:33:34 PM  [*] Sat Jan 28 18:33:34 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.130131 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8429 & F1 0.9147 | AUC 0.9933
01/28/2023 06:33:36 PM  [*] Sat Jan 28 18:33:36 2023:    3    | Tr.loss: 0.193876 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.45 & F1: 0.62 | AUC: 0.9743
01/28/2023 06:33:41 PM  [!] Evaluating model from split: 2 | epoch: 6
01/28/2023 06:33:42 PM  [*] Started epoch: 1
01/28/2023 06:33:42 PM  [*] Sat Jan 28 18:33:42 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 4.141273 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.2396
01/28/2023 06:33:48 PM  [*] Sat Jan 28 18:33:48 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.499096 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.0517 & F1 0.0984 | AUC 0.8173
01/28/2023 06:33:54 PM  [*] Sat Jan 28 18:33:54 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.428752 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3077 & F1 0.4706 | AUC 0.8664
01/28/2023 06:33:57 PM  [*] Sat Jan 28 18:33:57 2023:    1    | Tr.loss: 0.493647 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8251
01/28/2023 06:33:57 PM  [*] Started epoch: 2
01/28/2023 06:33:57 PM  [*] Sat Jan 28 18:33:57 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.267444 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.8298 & F1 0.9070 | AUC 0.9574
01/28/2023 06:34:03 PM  [*] Sat Jan 28 18:34:03 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.177433 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8382 & F1 0.9120 | AUC 0.9825
01/28/2023 06:34:09 PM  [*] Sat Jan 28 18:34:09 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.148564 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7576 & F1 0.8621 | AUC 0.9733
01/28/2023 06:34:12 PM  [*] Sat Jan 28 18:34:12 2023:    2    | Tr.loss: 0.280489 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.36 | AUC: 0.9428
01/28/2023 06:34:12 PM  [*] Started epoch: 3
01/28/2023 06:34:12 PM  [*] Sat Jan 28 18:34:12 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.288430 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6250 & F1 0.7692 | AUC 0.9542
01/28/2023 06:34:18 PM  [*] Sat Jan 28 18:34:18 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.184462 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8082 & F1 0.8939 | AUC 0.9736
01/28/2023 06:34:24 PM  [*] Sat Jan 28 18:34:24 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.161629 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8696 & F1 0.9302 | AUC 0.9836
01/28/2023 06:34:27 PM  [*] Sat Jan 28 18:34:27 2023:    3    | Tr.loss: 0.204069 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.46 & F1: 0.63 | AUC: 0.9709
01/28/2023 06:34:32 PM  [!] Evaluating model from split: 2 | epoch: 7
01/28/2023 06:34:33 PM  [*] Started epoch: 1
01/28/2023 06:34:33 PM  [*] Sat Jan 28 18:34:33 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.755126 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.1500 & F1 0.2609 | AUC 0.5021
01/28/2023 06:34:39 PM  [*] Sat Jan 28 18:34:39 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.425664 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.1471 & F1 0.2564 | AUC 0.7808
01/28/2023 06:34:45 PM  [*] Sat Jan 28 18:34:45 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.330695 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.2267 & F1 0.3696 | AUC 0.8848
01/28/2023 06:34:48 PM  [*] Sat Jan 28 18:34:48 2023:    1    | Tr.loss: 0.490251 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8255
01/28/2023 06:34:48 PM  [*] Started epoch: 2
01/28/2023 06:34:48 PM  [*] Sat Jan 28 18:34:48 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.395996 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.4286 & F1 0.6000 | AUC 0.8723
01/28/2023 06:34:54 PM  [*] Sat Jan 28 18:34:54 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.242570 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.7826 & F1 0.8780 | AUC 0.9645
01/28/2023 06:35:00 PM  [*] Sat Jan 28 18:35:00 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.240288 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5753 & F1 0.7304 | AUC 0.9528
01/28/2023 06:35:03 PM  [*] Sat Jan 28 18:35:03 2023:    2    | Tr.loss: 0.267266 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9483
01/28/2023 06:35:03 PM  [*] Started epoch: 3
01/28/2023 06:35:03 PM  [*] Sat Jan 28 18:35:03 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.196998 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.9143 & F1 0.9552 | AUC 0.9921
01/28/2023 06:35:09 PM  [*] Sat Jan 28 18:35:09 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.081886 | Elapsed: 6.15s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
01/28/2023 06:35:15 PM  [*] Sat Jan 28 18:35:15 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.114834 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.9000 & F1 0.9474 | AUC 0.9886
01/28/2023 06:35:18 PM  [*] Sat Jan 28 18:35:18 2023:    3    | Tr.loss: 0.195644 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9729
01/28/2023 06:35:23 PM  [!] Evaluating model from split: 2 | epoch: 8
01/28/2023 06:35:23 PM  [*] Started epoch: 1
01/28/2023 06:35:23 PM  [*] Sat Jan 28 18:35:23 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.787168 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0244 & F1 0.0476 | AUC 0.3902
01/28/2023 06:35:30 PM  [*] Sat Jan 28 18:35:30 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.408430 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3279 & F1 0.4938 | AUC 0.9037
01/28/2023 06:35:36 PM  [*] Sat Jan 28 18:35:36 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.341318 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4783 & F1 0.6471 | AUC 0.9252
01/28/2023 06:35:38 PM  [*] Sat Jan 28 18:35:38 2023:    1    | Tr.loss: 0.484217 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8308
01/28/2023 06:35:38 PM  [*] Started epoch: 2
01/28/2023 06:35:38 PM  [*] Sat Jan 28 18:35:38 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.305429 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6000 & F1 0.7500 | AUC 0.9521
01/28/2023 06:35:45 PM  [*] Sat Jan 28 18:35:45 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.279956 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6176 & F1 0.7636 | AUC 0.9403
01/28/2023 06:35:51 PM  [*] Sat Jan 28 18:35:51 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.311290 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.5294 & F1 0.6923 | AUC 0.9214
01/28/2023 06:35:53 PM  [*] Sat Jan 28 18:35:53 2023:    2    | Tr.loss: 0.270442 | Elapsed:   15.11  s | FPR 0.0003 -> TPR: 0.18 & F1: 0.30 | AUC: 0.9480
01/28/2023 06:35:53 PM  [*] Started epoch: 3
01/28/2023 06:35:53 PM  [*] Sat Jan 28 18:35:53 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.276372 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.3611 & F1 0.5306 | AUC 0.9623
01/28/2023 06:36:00 PM  [*] Sat Jan 28 18:36:00 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.192415 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7123 & F1 0.8320 | AUC 0.9807
01/28/2023 06:36:06 PM  [*] Sat Jan 28 18:36:06 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.147751 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.9211 & F1 0.9589 | AUC 0.9929
01/28/2023 06:36:08 PM  [*] Sat Jan 28 18:36:08 2023:    3    | Tr.loss: 0.195843 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9734
01/28/2023 06:36:13 PM  [!] Evaluating model from split: 2 | epoch: 9
01/28/2023 06:36:14 PM  [*] Started epoch: 1
01/28/2023 06:36:14 PM  [*] Sat Jan 28 18:36:14 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.045936 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0652 & F1 0.1224 | AUC 0.4577
01/28/2023 06:36:20 PM  [*] Sat Jan 28 18:36:20 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.471388 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.3438 & F1 0.5116 | AUC 0.8503
01/28/2023 06:36:27 PM  [*] Sat Jan 28 18:36:27 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.403297 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.4697 & F1 0.6392 | AUC 0.8997
01/28/2023 06:36:29 PM  [*] Sat Jan 28 18:36:29 2023:    1    | Tr.loss: 0.493599 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8200
01/28/2023 06:36:29 PM  [*] Started epoch: 2
01/28/2023 06:36:29 PM  [*] Sat Jan 28 18:36:29 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.356770 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6809 & F1 0.8101 | AUC 0.9174
01/28/2023 06:36:35 PM  [*] Sat Jan 28 18:36:35 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.313791 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5606 & F1 0.7184 | AUC 0.9332
01/28/2023 06:36:42 PM  [*] Sat Jan 28 18:36:42 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.350760 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.5714 & F1 0.7273 | AUC 0.9300
01/28/2023 06:36:44 PM  [*] Sat Jan 28 18:36:44 2023:    2    | Tr.loss: 0.266165 | Elapsed:   15.10  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.41 | AUC: 0.9494
01/28/2023 06:36:44 PM  [*] Started epoch: 3
01/28/2023 06:36:44 PM  [*] Sat Jan 28 18:36:44 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.119330 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.9714 & F1 0.9855 | AUC 0.9990
01/28/2023 06:36:50 PM  [*] Sat Jan 28 18:36:50 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.178408 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.5758 & F1 0.7308 | AUC 0.9724
01/28/2023 06:36:57 PM  [*] Sat Jan 28 18:36:57 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.225386 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7231 & F1 0.8393 | AUC 0.9622
01/28/2023 06:36:59 PM  [*] Sat Jan 28 18:36:59 2023:    3    | Tr.loss: 0.190428 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.41 & F1: 0.58 | AUC: 0.9750
01/28/2023 06:37:04 PM  [!] Evaluating model from split: 2 | epoch: 10
01/28/2023 06:37:05 PM  [*] Started epoch: 1
01/28/2023 06:37:05 PM  [*] Sat Jan 28 18:37:05 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.506309 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0435 & F1 0.0833 | AUC 0.2222
01/28/2023 06:37:11 PM  [*] Sat Jan 28 18:37:11 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.410433 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.1077 & F1 0.1944 | AUC 0.8615
01/28/2023 06:37:17 PM  [*] Sat Jan 28 18:37:17 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.380611 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4286 & F1 0.6000 | AUC 0.9090
01/28/2023 06:37:20 PM  [*] Sat Jan 28 18:37:20 2023:    1    | Tr.loss: 0.473112 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8405
01/28/2023 06:37:20 PM  [*] Started epoch: 2
01/28/2023 06:37:20 PM  [*] Sat Jan 28 18:37:20 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.362057 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5897 & F1 0.7419 | AUC 0.9323
01/28/2023 06:37:26 PM  [*] Sat Jan 28 18:37:26 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.206575 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8143 & F1 0.8976 | AUC 0.9767
01/28/2023 06:37:32 PM  [*] Sat Jan 28 18:37:32 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.326975 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7647 & F1 0.8667 | AUC 0.9467
01/28/2023 06:37:35 PM  [*] Sat Jan 28 18:37:35 2023:    2    | Tr.loss: 0.258127 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.37 | AUC: 0.9536
01/28/2023 06:37:35 PM  [*] Started epoch: 3
01/28/2023 06:37:35 PM  [*] Sat Jan 28 18:37:35 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.218063 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.8837 & F1 0.9383 | AUC 0.9756
01/28/2023 06:37:41 PM  [*] Sat Jan 28 18:37:41 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.226251 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6949 & F1 0.8200 | AUC 0.9744
01/28/2023 06:37:47 PM  [*] Sat Jan 28 18:37:47 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.236144 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4821 & F1 0.6506 | AUC 0.9627
01/28/2023 06:37:50 PM  [*] Sat Jan 28 18:37:50 2023:    3    | Tr.loss: 0.185426 | Elapsed:   15.08  s | FPR 0.0003 -> TPR: 0.40 & F1: 0.58 | AUC: 0.9766
01/28/2023 06:37:55 PM  [!] Evaluating model from split: 2 | epoch: 11
01/28/2023 06:37:56 PM  [*] Started epoch: 1
01/28/2023 06:37:56 PM  [*] Sat Jan 28 18:37:56 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.577255 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0222 & F1 0.0435 | AUC 0.4608
01/28/2023 06:38:02 PM  [*] Sat Jan 28 18:38:02 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.358914 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.3514 & F1 0.5200 | AUC 0.8805
01/28/2023 06:38:08 PM  [*] Sat Jan 28 18:38:08 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.290746 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.0156 & F1 0.0308 | AUC 0.8841
01/28/2023 06:38:11 PM  [*] Sat Jan 28 18:38:11 2023:    1    | Tr.loss: 0.481836 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8333
01/28/2023 06:38:11 PM  [*] Started epoch: 2
01/28/2023 06:38:11 PM  [*] Sat Jan 28 18:38:11 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.387600 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.3256 & F1 0.4912 | AUC 0.8682
01/28/2023 06:38:17 PM  [*] Sat Jan 28 18:38:17 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.179319 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4928 & F1 0.6602 | AUC 0.9640
01/28/2023 06:38:23 PM  [*] Sat Jan 28 18:38:23 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.160313 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8611 & F1 0.9254 | AUC 0.9802
01/28/2023 06:38:26 PM  [*] Sat Jan 28 18:38:26 2023:    2    | Tr.loss: 0.264663 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.44 | AUC: 0.9497
01/28/2023 06:38:26 PM  [*] Started epoch: 3
01/28/2023 06:38:26 PM  [*] Sat Jan 28 18:38:26 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.150416 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8542 & F1 0.9213 | AUC 0.9883
01/28/2023 06:38:32 PM  [*] Sat Jan 28 18:38:32 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.151245 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.9538 & F1 0.9764 | AUC 0.9916
01/28/2023 06:38:38 PM  [*] Sat Jan 28 18:38:38 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.200172 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.6486 & F1 0.7869 | AUC 0.9802
01/28/2023 06:38:41 PM  [*] Sat Jan 28 18:38:41 2023:    3    | Tr.loss: 0.187431 | Elapsed:   15.12  s | FPR 0.0003 -> TPR: 0.40 & F1: 0.57 | AUC: 0.9757
01/28/2023 06:38:46 PM  [!] Evaluating model from split: 2 | epoch: 12
01/28/2023 06:38:46 PM  [*] Started epoch: 1
01/28/2023 06:38:46 PM  [*] Sat Jan 28 18:38:46 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.397819 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0435 & F1 0.0833 | AUC 0.3623
01/28/2023 06:38:53 PM  [*] Sat Jan 28 18:38:53 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.474221 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.2254 & F1 0.3678 | AUC 0.7960
01/28/2023 06:38:59 PM  [*] Sat Jan 28 18:38:59 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.360816 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5522 & F1 0.7115 | AUC 0.9317
01/28/2023 06:39:01 PM  [*] Sat Jan 28 18:39:01 2023:    1    | Tr.loss: 0.485908 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8278
01/28/2023 06:39:01 PM  [*] Started epoch: 2
01/28/2023 06:39:01 PM  [*] Sat Jan 28 18:39:01 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.181236 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8889 & F1 0.9412 | AUC 0.9825
01/28/2023 06:39:08 PM  [*] Sat Jan 28 18:39:08 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.257354 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.4848 & F1 0.6531 | AUC 0.9492
01/28/2023 06:39:14 PM  [*] Sat Jan 28 18:39:14 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.157956 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.6212 & F1 0.7664 | AUC 0.9657
01/28/2023 06:39:16 PM  [*] Sat Jan 28 18:39:16 2023:    2    | Tr.loss: 0.266172 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.26 & F1: 0.41 | AUC: 0.9497
01/28/2023 06:39:16 PM  [*] Started epoch: 3
01/28/2023 06:39:16 PM  [*] Sat Jan 28 18:39:16 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.276999 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6818 & F1 0.8108 | AUC 0.9443
01/28/2023 06:39:23 PM  [*] Sat Jan 28 18:39:23 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.263613 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6429 & F1 0.7826 | AUC 0.9671
01/28/2023 06:39:29 PM  [*] Sat Jan 28 18:39:29 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.185875 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8636 & F1 0.9268 | AUC 0.9835
01/28/2023 06:39:31 PM  [*] Sat Jan 28 18:39:31 2023:    3    | Tr.loss: 0.193237 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9745
01/28/2023 06:39:36 PM  [!] Evaluating model from split: 2 | epoch: 13
01/28/2023 06:39:37 PM  [*] Started epoch: 1
01/28/2023 06:39:37 PM  [*] Sat Jan 28 18:39:37 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.067606 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0612 & F1 0.1154 | AUC 0.3592
01/28/2023 06:39:43 PM  [*] Sat Jan 28 18:39:43 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.449260 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.3582 & F1 0.5275 | AUC 0.8544
01/28/2023 06:39:49 PM  [*] Sat Jan 28 18:39:49 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.396742 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4068 & F1 0.5783 | AUC 0.9024
01/28/2023 06:39:52 PM  [*] Sat Jan 28 18:39:52 2023:    1    | Tr.loss: 0.489787 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8268
01/28/2023 06:39:52 PM  [*] Started epoch: 2
01/28/2023 06:39:52 PM  [*] Sat Jan 28 18:39:52 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.384734 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5789 & F1 0.7333 | AUC 0.9140
01/28/2023 06:39:58 PM  [*] Sat Jan 28 18:39:58 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.246601 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3649 & F1 0.5347 | AUC 0.8857
01/28/2023 06:40:04 PM  [*] Sat Jan 28 18:40:04 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.175678 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7857 & F1 0.8800 | AUC 0.9671
01/28/2023 06:40:07 PM  [*] Sat Jan 28 18:40:07 2023:    2    | Tr.loss: 0.265002 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.26 & F1: 0.42 | AUC: 0.9502
01/28/2023 06:40:07 PM  [*] Started epoch: 3
01/28/2023 06:40:07 PM  [*] Sat Jan 28 18:40:07 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.245116 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7045 & F1 0.8267 | AUC 0.9625
01/28/2023 06:40:13 PM  [*] Sat Jan 28 18:40:13 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.211135 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8395 & F1 0.9128 | AUC 0.9760
01/28/2023 06:40:19 PM  [*] Sat Jan 28 18:40:19 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.155491 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.9206 & F1 0.9587 | AUC 0.9867
01/28/2023 06:40:22 PM  [*] Sat Jan 28 18:40:22 2023:    3    | Tr.loss: 0.193210 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9746
01/28/2023 06:40:27 PM  [!] Evaluating model from split: 2 | epoch: 14
01/28/2023 06:40:28 PM  [*] Started epoch: 1
01/28/2023 06:40:28 PM  [*] Sat Jan 28 18:40:28 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.445565 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0222 & F1 0.0435 | AUC 0.3205
01/28/2023 06:40:34 PM  [*] Sat Jan 28 18:40:34 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.383197 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4603 & F1 0.6304 | AUC 0.8739
01/28/2023 06:40:40 PM  [*] Sat Jan 28 18:40:40 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.325353 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4697 & F1 0.6392 | AUC 0.9176
01/28/2023 06:40:43 PM  [*] Sat Jan 28 18:40:43 2023:    1    | Tr.loss: 0.481284 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8339
01/28/2023 06:40:43 PM  [*] Started epoch: 2
01/28/2023 06:40:43 PM  [*] Sat Jan 28 18:40:43 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.322085 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5227 & F1 0.6866 | AUC 0.9250
01/28/2023 06:40:49 PM  [*] Sat Jan 28 18:40:49 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.163261 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7429 & F1 0.8525 | AUC 0.9705
01/28/2023 06:40:55 PM  [*] Sat Jan 28 18:40:55 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.187970 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.9118 & F1 0.9538 | AUC 0.9835
01/28/2023 06:40:58 PM  [*] Sat Jan 28 18:40:58 2023:    2    | Tr.loss: 0.259665 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.39 | AUC: 0.9526
01/28/2023 06:40:58 PM  [*] Started epoch: 3
01/28/2023 06:40:58 PM  [*] Sat Jan 28 18:40:58 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.161619 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7021 & F1 0.8250 | AUC 0.9750
01/28/2023 06:41:04 PM  [*] Sat Jan 28 18:41:04 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.095723 | Elapsed: 6.16s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
01/28/2023 06:41:10 PM  [*] Sat Jan 28 18:41:10 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.096926 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9869
01/28/2023 06:41:13 PM  [*] Sat Jan 28 18:41:13 2023:    3    | Tr.loss: 0.187581 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.40 & F1: 0.57 | AUC: 0.9758
01/28/2023 06:41:18 PM  [!] Evaluating model from split: 2 | epoch: 15
01/28/2023 06:41:18 PM  [*] Started epoch: 1
01/28/2023 06:41:19 PM  [*] Sat Jan 28 18:41:19 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.817973 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0652 & F1 0.1224 | AUC 0.4734
01/28/2023 06:41:25 PM  [*] Sat Jan 28 18:41:25 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.377927 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.1571 & F1 0.2716 | AUC 0.8719
01/28/2023 06:41:31 PM  [*] Sat Jan 28 18:41:31 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.344235 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.5224 & F1 0.6863 | AUC 0.9290
01/28/2023 06:41:34 PM  [*] Sat Jan 28 18:41:34 2023:    1    | Tr.loss: 0.477763 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8391
01/28/2023 06:41:34 PM  [*] Started epoch: 2
01/28/2023 06:41:34 PM  [*] Sat Jan 28 18:41:34 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.269676 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.5400 & F1 0.7013 | AUC 0.9486
01/28/2023 06:41:40 PM  [*] Sat Jan 28 18:41:40 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.276416 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7681 & F1 0.8689 | AUC 0.9565
01/28/2023 06:41:46 PM  [*] Sat Jan 28 18:41:46 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.257490 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5833 & F1 0.7368 | AUC 0.9335
01/28/2023 06:41:49 PM  [*] Sat Jan 28 18:41:49 2023:    2    | Tr.loss: 0.258294 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.28 & F1: 0.44 | AUC: 0.9533
01/28/2023 06:41:49 PM  [*] Started epoch: 3
01/28/2023 06:41:49 PM  [*] Sat Jan 28 18:41:49 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.323660 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6571 & F1 0.7931 | AUC 0.9438
01/28/2023 06:41:55 PM  [*] Sat Jan 28 18:41:55 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.255885 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8525 & F1 0.9204 | AUC 0.9622
01/28/2023 06:42:01 PM  [*] Sat Jan 28 18:42:01 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.284801 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6769 & F1 0.8073 | AUC 0.9622
01/28/2023 06:42:04 PM  [*] Sat Jan 28 18:42:04 2023:    3    | Tr.loss: 0.189004 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.50 | AUC: 0.9754
01/28/2023 06:42:09 PM  [!] Evaluating model from split: 2 | epoch: 16
01/28/2023 06:42:09 PM  [*] Started epoch: 1
01/28/2023 06:42:09 PM  [*] Sat Jan 28 18:42:09 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.935475 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.2128 & F1 0.3509 | AUC 0.4856
01/28/2023 06:42:15 PM  [*] Sat Jan 28 18:42:15 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.450028 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.2254 & F1 0.3678 | AUC 0.8261
01/28/2023 06:42:22 PM  [*] Sat Jan 28 18:42:22 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.254399 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.5373 & F1 0.6990 | AUC 0.9525
01/28/2023 06:42:24 PM  [*] Sat Jan 28 18:42:24 2023:    1    | Tr.loss: 0.496431 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8191
01/28/2023 06:42:24 PM  [*] Started epoch: 2
01/28/2023 06:42:24 PM  [*] Sat Jan 28 18:42:24 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.355235 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.4211 & F1 0.5926 | AUC 0.9200
01/28/2023 06:42:31 PM  [*] Sat Jan 28 18:42:31 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.276688 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.2429 & F1 0.3908 | AUC 0.9171
01/28/2023 06:42:37 PM  [*] Sat Jan 28 18:42:37 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.230176 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7500 & F1 0.8571 | AUC 0.9757
01/28/2023 06:42:39 PM  [*] Sat Jan 28 18:42:39 2023:    2    | Tr.loss: 0.284292 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.16 & F1: 0.27 | AUC: 0.9407
01/28/2023 06:42:39 PM  [*] Started epoch: 3
01/28/2023 06:42:39 PM  [*] Sat Jan 28 18:42:39 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.255174 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.5849 & F1 0.7381 | AUC 0.9211
01/28/2023 06:42:46 PM  [*] Sat Jan 28 18:42:46 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.187209 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8676 & F1 0.9291 | AUC 0.9697
01/28/2023 06:42:52 PM  [*] Sat Jan 28 18:42:52 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.199058 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7887 & F1 0.8819 | AUC 0.9752
01/28/2023 06:42:54 PM  [*] Sat Jan 28 18:42:54 2023:    3    | Tr.loss: 0.201829 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9715
01/28/2023 06:42:59 PM  [!] Evaluating model from split: 2 | epoch: 17
01/28/2023 06:43:00 PM  [*] Started epoch: 1
01/28/2023 06:43:00 PM  [*] Sat Jan 28 18:43:00 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.987280 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0952 & F1 0.1739 | AUC 0.3918
01/28/2023 06:43:06 PM  [*] Sat Jan 28 18:43:06 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.418190 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.1948 & F1 0.3261 | AUC 0.8007
01/28/2023 06:43:12 PM  [*] Sat Jan 28 18:43:12 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.389136 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6087 & F1 0.7568 | AUC 0.9397
01/28/2023 06:43:15 PM  [*] Sat Jan 28 18:43:15 2023:    1    | Tr.loss: 0.476258 | Elapsed:   14.98  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8359
01/28/2023 06:43:15 PM  [*] Started epoch: 2
01/28/2023 06:43:15 PM  [*] Sat Jan 28 18:43:15 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.373426 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5909 & F1 0.7429 | AUC 0.9091
01/28/2023 06:43:21 PM  [*] Sat Jan 28 18:43:21 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.284021 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6857 & F1 0.8136 | AUC 0.9529
01/28/2023 06:43:27 PM  [*] Sat Jan 28 18:43:27 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.156911 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.9041 & F1 0.9496 | AUC 0.9827
01/28/2023 06:43:30 PM  [*] Sat Jan 28 18:43:30 2023:    2    | Tr.loss: 0.263018 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9514
01/28/2023 06:43:30 PM  [*] Started epoch: 3
01/28/2023 06:43:30 PM  [*] Sat Jan 28 18:43:30 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.341408 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5250 & F1 0.6885 | AUC 0.9375
01/28/2023 06:43:36 PM  [*] Sat Jan 28 18:43:36 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.119524 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8261 & F1 0.9048 | AUC 0.9864
01/28/2023 06:43:42 PM  [*] Sat Jan 28 18:43:42 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.131797 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7808 & F1 0.8769 | AUC 0.9838
01/28/2023 06:43:45 PM  [*] Sat Jan 28 18:43:45 2023:    3    | Tr.loss: 0.196127 | Elapsed:   15.12  s | FPR 0.0003 -> TPR: 0.33 & F1: 0.50 | AUC: 0.9735
01/28/2023 06:43:50 PM  [!] Evaluating model from split: 2 | epoch: 18
01/28/2023 06:43:51 PM  [*] Started epoch: 1
01/28/2023 06:43:51 PM  [*] Sat Jan 28 18:43:51 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.645838 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0732 & F1 0.1364 | AUC 0.3107
01/28/2023 06:43:57 PM  [*] Sat Jan 28 18:43:57 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.450287 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.2208 & F1 0.3617 | AUC 0.8148
01/28/2023 06:44:03 PM  [*] Sat Jan 28 18:44:03 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.297730 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.6111 & F1 0.7586 | AUC 0.9410
01/28/2023 06:44:06 PM  [*] Sat Jan 28 18:44:06 2023:    1    | Tr.loss: 0.495493 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8234
01/28/2023 06:44:06 PM  [*] Started epoch: 2
01/28/2023 06:44:06 PM  [*] Sat Jan 28 18:44:06 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.257115 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6591 & F1 0.7945 | AUC 0.9636
01/28/2023 06:44:12 PM  [*] Sat Jan 28 18:44:12 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.214246 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6197 & F1 0.7652 | AUC 0.9529
01/28/2023 06:44:18 PM  [*] Sat Jan 28 18:44:18 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.203530 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.3875 & F1 0.5586 | AUC 0.9381
01/28/2023 06:44:21 PM  [*] Sat Jan 28 18:44:21 2023:    2    | Tr.loss: 0.271020 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.21 & F1: 0.34 | AUC: 0.9474
01/28/2023 06:44:21 PM  [*] Started epoch: 3
01/28/2023 06:44:21 PM  [*] Sat Jan 28 18:44:21 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.132292 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.9524 & F1 0.9756 | AUC 0.9968
01/28/2023 06:44:27 PM  [*] Sat Jan 28 18:44:27 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.191553 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.9265 & F1 0.9618 | AUC 0.9756
01/28/2023 06:44:33 PM  [*] Sat Jan 28 18:44:33 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.199593 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8676 & F1 0.9291 | AUC 0.9798
01/28/2023 06:44:36 PM  [*] Sat Jan 28 18:44:36 2023:    3    | Tr.loss: 0.193765 | Elapsed:   15.05  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.60 | AUC: 0.9741
01/28/2023 06:44:41 PM  [!] Evaluating model from split: 2 | epoch: 19
01/28/2023 06:44:41 PM  [*] Started epoch: 1
01/28/2023 06:44:42 PM  [*] Sat Jan 28 18:44:42 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.275205 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0227 & F1 0.0444 | AUC 0.3170
01/28/2023 06:44:48 PM  [*] Sat Jan 28 18:44:48 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.361369 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.3286 & F1 0.4946 | AUC 0.8290
01/28/2023 06:44:54 PM  [*] Sat Jan 28 18:44:54 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.373273 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.3768 & F1 0.5474 | AUC 0.9009
01/28/2023 06:44:57 PM  [*] Sat Jan 28 18:44:57 2023:    1    | Tr.loss: 0.492309 | Elapsed:   15.07  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8256
01/28/2023 06:44:57 PM  [*] Started epoch: 2
01/28/2023 06:44:57 PM  [*] Sat Jan 28 18:44:57 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.219477 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5217 & F1 0.6857 | AUC 0.9638
01/28/2023 06:45:03 PM  [*] Sat Jan 28 18:45:03 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.339973 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.4783 & F1 0.6471 | AUC 0.9294
01/28/2023 06:45:09 PM  [*] Sat Jan 28 18:45:09 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.264796 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7403 & F1 0.8507 | AUC 0.9565
01/28/2023 06:45:12 PM  [*] Sat Jan 28 18:45:12 2023:    2    | Tr.loss: 0.268758 | Elapsed:   15.10  s | FPR 0.0003 -> TPR: 0.27 & F1: 0.42 | AUC: 0.9475
01/28/2023 06:45:12 PM  [*] Started epoch: 3
01/28/2023 06:45:12 PM  [*] Sat Jan 28 18:45:12 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.184880 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7556 & F1 0.8608 | AUC 0.9696
01/28/2023 06:45:18 PM  [*] Sat Jan 28 18:45:18 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.211924 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7627 & F1 0.8654 | AUC 0.9661
01/28/2023 06:45:24 PM  [*] Sat Jan 28 18:45:24 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.159569 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.4559 & F1 0.6263 | AUC 0.9710
01/28/2023 06:45:27 PM  [*] Sat Jan 28 18:45:27 2023:    3    | Tr.loss: 0.195749 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9738
01/28/2023 06:45:32 PM  [!] Evaluating model from split: 2 | epoch: 20
01/28/2023 06:45:32 PM  [*] Started epoch: 1
01/28/2023 06:45:32 PM  [*] Sat Jan 28 18:45:32 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.773957 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3855
01/28/2023 06:45:39 PM  [*] Sat Jan 28 18:45:39 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.344486 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.2639 & F1 0.4176 | AUC 0.8566
01/28/2023 06:45:45 PM  [*] Sat Jan 28 18:45:45 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.436402 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4545 & F1 0.6250 | AUC 0.8681
01/28/2023 06:45:47 PM  [*] Sat Jan 28 18:45:47 2023:    1    | Tr.loss: 0.492055 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8300
01/28/2023 06:45:47 PM  [*] Started epoch: 2
01/28/2023 06:45:47 PM  [*] Sat Jan 28 18:45:47 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.401191 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5714 & F1 0.7273 | AUC 0.9080
01/28/2023 06:45:54 PM  [*] Sat Jan 28 18:45:54 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.244379 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.7162 & F1 0.8346 | AUC 0.9610
01/28/2023 06:46:00 PM  [*] Sat Jan 28 18:46:00 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.208826 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.8485 & F1 0.9180 | AUC 0.9742
01/28/2023 06:46:02 PM  [*] Sat Jan 28 18:46:02 2023:    2    | Tr.loss: 0.265200 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.31 & F1: 0.48 | AUC: 0.9500
01/28/2023 06:46:02 PM  [*] Started epoch: 3
01/28/2023 06:46:02 PM  [*] Sat Jan 28 18:46:02 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.242149 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.9184 & F1 0.9574 | AUC 0.9646
01/28/2023 06:46:09 PM  [*] Sat Jan 28 18:46:09 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.256139 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6452 & F1 0.7843 | AUC 0.9520
01/28/2023 06:46:15 PM  [*] Sat Jan 28 18:46:15 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.089700 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.9853 & F1 0.9926 | AUC 0.9991
01/28/2023 06:46:17 PM  [*] Sat Jan 28 18:46:17 2023:    3    | Tr.loss: 0.194256 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.36 & F1: 0.53 | AUC: 0.9743
01/28/2023 06:46:22 PM  [!] Evaluating model from split: 2 | epoch: 21
01/28/2023 06:46:23 PM  [*] Started epoch: 1
01/28/2023 06:46:23 PM  [*] Sat Jan 28 18:46:23 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.851668 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0556 & F1 0.1053 | AUC 0.4444
01/28/2023 06:46:29 PM  [*] Sat Jan 28 18:46:29 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.431984 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.1739 & F1 0.2963 | AUC 0.8728
01/28/2023 06:46:35 PM  [*] Sat Jan 28 18:46:35 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.353148 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5333 & F1 0.6957 | AUC 0.8965
01/28/2023 06:46:38 PM  [*] Sat Jan 28 18:46:38 2023:    1    | Tr.loss: 0.469696 | Elapsed:   14.95  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8447
01/28/2023 06:46:38 PM  [*] Started epoch: 2
01/28/2023 06:46:38 PM  [*] Sat Jan 28 18:46:38 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.375373 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.4565 & F1 0.6269 | AUC 0.8841
01/28/2023 06:46:44 PM  [*] Sat Jan 28 18:46:44 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.222416 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7143 & F1 0.8333 | AUC 0.9519
01/28/2023 06:46:50 PM  [*] Sat Jan 28 18:46:50 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.252736 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8667 & F1 0.9286 | AUC 0.9744
01/28/2023 06:46:53 PM  [*] Sat Jan 28 18:46:53 2023:    2    | Tr.loss: 0.260438 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.26 & F1: 0.41 | AUC: 0.9519
01/28/2023 06:46:53 PM  [*] Started epoch: 3
01/28/2023 06:46:53 PM  [*] Sat Jan 28 18:46:53 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.120243 | Elapsed: 0.05s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
01/28/2023 06:46:59 PM  [*] Sat Jan 28 18:46:59 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.171330 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.7000 & F1 0.8235 | AUC 0.9824
01/28/2023 06:47:05 PM  [*] Sat Jan 28 18:47:05 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.143263 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8060 & F1 0.8926 | AUC 0.9887
01/28/2023 06:47:08 PM  [*] Sat Jan 28 18:47:08 2023:    3    | Tr.loss: 0.187098 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.35 & F1: 0.52 | AUC: 0.9761
01/28/2023 06:47:13 PM  [!] Evaluating model from split: 2 | epoch: 22
01/28/2023 06:47:13 PM  [*] Started epoch: 1
01/28/2023 06:47:13 PM  [*] Sat Jan 28 18:47:13 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.355205 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0208 & F1 0.0408 | AUC 0.4023
01/28/2023 06:47:20 PM  [*] Sat Jan 28 18:47:20 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.430758 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.1143 & F1 0.2051 | AUC 0.8100
01/28/2023 06:47:26 PM  [*] Sat Jan 28 18:47:26 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.303631 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.4058 & F1 0.5773 | AUC 0.9037
01/28/2023 06:47:28 PM  [*] Sat Jan 28 18:47:28 2023:    1    | Tr.loss: 0.490147 | Elapsed:   14.97  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8343
01/28/2023 06:47:28 PM  [*] Started epoch: 2
01/28/2023 06:47:28 PM  [*] Sat Jan 28 18:47:28 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.362237 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5208 & F1 0.6849 | AUC 0.9271
01/28/2023 06:47:35 PM  [*] Sat Jan 28 18:47:35 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.495790 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6479 & F1 0.7863 | AUC 0.9155
01/28/2023 06:47:41 PM  [*] Sat Jan 28 18:47:41 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.371494 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.3385 & F1 0.5057 | AUC 0.9530
01/28/2023 06:47:43 PM  [*] Sat Jan 28 18:47:43 2023:    2    | Tr.loss: 0.258097 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.24 & F1: 0.39 | AUC: 0.9534
01/28/2023 06:47:43 PM  [*] Started epoch: 3
01/28/2023 06:47:43 PM  [*] Sat Jan 28 18:47:43 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.191491 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8372 & F1 0.9114 | AUC 0.9745
01/28/2023 06:47:50 PM  [*] Sat Jan 28 18:47:50 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.248087 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6462 & F1 0.7850 | AUC 0.9574
01/28/2023 06:47:56 PM  [*] Sat Jan 28 18:47:56 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.169790 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6286 & F1 0.7719 | AUC 0.9781
01/28/2023 06:47:58 PM  [*] Sat Jan 28 18:47:58 2023:    3    | Tr.loss: 0.189527 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.41 & F1: 0.58 | AUC: 0.9753
01/28/2023 06:48:03 PM  [!] Evaluating model from split: 2 | epoch: 23
01/28/2023 06:48:04 PM  [*] Started epoch: 1
01/28/2023 06:48:04 PM  [*] Sat Jan 28 18:48:04 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.027428 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0750 & F1 0.1395 | AUC 0.3698
01/28/2023 06:48:10 PM  [*] Sat Jan 28 18:48:10 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.386918 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.2400 & F1 0.3871 | AUC 0.9227
01/28/2023 06:48:16 PM  [*] Sat Jan 28 18:48:16 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.451030 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.3231 & F1 0.4884 | AUC 0.8862
01/28/2023 06:48:19 PM  [*] Sat Jan 28 18:48:19 2023:    1    | Tr.loss: 0.472803 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8403
01/28/2023 06:48:19 PM  [*] Started epoch: 2
01/28/2023 06:48:19 PM  [*] Sat Jan 28 18:48:19 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.350766 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.2927 & F1 0.4528 | AUC 0.8993
01/28/2023 06:48:25 PM  [*] Sat Jan 28 18:48:25 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.241941 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.8065 & F1 0.8929 | AUC 0.9699
01/28/2023 06:48:31 PM  [*] Sat Jan 28 18:48:31 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.336189 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4655 & F1 0.6353 | AUC 0.9298
01/28/2023 06:48:34 PM  [*] Sat Jan 28 18:48:34 2023:    2    | Tr.loss: 0.256024 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.21 & F1: 0.34 | AUC: 0.9534
01/28/2023 06:48:34 PM  [*] Started epoch: 3
01/28/2023 06:48:34 PM  [*] Sat Jan 28 18:48:34 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.234465 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9570
01/28/2023 06:48:40 PM  [*] Sat Jan 28 18:48:40 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.095125 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.7568 & F1 0.8615 | AUC 0.9818
01/28/2023 06:48:46 PM  [*] Sat Jan 28 18:48:46 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.107419 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.7344 & F1 0.8468 | AUC 0.9865
01/28/2023 06:48:49 PM  [*] Sat Jan 28 18:48:49 2023:    3    | Tr.loss: 0.184347 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.50 | AUC: 0.9766
01/28/2023 06:48:54 PM  [!] Evaluating model from split: 2 | epoch: 24
01/28/2023 06:48:55 PM  [*] Started epoch: 1
01/28/2023 06:48:55 PM  [*] Sat Jan 28 18:48:55 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.720632 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0870 & F1 0.1600 | AUC 0.3684
01/28/2023 06:49:01 PM  [*] Sat Jan 28 18:49:01 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.496216 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.1304 & F1 0.2308 | AUC 0.7817
01/28/2023 06:49:07 PM  [*] Sat Jan 28 18:49:07 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.350222 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4333 & F1 0.6047 | AUC 0.8796
01/28/2023 06:49:09 PM  [*] Sat Jan 28 18:49:09 2023:    1    | Tr.loss: 0.492948 | Elapsed:   14.94  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8224
01/28/2023 06:49:10 PM  [*] Started epoch: 2
01/28/2023 06:49:10 PM  [*] Sat Jan 28 18:49:10 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.337006 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.3902 & F1 0.5614 | AUC 0.9141
01/28/2023 06:49:16 PM  [*] Sat Jan 28 18:49:16 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.204039 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.6429 & F1 0.7826 | AUC 0.9576
01/28/2023 06:49:22 PM  [*] Sat Jan 28 18:49:22 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.241993 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4444 & F1 0.6154 | AUC 0.9370
01/28/2023 06:49:25 PM  [*] Sat Jan 28 18:49:25 2023:    2    | Tr.loss: 0.272829 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.30 & F1: 0.47 | AUC: 0.9469
01/28/2023 06:49:25 PM  [*] Started epoch: 3
01/28/2023 06:49:25 PM  [*] Sat Jan 28 18:49:25 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.217286 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.7222 & F1 0.8387 | AUC 0.9702
01/28/2023 06:49:31 PM  [*] Sat Jan 28 18:49:31 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.188011 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5882 & F1 0.7407 | AUC 0.9609
01/28/2023 06:49:37 PM  [*] Sat Jan 28 18:49:37 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.106637 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.9155 & F1 0.9559 | AUC 0.9908
01/28/2023 06:49:39 PM  [*] Sat Jan 28 18:49:39 2023:    3    | Tr.loss: 0.196579 | Elapsed:   14.98  s | FPR 0.0003 -> TPR: 0.37 & F1: 0.54 | AUC: 0.9732
01/28/2023 06:49:44 PM  [!] Evaluating model from split: 2 | epoch: 25
01/28/2023 06:49:45 PM  [*] Started epoch: 1
01/28/2023 06:49:45 PM  [*] Sat Jan 28 18:49:45 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.373577 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.3774
01/28/2023 06:49:51 PM  [*] Sat Jan 28 18:49:51 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.477509 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.4590 & F1 0.6292 | AUC 0.8630
01/28/2023 06:49:58 PM  [*] Sat Jan 28 18:49:58 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.350232 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3833 & F1 0.5542 | AUC 0.9079
01/28/2023 06:50:00 PM  [*] Sat Jan 28 18:50:00 2023:    1    | Tr.loss: 0.498137 | Elapsed:   14.96  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8162
01/28/2023 06:50:00 PM  [*] Started epoch: 2
01/28/2023 06:50:00 PM  [*] Sat Jan 28 18:50:00 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.348208 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7021 & F1 0.8250 | AUC 0.9349
01/28/2023 06:50:06 PM  [*] Sat Jan 28 18:50:06 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.228285 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5753 & F1 0.7304 | AUC 0.9356
01/28/2023 06:50:13 PM  [*] Sat Jan 28 18:50:13 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.268115 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.5600 & F1 0.7179 | AUC 0.9596
01/28/2023 06:50:15 PM  [*] Sat Jan 28 18:50:15 2023:    2    | Tr.loss: 0.279111 | Elapsed:   15.04  s | FPR 0.0003 -> TPR: 0.26 & F1: 0.41 | AUC: 0.9433
01/28/2023 06:50:15 PM  [*] Started epoch: 3
01/28/2023 06:50:15 PM  [*] Sat Jan 28 18:50:15 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.235790 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.5208 & F1 0.6849 | AUC 0.9544
01/28/2023 06:50:21 PM  [*] Sat Jan 28 18:50:21 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.147639 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8649 & F1 0.9275 | AUC 0.9844
01/28/2023 06:50:28 PM  [*] Sat Jan 28 18:50:28 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.280634 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.6935 & F1 0.8190 | AUC 0.9713
01/28/2023 06:50:30 PM  [*] Sat Jan 28 18:50:30 2023:    3    | Tr.loss: 0.202269 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.40 & F1: 0.57 | AUC: 0.9720
01/28/2023 06:50:35 PM  [!] Evaluating model from split: 2 | epoch: 26
01/28/2023 06:50:36 PM  [*] Started epoch: 1
01/28/2023 06:50:36 PM  [*] Sat Jan 28 18:50:36 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.757929 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0217 & F1 0.0426 | AUC 0.4553
01/28/2023 06:50:42 PM  [*] Sat Jan 28 18:50:42 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.340665 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.5479 & F1 0.7080 | AUC 0.9274
01/28/2023 06:50:48 PM  [*] Sat Jan 28 18:50:48 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.304548 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.3385 & F1 0.5057 | AUC 0.9130
01/28/2023 06:50:51 PM  [*] Sat Jan 28 18:50:51 2023:    1    | Tr.loss: 0.504160 | Elapsed:   14.93  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8157
01/28/2023 06:50:51 PM  [*] Started epoch: 2
01/28/2023 06:50:51 PM  [*] Sat Jan 28 18:50:51 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.271578 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.8980 & F1 0.9462 | AUC 0.9755
01/28/2023 06:50:57 PM  [*] Sat Jan 28 18:50:57 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.215314 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.8289 & F1 0.9065 | AUC 0.9792
01/28/2023 06:51:03 PM  [*] Sat Jan 28 18:51:03 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.302826 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5890 & F1 0.7414 | AUC 0.9508
01/28/2023 06:51:06 PM  [*] Sat Jan 28 18:51:06 2023:    2    | Tr.loss: 0.272756 | Elapsed:   15.00  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.45 | AUC: 0.9472
01/28/2023 06:51:06 PM  [*] Started epoch: 3
01/28/2023 06:51:06 PM  [*] Sat Jan 28 18:51:06 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.311441 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.5789 & F1 0.7333 | AUC 0.9464
01/28/2023 06:51:12 PM  [*] Sat Jan 28 18:51:12 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.213072 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8125 & F1 0.8966 | AUC 0.9818
01/28/2023 06:51:18 PM  [*] Sat Jan 28 18:51:18 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.215083 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5303 & F1 0.6931 | AUC 0.9782
01/28/2023 06:51:21 PM  [*] Sat Jan 28 18:51:21 2023:    3    | Tr.loss: 0.196281 | Elapsed:   15.02  s | FPR 0.0003 -> TPR: 0.30 & F1: 0.46 | AUC: 0.9733
01/28/2023 06:51:26 PM  [!] Evaluating model from split: 2 | epoch: 27
01/28/2023 06:51:26 PM  [*] Started epoch: 1
01/28/2023 06:51:26 PM  [*] Sat Jan 28 18:51:26 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.811605 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0213 & F1 0.0417 | AUC 0.3041
01/28/2023 06:51:33 PM  [*] Sat Jan 28 18:51:33 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.419610 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.4615 & F1 0.6316 | AUC 0.8725
01/28/2023 06:51:39 PM  [*] Sat Jan 28 18:51:39 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.393497 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.5211 & F1 0.6852 | AUC 0.8980
01/28/2023 06:51:41 PM  [*] Sat Jan 28 18:51:41 2023:    1    | Tr.loss: 0.483537 | Elapsed:   14.95  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8349
01/28/2023 06:51:41 PM  [*] Started epoch: 2
01/28/2023 06:51:41 PM  [*] Sat Jan 28 18:51:41 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.209759 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.9459 & F1 0.9722 | AUC 0.9890
01/28/2023 06:51:47 PM  [*] Sat Jan 28 18:51:47 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.239477 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5211 & F1 0.6852 | AUC 0.9621
01/28/2023 06:51:54 PM  [*] Sat Jan 28 18:51:54 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.209062 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.7639 & F1 0.8661 | AUC 0.9717
01/28/2023 06:51:56 PM  [*] Sat Jan 28 18:51:56 2023:    2    | Tr.loss: 0.264471 | Elapsed:   15.01  s | FPR 0.0003 -> TPR: 0.22 & F1: 0.36 | AUC: 0.9499
01/28/2023 06:51:56 PM  [*] Started epoch: 3
01/28/2023 06:51:56 PM  [*] Sat Jan 28 18:51:56 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.268479 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.4894 & F1 0.6571 | AUC 0.9362
01/28/2023 06:52:03 PM  [*] Sat Jan 28 18:52:03 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.127785 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.7619 & F1 0.8649 | AUC 0.9936
01/28/2023 06:52:09 PM  [*] Sat Jan 28 18:52:09 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.266133 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.6250 & F1 0.7692 | AUC 0.9748
01/28/2023 06:52:11 PM  [*] Sat Jan 28 18:52:11 2023:    3    | Tr.loss: 0.195352 | Elapsed:   15.03  s | FPR 0.0003 -> TPR: 0.41 & F1: 0.58 | AUC: 0.9737
01/28/2023 06:52:16 PM  [!] Evaluating model from split: 2 | epoch: 28
01/28/2023 06:52:17 PM  [*] Started epoch: 1
01/28/2023 06:52:17 PM  [*] Sat Jan 28 18:52:17 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.053164 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0222 & F1 0.0435 | AUC 0.4351
01/28/2023 06:52:23 PM  [*] Sat Jan 28 18:52:23 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.443404 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.2879 & F1 0.4471 | AUC 0.8111
01/28/2023 06:52:29 PM  [*] Sat Jan 28 18:52:29 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.326732 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.2973 & F1 0.4583 | AUC 0.8888
01/28/2023 06:52:32 PM  [*] Sat Jan 28 18:52:32 2023:    1    | Tr.loss: 0.482326 | Elapsed:   14.97  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8263
01/28/2023 06:52:32 PM  [*] Started epoch: 2
01/28/2023 06:52:32 PM  [*] Sat Jan 28 18:52:32 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.168373 | Elapsed: 0.06s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
01/28/2023 06:52:38 PM  [*] Sat Jan 28 18:52:38 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.262197 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.5479 & F1 0.7080 | AUC 0.9528
01/28/2023 06:52:44 PM  [*] Sat Jan 28 18:52:44 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.232813 | Elapsed: 6.15s | FPR 0.0003 -> TPR 0.8028 & F1 0.8906 | AUC 0.9709
01/28/2023 06:52:47 PM  [*] Sat Jan 28 18:52:47 2023:    2    | Tr.loss: 0.272305 | Elapsed:   14.99  s | FPR 0.0003 -> TPR: 0.28 & F1: 0.44 | AUC: 0.9467
01/28/2023 06:52:47 PM  [*] Started epoch: 3
01/28/2023 06:52:47 PM  [*] Sat Jan 28 18:52:47 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.167282 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.6444 & F1 0.7838 | AUC 0.9696
01/28/2023 06:52:53 PM  [*] Sat Jan 28 18:52:53 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.165594 | Elapsed: 6.13s | FPR 0.0003 -> TPR 0.8630 & F1 0.9265 | AUC 0.9716
01/28/2023 06:52:59 PM  [*] Sat Jan 28 18:52:59 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.134421 | Elapsed: 6.14s | FPR 0.0003 -> TPR 0.8806 & F1 0.9365 | AUC 0.9910
01/28/2023 06:53:02 PM  [*] Sat Jan 28 18:53:02 2023:    3    | Tr.loss: 0.191284 | Elapsed:   15.06  s | FPR 0.0003 -> TPR: 0.41 & F1: 0.58 | AUC: 0.9745
01/28/2023 06:53:07 PM  [!] Evaluating model from split: 2 | epoch: 29
01/28/2023 06:53:08 PM  [*] Started epoch: 1
01/28/2023 06:53:08 PM  [*] Sat Jan 28 18:53:08 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.328122 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0250 & F1 0.0488 | AUC 0.3719
01/28/2023 06:53:14 PM  [*] Sat Jan 28 18:53:14 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.302735 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.4133 & F1 0.5849 | AUC 0.9413
01/28/2023 06:53:20 PM  [*] Sat Jan 28 18:53:20 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.426387 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.4219 & F1 0.5934 | AUC 0.8481
01/28/2023 06:53:23 PM  [*] Sat Jan 28 18:53:23 2023:    1    | Tr.loss: 0.486439 | Elapsed:   15.09  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8267
01/28/2023 06:53:23 PM  [*] Started epoch: 2
01/28/2023 06:53:23 PM  [*] Sat Jan 28 18:53:23 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.347083 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.4286 & F1 0.6000 | AUC 0.8972
01/28/2023 06:53:29 PM  [*] Sat Jan 28 18:53:29 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.327820 | Elapsed: 6.16s | FPR 0.0003 -> TPR 0.5606 & F1 0.7184 | AUC 0.9086
01/28/2023 06:53:35 PM  [*] Sat Jan 28 18:53:35 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.240732 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.7460 & F1 0.8545 | AUC 0.9451
01/28/2023 06:53:38 PM  [*] Sat Jan 28 18:53:38 2023:    2    | Tr.loss: 0.270283 | Elapsed:   15.15  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.32 | AUC: 0.9472
01/28/2023 06:53:38 PM  [*] Started epoch: 3
01/28/2023 06:53:38 PM  [*] Sat Jan 28 18:53:38 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.189728 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7442 & F1 0.8533 | AUC 0.9745
01/28/2023 06:53:44 PM  [*] Sat Jan 28 18:53:44 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.263538 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.6087 & F1 0.7568 | AUC 0.9654
01/28/2023 06:53:50 PM  [*] Sat Jan 28 18:53:50 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.195633 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.8382 & F1 0.9120 | AUC 0.9881
01/28/2023 06:53:53 PM  [*] Sat Jan 28 18:53:53 2023:    3    | Tr.loss: 0.195115 | Elapsed:   15.18  s | FPR 0.0003 -> TPR: 0.38 & F1: 0.55 | AUC: 0.9736
01/28/2023 06:53:58 PM  [!] Evaluating model from split: 2 | epoch: 30
01/28/2023 06:53:59 PM  [*] Started epoch: 1
01/28/2023 06:53:59 PM  [*] Sat Jan 28 18:53:59 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 3.256254 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.0222 & F1 0.0435 | AUC 0.3018
01/28/2023 06:54:05 PM  [*] Sat Jan 28 18:54:05 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.599221 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.1613 & F1 0.2778 | AUC 0.7835
01/28/2023 06:54:11 PM  [*] Sat Jan 28 18:54:11 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.267471 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.4928 & F1 0.6602 | AUC 0.9331
01/28/2023 06:54:14 PM  [*] Sat Jan 28 18:54:14 2023:    1    | Tr.loss: 0.478146 | Elapsed:   15.29  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8312
01/28/2023 06:54:14 PM  [*] Started epoch: 2
01/28/2023 06:54:14 PM  [*] Sat Jan 28 18:54:14 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.418172 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.4651 & F1 0.6349 | AUC 0.8926
01/28/2023 06:54:20 PM  [*] Sat Jan 28 18:54:20 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.201369 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.6761 & F1 0.8067 | AUC 0.9752
01/28/2023 06:54:26 PM  [*] Sat Jan 28 18:54:26 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.239789 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8873 & F1 0.9403 | AUC 0.9757
01/28/2023 06:54:29 PM  [*] Sat Jan 28 18:54:29 2023:    2    | Tr.loss: 0.266653 | Elapsed:   15.16  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.45 | AUC: 0.9493
01/28/2023 06:54:29 PM  [*] Started epoch: 3
01/28/2023 06:54:29 PM  [*] Sat Jan 28 18:54:29 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.117421 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.9744 & F1 0.9870 | AUC 0.9990
01/28/2023 06:54:35 PM  [*] Sat Jan 28 18:54:35 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.157766 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.9032 & F1 0.9492 | AUC 0.9822
01/28/2023 06:54:42 PM  [*] Sat Jan 28 18:54:42 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.097116 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.9254 & F1 0.9612 | AUC 0.9937
01/28/2023 06:54:44 PM  [*] Sat Jan 28 18:54:44 2023:    3    | Tr.loss: 0.190253 | Elapsed:   15.13  s | FPR 0.0003 -> TPR: 0.41 & F1: 0.58 | AUC: 0.9749
