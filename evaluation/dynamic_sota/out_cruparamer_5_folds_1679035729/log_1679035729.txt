2023-03-17 07:48:49,052 WARNING  [*] Parsing malicious reports...
2023-03-17 07:58:35,853 WARNING  [*] Parsing benign reports...
2023-03-17 08:02:21,732 WARNING  [*] Parsing test reports...
2023-03-17 08:09:20,216 WARNING  [*] Preprocessing reports Nebula style...
2023-03-17 08:09:20,216 WARNING  [!] Initialized tokenizer without pre-trained model.
	You need to train tokenizer with .train() or specify 'model_path=' during initialization!
2023-03-17 08:09:20,216 WARNING  [*] Initializing tokenizer training...
2023-03-17 08:09:20,216 WARNING  [*] Data preparation for SentencePiece tokenizer...
2023-03-17 08:15:07,976 WARNING  [*] Saving to disk...
2023-03-17 08:15:21,199 WARNING  [!] Training tokenizer with command: --input=out_cruparamer_5_folds_1679035729\nebula_vocab_50000_seqlen_512\tokenizer_50000_trainset_1679037307.txt --model_prefix=out_cruparamer_5_folds_1679035729\nebula_vocab_50000_seqlen_512\tokenizer_50000 --vocab_size=50000 --model_type=bpe --split_by_number=False --max_sentence_length=4192 --max_sentencepiece_length=64
2023-03-17 08:28:59,190 WARNING  [!] Loaded vocab with size 50001 from out_cruparamer_5_folds_1679035729\nebula_vocab_50000_seqlen_512\tokenizer_50000.vocab
2023-03-17 08:29:00,812 WARNING  [*] Encoding and padding...
