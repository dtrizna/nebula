2023-03-30 09:05:39,391 WARNING  [!] Working on full!
2023-03-30 09:05:39,392 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\full_vocab_50000_seqlen_512\x_train_full.npy
2023-03-30 09:05:39,475 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\full_vocab_50000_seqlen_512\x_test_full.npy
2023-03-30 09:05:39,514 WARNING  [!!!] Starting CV over full!
2023-03-30 09:05:39,598 WARNING  [!] CV output folder out_fields_whitespace_1680006591\cv_full_limNone_r1763_t5 already exists, skipping!
2023-03-30 09:05:39,598 WARNING  [!] Working on api_only_name!
2023-03-30 09:05:39,599 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\api_only_name_vocab_50000_seqlen_512\x_train_full.npy
2023-03-30 09:05:39,691 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\api_only_name_vocab_50000_seqlen_512\x_test_full.npy
2023-03-30 09:05:39,712 WARNING  [!!!] Starting CV over api_only_name!
2023-03-30 09:05:39,798 WARNING  [!] CV output folder out_fields_whitespace_1680006591\cv_api_only_name_limNone_r1763_t5 already exists, skipping!
2023-03-30 09:05:39,798 WARNING  [!] Working on api_only_full!
2023-03-30 09:05:39,799 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\api_only_full_vocab_50000_seqlen_512\x_train_full.npy
2023-03-30 09:05:39,884 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\api_only_full_vocab_50000_seqlen_512\x_test_full.npy
2023-03-30 09:05:39,921 WARNING  [!!!] Starting CV over api_only_full!
2023-03-30 09:05:40,010 WARNING  [!] CV output folder out_fields_whitespace_1680006591\cv_api_only_full_limNone_r1763_t5 already exists, skipping!
2023-03-30 09:05:40,011 WARNING  [!] Working on file!
2023-03-30 09:05:40,011 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\file_vocab_50000_seqlen_512\x_train_full.npy
2023-03-30 09:05:40,124 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\file_vocab_50000_seqlen_512\x_test_full.npy
2023-03-30 09:05:40,149 WARNING  [!!!] Starting CV over file!
2023-03-30 09:05:40,275 WARNING  [!] CV output folder out_fields_whitespace_1680006591\cv_file_limNone_r1763_t5 already exists, skipping!
2023-03-30 09:05:40,275 WARNING  [!] Working on network!
2023-03-30 09:05:40,276 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\network_vocab_50000_seqlen_512\x_train_full.npy
2023-03-30 09:05:40,391 WARNING  [!] Skipping since exists: out_fields_whitespace_1680006591\network_vocab_50000_seqlen_512\x_test_full.npy
2023-03-30 09:05:40,408 WARNING  [!!!] Starting CV over network!
2023-03-30 09:05:40,517 WARNING  [!] Training time budget: 300min
2023-03-30 09:05:40,517 WARNING  [!] Model config: {'vocab_size': 152, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-03-30 09:05:40,615 WARNING  [1/3] Train set size: 60730, Validation set size: 30366
2023-03-30 09:05:41,459 WARNING  [!] Saved dataset splits to dataset_splits_1680159940.npz
2023-03-30 09:05:41,596 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.2070e6
2023-03-30 09:05:41,597 WARNING  [*] Training time budget set: 5.0 min
2023-03-30 09:05:41,629 WARNING  [*] Started epoch: 1
2023-03-30 09:05:43,418 WARNING  [*] 09:05:43: Train Epoch: 1 [  0  /60730 (0 %)] | Loss: 1.707461 | Elapsed: 1.78s | FPR 0.0003 -> TPR: 0.04 & F1: 0.08 | AUC: 0.5242
2023-03-30 09:05:53,339 WARNING  [*] 09:05:53: Train Epoch: 1 [9600 /60730 (16%)] | Loss: 0.679264 | Elapsed: 9.91s | FPR 0.0003 -> TPR: 0.07 & F1: 0.14 | AUC: 0.3928
2023-03-30 09:06:04,082 WARNING  [*] 09:06:04: Train Epoch: 1 [19200/60730 (32%)] | Loss: 0.629724 | Elapsed: 10.74s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4489
2023-03-30 09:06:15,361 WARNING  [*] 09:06:15: Train Epoch: 1 [28800/60730 (47%)] | Loss: 0.575447 | Elapsed: 11.27s | FPR 0.0003 -> TPR: 0.08 & F1: 0.14 | AUC: 0.4537
2023-03-30 09:06:26,238 WARNING  [*] 09:06:26: Train Epoch: 1 [38400/60730 (63%)] | Loss: 0.602708 | Elapsed: 10.88s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5320
2023-03-30 09:06:36,852 WARNING  [*] 09:06:36: Train Epoch: 1 [48000/60730 (79%)] | Loss: 0.594478 | Elapsed: 10.59s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5363
2023-03-30 09:06:47,750 WARNING  [*] 09:06:47: Train Epoch: 1 [57600/60730 (95%)] | Loss: 0.620208 | Elapsed: 10.88s | FPR 0.0003 -> TPR: 0.07 & F1: 0.14 | AUC: 0.5236
2023-03-30 09:06:52,717 WARNING  [*] Thu Mar 30 09:06:52 2023:    1    | Tr.loss: 0.635535 | Elapsed:   71.09  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4974
2023-03-30 09:06:52,717 WARNING  [*] Started epoch: 2
2023-03-30 09:06:52,851 WARNING  [*] 09:06:52: Train Epoch: 2 [  0  /60730 (0 %)] | Loss: 0.589331 | Elapsed: 0.12s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5390
2023-03-30 09:07:03,222 WARNING  [*] 09:07:03: Train Epoch: 2 [9600 /60730 (16%)] | Loss: 0.585015 | Elapsed: 10.37s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5495
2023-03-30 09:07:13,750 WARNING  [*] 09:07:13: Train Epoch: 2 [19200/60730 (32%)] | Loss: 0.623857 | Elapsed: 10.52s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.3922
2023-03-30 09:07:24,391 WARNING  [*] 09:07:24: Train Epoch: 2 [28800/60730 (47%)] | Loss: 0.620419 | Elapsed: 10.63s | FPR 0.0003 -> TPR: 0.09 & F1: 0.16 | AUC: 0.5273
2023-03-30 09:07:35,188 WARNING  [*] 09:07:35: Train Epoch: 2 [38400/60730 (63%)] | Loss: 0.618093 | Elapsed: 10.79s | FPR 0.0003 -> TPR: 0.03 & F1: 0.06 | AUC: 0.5574
2023-03-30 09:07:46,085 WARNING  [*] 09:07:46: Train Epoch: 2 [48000/60730 (79%)] | Loss: 0.687559 | Elapsed: 10.88s | FPR 0.0003 -> TPR: 0.05 & F1: 0.09 | AUC: 0.5229
2023-03-30 09:07:56,897 WARNING  [*] 09:07:56: Train Epoch: 2 [57600/60730 (95%)] | Loss: 0.608633 | Elapsed: 10.81s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4308
2023-03-30 09:08:02,048 WARNING  [*] Thu Mar 30 09:08:02 2023:    2    | Tr.loss: 0.604309 | Elapsed:   69.32  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4974
2023-03-30 09:08:02,048 WARNING  [*] Started epoch: 3
2023-03-30 09:08:02,165 WARNING  [*] 09:08:02: Train Epoch: 3 [  0  /60730 (0 %)] | Loss: 0.561765 | Elapsed: 0.10s | FPR 0.0003 -> TPR: 0.07 & F1: 0.13 | AUC: 0.5995
2023-03-30 09:08:13,015 WARNING  [*] 09:08:13: Train Epoch: 3 [9600 /60730 (16%)] | Loss: 0.533027 | Elapsed: 10.83s | FPR 0.0003 -> TPR: 0.04 & F1: 0.07 | AUC: 0.4275
2023-03-30 09:08:23,907 WARNING  [*] 09:08:23: Train Epoch: 3 [19200/60730 (32%)] | Loss: 0.609390 | Elapsed: 10.88s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.4956
2023-03-30 09:08:34,874 WARNING  [*] 09:08:34: Train Epoch: 3 [28800/60730 (47%)] | Loss: 0.610119 | Elapsed: 10.97s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5253
2023-03-30 09:08:45,760 WARNING  [*] 09:08:45: Train Epoch: 3 [38400/60730 (63%)] | Loss: 0.636121 | Elapsed: 10.85s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.4812
2023-03-30 09:08:56,649 WARNING  [*] 09:08:56: Train Epoch: 3 [48000/60730 (79%)] | Loss: 0.542601 | Elapsed: 10.87s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5932
2023-03-30 09:09:07,616 WARNING  [*] 09:09:07: Train Epoch: 3 [57600/60730 (95%)] | Loss: 0.639714 | Elapsed: 10.95s | FPR 0.0003 -> TPR: 0.02 & F1: 0.03 | AUC: 0.5385
2023-03-30 09:09:12,713 WARNING  [*] Thu Mar 30 09:09:12 2023:    3    | Tr.loss: 0.600356 | Elapsed:   70.66  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5044
2023-03-30 09:09:12,713 WARNING  [*] Started epoch: 4
2023-03-30 09:09:12,836 WARNING  [*] 09:09:12: Train Epoch: 4 [  0  /60730 (0 %)] | Loss: 0.606230 | Elapsed: 0.11s | FPR 0.0003 -> TPR: 0.07 & F1: 0.14 | AUC: 0.5126
2023-03-30 09:09:23,708 WARNING  [*] 09:09:23: Train Epoch: 4 [9600 /60730 (16%)] | Loss: 0.562656 | Elapsed: 10.87s | FPR 0.0003 -> TPR: 0.06 & F1: 0.12 | AUC: 0.5111
2023-03-30 09:09:34,680 WARNING  [*] 09:09:34: Train Epoch: 4 [19200/60730 (32%)] | Loss: 0.635952 | Elapsed: 10.96s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.3425
2023-03-30 09:09:45,786 WARNING  [*] 09:09:45: Train Epoch: 4 [28800/60730 (47%)] | Loss: 0.597233 | Elapsed: 11.09s | FPR 0.0003 -> TPR: 0.04 & F1: 0.08 | AUC: 0.4278
2023-03-30 09:09:56,818 WARNING  [*] 09:09:56: Train Epoch: 4 [38400/60730 (63%)] | Loss: 0.589738 | Elapsed: 11.03s | FPR 0.0003 -> TPR: 0.08 & F1: 0.15 | AUC: 0.6260
2023-03-30 09:10:07,799 WARNING  [*] 09:10:07: Train Epoch: 4 [48000/60730 (79%)] | Loss: 0.617727 | Elapsed: 10.95s | FPR 0.0003 -> TPR: 0.03 & F1: 0.06 | AUC: 0.6033
2023-03-30 09:10:18,463 WARNING  [*] 09:10:18: Train Epoch: 4 [57600/60730 (95%)] | Loss: 0.500969 | Elapsed: 10.65s | FPR 0.0003 -> TPR: 0.11 & F1: 0.20 | AUC: 0.5348
2023-03-30 09:10:23,438 WARNING  [*] Thu Mar 30 09:10:23 2023:    4    | Tr.loss: 0.598891 | Elapsed:   70.73  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5046
2023-03-30 09:10:23,438 WARNING  [*] Started epoch: 5
2023-03-30 09:10:23,561 WARNING  [*] 09:10:23: Train Epoch: 5 [  0  /60730 (0 %)] | Loss: 0.614024 | Elapsed: 0.11s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5039
2023-03-30 09:10:34,221 WARNING  [*] 09:10:34: Train Epoch: 5 [9600 /60730 (16%)] | Loss: 0.607160 | Elapsed: 10.64s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5005
2023-03-30 09:10:41,630 WARNING  [!] Time budget exceeded, training stopped.
2023-03-30 09:10:41,666 WARNING  [!] Thu Mar 30 09:10:41 2023: Dumped results:
                model       : 1680159940-model.torch
		train time  : 1680159940-trainTime.npy
		train losses: 1680159940-trainLosses.npy
		train AUC   : 1680159940-auc.npy
		train F1s   : 1680159940-trainF1s.npy
		train TPRs  : 1680159940-trainTPRs.npy
2023-03-30 09:10:41,685 WARNING  [!] Evaluating model on training set...
2023-03-30 09:10:58,935 WARNING  [!] This fold metrics on training set:
2023-03-30 09:10:58,945 WARNING 	AUC: 0.5005
2023-03-30 09:10:58,948 WARNING 	FPR: 0.0001 | TPR: 0.0002 | F1: 0.0004
2023-03-30 09:10:58,972 WARNING 	FPR: 0.0003 | TPR: 0.0002 | F1: 0.0004
2023-03-30 09:10:58,977 WARNING 	FPR: 0.001 | TPR: 0.0002 | F1: 0.0004
2023-03-30 09:10:58,988 WARNING 	FPR: 0.003 | TPR: 0.0002 | F1: 0.0004
2023-03-30 09:10:59,013 WARNING 	FPR: 0.01 | TPR: 0.0002 | F1: 0.0004
2023-03-30 09:10:59,018 WARNING 	FPR: 0.03 | TPR: 0.0209 | F1: 0.0407
2023-03-30 09:10:59,029 WARNING 	FPR: 0.1 | TPR: 0.0946 | F1: 0.1673
2023-03-30 09:10:59,029 WARNING  [!] Evaluating model on validation set...
2023-03-30 09:11:07,657 WARNING  [!] This fold metrics on validation set:
2023-03-30 09:11:07,667 WARNING 	AUC: 0.5005
2023-03-30 09:11:07,674 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:11:07,677 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:11:07,684 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:11:07,694 WARNING 	FPR: 0.003 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:11:07,702 WARNING 	FPR: 0.01 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:11:07,707 WARNING 	FPR: 0.03 | TPR: 0.0209 | F1: 0.0407
2023-03-30 09:11:07,712 WARNING 	FPR: 0.1 | TPR: 0.0943 | F1: 0.1667
2023-03-30 09:11:07,882 WARNING  [2/3] Train set size: 60731, Validation set size: 30365
2023-03-30 09:11:08,841 WARNING  [!] Saved dataset splits to dataset_splits_1680160267.npz
2023-03-30 09:11:08,884 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.2070e6
2023-03-30 09:11:08,884 WARNING  [*] Training time budget set: 5.0 min
2023-03-30 09:11:08,905 WARNING  [*] Started epoch: 1
2023-03-30 09:11:09,061 WARNING  [*] 09:11:09: Train Epoch: 1 [  0  /60731 (0 %)] | Loss: 2.123653 | Elapsed: 0.15s | FPR 0.0003 -> TPR: 0.03 & F1: 0.05 | AUC: 0.4479
2023-03-30 09:11:19,591 WARNING  [*] 09:11:19: Train Epoch: 1 [9600 /60731 (16%)] | Loss: 0.585546 | Elapsed: 10.51s | FPR 0.0003 -> TPR: 0.05 & F1: 0.10 | AUC: 0.4568
2023-03-30 09:11:30,069 WARNING  [*] 09:11:30: Train Epoch: 1 [19200/60731 (32%)] | Loss: 0.609303 | Elapsed: 10.46s | FPR 0.0003 -> TPR: 0.06 & F1: 0.11 | AUC: 0.5313
2023-03-30 09:11:40,635 WARNING  [*] 09:11:40: Train Epoch: 1 [28800/60731 (47%)] | Loss: 0.579951 | Elapsed: 10.57s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.4816
2023-03-30 09:11:51,145 WARNING  [*] 09:11:51: Train Epoch: 1 [38400/60731 (63%)] | Loss: 0.613734 | Elapsed: 10.49s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4407
2023-03-30 09:12:01,659 WARNING  [*] 09:12:01: Train Epoch: 1 [48000/60731 (79%)] | Loss: 0.637815 | Elapsed: 10.50s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5334
2023-03-30 09:12:12,220 WARNING  [*] 09:12:12: Train Epoch: 1 [57600/60731 (95%)] | Loss: 0.645590 | Elapsed: 10.56s | FPR 0.0003 -> TPR: 0.02 & F1: 0.03 | AUC: 0.5514
2023-03-30 09:12:17,076 WARNING  [*] Thu Mar 30 09:12:17 2023:    1    | Tr.loss: 0.630833 | Elapsed:   68.17  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5031
2023-03-30 09:12:17,076 WARNING  [*] Started epoch: 2
2023-03-30 09:12:17,209 WARNING  [*] 09:12:17: Train Epoch: 2 [  0  /60731 (0 %)] | Loss: 0.619831 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.06 & F1: 0.11 | AUC: 0.4837
2023-03-30 09:12:27,960 WARNING  [*] 09:12:27: Train Epoch: 2 [9600 /60731 (16%)] | Loss: 0.570949 | Elapsed: 10.75s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5205
2023-03-30 09:12:38,645 WARNING  [*] 09:12:38: Train Epoch: 2 [19200/60731 (32%)] | Loss: 0.581881 | Elapsed: 10.65s | FPR 0.0003 -> TPR: 0.03 & F1: 0.05 | AUC: 0.5416
2023-03-30 09:12:49,153 WARNING  [*] 09:12:49: Train Epoch: 2 [28800/60731 (47%)] | Loss: 0.679504 | Elapsed: 10.50s | FPR 0.0003 -> TPR: 0.02 & F1: 0.03 | AUC: 0.3976
2023-03-30 09:12:59,927 WARNING  [*] 09:12:59: Train Epoch: 2 [38400/60731 (63%)] | Loss: 0.603322 | Elapsed: 10.76s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4834
2023-03-30 09:13:10,501 WARNING  [*] 09:13:10: Train Epoch: 2 [48000/60731 (79%)] | Loss: 0.558282 | Elapsed: 10.57s | FPR 0.0003 -> TPR: 0.08 & F1: 0.14 | AUC: 0.5243
2023-03-30 09:13:20,970 WARNING  [*] 09:13:20: Train Epoch: 2 [57600/60731 (95%)] | Loss: 0.552123 | Elapsed: 10.45s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.6505
2023-03-30 09:13:25,818 WARNING  [*] Thu Mar 30 09:13:25 2023:    2    | Tr.loss: 0.606020 | Elapsed:   68.74  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5019
2023-03-30 09:13:25,818 WARNING  [*] Started epoch: 3
2023-03-30 09:13:25,937 WARNING  [*] 09:13:25: Train Epoch: 3 [  0  /60731 (0 %)] | Loss: 0.630881 | Elapsed: 0.11s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4826
2023-03-30 09:13:36,469 WARNING  [*] 09:13:36: Train Epoch: 3 [9600 /60731 (16%)] | Loss: 0.625584 | Elapsed: 10.52s | FPR 0.0003 -> TPR: 0.03 & F1: 0.06 | AUC: 0.4419
2023-03-30 09:13:46,976 WARNING  [*] 09:13:46: Train Epoch: 3 [19200/60731 (32%)] | Loss: 0.634741 | Elapsed: 10.50s | FPR 0.0003 -> TPR: 0.10 & F1: 0.18 | AUC: 0.5292
2023-03-30 09:13:57,541 WARNING  [*] 09:13:57: Train Epoch: 3 [28800/60731 (47%)] | Loss: 0.602705 | Elapsed: 10.55s | FPR 0.0003 -> TPR: 0.05 & F1: 0.10 | AUC: 0.4962
2023-03-30 09:14:08,108 WARNING  [*] 09:14:08: Train Epoch: 3 [38400/60731 (63%)] | Loss: 0.636109 | Elapsed: 10.56s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.4678
2023-03-30 09:14:18,983 WARNING  [*] 09:14:18: Train Epoch: 3 [48000/60731 (79%)] | Loss: 0.554472 | Elapsed: 10.86s | FPR 0.0003 -> TPR: 0.06 & F1: 0.12 | AUC: 0.5220
2023-03-30 09:14:29,470 WARNING  [*] 09:14:29: Train Epoch: 3 [57600/60731 (95%)] | Loss: 0.612097 | Elapsed: 10.47s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4458
2023-03-30 09:14:34,279 WARNING  [*] Thu Mar 30 09:14:34 2023:    3    | Tr.loss: 0.605709 | Elapsed:   68.46  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4960
2023-03-30 09:14:34,279 WARNING  [*] Started epoch: 4
2023-03-30 09:14:34,410 WARNING  [*] 09:14:34: Train Epoch: 4 [  0  /60731 (0 %)] | Loss: 0.668081 | Elapsed: 0.12s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5461
2023-03-30 09:14:44,886 WARNING  [*] 09:14:44: Train Epoch: 4 [9600 /60731 (16%)] | Loss: 0.624566 | Elapsed: 10.48s | FPR 0.0003 -> TPR: 0.11 & F1: 0.19 | AUC: 0.6582
2023-03-30 09:14:55,438 WARNING  [*] 09:14:55: Train Epoch: 4 [19200/60731 (32%)] | Loss: 0.566216 | Elapsed: 10.55s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.3707
2023-03-30 09:15:06,853 WARNING  [*] 09:15:06: Train Epoch: 4 [28800/60731 (47%)] | Loss: 0.605726 | Elapsed: 11.40s | FPR 0.0003 -> TPR: 0.08 & F1: 0.15 | AUC: 0.4849
2023-03-30 09:15:17,475 WARNING  [*] 09:15:17: Train Epoch: 4 [38400/60731 (63%)] | Loss: 0.543329 | Elapsed: 10.61s | FPR 0.0003 -> TPR: 0.15 & F1: 0.26 | AUC: 0.5349
2023-03-30 09:15:28,705 WARNING  [*] 09:15:28: Train Epoch: 4 [48000/60731 (79%)] | Loss: 0.581960 | Elapsed: 11.22s | FPR 0.0003 -> TPR: 0.14 & F1: 0.25 | AUC: 0.6821
2023-03-30 09:15:39,285 WARNING  [*] 09:15:39: Train Epoch: 4 [57600/60731 (95%)] | Loss: 0.616587 | Elapsed: 10.56s | FPR 0.0003 -> TPR: 0.04 & F1: 0.09 | AUC: 0.5767
2023-03-30 09:15:44,093 WARNING  [*] Thu Mar 30 09:15:44 2023:    4    | Tr.loss: 0.603354 | Elapsed:   69.81  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4994
2023-03-30 09:15:44,093 WARNING  [*] Started epoch: 5
2023-03-30 09:15:44,216 WARNING  [*] 09:15:44: Train Epoch: 5 [  0  /60731 (0 %)] | Loss: 0.641528 | Elapsed: 0.11s | FPR 0.0003 -> TPR: 0.05 & F1: 0.09 | AUC: 0.4364
2023-03-30 09:15:55,057 WARNING  [*] 09:15:55: Train Epoch: 5 [9600 /60731 (16%)] | Loss: 0.573258 | Elapsed: 10.83s | FPR 0.0003 -> TPR: 0.10 & F1: 0.18 | AUC: 0.5893
2023-03-30 09:16:05,985 WARNING  [*] 09:16:05: Train Epoch: 5 [19200/60731 (32%)] | Loss: 0.557098 | Elapsed: 10.93s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4585
2023-03-30 09:16:08,915 WARNING  [!] Time budget exceeded, training stopped.
2023-03-30 09:16:08,959 WARNING  [!] Thu Mar 30 09:16:08 2023: Dumped results:
                model       : 1680160267-model.torch
		train time  : 1680160267-trainTime.npy
		train losses: 1680160267-trainLosses.npy
		train AUC   : 1680160267-auc.npy
		train F1s   : 1680160267-trainF1s.npy
		train TPRs  : 1680160267-trainTPRs.npy
2023-03-30 09:16:08,988 WARNING  [!] Evaluating model on training set...
2023-03-30 09:16:26,646 WARNING  [!] This fold metrics on training set:
2023-03-30 09:16:26,671 WARNING 	AUC: 0.5002
2023-03-30 09:16:26,674 WARNING 	FPR: 0.0001 | TPR: 0.0001 | F1: 0.0001
2023-03-30 09:16:26,700 WARNING 	FPR: 0.0003 | TPR: 0.0001 | F1: 0.0001
2023-03-30 09:16:26,705 WARNING 	FPR: 0.001 | TPR: 0.0001 | F1: 0.0001
2023-03-30 09:16:26,717 WARNING 	FPR: 0.003 | TPR: 0.0001 | F1: 0.0001
2023-03-30 09:16:26,744 WARNING 	FPR: 0.01 | TPR: 0.0001 | F1: 0.0001
2023-03-30 09:16:26,748 WARNING 	FPR: 0.03 | TPR: 0.0209 | F1: 0.0406
2023-03-30 09:16:26,772 WARNING 	FPR: 0.1 | TPR: 0.0938 | F1: 0.1659
2023-03-30 09:16:26,773 WARNING  [!] Evaluating model on validation set...
2023-03-30 09:16:35,653 WARNING  [!] This fold metrics on validation set:
2023-03-30 09:16:35,663 WARNING 	AUC: 0.5010
2023-03-30 09:16:35,670 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-30 09:16:35,673 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-30 09:16:35,684 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-30 09:16:35,691 WARNING 	FPR: 0.003 | TPR: 0.0000 | F1: 0.0000
2023-03-30 09:16:35,698 WARNING 	FPR: 0.01 | TPR: 0.0000 | F1: 0.0000
2023-03-30 09:16:35,702 WARNING 	FPR: 0.03 | TPR: 0.0209 | F1: 0.0406
2023-03-30 09:16:35,709 WARNING 	FPR: 0.1 | TPR: 0.0940 | F1: 0.1664
2023-03-30 09:16:35,867 WARNING  [3/3] Train set size: 60731, Validation set size: 30365
2023-03-30 09:16:36,843 WARNING  [!] Saved dataset splits to dataset_splits_1680160595.npz
2023-03-30 09:16:36,868 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.2070e6
2023-03-30 09:16:36,868 WARNING  [*] Training time budget set: 5.0 min
2023-03-30 09:16:36,903 WARNING  [*] Started epoch: 1
2023-03-30 09:16:37,069 WARNING  [*] 09:16:37: Train Epoch: 1 [  0  /60731 (0 %)] | Loss: 2.524776 | Elapsed: 0.15s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5555
2023-03-30 09:16:47,767 WARNING  [*] 09:16:47: Train Epoch: 1 [9600 /60731 (16%)] | Loss: 0.562356 | Elapsed: 10.70s | FPR 0.0003 -> TPR: 0.05 & F1: 0.10 | AUC: 0.5384
2023-03-30 09:16:58,534 WARNING  [*] 09:16:58: Train Epoch: 1 [19200/60731 (32%)] | Loss: 0.550019 | Elapsed: 10.76s | FPR 0.0003 -> TPR: 0.15 & F1: 0.26 | AUC: 0.6414
2023-03-30 09:17:09,244 WARNING  [*] 09:17:09: Train Epoch: 1 [28800/60731 (47%)] | Loss: 0.552221 | Elapsed: 10.71s | FPR 0.0003 -> TPR: 0.03 & F1: 0.05 | AUC: 0.5396
2023-03-30 09:17:20,106 WARNING  [*] 09:17:20: Train Epoch: 1 [38400/60731 (63%)] | Loss: 0.617384 | Elapsed: 10.85s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5053
2023-03-30 09:17:30,819 WARNING  [*] 09:17:30: Train Epoch: 1 [48000/60731 (79%)] | Loss: 0.530198 | Elapsed: 10.71s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.4682
2023-03-30 09:17:41,767 WARNING  [*] 09:17:41: Train Epoch: 1 [57600/60731 (95%)] | Loss: 0.587519 | Elapsed: 10.93s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5454
2023-03-30 09:17:46,717 WARNING  [*] Thu Mar 30 09:17:46 2023:    1    | Tr.loss: 0.633802 | Elapsed:   69.81  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5027
2023-03-30 09:17:46,717 WARNING  [*] Started epoch: 2
2023-03-30 09:17:46,832 WARNING  [*] 09:17:46: Train Epoch: 2 [  0  /60731 (0 %)] | Loss: 0.647807 | Elapsed: 0.10s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5052
2023-03-30 09:17:57,443 WARNING  [*] 09:17:57: Train Epoch: 2 [9600 /60731 (16%)] | Loss: 0.587239 | Elapsed: 10.59s | FPR 0.0003 -> TPR: 0.03 & F1: 0.05 | AUC: 0.5585
2023-03-30 09:18:08,300 WARNING  [*] 09:18:08: Train Epoch: 2 [19200/60731 (32%)] | Loss: 0.660902 | Elapsed: 10.86s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.3331
2023-03-30 09:18:19,095 WARNING  [*] 09:18:19: Train Epoch: 2 [28800/60731 (47%)] | Loss: 0.577690 | Elapsed: 10.79s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.4523
2023-03-30 09:18:29,941 WARNING  [*] 09:18:29: Train Epoch: 2 [38400/60731 (63%)] | Loss: 0.601570 | Elapsed: 10.83s | FPR 0.0003 -> TPR: 0.06 & F1: 0.11 | AUC: 0.5595
2023-03-30 09:18:40,573 WARNING  [*] 09:18:40: Train Epoch: 2 [48000/60731 (79%)] | Loss: 0.673172 | Elapsed: 10.63s | FPR 0.0003 -> TPR: 0.02 & F1: 0.03 | AUC: 0.4975
2023-03-30 09:18:51,338 WARNING  [*] 09:18:51: Train Epoch: 2 [57600/60731 (95%)] | Loss: 0.550353 | Elapsed: 10.75s | FPR 0.0003 -> TPR: 0.04 & F1: 0.07 | AUC: 0.4469
2023-03-30 09:18:56,344 WARNING  [*] Thu Mar 30 09:18:56 2023:    2    | Tr.loss: 0.602017 | Elapsed:   69.63  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5029
2023-03-30 09:18:56,344 WARNING  [*] Started epoch: 3
2023-03-30 09:18:56,467 WARNING  [*] 09:18:56: Train Epoch: 3 [  0  /60731 (0 %)] | Loss: 0.606100 | Elapsed: 0.11s | FPR 0.0003 -> TPR: 0.04 & F1: 0.08 | AUC: 0.4970
2023-03-30 09:19:07,034 WARNING  [*] 09:19:07: Train Epoch: 3 [9600 /60731 (16%)] | Loss: 0.598600 | Elapsed: 10.57s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5189
2023-03-30 09:19:17,889 WARNING  [*] 09:19:17: Train Epoch: 3 [19200/60731 (32%)] | Loss: 0.630605 | Elapsed: 10.85s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.4850
2023-03-30 09:19:28,890 WARNING  [*] 09:19:28: Train Epoch: 3 [28800/60731 (47%)] | Loss: 0.646909 | Elapsed: 10.99s | FPR 0.0003 -> TPR: 0.03 & F1: 0.06 | AUC: 0.5000
2023-03-30 09:19:39,738 WARNING  [*] 09:19:39: Train Epoch: 3 [38400/60731 (63%)] | Loss: 0.605622 | Elapsed: 10.85s | FPR 0.0003 -> TPR: 0.11 & F1: 0.20 | AUC: 0.4876
2023-03-30 09:19:50,630 WARNING  [*] 09:19:50: Train Epoch: 3 [48000/60731 (79%)] | Loss: 0.652861 | Elapsed: 10.89s | FPR 0.0003 -> TPR: 0.02 & F1: 0.03 | AUC: 0.5312
2023-03-30 09:20:01,362 WARNING  [*] 09:20:01: Train Epoch: 3 [57600/60731 (95%)] | Loss: 0.652209 | Elapsed: 10.72s | FPR 0.0003 -> TPR: 0.05 & F1: 0.09 | AUC: 0.3864
2023-03-30 09:20:06,259 WARNING  [*] Thu Mar 30 09:20:06 2023:    3    | Tr.loss: 0.600989 | Elapsed:   69.92  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5003
2023-03-30 09:20:06,259 WARNING  [*] Started epoch: 4
2023-03-30 09:20:06,377 WARNING  [*] 09:20:06: Train Epoch: 4 [  0  /60731 (0 %)] | Loss: 0.583984 | Elapsed: 0.11s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4958
2023-03-30 09:20:17,108 WARNING  [*] 09:20:17: Train Epoch: 4 [9600 /60731 (16%)] | Loss: 0.611180 | Elapsed: 10.72s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.5802
2023-03-30 09:20:27,769 WARNING  [*] 09:20:27: Train Epoch: 4 [19200/60731 (32%)] | Loss: 0.614467 | Elapsed: 10.64s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4241
2023-03-30 09:20:38,426 WARNING  [*] 09:20:38: Train Epoch: 4 [28800/60731 (47%)] | Loss: 0.551917 | Elapsed: 10.65s | FPR 0.0003 -> TPR: 0.03 & F1: 0.05 | AUC: 0.3695
2023-03-30 09:20:49,063 WARNING  [*] 09:20:49: Train Epoch: 4 [38400/60731 (63%)] | Loss: 0.527430 | Elapsed: 10.64s | FPR 0.0003 -> TPR: 0.09 & F1: 0.17 | AUC: 0.5318
2023-03-30 09:20:59,710 WARNING  [*] 09:20:59: Train Epoch: 4 [48000/60731 (79%)] | Loss: 0.652277 | Elapsed: 10.63s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4555
2023-03-30 09:21:10,411 WARNING  [*] 09:21:10: Train Epoch: 4 [57600/60731 (95%)] | Loss: 0.635401 | Elapsed: 10.69s | FPR 0.0003 -> TPR: 0.03 & F1: 0.06 | AUC: 0.5074
2023-03-30 09:21:15,401 WARNING  [*] Thu Mar 30 09:21:15 2023:    4    | Tr.loss: 0.599375 | Elapsed:   69.14  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4978
2023-03-30 09:21:15,401 WARNING  [*] Started epoch: 5
2023-03-30 09:21:15,513 WARNING  [*] 09:21:15: Train Epoch: 5 [  0  /60731 (0 %)] | Loss: 0.607653 | Elapsed: 0.10s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4461
2023-03-30 09:21:26,508 WARNING  [*] 09:21:26: Train Epoch: 5 [9600 /60731 (16%)] | Loss: 0.657216 | Elapsed: 11.00s | FPR 0.0003 -> TPR: 0.04 & F1: 0.08 | AUC: 0.4591
2023-03-30 09:21:36,879 WARNING  [!] Time budget exceeded, training stopped.
2023-03-30 09:21:36,909 WARNING  [!] Thu Mar 30 09:21:36 2023: Dumped results:
                model       : 1680160595-model.torch
		train time  : 1680160595-trainTime.npy
		train losses: 1680160595-trainLosses.npy
		train AUC   : 1680160595-auc.npy
		train F1s   : 1680160595-trainF1s.npy
		train TPRs  : 1680160595-trainTPRs.npy
2023-03-30 09:21:36,940 WARNING  [!] Evaluating model on training set...
2023-03-30 09:21:55,075 WARNING  [!] This fold metrics on training set:
2023-03-30 09:21:55,098 WARNING 	AUC: 0.4998
2023-03-30 09:21:55,104 WARNING 	FPR: 0.0001 | TPR: 0.0001 | F1: 0.0002
2023-03-30 09:21:55,130 WARNING 	FPR: 0.0003 | TPR: 0.0001 | F1: 0.0002
2023-03-30 09:21:55,133 WARNING 	FPR: 0.001 | TPR: 0.0001 | F1: 0.0002
2023-03-30 09:21:55,162 WARNING 	FPR: 0.003 | TPR: 0.0001 | F1: 0.0002
2023-03-30 09:21:55,165 WARNING 	FPR: 0.01 | TPR: 0.0001 | F1: 0.0002
2023-03-30 09:21:55,194 WARNING 	FPR: 0.03 | TPR: 0.0213 | F1: 0.0414
2023-03-30 09:21:55,197 WARNING 	FPR: 0.1 | TPR: 0.0940 | F1: 0.1662
2023-03-30 09:21:55,197 WARNING  [!] Evaluating model on validation set...
2023-03-30 09:22:05,841 WARNING  [!] This fold metrics on validation set:
2023-03-30 09:22:05,848 WARNING 	AUC: 0.5017
2023-03-30 09:22:05,858 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:22:05,862 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:22:05,871 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:22:05,888 WARNING 	FPR: 0.003 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:22:05,897 WARNING 	FPR: 0.01 | TPR: 0.0000 | F1: 0.0001
2023-03-30 09:22:05,902 WARNING 	FPR: 0.03 | TPR: 0.0210 | F1: 0.0407
2023-03-30 09:22:05,922 WARNING 	FPR: 0.1 | TPR: 0.0942 | F1: 0.1666
2023-03-30 09:22:06,047 WARNING  [!] Metrics saved to out_fields_whitespace_1680006591\cv_network_limNone_r1763_t5\network_metrics_validation.json
2023-03-30 09:22:06,050 WARNING  [!] Metrics saved to out_fields_whitespace_1680006591\cv_network_limNone_r1763_t5\network_metrics_training.json
2023-03-30 09:22:06,050 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.5011
	FPR: 0.0001 -- TPR: 0.0000 -- F1: 0.0001
	FPR: 0.0003 -- TPR: 0.0000 -- F1: 0.0001
	FPR:  0.001 -- TPR: 0.0000 -- F1: 0.0001
	FPR:  0.003 -- TPR: 0.0000 -- F1: 0.0001
	FPR:   0.01 -- TPR: 0.0000 -- F1: 0.0001
	FPR:   0.03 -- TPR: 0.0209 -- F1: 0.0407
	FPR:    0.1 -- TPR: 0.0942 -- F1: 0.1666

2023-03-30 09:22:06,112 WARNING  [!] Working on registry!
2023-03-30 09:22:06,130 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-03-30 09:23:43,172 WARNING Finished... Took: 97.04s
2023-03-30 09:23:43,172 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-03-30 09:26:11,803 WARNING Finished... Took: 148.63s
2023-03-30 09:26:11,803 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-03-30 09:26:52,563 WARNING Finished... Took: 40.76s
2023-03-30 09:26:52,564 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-03-30 09:27:37,920 WARNING Finished... Took: 45.36s
2023-03-30 09:27:37,920 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-03-30 09:28:07,260 WARNING Finished... Took: 29.34s
2023-03-30 09:28:07,260 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
