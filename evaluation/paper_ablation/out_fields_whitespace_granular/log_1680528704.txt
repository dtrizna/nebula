2023-04-03 15:31:44,362 WARNING  [!] Working on full!
2023-04-03 15:31:44,383 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\full_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 15:31:44,496 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\full_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 15:31:44,559 WARNING  [!!!] Starting CV over full!
2023-04-03 15:31:44,696 WARNING  [!] CV output folder out_fields_whitespace_granular\cv_full_limNone_r1763_t5 already exists, skipping!
2023-04-03 15:31:44,696 WARNING  [!] Working on api_only_name!
2023-04-03 15:31:44,715 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\api_only_name_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 15:31:44,835 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\api_only_name_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 15:31:44,882 WARNING  [!!!] Starting CV over api_only_name!
2023-04-03 15:31:45,034 WARNING  [!] CV output folder out_fields_whitespace_granular\cv_api_only_name_limNone_r1763_t5 already exists, skipping!
2023-04-03 15:31:45,035 WARNING  [!] Working on api_only_full!
2023-04-03 15:31:45,043 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\api_only_full_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 15:31:45,186 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\api_only_full_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 15:31:45,249 WARNING  [!!!] Starting CV over api_only_full!
2023-04-03 15:31:45,380 WARNING  [!] CV output folder out_fields_whitespace_granular\cv_api_only_full_limNone_r1763_t5 already exists, skipping!
2023-04-03 15:31:45,380 WARNING  [!] Working on file!
2023-04-03 15:31:45,396 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\file_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 15:31:45,531 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\file_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 15:31:45,577 WARNING  [!!!] Starting CV over file!
2023-04-03 15:31:45,761 WARNING  [!] CV output folder out_fields_whitespace_granular\cv_file_limNone_r1763_t5 already exists, skipping!
2023-04-03 15:31:45,761 WARNING  [!] Working on network!
2023-04-03 15:31:45,761 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\network_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 15:31:45,899 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\network_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 15:31:45,948 WARNING  [!!!] Starting CV over network!
2023-04-03 15:31:46,129 WARNING  [!] CV output folder out_fields_whitespace_granular\cv_network_limNone_r1763_t5 already exists, skipping!
2023-04-03 15:31:46,129 WARNING  [!] Working on registry!
2023-04-03 15:31:46,132 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\registry_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 15:31:46,264 WARNING  [!] Skipping since exists: out_fields_whitespace_granular\registry_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 15:31:46,313 WARNING  [!!!] Starting CV over registry!
2023-04-03 15:31:46,494 WARNING  [!] CV output folder out_fields_whitespace_granular\cv_registry_limNone_r1763_t5 already exists, skipping!
2023-04-03 15:31:46,494 WARNING  [!] Working on all!
2023-04-03 15:31:46,497 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-04-03 15:33:18,019 WARNING Finished... Took: 91.52s
2023-04-03 15:33:18,019 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-04-03 15:36:42,127 WARNING Finished... Took: 204.11s
2023-04-03 15:36:42,128 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-04-03 15:37:35,957 WARNING Finished... Took: 53.83s
2023-04-03 15:37:35,957 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-04-03 15:38:27,512 WARNING Finished... Took: 51.55s
2023-04-03 15:38:27,512 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-04-03 15:39:00,598 WARNING Finished... Took: 33.09s
2023-04-03 15:39:00,598 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-04-03 15:40:12,487 WARNING Finished... Took: 71.89s
2023-04-03 15:40:12,487 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-04-03 15:40:32,700 WARNING Finished... Took: 20.21s
2023-04-03 15:40:32,700 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-04-03 15:41:47,580 WARNING Finished... Took: 74.88s
2023-04-03 15:41:47,580 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-04-03 15:41:49,067 WARNING Finished... Took: 1.49s
2023-04-03 15:41:49,085 WARNING  [!] Saved Y as out_fields_whitespace_granular\all_vocab_50000_seqlen_512\y_train_full.npy
2023-04-03 15:41:49,376 WARNING  [!] Saved Y names as out_fields_whitespace_granular\all_vocab_50000_seqlen_512\y_names_train_full.json
2023-04-03 15:41:49,376 WARNING  [*] Initializing tokenizer training...
2023-04-03 15:45:11,544 WARNING Dumped vocab to out_fields_whitespace_granular\all_vocab_50000_seqlen_512\tokenizer_50000_vocab.json
2023-04-03 15:45:16,524 WARNING Dumped vocab counter to out_fields_whitespace_granular\all_vocab_50000_seqlen_512\tokenizer_50000_counter.json
2023-04-03 15:45:16,524 WARNING  [*] Encoding and padding...
2023-04-03 15:50:03,730 WARNING  [!] Saved X as out_fields_whitespace_granular\all_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 15:50:07,241 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-04-03 15:50:53,216 WARNING Finished... Took: 45.97s
2023-04-03 15:50:53,216 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-04-03 15:52:47,913 WARNING Finished... Took: 114.70s
2023-04-03 15:52:47,913 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-04-03 15:53:19,977 WARNING Finished... Took: 32.06s
2023-04-03 15:53:19,977 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-04-03 15:53:23,788 WARNING Finished... Took: 3.81s
2023-04-03 15:53:23,788 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-04-03 15:53:39,786 WARNING Finished... Took: 16.00s
2023-04-03 15:53:39,786 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-04-03 15:56:48,939 WARNING Finished... Took: 189.15s
2023-04-03 15:56:48,940 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-04-03 15:57:13,502 WARNING Finished... Took: 24.56s
2023-04-03 15:57:13,502 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-04-03 15:57:31,967 WARNING Finished... Took: 18.46s
2023-04-03 15:57:31,968 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-04-03 15:57:32,815 WARNING Finished... Took: 0.85s
2023-04-03 15:57:32,818 WARNING  [!] Saved Y as out_fields_whitespace_granular\all_vocab_50000_seqlen_512\y_test_full.npy
2023-04-03 15:57:32,890 WARNING  [!] Saved Y names as out_fields_whitespace_granular\all_vocab_50000_seqlen_512\y_names_test_full.json
2023-04-03 15:57:32,935 WARNING  [*] Encoding and padding...
2023-04-03 15:57:59,123 WARNING  [!] Saved X as out_fields_whitespace_granular\all_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 15:57:59,762 WARNING  [!!!] Starting CV over all!
2023-04-03 15:57:59,879 WARNING  [!] Training time budget: 300min
2023-04-03 15:57:59,879 WARNING  [!] Model config: {'vocab_size': 50000, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-04-03 15:57:59,969 WARNING  [1/3] Train set size: 60730, Validation set size: 30366
2023-04-03 15:58:03,127 WARNING  [!] Saved dataset splits to dataset_splits_1680530279.npz
2023-04-03 15:58:03,351 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3972e6
2023-04-03 15:58:03,351 WARNING  [*] Training time budget set: 5.0 min
2023-04-03 15:58:03,392 WARNING  [*] Started epoch: 1
2023-04-03 15:58:05,760 WARNING  [*] 15:58:05: Train Epoch: 1 [  0  / 633  (0 %)] | Loss: 1.407412 | Elapsed: 2.35s | FPR 0.0003 -> TPR: 0.13 & F1: 0.23 | AUC: 0.5923
2023-04-03 15:58:16,384 WARNING  [*] 15:58:16: Train Epoch: 1 [9600 / 633  (16%)] | Loss: 0.387136 | Elapsed: 10.61s | FPR 0.0003 -> TPR: 0.38 & F1: 0.55 | AUC: 0.8757
2023-04-03 15:58:27,230 WARNING  [*] 15:58:27: Train Epoch: 1 [19200/ 633  (32%)] | Loss: 0.275269 | Elapsed: 10.83s | FPR 0.0003 -> TPR: 0.57 & F1: 0.73 | AUC: 0.9415
2023-04-03 15:58:38,991 WARNING  [*] 15:58:38: Train Epoch: 1 [28800/ 633  (47%)] | Loss: 0.158139 | Elapsed: 11.75s | FPR 0.0003 -> TPR: 0.72 & F1: 0.84 | AUC: 0.9787
2023-04-03 15:58:51,464 WARNING  [*] 15:58:51: Train Epoch: 1 [38400/ 633  (63%)] | Loss: 0.095298 | Elapsed: 12.47s | FPR 0.0003 -> TPR: 0.87 & F1: 0.93 | AUC: 0.9957
2023-04-03 15:59:03,512 WARNING  [*] 15:59:03: Train Epoch: 1 [48000/ 633  (79%)] | Loss: 0.113006 | Elapsed: 12.04s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9881
2023-04-03 15:59:16,259 WARNING  [*] 15:59:16: Train Epoch: 1 [57600/ 633  (95%)] | Loss: 0.155296 | Elapsed: 12.75s | FPR 0.0003 -> TPR: 0.88 & F1: 0.94 | AUC: 0.9854
2023-04-03 15:59:21,758 WARNING  [*] Mon Apr  3 15:59:21 2023:    1    | Tr.loss: 0.228835 | Elapsed:   78.37  s | FPR 0.0003 -> TPR: 0.13 & F1: 0.23 | AUC: 0.9602
2023-04-03 15:59:21,758 WARNING  [*] Started epoch: 2
2023-04-03 15:59:21,886 WARNING  [*] 15:59:21: Train Epoch: 2 [  0  / 633  (0 %)] | Loss: 0.061160 | Elapsed: 0.12s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9962
2023-04-03 15:59:34,387 WARNING  [*] 15:59:34: Train Epoch: 2 [9600 / 633  (16%)] | Loss: 0.074580 | Elapsed: 12.49s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9975
2023-04-03 15:59:46,715 WARNING  [*] 15:59:46: Train Epoch: 2 [19200/ 633  (32%)] | Loss: 0.178396 | Elapsed: 12.32s | FPR 0.0003 -> TPR: 0.72 & F1: 0.84 | AUC: 0.9808
2023-04-03 15:59:58,700 WARNING  [*] 15:59:58: Train Epoch: 2 [28800/ 633  (47%)] | Loss: 0.167150 | Elapsed: 11.97s | FPR 0.0003 -> TPR: 0.88 & F1: 0.94 | AUC: 0.9857
2023-04-03 16:00:10,792 WARNING  [*] 16:00:10: Train Epoch: 2 [38400/ 633  (63%)] | Loss: 0.089808 | Elapsed: 12.08s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9952
2023-04-03 16:00:23,026 WARNING  [*] 16:00:23: Train Epoch: 2 [48000/ 633  (79%)] | Loss: 0.098148 | Elapsed: 12.23s | FPR 0.0003 -> TPR: 0.88 & F1: 0.94 | AUC: 0.9953
2023-04-03 16:00:35,728 WARNING  [*] 16:00:35: Train Epoch: 2 [57600/ 633  (95%)] | Loss: 0.130514 | Elapsed: 12.70s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9897
2023-04-03 16:00:41,335 WARNING  [*] Mon Apr  3 16:00:41 2023:    2    | Tr.loss: 0.082909 | Elapsed:   79.58  s | FPR 0.0003 -> TPR: 0.77 & F1: 0.87 | AUC: 0.9945
2023-04-03 16:00:41,336 WARNING  [*] Started epoch: 3
2023-04-03 16:00:41,463 WARNING  [*] 16:00:41: Train Epoch: 3 [  0  / 633  (0 %)] | Loss: 0.060311 | Elapsed: 0.12s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9958
2023-04-03 16:00:53,353 WARNING  [*] 16:00:53: Train Epoch: 3 [9600 / 633  (16%)] | Loss: 0.029859 | Elapsed: 11.87s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:01:05,389 WARNING  [*] 16:01:05: Train Epoch: 3 [19200/ 633  (32%)] | Loss: 0.079439 | Elapsed: 12.01s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9963
2023-04-03 16:01:17,878 WARNING  [*] 16:01:17: Train Epoch: 3 [28800/ 633  (47%)] | Loss: 0.030207 | Elapsed: 12.49s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9994
2023-04-03 16:01:29,849 WARNING  [*] 16:01:29: Train Epoch: 3 [38400/ 633  (63%)] | Loss: 0.001737 | Elapsed: 11.97s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:01:42,208 WARNING  [*] 16:01:42: Train Epoch: 3 [48000/ 633  (79%)] | Loss: 0.032057 | Elapsed: 12.36s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:01:55,098 WARNING  [*] 16:01:55: Train Epoch: 3 [57600/ 633  (95%)] | Loss: 0.111326 | Elapsed: 12.88s | FPR 0.0003 -> TPR: 0.91 & F1: 0.95 | AUC: 0.9938
2023-04-03 16:02:00,573 WARNING  [*] Mon Apr  3 16:02:00 2023:    3    | Tr.loss: 0.051198 | Elapsed:   79.24  s | FPR 0.0003 -> TPR: 0.81 & F1: 0.89 | AUC: 0.9978
2023-04-03 16:02:00,573 WARNING  [*] Started epoch: 4
2023-04-03 16:02:00,717 WARNING  [*] 16:02:00: Train Epoch: 4 [  0  / 633  (0 %)] | Loss: 0.041793 | Elapsed: 0.14s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9987
2023-04-03 16:02:12,960 WARNING  [*] 16:02:12: Train Epoch: 4 [9600 / 633  (16%)] | Loss: 0.040200 | Elapsed: 12.24s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9988
2023-04-03 16:02:25,067 WARNING  [*] 16:02:25: Train Epoch: 4 [19200/ 633  (32%)] | Loss: 0.005204 | Elapsed: 12.10s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:02:38,226 WARNING  [*] 16:02:38: Train Epoch: 4 [28800/ 633  (47%)] | Loss: 0.067502 | Elapsed: 13.15s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9990
2023-04-03 16:02:51,079 WARNING  [*] 16:02:51: Train Epoch: 4 [38400/ 633  (63%)] | Loss: 0.009732 | Elapsed: 12.84s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:03:03,370 WARNING  [!] Time budget exceeded, training stopped.
2023-04-03 16:03:03,437 WARNING  [!] Mon Apr  3 16:03:03 2023: Dumped results:
                model       : 1680530279-model.torch
		train time  : 1680530279-trainTime.npy
		train losses: 1680530279-trainLosses.npy
		train AUC   : 1680530279-auc.npy
		train F1s   : 1680530279-trainF1s.npy
		train TPRs  : 1680530279-trainTPRs.npy
2023-04-03 16:03:03,487 WARNING  [!] Evaluating model on training set...
2023-04-03 16:03:24,852 WARNING  [!] This fold metrics on training set:
2023-04-03 16:03:24,860 WARNING 	AUC: 0.9995
2023-04-03 16:03:24,885 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-03 16:03:24,903 WARNING 	FPR: 0.0003 | TPR: 0.9352 | F1: 0.9664
2023-04-03 16:03:24,921 WARNING 	FPR: 0.001 | TPR: 0.9685 | F1: 0.9838
2023-04-03 16:03:24,939 WARNING 	FPR: 0.003 | TPR: 0.9872 | F1: 0.9930
2023-04-03 16:03:24,957 WARNING 	FPR: 0.01 | TPR: 0.9925 | F1: 0.9943
2023-04-03 16:03:24,975 WARNING 	FPR: 0.03 | TPR: 0.9965 | F1: 0.9926
2023-04-03 16:03:24,994 WARNING 	FPR: 0.1 | TPR: 0.9990 | F1: 0.9814
2023-04-03 16:03:24,995 WARNING  [!] Evaluating model on validation set...
2023-04-03 16:03:36,398 WARNING  [!] This fold metrics on validation set:
2023-04-03 16:03:36,404 WARNING 	AUC: 0.9986
2023-04-03 16:03:36,413 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-03 16:03:36,424 WARNING 	FPR: 0.0003 | TPR: 0.9261 | F1: 0.9616
2023-04-03 16:03:36,434 WARNING 	FPR: 0.001 | TPR: 0.9499 | F1: 0.9741
2023-04-03 16:03:36,444 WARNING 	FPR: 0.003 | TPR: 0.9722 | F1: 0.9854
2023-04-03 16:03:36,453 WARNING 	FPR: 0.01 | TPR: 0.9848 | F1: 0.9905
2023-04-03 16:03:36,463 WARNING 	FPR: 0.03 | TPR: 0.9916 | F1: 0.9899
2023-04-03 16:03:36,473 WARNING 	FPR: 0.1 | TPR: 0.9961 | F1: 0.9789
2023-04-03 16:03:36,691 WARNING  [2/3] Train set size: 60731, Validation set size: 30365
2023-04-03 16:03:40,971 WARNING  [!] Saved dataset splits to dataset_splits_1680530616.npz
2023-04-03 16:03:41,114 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3972e6
2023-04-03 16:03:41,115 WARNING  [*] Training time budget set: 5.0 min
2023-04-03 16:03:41,161 WARNING  [*] Started epoch: 1
2023-04-03 16:03:41,431 WARNING  [*] 16:03:41: Train Epoch: 1 [  0  / 633  (0 %)] | Loss: 2.651847 | Elapsed: 0.26s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5341
2023-04-03 16:03:54,755 WARNING  [*] 16:03:54: Train Epoch: 1 [9600 / 633  (16%)] | Loss: 0.324855 | Elapsed: 13.31s | FPR 0.0003 -> TPR: 0.55 & F1: 0.71 | AUC: 0.9168
2023-04-03 16:04:08,701 WARNING  [*] 16:04:08: Train Epoch: 1 [19200/ 633  (32%)] | Loss: 0.182618 | Elapsed: 13.94s | FPR 0.0003 -> TPR: 0.58 & F1: 0.73 | AUC: 0.9716
2023-04-03 16:04:22,088 WARNING  [*] 16:04:22: Train Epoch: 1 [28800/ 633  (47%)] | Loss: 0.239647 | Elapsed: 13.38s | FPR 0.0003 -> TPR: 0.82 & F1: 0.90 | AUC: 0.9780
2023-04-03 16:04:34,260 WARNING  [*] 16:04:34: Train Epoch: 1 [38400/ 633  (63%)] | Loss: 0.130390 | Elapsed: 12.17s | FPR 0.0003 -> TPR: 0.89 & F1: 0.94 | AUC: 0.9899
2023-04-03 16:04:46,306 WARNING  [*] 16:04:46: Train Epoch: 1 [48000/ 633  (79%)] | Loss: 0.114039 | Elapsed: 12.04s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9872
2023-04-03 16:04:59,871 WARNING  [*] 16:04:59: Train Epoch: 1 [57600/ 633  (95%)] | Loss: 0.148325 | Elapsed: 13.56s | FPR 0.0003 -> TPR: 0.87 & F1: 0.93 | AUC: 0.9794
2023-04-03 16:05:06,210 WARNING  [*] Mon Apr  3 16:05:06 2023:    1    | Tr.loss: 0.218882 | Elapsed:   85.05  s | FPR 0.0003 -> TPR: 0.09 & F1: 0.17 | AUC: 0.9645
2023-04-03 16:05:06,210 WARNING  [*] Started epoch: 2
2023-04-03 16:05:06,376 WARNING  [*] 16:05:06: Train Epoch: 2 [  0  / 633  (0 %)] | Loss: 0.104568 | Elapsed: 0.15s | FPR 0.0003 -> TPR: 0.93 & F1: 0.96 | AUC: 0.9927
2023-04-03 16:05:20,673 WARNING  [*] 16:05:20: Train Epoch: 2 [9600 / 633  (16%)] | Loss: 0.123530 | Elapsed: 14.28s | FPR 0.0003 -> TPR: 0.87 & F1: 0.93 | AUC: 0.9888
2023-04-03 16:05:33,701 WARNING  [*] 16:05:33: Train Epoch: 2 [19200/ 633  (32%)] | Loss: 0.120224 | Elapsed: 13.02s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9883
2023-04-03 16:05:47,070 WARNING  [*] 16:05:47: Train Epoch: 2 [28800/ 633  (47%)] | Loss: 0.035507 | Elapsed: 13.36s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9996
2023-04-03 16:06:00,295 WARNING  [*] 16:06:00: Train Epoch: 2 [38400/ 633  (63%)] | Loss: 0.106075 | Elapsed: 13.22s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9964
2023-04-03 16:06:13,172 WARNING  [*] 16:06:13: Train Epoch: 2 [48000/ 633  (79%)] | Loss: 0.089649 | Elapsed: 12.88s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9915
2023-04-03 16:06:26,493 WARNING  [*] 16:06:26: Train Epoch: 2 [57600/ 633  (95%)] | Loss: 0.088996 | Elapsed: 13.31s | FPR 0.0003 -> TPR: 0.95 & F1: 0.97 | AUC: 0.9949
2023-04-03 16:06:32,081 WARNING  [*] Mon Apr  3 16:06:32 2023:    2    | Tr.loss: 0.077952 | Elapsed:   85.87  s | FPR 0.0003 -> TPR: 0.71 & F1: 0.83 | AUC: 0.9951
2023-04-03 16:06:32,081 WARNING  [*] Started epoch: 3
2023-04-03 16:06:32,225 WARNING  [*] 16:06:32: Train Epoch: 3 [  0  / 633  (0 %)] | Loss: 0.045928 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-03 16:06:45,771 WARNING  [*] 16:06:45: Train Epoch: 3 [9600 / 633  (16%)] | Loss: 0.013827 | Elapsed: 13.54s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:06:58,834 WARNING  [*] 16:06:58: Train Epoch: 3 [19200/ 633  (32%)] | Loss: 0.031252 | Elapsed: 13.06s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-03 16:07:12,582 WARNING  [*] 16:07:12: Train Epoch: 3 [28800/ 633  (47%)] | Loss: 0.048483 | Elapsed: 13.73s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9983
2023-04-03 16:07:25,880 WARNING  [*] 16:07:25: Train Epoch: 3 [38400/ 633  (63%)] | Loss: 0.083215 | Elapsed: 13.29s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9945
2023-04-03 16:07:39,818 WARNING  [*] 16:07:39: Train Epoch: 3 [48000/ 633  (79%)] | Loss: 0.030490 | Elapsed: 13.93s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:07:53,182 WARNING  [*] 16:07:53: Train Epoch: 3 [57600/ 633  (95%)] | Loss: 0.016415 | Elapsed: 13.35s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:07:58,904 WARNING  [*] Mon Apr  3 16:07:58 2023:    3    | Tr.loss: 0.049658 | Elapsed:   86.82  s | FPR 0.0003 -> TPR: 0.85 & F1: 0.92 | AUC: 0.9978
2023-04-03 16:07:58,904 WARNING  [*] Started epoch: 4
2023-04-03 16:07:59,027 WARNING  [*] 16:07:59: Train Epoch: 4 [  0  / 633  (0 %)] | Loss: 0.069115 | Elapsed: 0.12s | FPR 0.0003 -> TPR: 0.95 & F1: 0.97 | AUC: 0.9976
2023-04-03 16:08:12,036 WARNING  [*] 16:08:12: Train Epoch: 4 [9600 / 633  (16%)] | Loss: 0.010697 | Elapsed: 13.00s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:08:24,727 WARNING  [*] 16:08:24: Train Epoch: 4 [19200/ 633  (32%)] | Loss: 0.012288 | Elapsed: 12.69s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:08:38,050 WARNING  [*] 16:08:38: Train Epoch: 4 [28800/ 633  (47%)] | Loss: 0.035702 | Elapsed: 13.31s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:08:41,136 WARNING  [!] Time budget exceeded, training stopped.
2023-04-03 16:08:41,183 WARNING  [!] Mon Apr  3 16:08:41 2023: Dumped results:
                model       : 1680530616-model.torch
		train time  : 1680530616-trainTime.npy
		train losses: 1680530616-trainLosses.npy
		train AUC   : 1680530616-auc.npy
		train F1s   : 1680530616-trainF1s.npy
		train TPRs  : 1680530616-trainTPRs.npy
2023-04-03 16:08:41,223 WARNING  [!] Evaluating model on training set...
2023-04-03 16:09:02,840 WARNING  [!] This fold metrics on training set:
2023-04-03 16:09:02,850 WARNING 	AUC: 0.9992
2023-04-03 16:09:02,867 WARNING 	FPR: 0.0001 | TPR: 0.8763 | F1: 0.9340
2023-04-03 16:09:02,885 WARNING 	FPR: 0.0003 | TPR: 0.9390 | F1: 0.9685
2023-04-03 16:09:02,901 WARNING 	FPR: 0.001 | TPR: 0.9681 | F1: 0.9836
2023-04-03 16:09:02,919 WARNING 	FPR: 0.003 | TPR: 0.9850 | F1: 0.9919
2023-04-03 16:09:02,936 WARNING 	FPR: 0.01 | TPR: 0.9904 | F1: 0.9932
2023-04-03 16:09:02,954 WARNING 	FPR: 0.03 | TPR: 0.9944 | F1: 0.9913
2023-04-03 16:09:02,971 WARNING 	FPR: 0.1 | TPR: 0.9981 | F1: 0.9799
2023-04-03 16:09:02,972 WARNING  [!] Evaluating model on validation set...
2023-04-03 16:09:14,199 WARNING  [!] This fold metrics on validation set:
2023-04-03 16:09:14,202 WARNING 	AUC: 0.9984
2023-04-03 16:09:14,207 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-03 16:09:14,219 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-04-03 16:09:14,227 WARNING 	FPR: 0.001 | TPR: 0.9334 | F1: 0.9654
2023-04-03 16:09:14,236 WARNING 	FPR: 0.003 | TPR: 0.9700 | F1: 0.9842
2023-04-03 16:09:14,244 WARNING 	FPR: 0.01 | TPR: 0.9826 | F1: 0.9893
2023-04-03 16:09:14,252 WARNING 	FPR: 0.03 | TPR: 0.9898 | F1: 0.9892
2023-04-03 16:09:14,261 WARNING 	FPR: 0.1 | TPR: 0.9961 | F1: 0.9793
2023-04-03 16:09:14,440 WARNING  [3/3] Train set size: 60731, Validation set size: 30365
2023-04-03 16:09:17,581 WARNING  [!] Saved dataset splits to dataset_splits_1680530954.npz
2023-04-03 16:09:17,669 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3972e6
2023-04-03 16:09:17,669 WARNING  [*] Training time budget set: 5.0 min
2023-04-03 16:09:17,711 WARNING  [*] Started epoch: 1
2023-04-03 16:09:17,995 WARNING  [*] 16:09:17: Train Epoch: 1 [  0  / 633  (0 %)] | Loss: 1.866825 | Elapsed: 0.27s | FPR 0.0003 -> TPR: 0.08 & F1: 0.14 | AUC: 0.5059
2023-04-03 16:09:29,828 WARNING  [*] 16:09:29: Train Epoch: 1 [9600 / 633  (16%)] | Loss: 0.349581 | Elapsed: 11.83s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9171
2023-04-03 16:09:41,719 WARNING  [*] 16:09:41: Train Epoch: 1 [19200/ 633  (32%)] | Loss: 0.169803 | Elapsed: 11.89s | FPR 0.0003 -> TPR: 0.82 & F1: 0.90 | AUC: 0.9782
2023-04-03 16:09:54,008 WARNING  [*] 16:09:54: Train Epoch: 1 [28800/ 633  (47%)] | Loss: 0.140168 | Elapsed: 12.28s | FPR 0.0003 -> TPR: 0.86 & F1: 0.93 | AUC: 0.9854
2023-04-03 16:10:06,875 WARNING  [*] 16:10:06: Train Epoch: 1 [38400/ 633  (63%)] | Loss: 0.203325 | Elapsed: 12.85s | FPR 0.0003 -> TPR: 0.81 & F1: 0.89 | AUC: 0.9762
2023-04-03 16:10:20,681 WARNING  [*] 16:10:20: Train Epoch: 1 [48000/ 633  (79%)] | Loss: 0.104385 | Elapsed: 13.79s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9936
2023-04-03 16:10:33,653 WARNING  [*] 16:10:33: Train Epoch: 1 [57600/ 633  (95%)] | Loss: 0.071743 | Elapsed: 12.96s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9987
2023-04-03 16:10:39,415 WARNING  [*] Mon Apr  3 16:10:39 2023:    1    | Tr.loss: 0.235113 | Elapsed:   81.70  s | FPR 0.0003 -> TPR: 0.10 & F1: 0.17 | AUC: 0.9584
2023-04-03 16:10:39,415 WARNING  [*] Started epoch: 2
2023-04-03 16:10:39,555 WARNING  [*] 16:10:39: Train Epoch: 2 [  0  / 633  (0 %)] | Loss: 0.143675 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.90 & F1: 0.95 | AUC: 0.9826
2023-04-03 16:10:53,932 WARNING  [*] 16:10:53: Train Epoch: 2 [9600 / 633  (16%)] | Loss: 0.100265 | Elapsed: 14.37s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9940
2023-04-03 16:11:08,328 WARNING  [*] 16:11:08: Train Epoch: 2 [19200/ 633  (32%)] | Loss: 0.123001 | Elapsed: 14.39s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9892
2023-04-03 16:11:23,069 WARNING  [*] 16:11:23: Train Epoch: 2 [28800/ 633  (47%)] | Loss: 0.106539 | Elapsed: 14.73s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9896
2023-04-03 16:11:37,859 WARNING  [*] 16:11:37: Train Epoch: 2 [38400/ 633  (63%)] | Loss: 0.053903 | Elapsed: 14.76s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9982
2023-04-03 16:11:51,970 WARNING  [*] 16:11:51: Train Epoch: 2 [48000/ 633  (79%)] | Loss: 0.029341 | Elapsed: 14.10s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:12:06,041 WARNING  [*] 16:12:06: Train Epoch: 2 [57600/ 633  (95%)] | Loss: 0.015066 | Elapsed: 14.06s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:12:12,111 WARNING  [*] Mon Apr  3 16:12:12 2023:    2    | Tr.loss: 0.081179 | Elapsed:   92.70  s | FPR 0.0003 -> TPR: 0.65 & F1: 0.79 | AUC: 0.9947
2023-04-03 16:12:12,111 WARNING  [*] Started epoch: 3
2023-04-03 16:12:12,252 WARNING  [*] 16:12:12: Train Epoch: 3 [  0  / 633  (0 %)] | Loss: 0.035772 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9995
2023-04-03 16:12:26,642 WARNING  [*] 16:12:26: Train Epoch: 3 [9600 / 633  (16%)] | Loss: 0.044487 | Elapsed: 14.38s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9989
2023-04-03 16:12:40,835 WARNING  [*] 16:12:40: Train Epoch: 3 [19200/ 633  (32%)] | Loss: 0.035053 | Elapsed: 14.18s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9989
2023-04-03 16:12:55,017 WARNING  [*] 16:12:55: Train Epoch: 3 [28800/ 633  (47%)] | Loss: 0.046011 | Elapsed: 14.17s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9982
2023-04-03 16:13:09,383 WARNING  [*] 16:13:09: Train Epoch: 3 [38400/ 633  (63%)] | Loss: 0.093025 | Elapsed: 14.36s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9941
2023-04-03 16:13:23,274 WARNING  [*] 16:13:23: Train Epoch: 3 [48000/ 633  (79%)] | Loss: 0.028848 | Elapsed: 13.88s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:13:36,996 WARNING  [*] 16:13:36: Train Epoch: 3 [57600/ 633  (95%)] | Loss: 0.044330 | Elapsed: 13.71s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:13:43,269 WARNING  [*] Mon Apr  3 16:13:43 2023:    3    | Tr.loss: 0.052929 | Elapsed:   91.16  s | FPR 0.0003 -> TPR: 0.77 & F1: 0.87 | AUC: 0.9976
2023-04-03 16:13:43,269 WARNING  [*] Started epoch: 4
2023-04-03 16:13:43,444 WARNING  [*] 16:13:43: Train Epoch: 4 [  0  / 633  (0 %)] | Loss: 0.029679 | Elapsed: 0.16s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:13:57,811 WARNING  [*] 16:13:57: Train Epoch: 4 [9600 / 633  (16%)] | Loss: 0.002255 | Elapsed: 14.36s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:14:12,044 WARNING  [*] 16:14:12: Train Epoch: 4 [19200/ 633  (32%)] | Loss: 0.020065 | Elapsed: 14.22s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 16:14:17,756 WARNING  [!] Time budget exceeded, training stopped.
2023-04-03 16:14:17,803 WARNING  [!] Mon Apr  3 16:14:17 2023: Dumped results:
                model       : 1680530954-model.torch
		train time  : 1680530954-trainTime.npy
		train losses: 1680530954-trainLosses.npy
		train AUC   : 1680530954-auc.npy
		train F1s   : 1680530954-trainF1s.npy
		train TPRs  : 1680530954-trainTPRs.npy
2023-04-03 16:14:17,841 WARNING  [!] Evaluating model on training set...
2023-04-03 16:14:41,977 WARNING  [!] This fold metrics on training set:
2023-04-03 16:14:41,987 WARNING 	AUC: 0.9989
2023-04-03 16:14:42,000 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-03 16:14:42,017 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-04-03 16:14:42,033 WARNING 	FPR: 0.001 | TPR: 0.9425 | F1: 0.9702
2023-04-03 16:14:42,051 WARNING 	FPR: 0.003 | TPR: 0.9788 | F1: 0.9887
2023-04-03 16:14:42,067 WARNING 	FPR: 0.01 | TPR: 0.9888 | F1: 0.9925
2023-04-03 16:14:42,083 WARNING 	FPR: 0.03 | TPR: 0.9949 | F1: 0.9917
2023-04-03 16:14:42,100 WARNING 	FPR: 0.1 | TPR: 0.9984 | F1: 0.9802
2023-04-03 16:14:42,100 WARNING  [!] Evaluating model on validation set...
2023-04-03 16:14:54,127 WARNING  [!] This fold metrics on validation set:
2023-04-03 16:14:54,132 WARNING 	AUC: 0.9978
2023-04-03 16:14:54,139 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-03 16:14:54,147 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-04-03 16:14:54,155 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-04-03 16:14:54,163 WARNING 	FPR: 0.003 | TPR: 0.9523 | F1: 0.9750
2023-04-03 16:14:54,171 WARNING 	FPR: 0.01 | TPR: 0.9797 | F1: 0.9878
2023-04-03 16:14:54,178 WARNING 	FPR: 0.03 | TPR: 0.9895 | F1: 0.9890
2023-04-03 16:14:54,187 WARNING 	FPR: 0.1 | TPR: 0.9958 | F1: 0.9787
2023-04-03 16:14:54,310 WARNING  [!] Metrics saved to out_fields_whitespace_granular\cv_all_limNone_r1763_t5\all_metrics_validation.json
2023-04-03 16:14:54,312 WARNING  [!] Metrics saved to out_fields_whitespace_granular\cv_all_limNone_r1763_t5\all_metrics_training.json
2023-04-03 16:14:54,313 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9983
	FPR: 0.0001 -- TPR: 0.0000 -- F1: 0.0000
	FPR: 0.0003 -- TPR: 0.3087 -- F1: 0.3205
	FPR:  0.001 -- TPR: 0.6278 -- F1: 0.6465
	FPR:  0.003 -- TPR: 0.9649 -- F1: 0.9815
	FPR:   0.01 -- TPR: 0.9824 -- F1: 0.9892
	FPR:   0.03 -- TPR: 0.9903 -- F1: 0.9894
	FPR:    0.1 -- TPR: 0.9960 -- F1: 0.9790

