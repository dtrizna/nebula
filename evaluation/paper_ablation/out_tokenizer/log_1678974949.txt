2023-03-16 14:55:49,741 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-03-16 15:00:49,265 WARNING Finished... Took: 299.52s
2023-03-16 15:00:49,266 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-03-16 15:06:57,741 WARNING Finished... Took: 368.47s
2023-03-16 15:06:57,741 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-03-16 15:08:56,105 WARNING Finished... Took: 118.36s
2023-03-16 15:08:56,105 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-03-16 15:10:43,554 WARNING Finished... Took: 107.45s
2023-03-16 15:10:43,554 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-03-16 15:11:58,251 WARNING Finished... Took: 74.70s
2023-03-16 15:11:58,251 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-03-16 15:15:26,390 WARNING Finished... Took: 208.14s
2023-03-16 15:15:26,391 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-03-16 15:16:21,181 WARNING Finished... Took: 54.79s
2023-03-16 15:16:21,181 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-03-16 15:19:04,845 WARNING Finished... Took: 163.66s
2023-03-16 15:19:04,845 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-03-16 15:19:07,053 WARNING Finished... Took: 2.21s
2023-03-16 15:19:07,056 WARNING  [!] Saved Y as out_tokenizer_1678974949\nebula_whitespace_vocab_50000_seqlen_512\y_train_full.npy
2023-03-16 15:19:07,260 WARNING  [!] Saved Y names as out_tokenizer_1678974949\nebula_whitespace_vocab_50000_seqlen_512\y_names_train_full.json
2023-03-16 15:19:07,260 WARNING  [*] Initializing tokenizer training...
2023-03-16 15:20:30,187 WARNING  [*] Encoding and padding...
2023-03-16 15:22:27,423 WARNING  [!] Saved X as out_tokenizer_1678974949\nebula_whitespace_vocab_50000_seqlen_512\x_train_full.npy
2023-03-16 15:22:30,443 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-03-16 15:23:05,436 WARNING Finished... Took: 34.99s
2023-03-16 15:23:05,436 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-03-16 15:24:51,692 WARNING Finished... Took: 106.26s
2023-03-16 15:24:51,692 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-03-16 15:25:22,900 WARNING Finished... Took: 31.21s
2023-03-16 15:25:22,900 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-03-16 15:25:25,916 WARNING Finished... Took: 3.02s
2023-03-16 15:25:25,916 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-03-16 15:25:37,871 WARNING Finished... Took: 11.95s
2023-03-16 15:25:37,871 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-03-16 15:27:58,775 WARNING Finished... Took: 140.90s
2023-03-16 15:27:58,775 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-03-16 15:28:19,176 WARNING Finished... Took: 20.40s
2023-03-16 15:28:19,176 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-03-16 15:28:32,320 WARNING Finished... Took: 13.14s
2023-03-16 15:28:32,321 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-03-16 15:28:32,821 WARNING Finished... Took: 0.50s
2023-03-16 15:28:32,828 WARNING  [!] Saved Y as out_tokenizer_1678974949\nebula_whitespace_vocab_50000_seqlen_512\y_test_full.npy
2023-03-16 15:28:32,864 WARNING  [!] Saved Y names as out_tokenizer_1678974949\nebula_whitespace_vocab_50000_seqlen_512\y_names_test_full.json
2023-03-16 15:28:32,893 WARNING  [*] Encoding and padding...
2023-03-16 15:28:54,669 WARNING  [!] Saved X as out_tokenizer_1678974949\nebula_whitespace_vocab_50000_seqlen_512\x_test_full.npy
2023-03-16 15:28:55,307 WARNING  [!!!] Starting CV over whitespace!
2023-03-16 15:28:55,396 WARNING  [!] Training time budget: 300min
2023-03-16 15:28:55,396 WARNING  [!] Model config: {'vocab_size': 50000, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-03-16 15:28:55,454 WARNING  [1/3] Train set size: 50750, Validation set size: 25376
2023-03-16 15:28:57,446 WARNING  [!] Saved dataset splits to dataset_splits_1678976935.npz
2023-03-16 15:28:57,652 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3972e6
2023-03-16 15:28:57,652 WARNING  [*] Training time budget set: 5.0 min
2023-03-16 15:28:57,683 WARNING  [*] Started epoch: 1
2023-03-16 15:28:59,679 WARNING  [*] 15:28:59: Train Epoch: 1 [  0  /50750 (0 %)] | Loss: 1.823555 | Elapsed: 1.99s | FPR 0.0003 -> TPR 0.0370 & F1 0.0714 | AUC 0.5432
2023-03-16 15:29:09,500 WARNING  [*] 15:29:09: Train Epoch: 1 [9600 /50750 (19%)] | Loss: 0.322223 | Elapsed: 9.81s | FPR 0.0003 -> TPR 0.5195 & F1 0.6838 | AUC 0.9108
2023-03-16 15:29:19,372 WARNING  [*] 15:29:19: Train Epoch: 1 [19200/50750 (38%)] | Loss: 0.374834 | Elapsed: 9.86s | FPR 0.0003 -> TPR 0.4035 & F1 0.5750 | AUC 0.9217
2023-03-16 15:29:29,393 WARNING  [*] 15:29:29: Train Epoch: 1 [28800/50750 (57%)] | Loss: 0.317715 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.3492 & F1 0.5176 | AUC 0.9455
2023-03-16 15:29:40,064 WARNING  [*] 15:29:40: Train Epoch: 1 [38400/50750 (76%)] | Loss: 0.178709 | Elapsed: 10.65s | FPR 0.0003 -> TPR 0.6857 & F1 0.8136 | AUC 0.9781
2023-03-16 15:29:50,920 WARNING  [*] 15:29:50: Train Epoch: 1 [48000/50750 (95%)] | Loss: 0.253524 | Elapsed: 10.85s | FPR 0.0003 -> TPR 0.6765 & F1 0.8070 | AUC 0.9586
2023-03-16 15:29:55,028 WARNING  [*] Thu Mar 16 15:29:55 2023:    1    | Tr.loss: 0.316258 | Elapsed:   57.35  s | FPR 0.0003 -> TPR: 0.10 & F1: 0.18 | AUC: 0.9275
2023-03-16 15:29:55,029 WARNING  [*] Started epoch: 2
2023-03-16 15:29:55,142 WARNING  [*] 15:29:55: Train Epoch: 2 [  0  /50750 (0 %)] | Loss: 0.172830 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.8571 & F1 0.9231 | AUC 0.9841
2023-03-16 15:30:06,103 WARNING  [*] 15:30:06: Train Epoch: 2 [9600 /50750 (19%)] | Loss: 0.193546 | Elapsed: 10.95s | FPR 0.0003 -> TPR 0.7969 & F1 0.8870 | AUC 0.9796
2023-03-16 15:30:17,235 WARNING  [*] 15:30:17: Train Epoch: 2 [19200/50750 (38%)] | Loss: 0.156047 | Elapsed: 11.12s | FPR 0.0003 -> TPR 0.7647 & F1 0.8667 | AUC 0.9770
2023-03-16 15:30:28,392 WARNING  [*] 15:30:28: Train Epoch: 2 [28800/50750 (57%)] | Loss: 0.179230 | Elapsed: 11.14s | FPR 0.0003 -> TPR 0.8939 & F1 0.9440 | AUC 0.9866
2023-03-16 15:30:39,629 WARNING  [*] 15:30:39: Train Epoch: 2 [38400/50750 (76%)] | Loss: 0.151930 | Elapsed: 11.23s | FPR 0.0003 -> TPR 0.9286 & F1 0.9630 | AUC 0.9857
2023-03-16 15:30:50,657 WARNING  [*] 15:30:50: Train Epoch: 2 [48000/50750 (95%)] | Loss: 0.067477 | Elapsed: 11.01s | FPR 0.0003 -> TPR 0.9138 & F1 0.9550 | AUC 0.9955
2023-03-16 15:30:54,720 WARNING  [*] Thu Mar 16 15:30:54 2023:    2    | Tr.loss: 0.138060 | Elapsed:   59.69  s | FPR 0.0003 -> TPR: 0.46 & F1: 0.63 | AUC: 0.9869
2023-03-16 15:30:54,720 WARNING  [*] Started epoch: 3
2023-03-16 15:30:54,847 WARNING  [*] 15:30:54: Train Epoch: 3 [  0  /50750 (0 %)] | Loss: 0.145669 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.8621 & F1 0.9259 | AUC 0.9864
2023-03-16 15:31:05,879 WARNING  [*] 15:31:05: Train Epoch: 3 [9600 /50750 (19%)] | Loss: 0.046090 | Elapsed: 11.02s | FPR 0.0003 -> TPR 0.9853 & F1 0.9926 | AUC 0.9991
2023-03-16 15:31:16,751 WARNING  [*] 15:31:16: Train Epoch: 3 [19200/50750 (38%)] | Loss: 0.142716 | Elapsed: 10.86s | FPR 0.0003 -> TPR 0.8548 & F1 0.9217 | AUC 0.9907
2023-03-16 15:31:27,615 WARNING  [*] 15:31:27: Train Epoch: 3 [28800/50750 (57%)] | Loss: 0.076894 | Elapsed: 10.86s | FPR 0.0003 -> TPR 0.9206 & F1 0.9587 | AUC 0.9974
2023-03-16 15:31:38,512 WARNING  [*] 15:31:38: Train Epoch: 3 [38400/50750 (76%)] | Loss: 0.049293 | Elapsed: 10.89s | FPR 0.0003 -> TPR 0.9868 & F1 0.9934 | AUC 0.9989
2023-03-16 15:31:49,698 WARNING  [*] 15:31:49: Train Epoch: 3 [48000/50750 (95%)] | Loss: 0.137731 | Elapsed: 11.16s | FPR 0.0003 -> TPR 0.9344 & F1 0.9661 | AUC 0.9899
2023-03-16 15:31:53,905 WARNING  [*] Thu Mar 16 15:31:53 2023:    3    | Tr.loss: 0.079810 | Elapsed:   59.19  s | FPR 0.0003 -> TPR: 0.76 & F1: 0.86 | AUC: 0.9953
2023-03-16 15:31:53,905 WARNING  [*] Started epoch: 4
2023-03-16 15:31:54,027 WARNING  [*] 15:31:54: Train Epoch: 4 [  0  /50750 (0 %)] | Loss: 0.032824 | Elapsed: 0.11s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:32:05,721 WARNING  [*] 15:32:05: Train Epoch: 4 [9600 /50750 (19%)] | Loss: 0.026944 | Elapsed: 11.68s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:32:17,403 WARNING  [*] 15:32:17: Train Epoch: 4 [19200/50750 (38%)] | Loss: 0.153190 | Elapsed: 11.67s | FPR 0.0003 -> TPR 0.9412 & F1 0.9697 | AUC 0.9862
2023-03-16 15:32:28,498 WARNING  [*] 15:32:28: Train Epoch: 4 [28800/50750 (57%)] | Loss: 0.049576 | Elapsed: 11.08s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9974
2023-03-16 15:32:39,787 WARNING  [*] 15:32:39: Train Epoch: 4 [38400/50750 (76%)] | Loss: 0.009976 | Elapsed: 11.28s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:32:51,137 WARNING  [*] 15:32:51: Train Epoch: 4 [48000/50750 (95%)] | Loss: 0.026457 | Elapsed: 11.34s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:32:55,385 WARNING  [*] Thu Mar 16 15:32:55 2023:    4    | Tr.loss: 0.059556 | Elapsed:   61.48  s | FPR 0.0003 -> TPR: 0.76 & F1: 0.86 | AUC: 0.9972
2023-03-16 15:32:55,386 WARNING  [*] Started epoch: 5
2023-03-16 15:32:55,513 WARNING  [*] 15:32:55: Train Epoch: 5 [  0  /50750 (0 %)] | Loss: 0.022958 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9841 & F1 0.9920 | AUC 0.9995
2023-03-16 15:33:06,608 WARNING  [*] 15:33:06: Train Epoch: 5 [9600 /50750 (19%)] | Loss: 0.098918 | Elapsed: 11.09s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9939
2023-03-16 15:33:17,548 WARNING  [*] 15:33:17: Train Epoch: 5 [19200/50750 (38%)] | Loss: 0.054128 | Elapsed: 10.93s | FPR 0.0003 -> TPR 0.9714 & F1 0.9855 | AUC 0.9976
2023-03-16 15:33:28,292 WARNING  [*] 15:33:28: Train Epoch: 5 [28800/50750 (57%)] | Loss: 0.023170 | Elapsed: 10.73s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:33:39,073 WARNING  [*] 15:33:39: Train Epoch: 5 [38400/50750 (76%)] | Loss: 0.032789 | Elapsed: 10.78s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:33:49,877 WARNING  [*] 15:33:49: Train Epoch: 5 [48000/50750 (95%)] | Loss: 0.066901 | Elapsed: 10.78s | FPR 0.0003 -> TPR 0.9744 & F1 0.9870 | AUC 0.9971
2023-03-16 15:33:54,192 WARNING  [*] Thu Mar 16 15:33:54 2023:    5    | Tr.loss: 0.048011 | Elapsed:   58.81  s | FPR 0.0003 -> TPR: 0.82 & F1: 0.90 | AUC: 0.9981
2023-03-16 15:33:54,192 WARNING  [*] Started epoch: 6
2023-03-16 15:33:54,322 WARNING  [*] 15:33:54: Train Epoch: 6 [  0  /50750 (0 %)] | Loss: 0.008921 | Elapsed: 0.12s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:33:57,680 WARNING  [!] Time budget exceeded, training stopped.
2023-03-16 15:33:57,731 WARNING  [!] Thu Mar 16 15:33:57 2023: Dumped results:
                model       : 1678976935-model.torch
		train time  : 1678976935-trainTime.npy
		train losses: 1678976935-trainLosses.npy
		train AUC   : 1678976935-auc.npy
		train F1s   : 1678976935-trainF1s.npy
		train TPRs  : 1678976935-trainTPRs.npy
2023-03-16 15:33:57,748 WARNING  [!] Evaluating model on training set...
2023-03-16 15:34:12,927 WARNING  [!] This fold metrics on training set:
2023-03-16 15:34:12,930 WARNING 	AUC: 0.9988
2023-03-16 15:34:12,942 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 15:34:12,963 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-16 15:34:12,969 WARNING 	FPR: 0.001 | TPR: 0.9537 | F1: 0.9761
2023-03-16 15:34:12,979 WARNING 	FPR: 0.003 | TPR: 0.9655 | F1: 0.9817
2023-03-16 15:34:13,003 WARNING 	FPR: 0.01 | TPR: 0.9838 | F1: 0.9894
2023-03-16 15:34:13,009 WARNING 	FPR: 0.03 | TPR: 0.9916 | F1: 0.9887
2023-03-16 15:34:13,041 WARNING 	FPR: 0.1 | TPR: 0.9977 | F1: 0.9756
2023-03-16 15:34:13,041 WARNING  [!] Evaluating model on validation set...
2023-03-16 15:34:20,737 WARNING  [!] This fold metrics on validation set:
2023-03-16 15:34:20,741 WARNING 	AUC: 0.9972
2023-03-16 15:34:20,746 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 15:34:20,755 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-16 15:34:20,760 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 15:34:20,767 WARNING 	FPR: 0.003 | TPR: 0.9355 | F1: 0.9660
2023-03-16 15:34:20,774 WARNING 	FPR: 0.01 | TPR: 0.9683 | F1: 0.9816
2023-03-16 15:34:20,781 WARNING 	FPR: 0.03 | TPR: 0.9857 | F1: 0.9857
2023-03-16 15:34:20,790 WARNING 	FPR: 0.1 | TPR: 0.9952 | F1: 0.9745
2023-03-16 15:34:20,973 WARNING  [2/3] Train set size: 50751, Validation set size: 25375
2023-03-16 15:34:23,217 WARNING  [!] Saved dataset splits to dataset_splits_1678977260.npz
2023-03-16 15:34:23,290 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3972e6
2023-03-16 15:34:23,290 WARNING  [*] Training time budget set: 5.0 min
2023-03-16 15:34:23,323 WARNING  [*] Started epoch: 1
2023-03-16 15:34:23,559 WARNING  [*] 15:34:23: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 1.339888 | Elapsed: 0.23s | FPR 0.0003 -> TPR 0.0615 & F1 0.1159 | AUC 0.5747
2023-03-16 15:34:34,566 WARNING  [*] 15:34:34: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.293261 | Elapsed: 11.01s | FPR 0.0003 -> TPR 0.4722 & F1 0.6415 | AUC 0.9390
2023-03-16 15:34:45,690 WARNING  [*] 15:34:45: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.265726 | Elapsed: 11.10s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9412
2023-03-16 15:34:56,823 WARNING  [*] 15:34:56: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.330937 | Elapsed: 11.12s | FPR 0.0003 -> TPR 0.5634 & F1 0.7207 | AUC 0.9407
2023-03-16 15:35:08,000 WARNING  [*] 15:35:08: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.158853 | Elapsed: 11.17s | FPR 0.0003 -> TPR 0.8493 & F1 0.9185 | AUC 0.9817
2023-03-16 15:35:19,148 WARNING  [*] 15:35:19: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.115794 | Elapsed: 11.14s | FPR 0.0003 -> TPR 0.9367 & F1 0.9673 | AUC 0.9928
2023-03-16 15:35:23,706 WARNING  [*] Thu Mar 16 15:35:23 2023:    1    | Tr.loss: 0.311773 | Elapsed:   60.38  s | FPR 0.0003 -> TPR: 0.05 & F1: 0.09 | AUC: 0.9297
2023-03-16 15:35:23,706 WARNING  [*] Started epoch: 2
2023-03-16 15:35:23,841 WARNING  [*] 15:35:23: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.252063 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.6250 & F1 0.7692 | AUC 0.9521
2023-03-16 15:35:34,938 WARNING  [*] 15:35:34: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.113505 | Elapsed: 11.09s | FPR 0.0003 -> TPR 0.8788 & F1 0.9355 | AUC 0.9947
2023-03-16 15:35:46,093 WARNING  [*] 15:35:46: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.097938 | Elapsed: 11.15s | FPR 0.0003 -> TPR 0.9180 & F1 0.9573 | AUC 0.9962
2023-03-16 15:35:57,235 WARNING  [*] 15:35:57: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.152279 | Elapsed: 11.13s | FPR 0.0003 -> TPR 0.9565 & F1 0.9778 | AUC 0.9897
2023-03-16 15:36:08,316 WARNING  [*] 15:36:08: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.118994 | Elapsed: 11.08s | FPR 0.0003 -> TPR 0.9595 & F1 0.9793 | AUC 0.9901
2023-03-16 15:36:19,448 WARNING  [*] 15:36:19: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.146415 | Elapsed: 11.13s | FPR 0.0003 -> TPR 0.9559 & F1 0.9774 | AUC 0.9867
2023-03-16 15:36:23,865 WARNING  [*] Thu Mar 16 15:36:23 2023:    2    | Tr.loss: 0.136500 | Elapsed:   60.16  s | FPR 0.0003 -> TPR: 0.37 & F1: 0.54 | AUC: 0.9870
2023-03-16 15:36:23,866 WARNING  [*] Started epoch: 3
2023-03-16 15:36:23,989 WARNING  [*] 15:36:23: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.119353 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.7671 & F1 0.8682 | AUC 0.9875
2023-03-16 15:36:35,126 WARNING  [*] 15:36:35: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.054937 | Elapsed: 11.14s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:36:46,232 WARNING  [*] 15:36:46: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.049844 | Elapsed: 11.11s | FPR 0.0003 -> TPR 0.9677 & F1 0.9836 | AUC 0.9992
2023-03-16 15:36:57,387 WARNING  [*] 15:36:57: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.107304 | Elapsed: 11.16s | FPR 0.0003 -> TPR 0.9508 & F1 0.9748 | AUC 0.9941
2023-03-16 15:37:08,571 WARNING  [*] 15:37:08: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.051323 | Elapsed: 11.18s | FPR 0.0003 -> TPR 0.9538 & F1 0.9764 | AUC 0.9987
2023-03-16 15:37:19,702 WARNING  [*] 15:37:19: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.077020 | Elapsed: 11.13s | FPR 0.0003 -> TPR 0.9853 & F1 0.9926 | AUC 0.9977
2023-03-16 15:37:24,201 WARNING  [*] Thu Mar 16 15:37:24 2023:    3    | Tr.loss: 0.078271 | Elapsed:   60.33  s | FPR 0.0003 -> TPR: 0.56 & F1: 0.71 | AUC: 0.9954
2023-03-16 15:37:24,201 WARNING  [*] Started epoch: 4
2023-03-16 15:37:24,327 WARNING  [*] 15:37:24: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.092679 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9286 & F1 0.9630 | AUC 0.9945
2023-03-16 15:37:35,442 WARNING  [*] 15:37:35: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.034918 | Elapsed: 11.09s | FPR 0.0003 -> TPR 0.9706 & F1 0.9851 | AUC 0.9991
2023-03-16 15:37:46,599 WARNING  [*] 15:37:46: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.158020 | Elapsed: 11.14s | FPR 0.0003 -> TPR 0.9552 & F1 0.9771 | AUC 0.9937
2023-03-16 15:37:57,719 WARNING  [*] 15:37:57: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.048081 | Elapsed: 11.12s | FPR 0.0003 -> TPR 0.9683 & F1 0.9839 | AUC 0.9991
2023-03-16 15:38:08,863 WARNING  [*] 15:38:08: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.086190 | Elapsed: 11.14s | FPR 0.0003 -> TPR 0.9692 & F1 0.9844 | AUC 0.9947
2023-03-16 15:38:19,993 WARNING  [*] 15:38:19: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.028753 | Elapsed: 11.13s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:38:24,354 WARNING  [*] Thu Mar 16 15:38:24 2023:    4    | Tr.loss: 0.058566 | Elapsed:   60.15  s | FPR 0.0003 -> TPR: 0.64 & F1: 0.78 | AUC: 0.9973
2023-03-16 15:38:24,354 WARNING  [*] Started epoch: 5
2023-03-16 15:38:24,489 WARNING  [*] 15:38:24: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.076795 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9980
2023-03-16 15:38:35,689 WARNING  [*] 15:38:35: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.022405 | Elapsed: 11.19s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:38:46,524 WARNING  [*] 15:38:46: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.088828 | Elapsed: 10.81s | FPR 0.0003 -> TPR 0.9516 & F1 0.9752 | AUC 0.9953
2023-03-16 15:38:57,675 WARNING  [*] 15:38:57: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.017932 | Elapsed: 11.14s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:39:08,902 WARNING  [*] 15:39:08: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.015666 | Elapsed: 11.22s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:39:20,182 WARNING  [*] 15:39:20: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.026642 | Elapsed: 11.27s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:39:23,290 WARNING  [!] Time budget exceeded, training stopped.
2023-03-16 15:39:23,333 WARNING  [!] Thu Mar 16 15:39:23 2023: Dumped results:
                model       : 1678977260-model.torch
		train time  : 1678977260-trainTime.npy
		train losses: 1678977260-trainLosses.npy
		train AUC   : 1678977260-auc.npy
		train F1s   : 1678977260-trainF1s.npy
		train TPRs  : 1678977260-trainTPRs.npy
2023-03-16 15:39:23,416 WARNING  [!] Evaluating model on training set...
2023-03-16 15:39:38,393 WARNING  [!] This fold metrics on training set:
2023-03-16 15:39:38,397 WARNING 	AUC: 0.9990
2023-03-16 15:39:38,424 WARNING 	FPR: 0.0001 | TPR: 0.8259 | F1: 0.9046
2023-03-16 15:39:38,427 WARNING 	FPR: 0.0003 | TPR: 0.8374 | F1: 0.9114
2023-03-16 15:39:38,457 WARNING 	FPR: 0.001 | TPR: 0.9439 | F1: 0.9709
2023-03-16 15:39:38,462 WARNING 	FPR: 0.003 | TPR: 0.9727 | F1: 0.9854
2023-03-16 15:39:38,490 WARNING 	FPR: 0.01 | TPR: 0.9862 | F1: 0.9907
2023-03-16 15:39:38,493 WARNING 	FPR: 0.03 | TPR: 0.9936 | F1: 0.9897
2023-03-16 15:39:38,523 WARNING 	FPR: 0.1 | TPR: 0.9977 | F1: 0.9756
2023-03-16 15:39:38,524 WARNING  [!] Evaluating model on validation set...
2023-03-16 15:39:45,920 WARNING  [!] This fold metrics on validation set:
2023-03-16 15:39:45,923 WARNING 	AUC: 0.9981
2023-03-16 15:39:45,933 WARNING 	FPR: 0.0001 | TPR: 0.7185 | F1: 0.8362
2023-03-16 15:39:45,942 WARNING 	FPR: 0.0003 | TPR: 0.7765 | F1: 0.8741
2023-03-16 15:39:45,950 WARNING 	FPR: 0.001 | TPR: 0.9343 | F1: 0.9658
2023-03-16 15:39:45,953 WARNING 	FPR: 0.003 | TPR: 0.9651 | F1: 0.9815
2023-03-16 15:39:45,961 WARNING 	FPR: 0.01 | TPR: 0.9817 | F1: 0.9884
2023-03-16 15:39:45,973 WARNING 	FPR: 0.03 | TPR: 0.9891 | F1: 0.9875
2023-03-16 15:39:45,981 WARNING 	FPR: 0.1 | TPR: 0.9947 | F1: 0.9744
2023-03-16 15:39:46,140 WARNING  [3/3] Train set size: 50751, Validation set size: 25375
2023-03-16 15:39:48,418 WARNING  [!] Saved dataset splits to dataset_splits_1678977586.npz
2023-03-16 15:39:48,488 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3972e6
2023-03-16 15:39:48,488 WARNING  [*] Training time budget set: 5.0 min
2023-03-16 15:39:48,522 WARNING  [*] Started epoch: 1
2023-03-16 15:39:48,816 WARNING  [*] 15:39:48: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 2.232380 | Elapsed: 0.28s | FPR 0.0003 -> TPR 0.0794 & F1 0.1471 | AUC 0.5368
2023-03-16 15:40:00,050 WARNING  [*] 15:40:00: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.406324 | Elapsed: 11.23s | FPR 0.0003 -> TPR 0.3750 & F1 0.5455 | AUC 0.9034
2023-03-16 15:40:11,252 WARNING  [*] 15:40:11: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.297029 | Elapsed: 11.19s | FPR 0.0003 -> TPR 0.3333 & F1 0.5000 | AUC 0.9421
2023-03-16 15:40:22,433 WARNING  [*] 15:40:22: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.249845 | Elapsed: 11.18s | FPR 0.0003 -> TPR 0.3538 & F1 0.5227 | AUC 0.9451
2023-03-16 15:40:33,612 WARNING  [*] 15:40:33: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.283307 | Elapsed: 11.18s | FPR 0.0003 -> TPR 0.4925 & F1 0.6600 | AUC 0.9403
2023-03-16 15:40:44,929 WARNING  [*] 15:40:44: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.274942 | Elapsed: 11.31s | FPR 0.0003 -> TPR 0.0946 & F1 0.1728 | AUC 0.9527
2023-03-16 15:40:49,503 WARNING  [*] Thu Mar 16 15:40:49 2023:    1    | Tr.loss: 0.333424 | Elapsed:   60.98  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.9236
2023-03-16 15:40:49,503 WARNING  [*] Started epoch: 2
2023-03-16 15:40:49,635 WARNING  [*] 15:40:49: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.184402 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9420 & F1 0.9701 | AUC 0.9775
2023-03-16 15:41:00,822 WARNING  [*] 15:41:00: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.105760 | Elapsed: 11.18s | FPR 0.0003 -> TPR 0.9545 & F1 0.9767 | AUC 0.9924
2023-03-16 15:41:12,044 WARNING  [*] 15:41:12: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.226441 | Elapsed: 11.21s | FPR 0.0003 -> TPR 0.7879 & F1 0.8814 | AUC 0.9719
2023-03-16 15:41:23,295 WARNING  [*] 15:41:23: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.118817 | Elapsed: 11.23s | FPR 0.0003 -> TPR 0.8857 & F1 0.9394 | AUC 0.9914
2023-03-16 15:41:34,479 WARNING  [*] 15:41:34: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.145547 | Elapsed: 11.17s | FPR 0.0003 -> TPR 0.7895 & F1 0.8824 | AUC 0.9894
2023-03-16 15:41:45,676 WARNING  [*] 15:41:45: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.122470 | Elapsed: 11.20s | FPR 0.0003 -> TPR 0.7647 & F1 0.8667 | AUC 0.9917
2023-03-16 15:41:50,147 WARNING  [*] Thu Mar 16 15:41:50 2023:    2    | Tr.loss: 0.132199 | Elapsed:   60.64  s | FPR 0.0003 -> TPR: 0.49 & F1: 0.66 | AUC: 0.9878
2023-03-16 15:41:50,163 WARNING  [*] Started epoch: 3
2023-03-16 15:41:50,279 WARNING  [*] 15:41:50: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.064129 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9995
2023-03-16 15:42:01,522 WARNING  [*] 15:42:01: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.051563 | Elapsed: 11.22s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9977
2023-03-16 15:42:12,707 WARNING  [*] 15:42:12: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.025953 | Elapsed: 11.17s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:42:23,825 WARNING  [*] 15:42:23: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.032033 | Elapsed: 11.12s | FPR 0.0003 -> TPR 0.9859 & F1 0.9929 | AUC 0.9995
2023-03-16 15:42:34,835 WARNING  [*] 15:42:34: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.045801 | Elapsed: 11.00s | FPR 0.0003 -> TPR 0.9859 & F1 0.9929 | AUC 0.9990
2023-03-16 15:42:45,569 WARNING  [*] 15:42:45: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.055235 | Elapsed: 10.73s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:42:49,943 WARNING  [*] Thu Mar 16 15:42:49 2023:    3    | Tr.loss: 0.077627 | Elapsed:   59.78  s | FPR 0.0003 -> TPR: 0.55 & F1: 0.71 | AUC: 0.9955
2023-03-16 15:42:49,943 WARNING  [*] Started epoch: 4
2023-03-16 15:42:50,066 WARNING  [*] 15:42:50: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.087119 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9492 & F1 0.9739 | AUC 0.9963
2023-03-16 15:43:01,316 WARNING  [*] 15:43:01: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.048792 | Elapsed: 11.24s | FPR 0.0003 -> TPR 0.9857 & F1 0.9928 | AUC 0.9990
2023-03-16 15:43:12,114 WARNING  [*] 15:43:12: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.014369 | Elapsed: 10.79s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:43:22,848 WARNING  [*] 15:43:22: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.009787 | Elapsed: 10.73s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:43:33,588 WARNING  [*] 15:43:33: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.030184 | Elapsed: 10.73s | FPR 0.0003 -> TPR 0.9688 & F1 0.9841 | AUC 0.9991
2023-03-16 15:43:44,526 WARNING  [*] 15:43:44: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.067304 | Elapsed: 10.92s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9986
2023-03-16 15:43:48,854 WARNING  [*] Thu Mar 16 15:43:48 2023:    4    | Tr.loss: 0.057911 | Elapsed:   58.91  s | FPR 0.0003 -> TPR: 0.70 & F1: 0.83 | AUC: 0.9974
2023-03-16 15:43:48,854 WARNING  [*] Started epoch: 5
2023-03-16 15:43:48,967 WARNING  [*] 15:43:48: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.109168 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9254 & F1 0.9612 | AUC 0.9928
2023-03-16 15:43:59,857 WARNING  [*] 15:43:59: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.042455 | Elapsed: 10.89s | FPR 0.0003 -> TPR 0.9661 & F1 0.9828 | AUC 0.9992
2023-03-16 15:44:10,700 WARNING  [*] 15:44:10: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.028525 | Elapsed: 10.84s | FPR 0.0003 -> TPR 0.9865 & F1 0.9932 | AUC 0.9995
2023-03-16 15:44:21,602 WARNING  [*] 15:44:21: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.100392 | Elapsed: 10.88s | FPR 0.0003 -> TPR 0.8986 & F1 0.9466 | AUC 0.9967
2023-03-16 15:44:32,382 WARNING  [*] 15:44:32: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.075830 | Elapsed: 10.76s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9959
2023-03-16 15:44:43,398 WARNING  [*] 15:44:43: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.010797 | Elapsed: 11.01s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 15:44:47,813 WARNING  [*] Thu Mar 16 15:44:47 2023:    5    | Tr.loss: 0.047939 | Elapsed:   58.96  s | FPR 0.0003 -> TPR: 0.82 & F1: 0.90 | AUC: 0.9982
2023-03-16 15:44:47,813 WARNING  [*] Started epoch: 6
2023-03-16 15:44:47,946 WARNING  [*] 15:44:47: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.057147 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9857 & F1 0.9928 | AUC 0.9962
2023-03-16 15:44:48,509 WARNING  [!] Time budget exceeded, training stopped.
2023-03-16 15:44:48,569 WARNING  [!] Thu Mar 16 15:44:48 2023: Dumped results:
                model       : 1678977586-model.torch
		train time  : 1678977586-trainTime.npy
		train losses: 1678977586-trainLosses.npy
		train AUC   : 1678977586-auc.npy
		train F1s   : 1678977586-trainF1s.npy
		train TPRs  : 1678977586-trainTPRs.npy
2023-03-16 15:44:48,584 WARNING  [!] Evaluating model on training set...
2023-03-16 15:45:03,697 WARNING  [!] This fold metrics on training set:
2023-03-16 15:45:03,706 WARNING 	AUC: 0.9990
2023-03-16 15:45:03,716 WARNING 	FPR: 0.0001 | TPR: 0.7556 | F1: 0.8608
2023-03-16 15:45:03,724 WARNING 	FPR: 0.0003 | TPR: 0.8985 | F1: 0.9465
2023-03-16 15:45:03,750 WARNING 	FPR: 0.001 | TPR: 0.9539 | F1: 0.9762
2023-03-16 15:45:03,755 WARNING 	FPR: 0.003 | TPR: 0.9673 | F1: 0.9827
2023-03-16 15:45:03,766 WARNING 	FPR: 0.01 | TPR: 0.9866 | F1: 0.9909
2023-03-16 15:45:03,791 WARNING 	FPR: 0.03 | TPR: 0.9923 | F1: 0.9890
2023-03-16 15:45:03,799 WARNING 	FPR: 0.1 | TPR: 0.9979 | F1: 0.9755
2023-03-16 15:45:03,799 WARNING  [!] Evaluating model on validation set...
2023-03-16 15:45:11,479 WARNING  [!] This fold metrics on validation set:
2023-03-16 15:45:11,482 WARNING 	AUC: 0.9979
2023-03-16 15:45:11,487 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 15:45:11,500 WARNING 	FPR: 0.0003 | TPR: 0.7443 | F1: 0.8534
2023-03-16 15:45:11,508 WARNING 	FPR: 0.001 | TPR: 0.8878 | F1: 0.9403
2023-03-16 15:45:11,511 WARNING 	FPR: 0.003 | TPR: 0.9536 | F1: 0.9756
2023-03-16 15:45:11,518 WARNING 	FPR: 0.01 | TPR: 0.9771 | F1: 0.9860
2023-03-16 15:45:11,530 WARNING 	FPR: 0.03 | TPR: 0.9872 | F1: 0.9865
2023-03-16 15:45:11,538 WARNING 	FPR: 0.1 | TPR: 0.9946 | F1: 0.9740
2023-03-16 15:45:11,640 WARNING  [!] Metrics saved to out_tokenizer_1678974949\cv_whitespace_limNone_r1763_t5\whitespace_metrics_validation.json
2023-03-16 15:45:11,649 WARNING  [!] Metrics saved to out_tokenizer_1678974949\cv_whitespace_limNone_r1763_t5\whitespace_metrics_training.json
2023-03-16 15:45:11,649 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9977
	FPR: 0.0001 -- TPR: 0.2395 -- F1: 0.2787
	FPR: 0.0003 -- TPR: 0.5069 -- F1: 0.5758
	FPR:  0.001 -- TPR: 0.6074 -- F1: 0.6354
	FPR:  0.003 -- TPR: 0.9514 -- F1: 0.9744
	FPR:   0.01 -- TPR: 0.9757 -- F1: 0.9853
	FPR:   0.03 -- TPR: 0.9874 -- F1: 0.9866
	FPR:    0.1 -- TPR: 0.9948 -- F1: 0.9743

2023-03-16 15:45:11,722 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-03-16 15:50:17,981 WARNING Finished... Took: 306.26s
2023-03-16 15:50:17,981 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-03-16 15:57:57,800 WARNING Finished... Took: 459.82s
2023-03-16 15:57:57,800 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-03-16 16:00:23,110 WARNING Finished... Took: 145.31s
2023-03-16 16:00:23,111 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-03-16 16:02:48,797 WARNING Finished... Took: 145.69s
2023-03-16 16:02:48,797 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-03-16 16:04:18,563 WARNING Finished... Took: 89.77s
2023-03-16 16:04:18,564 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-03-16 16:08:45,685 WARNING Finished... Took: 267.12s
2023-03-16 16:08:45,685 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-03-16 16:09:56,907 WARNING Finished... Took: 71.22s
2023-03-16 16:09:56,908 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-03-16 16:13:18,384 WARNING Finished... Took: 201.48s
2023-03-16 16:13:18,384 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-03-16 16:13:20,664 WARNING Finished... Took: 2.28s
2023-03-16 16:13:20,679 WARNING  [!] Saved Y as out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\y_train_full.npy
2023-03-16 16:13:20,922 WARNING  [!] Saved Y names as out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\y_names_train_full.json
2023-03-16 16:13:20,923 WARNING  [!] Initialized tokenizer without pre-trained model.
	You need to train tokenizer with .train() or specify 'model_path=' during initialization!
2023-03-16 16:13:20,923 WARNING  [*] Initializing tokenizer training...
2023-03-16 16:13:20,924 WARNING  [*] Data preparation for SentencePiece tokenizer...
2023-03-16 16:17:19,967 WARNING  [*] Saving to disk...
2023-03-16 16:17:31,725 WARNING  [!] Training tokenizer with command: --input=out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\tokenizer_50000_trainset_1678979839.txt --model_prefix=out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\tokenizer_50000 --vocab_size=50000 --model_type=bpe --split_by_number=False --max_sentence_length=4192 --max_sentencepiece_length=64
2023-03-16 16:20:52,144 WARNING  [!] Loaded vocab with size 50001 from out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\tokenizer_50000.vocab
2023-03-16 16:20:52,665 WARNING  [*] Encoding and padding...
2023-03-16 16:27:47,498 WARNING  [!] Saved X as out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\x_train_full.npy
2023-03-16 16:27:50,954 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-03-16 16:28:24,949 WARNING Finished... Took: 34.00s
2023-03-16 16:28:24,949 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-03-16 16:30:01,328 WARNING Finished... Took: 96.38s
2023-03-16 16:30:01,328 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-03-16 16:30:27,021 WARNING Finished... Took: 25.69s
2023-03-16 16:30:27,021 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-03-16 16:30:31,080 WARNING Finished... Took: 4.06s
2023-03-16 16:30:31,080 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-03-16 16:30:41,145 WARNING Finished... Took: 10.06s
2023-03-16 16:30:41,145 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-03-16 16:32:52,298 WARNING Finished... Took: 131.15s
2023-03-16 16:32:52,298 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-03-16 16:33:08,546 WARNING Finished... Took: 16.25s
2023-03-16 16:33:08,546 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-03-16 16:33:19,971 WARNING Finished... Took: 11.42s
2023-03-16 16:33:19,971 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-03-16 16:33:20,440 WARNING Finished... Took: 0.47s
2023-03-16 16:33:20,443 WARNING  [!] Saved Y as out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\y_test_full.npy
2023-03-16 16:33:20,479 WARNING  [!] Saved Y names as out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\y_names_test_full.json
2023-03-16 16:33:20,517 WARNING  [!] Successfully loaded pre-trained tokenizer model!
2023-03-16 16:33:20,549 WARNING  [!] Loaded vocab with size 50001 from out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\tokenizer_50000_vocab.json
2023-03-16 16:33:20,550 WARNING  [*] Encoding and padding...
2023-03-16 16:34:39,070 WARNING  [!] Saved X as out_tokenizer_1678974949\nebula_bpe_vocab_50000_seqlen_512\x_test_full.npy
2023-03-16 16:34:39,868 WARNING  [!!!] Starting CV over bpe!
2023-03-16 16:34:39,948 WARNING  [!] Training time budget: 300min
2023-03-16 16:34:39,948 WARNING  [!] Model config: {'vocab_size': 50001, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-03-16 16:34:40,007 WARNING  [1/3] Train set size: 50750, Validation set size: 25376
2023-03-16 16:34:42,028 WARNING  [!] Saved dataset splits to dataset_splits_1678980879.npz
2023-03-16 16:34:42,100 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3973e6
2023-03-16 16:34:42,100 WARNING  [*] Training time budget set: 5.0 min
2023-03-16 16:34:42,136 WARNING  [*] Started epoch: 1
2023-03-16 16:34:42,437 WARNING  [*] 16:34:42: Train Epoch: 1 [  0  /50750 (0 %)] | Loss: 3.402544 | Elapsed: 0.29s | FPR 0.0003 -> TPR 0.0615 & F1 0.1159 | AUC 0.6283
2023-03-16 16:34:52,231 WARNING  [*] 16:34:52: Train Epoch: 1 [9600 /50750 (19%)] | Loss: 0.414278 | Elapsed: 9.79s | FPR 0.0003 -> TPR 0.4333 & F1 0.6047 | AUC 0.8721
2023-03-16 16:35:02,030 WARNING  [*] 16:35:02: Train Epoch: 1 [19200/50750 (38%)] | Loss: 0.277653 | Elapsed: 9.80s | FPR 0.0003 -> TPR 0.5139 & F1 0.6789 | AUC 0.9375
2023-03-16 16:35:11,936 WARNING  [*] 16:35:11: Train Epoch: 1 [28800/50750 (57%)] | Loss: 0.258136 | Elapsed: 9.90s | FPR 0.0003 -> TPR 0.8116 & F1 0.8960 | AUC 0.9490
2023-03-16 16:35:21,858 WARNING  [*] 16:35:21: Train Epoch: 1 [38400/50750 (76%)] | Loss: 0.246128 | Elapsed: 9.92s | FPR 0.0003 -> TPR 0.5634 & F1 0.7207 | AUC 0.9359
2023-03-16 16:35:31,873 WARNING  [*] 16:35:31: Train Epoch: 1 [48000/50750 (95%)] | Loss: 0.204383 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.7812 & F1 0.8772 | AUC 0.9718
2023-03-16 16:35:35,754 WARNING  [*] Thu Mar 16 16:35:35 2023:    1    | Tr.loss: 0.350183 | Elapsed:   53.62  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9140
2023-03-16 16:35:35,754 WARNING  [*] Started epoch: 2
2023-03-16 16:35:35,873 WARNING  [*] 16:35:35: Train Epoch: 2 [  0  /50750 (0 %)] | Loss: 0.228129 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.7313 & F1 0.8448 | AUC 0.9732
2023-03-16 16:35:46,170 WARNING  [*] 16:35:46: Train Epoch: 2 [9600 /50750 (19%)] | Loss: 0.086396 | Elapsed: 10.29s | FPR 0.0003 -> TPR 0.9714 & F1 0.9855 | AUC 0.9971
2023-03-16 16:35:56,533 WARNING  [*] 16:35:56: Train Epoch: 2 [19200/50750 (38%)] | Loss: 0.231378 | Elapsed: 10.36s | FPR 0.0003 -> TPR 0.6508 & F1 0.7885 | AUC 0.9695
2023-03-16 16:36:07,105 WARNING  [*] 16:36:07: Train Epoch: 2 [28800/50750 (57%)] | Loss: 0.096966 | Elapsed: 10.55s | FPR 0.0003 -> TPR 0.9041 & F1 0.9496 | AUC 0.9934
2023-03-16 16:36:17,679 WARNING  [*] 16:36:17: Train Epoch: 2 [38400/50750 (76%)] | Loss: 0.177450 | Elapsed: 10.57s | FPR 0.0003 -> TPR 0.6333 & F1 0.7755 | AUC 0.9846
2023-03-16 16:36:28,293 WARNING  [*] 16:36:28: Train Epoch: 2 [48000/50750 (95%)] | Loss: 0.077115 | Elapsed: 10.61s | FPR 0.0003 -> TPR 0.9577 & F1 0.9784 | AUC 0.9937
2023-03-16 16:36:32,437 WARNING  [*] Thu Mar 16 16:36:32 2023:    2    | Tr.loss: 0.137369 | Elapsed:   56.68  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9869
2023-03-16 16:36:32,437 WARNING  [*] Started epoch: 3
2023-03-16 16:36:32,539 WARNING  [*] 16:36:32: Train Epoch: 3 [  0  /50750 (0 %)] | Loss: 0.090132 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.9722 & F1 0.9859 | AUC 0.9919
2023-03-16 16:36:43,164 WARNING  [*] 16:36:43: Train Epoch: 3 [9600 /50750 (19%)] | Loss: 0.122946 | Elapsed: 10.62s | FPR 0.0003 -> TPR 0.9206 & F1 0.9587 | AUC 0.9901
2023-03-16 16:36:53,807 WARNING  [*] 16:36:53: Train Epoch: 3 [19200/50750 (38%)] | Loss: 0.100747 | Elapsed: 10.64s | FPR 0.0003 -> TPR 0.9836 & F1 0.9917 | AUC 0.9954
2023-03-16 16:37:04,547 WARNING  [*] 16:37:04: Train Epoch: 3 [28800/50750 (57%)] | Loss: 0.075523 | Elapsed: 10.73s | FPR 0.0003 -> TPR 0.9333 & F1 0.9655 | AUC 0.9975
2023-03-16 16:37:15,218 WARNING  [*] 16:37:15: Train Epoch: 3 [38400/50750 (76%)] | Loss: 0.057136 | Elapsed: 10.65s | FPR 0.0003 -> TPR 0.9833 & F1 0.9916 | AUC 0.9971
2023-03-16 16:37:25,995 WARNING  [*] 16:37:25: Train Epoch: 3 [48000/50750 (95%)] | Loss: 0.052913 | Elapsed: 10.76s | FPR 0.0003 -> TPR 0.9733 & F1 0.9865 | AUC 0.9989
2023-03-16 16:37:30,118 WARNING  [*] Thu Mar 16 16:37:30 2023:    3    | Tr.loss: 0.077358 | Elapsed:   57.68  s | FPR 0.0003 -> TPR: 0.62 & F1: 0.77 | AUC: 0.9958
2023-03-16 16:37:30,118 WARNING  [*] Started epoch: 4
2023-03-16 16:37:30,233 WARNING  [*] 16:37:30: Train Epoch: 4 [  0  /50750 (0 %)] | Loss: 0.051007 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9688 & F1 0.9841 | AUC 0.9990
2023-03-16 16:37:40,938 WARNING  [*] 16:37:40: Train Epoch: 4 [9600 /50750 (19%)] | Loss: 0.069287 | Elapsed: 10.70s | FPR 0.0003 -> TPR 0.9516 & F1 0.9752 | AUC 0.9979
2023-03-16 16:37:51,687 WARNING  [*] 16:37:51: Train Epoch: 4 [19200/50750 (38%)] | Loss: 0.047350 | Elapsed: 10.74s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9991
2023-03-16 16:38:02,490 WARNING  [*] 16:38:02: Train Epoch: 4 [28800/50750 (57%)] | Loss: 0.079052 | Elapsed: 10.80s | FPR 0.0003 -> TPR 0.9857 & F1 0.9928 | AUC 0.9962
2023-03-16 16:38:13,250 WARNING  [*] 16:38:13: Train Epoch: 4 [38400/50750 (76%)] | Loss: 0.046425 | Elapsed: 10.74s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9991
2023-03-16 16:38:24,009 WARNING  [*] 16:38:24: Train Epoch: 4 [48000/50750 (95%)] | Loss: 0.019964 | Elapsed: 10.75s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:38:28,222 WARNING  [*] Thu Mar 16 16:38:28 2023:    4    | Tr.loss: 0.053260 | Elapsed:   58.10  s | FPR 0.0003 -> TPR: 0.70 & F1: 0.82 | AUC: 0.9979
2023-03-16 16:38:28,222 WARNING  [*] Started epoch: 5
2023-03-16 16:38:28,350 WARNING  [*] 16:38:28: Train Epoch: 5 [  0  /50750 (0 %)] | Loss: 0.002563 | Elapsed: 0.10s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:38:39,090 WARNING  [*] 16:38:39: Train Epoch: 5 [9600 /50750 (19%)] | Loss: 0.006935 | Elapsed: 10.74s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:38:49,939 WARNING  [*] 16:38:49: Train Epoch: 5 [19200/50750 (38%)] | Loss: 0.027127 | Elapsed: 10.85s | FPR 0.0003 -> TPR 0.9863 & F1 0.9931 | AUC 0.9990
2023-03-16 16:39:00,723 WARNING  [*] 16:39:00: Train Epoch: 5 [28800/50750 (57%)] | Loss: 0.025359 | Elapsed: 10.78s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9996
2023-03-16 16:39:11,559 WARNING  [*] 16:39:11: Train Epoch: 5 [38400/50750 (76%)] | Loss: 0.049433 | Elapsed: 10.83s | FPR 0.0003 -> TPR 0.9365 & F1 0.9672 | AUC 0.9983
2023-03-16 16:39:22,449 WARNING  [*] 16:39:22: Train Epoch: 5 [48000/50750 (95%)] | Loss: 0.004936 | Elapsed: 10.88s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:39:26,663 WARNING  [*] Thu Mar 16 16:39:26 2023:    5    | Tr.loss: 0.041280 | Elapsed:   58.44  s | FPR 0.0003 -> TPR: 0.84 & F1: 0.91 | AUC: 0.9987
2023-03-16 16:39:26,663 WARNING  [*] Started epoch: 6
2023-03-16 16:39:26,772 WARNING  [*] 16:39:26: Train Epoch: 6 [  0  /50750 (0 %)] | Loss: 0.015669 | Elapsed: 0.11s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:39:37,563 WARNING  [*] 16:39:37: Train Epoch: 6 [9600 /50750 (19%)] | Loss: 0.030968 | Elapsed: 10.79s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:39:42,159 WARNING  [!] Time budget exceeded, training stopped.
2023-03-16 16:39:42,196 WARNING  [!] Thu Mar 16 16:39:42 2023: Dumped results:
                model       : 1678980879-model.torch
		train time  : 1678980879-trainTime.npy
		train losses: 1678980879-trainLosses.npy
		train AUC   : 1678980879-auc.npy
		train F1s   : 1678980879-trainF1s.npy
		train TPRs  : 1678980879-trainTPRs.npy
2023-03-16 16:39:42,220 WARNING  [!] Evaluating model on training set...
2023-03-16 16:39:57,250 WARNING  [!] This fold metrics on training set:
2023-03-16 16:39:57,256 WARNING 	AUC: 0.9994
2023-03-16 16:39:57,261 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:39:57,281 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:39:57,289 WARNING 	FPR: 0.001 | TPR: 0.9380 | F1: 0.9678
2023-03-16 16:39:57,295 WARNING 	FPR: 0.003 | TPR: 0.9758 | F1: 0.9870
2023-03-16 16:39:57,316 WARNING 	FPR: 0.01 | TPR: 0.9926 | F1: 0.9939
2023-03-16 16:39:57,325 WARNING 	FPR: 0.03 | TPR: 0.9973 | F1: 0.9916
2023-03-16 16:39:57,330 WARNING 	FPR: 0.1 | TPR: 0.9994 | F1: 0.9763
2023-03-16 16:39:57,330 WARNING  [!] Evaluating model on validation set...
2023-03-16 16:40:04,870 WARNING  [!] This fold metrics on validation set:
2023-03-16 16:40:04,874 WARNING 	AUC: 0.9956
2023-03-16 16:40:04,880 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:40:04,883 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:40:04,890 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:40:04,900 WARNING 	FPR: 0.003 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:40:04,907 WARNING 	FPR: 0.01 | TPR: 0.9606 | F1: 0.9775
2023-03-16 16:40:04,913 WARNING 	FPR: 0.03 | TPR: 0.9834 | F1: 0.9845
2023-03-16 16:40:04,916 WARNING 	FPR: 0.1 | TPR: 0.9943 | F1: 0.9741
2023-03-16 16:40:05,068 WARNING  [2/3] Train set size: 50751, Validation set size: 25375
2023-03-16 16:40:07,226 WARNING  [!] Saved dataset splits to dataset_splits_1678981204.npz
2023-03-16 16:40:07,294 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3973e6
2023-03-16 16:40:07,294 WARNING  [*] Training time budget set: 5.0 min
2023-03-16 16:40:07,321 WARNING  [*] Started epoch: 1
2023-03-16 16:40:07,663 WARNING  [*] 16:40:07: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 2.259017 | Elapsed: 0.33s | FPR 0.0003 -> TPR 0.0143 & F1 0.0282 | AUC 0.5275
2023-03-16 16:40:18,330 WARNING  [*] 16:40:18: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.383233 | Elapsed: 10.65s | FPR 0.0003 -> TPR 0.1286 & F1 0.2278 | AUC 0.8821
2023-03-16 16:40:29,146 WARNING  [*] 16:40:29: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.298061 | Elapsed: 10.81s | FPR 0.0003 -> TPR 0.6250 & F1 0.7692 | AUC 0.9271
2023-03-16 16:40:40,100 WARNING  [*] 16:40:40: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.224097 | Elapsed: 10.95s | FPR 0.0003 -> TPR 0.5200 & F1 0.6842 | AUC 0.9488
2023-03-16 16:40:51,040 WARNING  [*] 16:40:51: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.212645 | Elapsed: 10.94s | FPR 0.0003 -> TPR 0.7656 & F1 0.8673 | AUC 0.9692
2023-03-16 16:41:01,985 WARNING  [*] 16:41:01: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.184169 | Elapsed: 10.94s | FPR 0.0003 -> TPR 0.6087 & F1 0.7568 | AUC 0.9757
2023-03-16 16:41:06,294 WARNING  [*] Thu Mar 16 16:41:06 2023:    1    | Tr.loss: 0.347243 | Elapsed:   58.97  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.9146
2023-03-16 16:41:06,294 WARNING  [*] Started epoch: 2
2023-03-16 16:41:06,402 WARNING  [*] 16:41:06: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.183972 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.8824 & F1 0.9375 | AUC 0.9816
2023-03-16 16:41:17,319 WARNING  [*] 16:41:17: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.208504 | Elapsed: 10.90s | FPR 0.0003 -> TPR 0.8769 & F1 0.9344 | AUC 0.9714
2023-03-16 16:41:28,248 WARNING  [*] 16:41:28: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.129716 | Elapsed: 10.92s | FPR 0.0003 -> TPR 0.8571 & F1 0.9231 | AUC 0.9895
2023-03-16 16:41:39,113 WARNING  [*] 16:41:39: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.094905 | Elapsed: 10.86s | FPR 0.0003 -> TPR 0.9697 & F1 0.9846 | AUC 0.9942
2023-03-16 16:41:50,050 WARNING  [*] 16:41:50: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.104032 | Elapsed: 10.93s | FPR 0.0003 -> TPR 0.9692 & F1 0.9844 | AUC 0.9925
2023-03-16 16:42:00,987 WARNING  [*] 16:42:00: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.157216 | Elapsed: 10.93s | FPR 0.0003 -> TPR 0.8788 & F1 0.9355 | AUC 0.9871
2023-03-16 16:42:05,203 WARNING  [*] Thu Mar 16 16:42:05 2023:    2    | Tr.loss: 0.155923 | Elapsed:   58.91  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9832
2023-03-16 16:42:05,204 WARNING  [*] Started epoch: 3
2023-03-16 16:42:05,305 WARNING  [*] 16:42:05: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.076842 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.9672 & F1 0.9833 | AUC 0.9977
2023-03-16 16:42:16,197 WARNING  [*] 16:42:16: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.075119 | Elapsed: 10.89s | FPR 0.0003 -> TPR 0.9722 & F1 0.9859 | AUC 0.9980
2023-03-16 16:42:27,091 WARNING  [*] 16:42:27: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.077803 | Elapsed: 10.89s | FPR 0.0003 -> TPR 0.9706 & F1 0.9851 | AUC 0.9968
2023-03-16 16:42:38,025 WARNING  [*] 16:42:38: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.135164 | Elapsed: 10.93s | FPR 0.0003 -> TPR 0.9130 & F1 0.9545 | AUC 0.9888
2023-03-16 16:42:48,902 WARNING  [*] 16:42:48: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.114927 | Elapsed: 10.88s | FPR 0.0003 -> TPR 0.9167 & F1 0.9565 | AUC 0.9921
2023-03-16 16:42:59,824 WARNING  [*] 16:42:59: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.093460 | Elapsed: 10.91s | FPR 0.0003 -> TPR 0.8095 & F1 0.8947 | AUC 0.9940
2023-03-16 16:43:04,079 WARNING  [*] Thu Mar 16 16:43:04 2023:    3    | Tr.loss: 0.089577 | Elapsed:   58.88  s | FPR 0.0003 -> TPR: 0.57 & F1: 0.72 | AUC: 0.9943
2023-03-16 16:43:04,080 WARNING  [*] Started epoch: 4
2023-03-16 16:43:04,201 WARNING  [*] 16:43:04: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.045138 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9995
2023-03-16 16:43:15,084 WARNING  [*] 16:43:15: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.031637 | Elapsed: 10.88s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:43:26,005 WARNING  [*] 16:43:26: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.056825 | Elapsed: 10.92s | FPR 0.0003 -> TPR 0.9697 & F1 0.9846 | AUC 0.9987
2023-03-16 16:43:36,921 WARNING  [*] 16:43:36: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.041265 | Elapsed: 10.92s | FPR 0.0003 -> TPR 0.9697 & F1 0.9846 | AUC 0.9960
2023-03-16 16:43:47,842 WARNING  [*] 16:43:47: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.009564 | Elapsed: 10.92s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:43:58,703 WARNING  [*] 16:43:58: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.032242 | Elapsed: 10.86s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9995
2023-03-16 16:44:02,900 WARNING  [*] Thu Mar 16 16:44:02 2023:    4    | Tr.loss: 0.061289 | Elapsed:   58.82  s | FPR 0.0003 -> TPR: 0.65 & F1: 0.78 | AUC: 0.9972
2023-03-16 16:44:02,900 WARNING  [*] Started epoch: 5
2023-03-16 16:44:03,017 WARNING  [*] 16:44:03: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.030916 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.9841 & F1 0.9920 | AUC 0.9995
2023-03-16 16:44:13,924 WARNING  [*] 16:44:13: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.055831 | Elapsed: 10.89s | FPR 0.0003 -> TPR 0.9726 & F1 0.9861 | AUC 0.9985
2023-03-16 16:44:24,810 WARNING  [*] 16:44:24: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.019460 | Elapsed: 10.88s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:44:35,673 WARNING  [*] 16:44:35: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.044069 | Elapsed: 10.86s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9996
2023-03-16 16:44:46,526 WARNING  [*] 16:44:46: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.031140 | Elapsed: 10.83s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:44:57,455 WARNING  [*] 16:44:57: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.063890 | Elapsed: 10.93s | FPR 0.0003 -> TPR 0.9683 & F1 0.9839 | AUC 0.9974
2023-03-16 16:45:01,821 WARNING  [*] Thu Mar 16 16:45:01 2023:    5    | Tr.loss: 0.044790 | Elapsed:   58.92  s | FPR 0.0003 -> TPR: 0.62 & F1: 0.77 | AUC: 0.9984
2023-03-16 16:45:01,821 WARNING  [*] Started epoch: 6
2023-03-16 16:45:01,945 WARNING  [*] 16:45:01: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.090090 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9062 & F1 0.9508 | AUC 0.9956
2023-03-16 16:45:07,357 WARNING  [!] Time budget exceeded, training stopped.
2023-03-16 16:45:07,423 WARNING  [!] Thu Mar 16 16:45:07 2023: Dumped results:
                model       : 1678981204-model.torch
		train time  : 1678981204-trainTime.npy
		train losses: 1678981204-trainLosses.npy
		train AUC   : 1678981204-auc.npy
		train F1s   : 1678981204-trainF1s.npy
		train TPRs  : 1678981204-trainTPRs.npy
2023-03-16 16:45:07,438 WARNING  [!] Evaluating model on training set...
2023-03-16 16:45:22,617 WARNING  [!] This fold metrics on training set:
2023-03-16 16:45:22,626 WARNING 	AUC: 0.9993
2023-03-16 16:45:22,628 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:45:22,640 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:45:22,661 WARNING 	FPR: 0.001 | TPR: 0.9512 | F1: 0.9747
2023-03-16 16:45:22,669 WARNING 	FPR: 0.003 | TPR: 0.9737 | F1: 0.9860
2023-03-16 16:45:22,675 WARNING 	FPR: 0.01 | TPR: 0.9906 | F1: 0.9929
2023-03-16 16:45:22,697 WARNING 	FPR: 0.03 | TPR: 0.9966 | F1: 0.9912
2023-03-16 16:45:22,704 WARNING 	FPR: 0.1 | TPR: 0.9993 | F1: 0.9764
2023-03-16 16:45:22,704 WARNING  [!] Evaluating model on validation set...
2023-03-16 16:45:30,269 WARNING  [!] This fold metrics on validation set:
2023-03-16 16:45:30,272 WARNING 	AUC: 0.9976
2023-03-16 16:45:30,276 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:45:30,282 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:45:30,288 WARNING 	FPR: 0.001 | TPR: 0.9186 | F1: 0.9573
2023-03-16 16:45:30,295 WARNING 	FPR: 0.003 | TPR: 0.9468 | F1: 0.9720
2023-03-16 16:45:30,302 WARNING 	FPR: 0.01 | TPR: 0.9720 | F1: 0.9834
2023-03-16 16:45:30,308 WARNING 	FPR: 0.03 | TPR: 0.9842 | F1: 0.9852
2023-03-16 16:45:30,315 WARNING 	FPR: 0.1 | TPR: 0.9947 | F1: 0.9739
2023-03-16 16:45:30,465 WARNING  [3/3] Train set size: 50751, Validation set size: 25375
2023-03-16 16:45:32,614 WARNING  [!] Saved dataset splits to dataset_splits_1678981530.npz
2023-03-16 16:45:32,703 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3973e6
2023-03-16 16:45:32,704 WARNING  [*] Training time budget set: 5.0 min
2023-03-16 16:45:32,717 WARNING  [*] Started epoch: 1
2023-03-16 16:45:33,049 WARNING  [*] 16:45:33: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 2.601968 | Elapsed: 0.33s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.4973
2023-03-16 16:45:43,798 WARNING  [*] 16:45:43: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.393746 | Elapsed: 10.73s | FPR 0.0003 -> TPR 0.2879 & F1 0.4471 | AUC 0.8922
2023-03-16 16:45:54,647 WARNING  [*] 16:45:54: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.482019 | Elapsed: 10.84s | FPR 0.0003 -> TPR 0.4062 & F1 0.5778 | AUC 0.8303
2023-03-16 16:46:05,601 WARNING  [*] 16:46:05: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.276743 | Elapsed: 10.95s | FPR 0.0003 -> TPR 0.5915 & F1 0.7434 | AUC 0.9437
2023-03-16 16:46:16,603 WARNING  [*] 16:46:16: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.346199 | Elapsed: 10.99s | FPR 0.0003 -> TPR 0.6818 & F1 0.8108 | AUC 0.9269
2023-03-16 16:46:27,581 WARNING  [*] 16:46:27: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.202033 | Elapsed: 10.97s | FPR 0.0003 -> TPR 0.6271 & F1 0.7708 | AUC 0.9748
2023-03-16 16:46:31,823 WARNING  [*] Thu Mar 16 16:46:31 2023:    1    | Tr.loss: 0.351594 | Elapsed:   59.11  s | FPR 0.0003 -> TPR: 0.02 & F1: 0.03 | AUC: 0.9125
2023-03-16 16:46:31,823 WARNING  [*] Started epoch: 2
2023-03-16 16:46:31,949 WARNING  [*] 16:46:31: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.418103 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.6190 & F1 0.7647 | AUC 0.9096
2023-03-16 16:46:42,862 WARNING  [*] 16:46:42: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.177922 | Elapsed: 10.91s | FPR 0.0003 -> TPR 0.9028 & F1 0.9489 | AUC 0.9782
2023-03-16 16:46:53,807 WARNING  [*] 16:46:53: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.226515 | Elapsed: 10.93s | FPR 0.0003 -> TPR 0.7500 & F1 0.8571 | AUC 0.9721
2023-03-16 16:47:04,752 WARNING  [*] 16:47:04: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.165487 | Elapsed: 10.94s | FPR 0.0003 -> TPR 0.8194 & F1 0.9008 | AUC 0.9816
2023-03-16 16:47:15,769 WARNING  [*] 16:47:15: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.129707 | Elapsed: 11.01s | FPR 0.0003 -> TPR 0.8194 & F1 0.9008 | AUC 0.9881
2023-03-16 16:47:26,782 WARNING  [*] 16:47:26: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.154870 | Elapsed: 10.99s | FPR 0.0003 -> TPR 0.8235 & F1 0.9032 | AUC 0.9881
2023-03-16 16:47:30,998 WARNING  [*] Thu Mar 16 16:47:30 2023:    2    | Tr.loss: 0.153496 | Elapsed:   59.17  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.32 | AUC: 0.9839
2023-03-16 16:47:30,998 WARNING  [*] Started epoch: 3
2023-03-16 16:47:31,103 WARNING  [*] 16:47:31: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.074321 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.9538 & F1 0.9764 | AUC 0.9965
2023-03-16 16:47:42,048 WARNING  [*] 16:47:42: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.054346 | Elapsed: 10.93s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:47:53,018 WARNING  [*] 16:47:53: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.058209 | Elapsed: 10.96s | FPR 0.0003 -> TPR 0.9538 & F1 0.9764 | AUC 0.9982
2023-03-16 16:48:03,983 WARNING  [*] 16:48:03: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.076463 | Elapsed: 10.96s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9973
2023-03-16 16:48:14,927 WARNING  [*] 16:48:14: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.024945 | Elapsed: 10.94s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:48:25,905 WARNING  [*] 16:48:25: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.043636 | Elapsed: 10.97s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9995
2023-03-16 16:48:30,193 WARNING  [*] Thu Mar 16 16:48:30 2023:    3    | Tr.loss: 0.083144 | Elapsed:   59.20  s | FPR 0.0003 -> TPR: 0.56 & F1: 0.72 | AUC: 0.9951
2023-03-16 16:48:30,193 WARNING  [*] Started epoch: 4
2023-03-16 16:48:30,297 WARNING  [*] 16:48:30: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.029589 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.9857 & F1 0.9928 | AUC 0.9995
2023-03-16 16:48:41,226 WARNING  [*] 16:48:41: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.044725 | Elapsed: 10.92s | FPR 0.0003 -> TPR 0.9643 & F1 0.9818 | AUC 0.9992
2023-03-16 16:48:52,164 WARNING  [*] 16:48:52: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.022361 | Elapsed: 10.93s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:49:03,083 WARNING  [*] 16:49:03: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.057761 | Elapsed: 10.91s | FPR 0.0003 -> TPR 0.9552 & F1 0.9771 | AUC 0.9977
2023-03-16 16:49:14,002 WARNING  [*] 16:49:14: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.029049 | Elapsed: 10.92s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9995
2023-03-16 16:49:25,001 WARNING  [*] 16:49:25: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.064710 | Elapsed: 10.99s | FPR 0.0003 -> TPR 0.9733 & F1 0.9865 | AUC 0.9973
2023-03-16 16:49:29,212 WARNING  [*] Thu Mar 16 16:49:29 2023:    4    | Tr.loss: 0.057019 | Elapsed:   59.02  s | FPR 0.0003 -> TPR: 0.70 & F1: 0.82 | AUC: 0.9976
2023-03-16 16:49:29,212 WARNING  [*] Started epoch: 5
2023-03-16 16:49:29,327 WARNING  [*] 16:49:29: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.041956 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9828 & F1 0.9913 | AUC 0.9986
2023-03-16 16:49:40,236 WARNING  [*] 16:49:40: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.025939 | Elapsed: 10.91s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:49:51,156 WARNING  [*] 16:49:51: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.030737 | Elapsed: 10.91s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:50:02,126 WARNING  [*] 16:50:02: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.012795 | Elapsed: 10.97s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:50:13,039 WARNING  [*] 16:50:13: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.027614 | Elapsed: 10.91s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:50:24,016 WARNING  [*] 16:50:24: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.021419 | Elapsed: 10.97s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-16 16:50:28,252 WARNING  [*] Thu Mar 16 16:50:28 2023:    5    | Tr.loss: 0.044624 | Elapsed:   59.04  s | FPR 0.0003 -> TPR: 0.77 & F1: 0.87 | AUC: 0.9985
2023-03-16 16:50:28,253 WARNING  [*] Started epoch: 6
2023-03-16 16:50:28,359 WARNING  [*] 16:50:28: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.097824 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.8750 & F1 0.9333 | AUC 0.9961
2023-03-16 16:50:32,721 WARNING  [!] Time budget exceeded, training stopped.
2023-03-16 16:50:32,757 WARNING  [!] Thu Mar 16 16:50:32 2023: Dumped results:
                model       : 1678981530-model.torch
		train time  : 1678981530-trainTime.npy
		train losses: 1678981530-trainLosses.npy
		train AUC   : 1678981530-auc.npy
		train F1s   : 1678981530-trainF1s.npy
		train TPRs  : 1678981530-trainTPRs.npy
2023-03-16 16:50:32,762 WARNING  [!] Evaluating model on training set...
2023-03-16 16:50:47,971 WARNING  [!] This fold metrics on training set:
2023-03-16 16:50:47,980 WARNING 	AUC: 0.9994
2023-03-16 16:50:47,990 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:50:47,993 WARNING 	FPR: 0.0003 | TPR: 0.8819 | F1: 0.9372
2023-03-16 16:50:48,014 WARNING 	FPR: 0.001 | TPR: 0.9589 | F1: 0.9788
2023-03-16 16:50:48,022 WARNING 	FPR: 0.003 | TPR: 0.9751 | F1: 0.9867
2023-03-16 16:50:48,029 WARNING 	FPR: 0.01 | TPR: 0.9921 | F1: 0.9937
2023-03-16 16:50:48,050 WARNING 	FPR: 0.03 | TPR: 0.9969 | F1: 0.9913
2023-03-16 16:50:48,062 WARNING 	FPR: 0.1 | TPR: 0.9993 | F1: 0.9766
2023-03-16 16:50:48,066 WARNING  [!] Evaluating model on validation set...
2023-03-16 16:50:55,451 WARNING  [!] This fold metrics on validation set:
2023-03-16 16:50:55,456 WARNING 	AUC: 0.9974
2023-03-16 16:50:55,460 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:50:55,466 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-16 16:50:55,477 WARNING 	FPR: 0.001 | TPR: 0.8460 | F1: 0.9164
2023-03-16 16:50:55,479 WARNING 	FPR: 0.003 | TPR: 0.9163 | F1: 0.9556
2023-03-16 16:50:55,486 WARNING 	FPR: 0.01 | TPR: 0.9658 | F1: 0.9802
2023-03-16 16:50:55,492 WARNING 	FPR: 0.03 | TPR: 0.9866 | F1: 0.9862
2023-03-16 16:50:55,499 WARNING 	FPR: 0.1 | TPR: 0.9948 | F1: 0.9746
2023-03-16 16:50:55,585 WARNING  [!] Metrics saved to out_tokenizer_1678974949\cv_bpe_limNone_r1763_t5\bpe_metrics_validation.json
2023-03-16 16:50:55,590 WARNING  [!] Metrics saved to out_tokenizer_1678974949\cv_bpe_limNone_r1763_t5\bpe_metrics_training.json
2023-03-16 16:50:55,590 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9969
	FPR: 0.0001 -- TPR: 0.0000 -- F1: 0.0000
	FPR: 0.0003 -- TPR: 0.0000 -- F1: 0.0000
	FPR:  0.001 -- TPR: 0.5882 -- F1: 0.6246
	FPR:  0.003 -- TPR: 0.6210 -- F1: 0.6425
	FPR:   0.01 -- TPR: 0.9661 -- F1: 0.9804
	FPR:   0.03 -- TPR: 0.9848 -- F1: 0.9853
	FPR:    0.1 -- TPR: 0.9946 -- F1: 0.9742

