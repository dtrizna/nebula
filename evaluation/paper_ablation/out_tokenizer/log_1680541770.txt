2023-04-03 19:09:30,654 WARNING  [!] Skipping since exists: out_tokenizer\nebula_whitespace_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 19:09:30,733 WARNING  [!] Skipping since exists: out_tokenizer\nebula_whitespace_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 19:09:30,769 WARNING  [!] CV output folder out_tokenizer\cv_whitespace_limNone_r1763_t5 already exists, skipping!
2023-04-03 19:09:30,769 WARNING  [!] Skipping since exists: out_tokenizer\nebula_bpe_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 19:09:30,850 WARNING  [!] Skipping since exists: out_tokenizer\nebula_bpe_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 19:09:30,911 WARNING  [!] CV output folder out_tokenizer\cv_bpe_limNone_r1763_t5 already exists, skipping!
2023-04-03 19:09:30,926 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-04-03 19:14:06,680 WARNING Finished... Took: 275.75s
2023-04-03 19:14:06,680 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-04-03 19:20:46,942 WARNING Finished... Took: 400.26s
2023-04-03 19:20:46,942 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-04-03 19:22:56,808 WARNING Finished... Took: 129.87s
2023-04-03 19:22:56,809 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-04-03 19:24:50,932 WARNING Finished... Took: 114.12s
2023-04-03 19:24:50,933 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-04-03 19:26:12,136 WARNING Finished... Took: 81.20s
2023-04-03 19:26:12,136 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-04-03 19:30:14,481 WARNING Finished... Took: 242.34s
2023-04-03 19:30:14,481 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-04-03 19:31:11,993 WARNING Finished... Took: 57.51s
2023-04-03 19:31:11,993 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-04-03 19:34:00,501 WARNING Finished... Took: 168.51s
2023-04-03 19:34:00,501 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-04-03 19:34:03,015 WARNING Finished... Took: 2.51s
2023-04-03 19:34:03,025 WARNING  [!] Saved Y as out_tokenizer\nebula_wordpunct_vocab_50000_seqlen_512\y_train_full.npy
2023-04-03 19:34:03,225 WARNING  [!] Saved Y names as out_tokenizer\nebula_wordpunct_vocab_50000_seqlen_512\y_names_train_full.json
2023-04-03 19:34:03,225 WARNING  [*] Initializing tokenizer training...
2023-04-03 19:35:23,991 WARNING Dumped vocab to out_tokenizer\nebula_wordpunct_vocab_50000_seqlen_512\tokenizer_50000_vocab.json
2023-04-03 19:35:24,923 WARNING Dumped vocab counter to out_tokenizer\nebula_wordpunct_vocab_50000_seqlen_512\tokenizer_50000_counter.json
2023-04-03 19:35:24,924 WARNING  [*] Encoding and padding...
2023-04-03 19:37:09,102 WARNING  [!] Saved X as out_tokenizer\nebula_wordpunct_vocab_50000_seqlen_512\x_train_full.npy
2023-04-03 19:37:12,131 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-04-03 19:37:53,220 WARNING Finished... Took: 41.09s
2023-04-03 19:37:53,220 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-04-03 19:39:46,459 WARNING Finished... Took: 113.24s
2023-04-03 19:39:46,459 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-04-03 19:40:17,127 WARNING Finished... Took: 30.67s
2023-04-03 19:40:17,128 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-04-03 19:40:20,575 WARNING Finished... Took: 3.45s
2023-04-03 19:40:20,575 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-04-03 19:40:34,297 WARNING Finished... Took: 13.72s
2023-04-03 19:40:34,297 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-04-03 19:42:57,845 WARNING Finished... Took: 143.55s
2023-04-03 19:42:57,845 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-04-03 19:43:19,250 WARNING Finished... Took: 21.40s
2023-04-03 19:43:19,250 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-04-03 19:43:34,844 WARNING Finished... Took: 15.59s
2023-04-03 19:43:34,844 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-04-03 19:43:35,465 WARNING Finished... Took: 0.62s
2023-04-03 19:43:35,467 WARNING  [!] Saved Y as out_tokenizer\nebula_wordpunct_vocab_50000_seqlen_512\y_test_full.npy
2023-04-03 19:43:35,524 WARNING  [!] Saved Y names as out_tokenizer\nebula_wordpunct_vocab_50000_seqlen_512\y_names_test_full.json
2023-04-03 19:43:35,547 WARNING  [*] Encoding and padding...
2023-04-03 19:43:56,275 WARNING  [!] Saved X as out_tokenizer\nebula_wordpunct_vocab_50000_seqlen_512\x_test_full.npy
2023-04-03 19:43:56,930 WARNING  [!!!] Starting CV over wordpunct!
2023-04-03 19:43:57,054 WARNING  [!] Training time budget: 300min
2023-04-03 19:43:57,054 WARNING  [!] Model config: {'vocab_size': 50000, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-04-03 19:43:57,142 WARNING  [1/3] Train set size: 50750, Validation set size: 25376
2023-04-03 19:43:58,995 WARNING  [!] Saved dataset splits to dataset_splits_1680543837.npz
2023-04-03 19:43:59,200 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3972e6
2023-04-03 19:43:59,200 WARNING  [*] Training time budget set: 5.0 min
2023-04-03 19:43:59,232 WARNING  [*] Started epoch: 1
2023-04-03 19:44:01,058 WARNING  [*] 19:44:01: Train Epoch: 1 [  0  / 529  (0 %)] | Loss: 1.833666 | Elapsed: 1.82s | FPR 0.0003 -> TPR: 0.04 & F1: 0.07 | AUC: 0.5441
2023-04-03 19:44:11,696 WARNING  [*] 19:44:11: Train Epoch: 1 [9600 / 529  (19%)] | Loss: 0.339497 | Elapsed: 10.64s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.8786
2023-04-03 19:44:21,846 WARNING  [*] 19:44:21: Train Epoch: 1 [19200/ 529  (38%)] | Loss: 0.395893 | Elapsed: 10.15s | FPR 0.0003 -> TPR: 0.33 & F1: 0.50 | AUC: 0.9098
2023-04-03 19:44:32,025 WARNING  [*] 19:44:32: Train Epoch: 1 [28800/ 529  (57%)] | Loss: 0.359556 | Elapsed: 10.17s | FPR 0.0003 -> TPR: 0.25 & F1: 0.41 | AUC: 0.9254
2023-04-03 19:44:42,349 WARNING  [*] 19:44:42: Train Epoch: 1 [38400/ 529  (76%)] | Loss: 0.177443 | Elapsed: 10.32s | FPR 0.0003 -> TPR: 0.86 & F1: 0.92 | AUC: 0.9819
2023-04-03 19:44:52,619 WARNING  [*] 19:44:52: Train Epoch: 1 [48000/ 529  (95%)] | Loss: 0.236998 | Elapsed: 10.27s | FPR 0.0003 -> TPR: 0.54 & F1: 0.70 | AUC: 0.9614
2023-04-03 19:44:56,530 WARNING  [*] Mon Apr  3 19:44:56 2023:    1    | Tr.loss: 0.315377 | Elapsed:   57.30  s | FPR 0.0003 -> TPR: 0.10 & F1: 0.18 | AUC: 0.9280
2023-04-03 19:44:56,530 WARNING  [*] Started epoch: 2
2023-04-03 19:44:56,671 WARNING  [*] 19:44:56: Train Epoch: 2 [  0  / 529  (0 %)] | Loss: 0.199390 | Elapsed: 0.12s | FPR 0.0003 -> TPR: 0.79 & F1: 0.88 | AUC: 0.9769
2023-04-03 19:45:07,039 WARNING  [*] 19:45:07: Train Epoch: 2 [9600 / 529  (19%)] | Loss: 0.235184 | Elapsed: 10.35s | FPR 0.0003 -> TPR: 0.72 & F1: 0.84 | AUC: 0.9688
2023-04-03 19:45:17,351 WARNING  [*] 19:45:17: Train Epoch: 2 [19200/ 529  (38%)] | Loss: 0.133865 | Elapsed: 10.31s | FPR 0.0003 -> TPR: 0.82 & F1: 0.90 | AUC: 0.9858
2023-04-03 19:45:27,763 WARNING  [*] 19:45:27: Train Epoch: 2 [28800/ 529  (57%)] | Loss: 0.159506 | Elapsed: 10.40s | FPR 0.0003 -> TPR: 0.83 & F1: 0.91 | AUC: 0.9871
2023-04-03 19:45:38,375 WARNING  [*] 19:45:38: Train Epoch: 2 [38400/ 529  (76%)] | Loss: 0.158884 | Elapsed: 10.60s | FPR 0.0003 -> TPR: 0.86 & F1: 0.92 | AUC: 0.9862
2023-04-03 19:45:48,966 WARNING  [*] 19:45:48: Train Epoch: 2 [48000/ 529  (95%)] | Loss: 0.076201 | Elapsed: 10.58s | FPR 0.0003 -> TPR: 0.86 & F1: 0.93 | AUC: 0.9938
2023-04-03 19:45:52,927 WARNING  [*] Mon Apr  3 19:45:52 2023:    2    | Tr.loss: 0.135826 | Elapsed:   56.40  s | FPR 0.0003 -> TPR: 0.45 & F1: 0.62 | AUC: 0.9872
2023-04-03 19:45:52,927 WARNING  [*] Started epoch: 3
2023-04-03 19:45:53,068 WARNING  [*] 19:45:53: Train Epoch: 3 [  0  / 529  (0 %)] | Loss: 0.174055 | Elapsed: 0.14s | FPR 0.0003 -> TPR: 0.79 & F1: 0.88 | AUC: 0.9837
2023-04-03 19:46:03,648 WARNING  [*] 19:46:03: Train Epoch: 3 [9600 / 529  (19%)] | Loss: 0.060967 | Elapsed: 10.58s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9968
2023-04-03 19:46:14,520 WARNING  [*] 19:46:14: Train Epoch: 3 [19200/ 529  (38%)] | Loss: 0.115041 | Elapsed: 10.86s | FPR 0.0003 -> TPR: 0.90 & F1: 0.95 | AUC: 0.9936
2023-04-03 19:46:25,816 WARNING  [*] 19:46:25: Train Epoch: 3 [28800/ 529  (57%)] | Loss: 0.061625 | Elapsed: 11.30s | FPR 0.0003 -> TPR: 0.95 & F1: 0.98 | AUC: 0.9983
2023-04-03 19:46:36,502 WARNING  [*] 19:46:36: Train Epoch: 3 [38400/ 529  (76%)] | Loss: 0.043172 | Elapsed: 10.69s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-03 19:46:47,701 WARNING  [*] 19:46:47: Train Epoch: 3 [48000/ 529  (95%)] | Loss: 0.106425 | Elapsed: 11.18s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9916
2023-04-03 19:46:51,767 WARNING  [*] Mon Apr  3 19:46:51 2023:    3    | Tr.loss: 0.077754 | Elapsed:   58.84  s | FPR 0.0003 -> TPR: 0.75 & F1: 0.86 | AUC: 0.9955
2023-04-03 19:46:51,767 WARNING  [*] Started epoch: 4
2023-04-03 19:46:51,915 WARNING  [*] 19:46:51: Train Epoch: 4 [  0  / 529  (0 %)] | Loss: 0.035305 | Elapsed: 0.14s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9995
2023-04-03 19:47:02,537 WARNING  [*] 19:47:02: Train Epoch: 4 [9600 / 529  (19%)] | Loss: 0.038662 | Elapsed: 10.62s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9987
2023-04-03 19:47:13,287 WARNING  [*] 19:47:13: Train Epoch: 4 [19200/ 529  (38%)] | Loss: 0.140922 | Elapsed: 10.75s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9881
2023-04-03 19:47:24,113 WARNING  [*] 19:47:24: Train Epoch: 4 [28800/ 529  (57%)] | Loss: 0.034696 | Elapsed: 10.81s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:47:34,889 WARNING  [*] 19:47:34: Train Epoch: 4 [38400/ 529  (76%)] | Loss: 0.009142 | Elapsed: 10.76s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:47:45,760 WARNING  [*] 19:47:45: Train Epoch: 4 [48000/ 529  (95%)] | Loss: 0.040664 | Elapsed: 10.86s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9991
2023-04-03 19:47:49,811 WARNING  [*] Mon Apr  3 19:47:49 2023:    4    | Tr.loss: 0.057425 | Elapsed:   58.04  s | FPR 0.0003 -> TPR: 0.81 & F1: 0.89 | AUC: 0.9974
2023-04-03 19:47:49,811 WARNING  [*] Started epoch: 5
2023-04-03 19:47:49,943 WARNING  [*] 19:47:49: Train Epoch: 5 [  0  / 529  (0 %)] | Loss: 0.022295 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:48:00,668 WARNING  [*] 19:48:00: Train Epoch: 5 [9600 / 529  (19%)] | Loss: 0.079296 | Elapsed: 10.71s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9963
2023-04-03 19:48:11,556 WARNING  [*] 19:48:11: Train Epoch: 5 [19200/ 529  (38%)] | Loss: 0.021095 | Elapsed: 10.87s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:48:22,591 WARNING  [*] 19:48:22: Train Epoch: 5 [28800/ 529  (57%)] | Loss: 0.027587 | Elapsed: 11.03s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:48:33,540 WARNING  [*] 19:48:33: Train Epoch: 5 [38400/ 529  (76%)] | Loss: 0.028707 | Elapsed: 10.94s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9996
2023-04-03 19:48:44,910 WARNING  [*] 19:48:44: Train Epoch: 5 [48000/ 529  (95%)] | Loss: 0.071952 | Elapsed: 11.36s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9959
2023-04-03 19:48:49,061 WARNING  [*] Mon Apr  3 19:48:49 2023:    5    | Tr.loss: 0.047271 | Elapsed:   59.25  s | FPR 0.0003 -> TPR: 0.77 & F1: 0.87 | AUC: 0.9982
2023-04-03 19:48:49,062 WARNING  [*] Started epoch: 6
2023-04-03 19:48:49,209 WARNING  [*] 19:48:49: Train Epoch: 6 [  0  / 529  (0 %)] | Loss: 0.018745 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:48:59,296 WARNING  [!] Time budget exceeded, training stopped.
2023-04-03 19:48:59,341 WARNING  [!] Mon Apr  3 19:48:59 2023: Dumped results:
                model       : 1680543837-model.torch
		train time  : 1680543837-trainTime.npy
		train losses: 1680543837-trainLosses.npy
		train AUC   : 1680543837-auc.npy
		train F1s   : 1680543837-trainF1s.npy
		train TPRs  : 1680543837-trainTPRs.npy
2023-04-03 19:48:59,367 WARNING  [!] Evaluating model on training set...
2023-04-03 19:49:13,831 WARNING  [!] This fold metrics on training set:
2023-04-03 19:49:13,852 WARNING 	AUC: 0.9990
2023-04-03 19:49:13,867 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-03 19:49:13,881 WARNING 	FPR: 0.0003 | TPR: 0.9110 | F1: 0.9534
2023-04-03 19:49:13,896 WARNING 	FPR: 0.001 | TPR: 0.9568 | F1: 0.9777
2023-04-03 19:49:13,908 WARNING 	FPR: 0.003 | TPR: 0.9681 | F1: 0.9831
2023-04-03 19:49:13,927 WARNING 	FPR: 0.01 | TPR: 0.9858 | F1: 0.9905
2023-04-03 19:49:13,939 WARNING 	FPR: 0.03 | TPR: 0.9937 | F1: 0.9897
2023-04-03 19:49:13,951 WARNING 	FPR: 0.1 | TPR: 0.9980 | F1: 0.9756
2023-04-03 19:49:13,951 WARNING  [!] Evaluating model on validation set...
2023-04-03 19:49:21,593 WARNING  [!] This fold metrics on validation set:
2023-04-03 19:49:21,598 WARNING 	AUC: 0.9973
2023-04-03 19:49:21,603 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-03 19:49:21,612 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-04-03 19:49:21,618 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-04-03 19:49:21,627 WARNING 	FPR: 0.003 | TPR: 0.9421 | F1: 0.9695
2023-04-03 19:49:21,634 WARNING 	FPR: 0.01 | TPR: 0.9691 | F1: 0.9819
2023-04-03 19:49:21,642 WARNING 	FPR: 0.03 | TPR: 0.9865 | F1: 0.9861
2023-04-03 19:49:21,650 WARNING 	FPR: 0.1 | TPR: 0.9952 | F1: 0.9743
2023-04-03 19:49:21,863 WARNING  [2/3] Train set size: 50751, Validation set size: 25375
2023-04-03 19:49:24,234 WARNING  [!] Saved dataset splits to dataset_splits_1680544161.npz
2023-04-03 19:49:24,346 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3972e6
2023-04-03 19:49:24,347 WARNING  [*] Training time budget set: 5.0 min
2023-04-03 19:49:24,385 WARNING  [*] Started epoch: 1
2023-04-03 19:49:24,628 WARNING  [*] 19:49:24: Train Epoch: 1 [  0  / 529  (0 %)] | Loss: 1.772887 | Elapsed: 0.23s | FPR 0.0003 -> TPR: 0.05 & F1: 0.09 | AUC: 0.5221
2023-04-03 19:49:35,658 WARNING  [*] 19:49:35: Train Epoch: 1 [9600 / 529  (19%)] | Loss: 0.336883 | Elapsed: 11.01s | FPR 0.0003 -> TPR: 0.40 & F1: 0.57 | AUC: 0.8899
2023-04-03 19:49:46,844 WARNING  [*] 19:49:46: Train Epoch: 1 [19200/ 529  (38%)] | Loss: 0.288166 | Elapsed: 11.17s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9358
2023-04-03 19:49:57,839 WARNING  [*] 19:49:57: Train Epoch: 1 [28800/ 529  (57%)] | Loss: 0.347421 | Elapsed: 10.98s | FPR 0.0003 -> TPR: 0.20 & F1: 0.33 | AUC: 0.9072
2023-04-03 19:50:08,510 WARNING  [*] 19:50:08: Train Epoch: 1 [38400/ 529  (76%)] | Loss: 0.202034 | Elapsed: 10.66s | FPR 0.0003 -> TPR: 0.41 & F1: 0.58 | AUC: 0.9604
2023-04-03 19:50:19,135 WARNING  [*] 19:50:19: Train Epoch: 1 [48000/ 529  (95%)] | Loss: 0.120799 | Elapsed: 10.62s | FPR 0.0003 -> TPR: 0.86 & F1: 0.93 | AUC: 0.9873
2023-04-03 19:50:23,589 WARNING  [*] Mon Apr  3 19:50:23 2023:    1    | Tr.loss: 0.310431 | Elapsed:   59.20  s | FPR 0.0003 -> TPR: 0.11 & F1: 0.20 | AUC: 0.9298
2023-04-03 19:50:23,589 WARNING  [*] Started epoch: 2
2023-04-03 19:50:23,728 WARNING  [*] 19:50:23: Train Epoch: 2 [  0  / 529  (0 %)] | Loss: 0.187467 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.89 & F1: 0.94 | AUC: 0.9771
2023-04-03 19:50:34,705 WARNING  [*] 19:50:34: Train Epoch: 2 [9600 / 529  (19%)] | Loss: 0.106697 | Elapsed: 10.98s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9960
2023-04-03 19:50:45,517 WARNING  [*] 19:50:45: Train Epoch: 2 [19200/ 529  (38%)] | Loss: 0.102967 | Elapsed: 10.80s | FPR 0.0003 -> TPR: 0.89 & F1: 0.94 | AUC: 0.9941
2023-04-03 19:50:56,241 WARNING  [*] 19:50:56: Train Epoch: 2 [28800/ 529  (57%)] | Loss: 0.128157 | Elapsed: 10.72s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9911
2023-04-03 19:51:06,997 WARNING  [*] 19:51:06: Train Epoch: 2 [38400/ 529  (76%)] | Loss: 0.074706 | Elapsed: 10.76s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9984
2023-04-03 19:51:17,962 WARNING  [*] 19:51:17: Train Epoch: 2 [48000/ 529  (95%)] | Loss: 0.161793 | Elapsed: 10.97s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9858
2023-04-03 19:51:22,197 WARNING  [*] Mon Apr  3 19:51:22 2023:    2    | Tr.loss: 0.140170 | Elapsed:   58.61  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9863
2023-04-03 19:51:22,197 WARNING  [*] Started epoch: 3
2023-04-03 19:51:22,337 WARNING  [*] 19:51:22: Train Epoch: 3 [  0  / 529  (0 %)] | Loss: 0.103619 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.93 & F1: 0.96 | AUC: 0.9899
2023-04-03 19:51:33,078 WARNING  [*] 19:51:33: Train Epoch: 3 [9600 / 529  (19%)] | Loss: 0.067578 | Elapsed: 10.73s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9976
2023-04-03 19:51:44,048 WARNING  [*] 19:51:44: Train Epoch: 3 [19200/ 529  (38%)] | Loss: 0.081545 | Elapsed: 10.97s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9958
2023-04-03 19:51:54,904 WARNING  [*] 19:51:54: Train Epoch: 3 [28800/ 529  (57%)] | Loss: 0.097002 | Elapsed: 10.86s | FPR 0.0003 -> TPR: 0.95 & F1: 0.97 | AUC: 0.9941
2023-04-03 19:52:05,793 WARNING  [*] 19:52:05: Train Epoch: 3 [38400/ 529  (76%)] | Loss: 0.056662 | Elapsed: 10.89s | FPR 0.0003 -> TPR: 0.92 & F1: 0.96 | AUC: 0.9978
2023-04-03 19:52:16,685 WARNING  [*] 19:52:16: Train Epoch: 3 [48000/ 529  (95%)] | Loss: 0.104613 | Elapsed: 10.89s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9945
2023-04-03 19:52:20,874 WARNING  [*] Mon Apr  3 19:52:20 2023:    3    | Tr.loss: 0.081738 | Elapsed:   58.68  s | FPR 0.0003 -> TPR: 0.47 & F1: 0.64 | AUC: 0.9951
2023-04-03 19:52:20,874 WARNING  [*] Started epoch: 4
2023-04-03 19:52:21,000 WARNING  [*] 19:52:21: Train Epoch: 4 [  0  / 529  (0 %)] | Loss: 0.073690 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9962
2023-04-03 19:52:31,707 WARNING  [*] 19:52:31: Train Epoch: 4 [9600 / 529  (19%)] | Loss: 0.076305 | Elapsed: 10.69s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9982
2023-04-03 19:52:42,392 WARNING  [*] 19:52:42: Train Epoch: 4 [19200/ 529  (38%)] | Loss: 0.092954 | Elapsed: 10.67s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9959
2023-04-03 19:52:54,097 WARNING  [*] 19:52:54: Train Epoch: 4 [28800/ 529  (57%)] | Loss: 0.040544 | Elapsed: 11.69s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9991
2023-04-03 19:53:05,185 WARNING  [*] 19:53:05: Train Epoch: 4 [38400/ 529  (76%)] | Loss: 0.072716 | Elapsed: 11.07s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9969
2023-04-03 19:53:16,386 WARNING  [*] 19:53:16: Train Epoch: 4 [48000/ 529  (95%)] | Loss: 0.061854 | Elapsed: 11.19s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9975
2023-04-03 19:53:20,490 WARNING  [*] Mon Apr  3 19:53:20 2023:    4    | Tr.loss: 0.059978 | Elapsed:   59.62  s | FPR 0.0003 -> TPR: 0.68 & F1: 0.81 | AUC: 0.9972
2023-04-03 19:53:20,490 WARNING  [*] Started epoch: 5
2023-04-03 19:53:20,630 WARNING  [*] 19:53:20: Train Epoch: 5 [  0  / 529  (0 %)] | Loss: 0.054032 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9980
2023-04-03 19:53:31,497 WARNING  [*] 19:53:31: Train Epoch: 5 [9600 / 529  (19%)] | Loss: 0.012623 | Elapsed: 10.86s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-03 19:53:42,193 WARNING  [*] 19:53:42: Train Epoch: 5 [19200/ 529  (38%)] | Loss: 0.104234 | Elapsed: 10.69s | FPR 0.0003 -> TPR: 0.90 & F1: 0.95 | AUC: 0.9953
2023-04-03 19:53:53,123 WARNING  [*] 19:53:53: Train Epoch: 5 [28800/ 529  (57%)] | Loss: 0.067180 | Elapsed: 10.91s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9967
2023-04-03 19:54:04,068 WARNING  [*] 19:54:04: Train Epoch: 5 [38400/ 529  (76%)] | Loss: 0.023884 | Elapsed: 10.93s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:54:15,072 WARNING  [*] 19:54:15: Train Epoch: 5 [48000/ 529  (95%)] | Loss: 0.027224 | Elapsed: 11.00s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:54:19,192 WARNING  [*] Mon Apr  3 19:54:19 2023:    5    | Tr.loss: 0.048947 | Elapsed:   58.70  s | FPR 0.0003 -> TPR: 0.79 & F1: 0.88 | AUC: 0.9981
2023-04-03 19:54:19,192 WARNING  [*] Started epoch: 6
2023-04-03 19:54:19,319 WARNING  [*] 19:54:19: Train Epoch: 6 [  0  / 529  (0 %)] | Loss: 0.064961 | Elapsed: 0.12s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9971
2023-04-03 19:54:24,369 WARNING  [!] Time budget exceeded, training stopped.
2023-04-03 19:54:24,423 WARNING  [!] Mon Apr  3 19:54:24 2023: Dumped results:
                model       : 1680544161-model.torch
		train time  : 1680544161-trainTime.npy
		train losses: 1680544161-trainLosses.npy
		train AUC   : 1680544161-auc.npy
		train F1s   : 1680544161-trainF1s.npy
		train TPRs  : 1680544161-trainTPRs.npy
2023-04-03 19:54:24,446 WARNING  [!] Evaluating model on training set...
2023-04-03 19:54:39,709 WARNING  [!] This fold metrics on training set:
2023-04-03 19:54:39,727 WARNING 	AUC: 0.9991
2023-04-03 19:54:39,745 WARNING 	FPR: 0.0001 | TPR: 0.8427 | F1: 0.9146
2023-04-03 19:54:39,764 WARNING 	FPR: 0.0003 | TPR: 0.8705 | F1: 0.9307
2023-04-03 19:54:39,785 WARNING 	FPR: 0.001 | TPR: 0.9455 | F1: 0.9718
2023-04-03 19:54:39,803 WARNING 	FPR: 0.003 | TPR: 0.9718 | F1: 0.9850
2023-04-03 19:54:39,821 WARNING 	FPR: 0.01 | TPR: 0.9853 | F1: 0.9902
2023-04-03 19:54:39,839 WARNING 	FPR: 0.03 | TPR: 0.9931 | F1: 0.9894
2023-04-03 19:54:39,861 WARNING 	FPR: 0.1 | TPR: 0.9983 | F1: 0.9758
2023-04-03 19:54:39,861 WARNING  [!] Evaluating model on validation set...
2023-04-03 19:54:47,215 WARNING  [!] This fold metrics on validation set:
2023-04-03 19:54:47,228 WARNING 	AUC: 0.9982
2023-04-03 19:54:47,235 WARNING 	FPR: 0.0001 | TPR: 0.6026 | F1: 0.7520
2023-04-03 19:54:47,239 WARNING 	FPR: 0.0003 | TPR: 0.8718 | F1: 0.9314
2023-04-03 19:54:47,254 WARNING 	FPR: 0.001 | TPR: 0.9276 | F1: 0.9622
2023-04-03 19:54:47,262 WARNING 	FPR: 0.003 | TPR: 0.9625 | F1: 0.9802
2023-04-03 19:54:47,264 WARNING 	FPR: 0.01 | TPR: 0.9795 | F1: 0.9873
2023-04-03 19:54:47,273 WARNING 	FPR: 0.03 | TPR: 0.9883 | F1: 0.9870
2023-04-03 19:54:47,281 WARNING 	FPR: 0.1 | TPR: 0.9955 | F1: 0.9743
2023-04-03 19:54:47,468 WARNING  [3/3] Train set size: 50751, Validation set size: 25375
2023-04-03 19:54:49,874 WARNING  [!] Saved dataset splits to dataset_splits_1680544487.npz
2023-04-03 19:54:49,990 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3972e6
2023-04-03 19:54:49,990 WARNING  [*] Training time budget set: 5.0 min
2023-04-03 19:54:50,040 WARNING  [*] Started epoch: 1
2023-04-03 19:54:50,266 WARNING  [*] 19:54:50: Train Epoch: 1 [  0  / 529  (0 %)] | Loss: 2.079878 | Elapsed: 0.21s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5268
2023-04-03 19:55:01,145 WARNING  [*] 19:55:01: Train Epoch: 1 [9600 / 529  (19%)] | Loss: 0.323827 | Elapsed: 10.86s | FPR 0.0003 -> TPR: 0.68 & F1: 0.81 | AUC: 0.9287
2023-04-03 19:55:11,867 WARNING  [*] 19:55:11: Train Epoch: 1 [19200/ 529  (38%)] | Loss: 0.348995 | Elapsed: 10.72s | FPR 0.0003 -> TPR: 0.62 & F1: 0.77 | AUC: 0.9113
2023-04-03 19:55:22,830 WARNING  [*] 19:55:22: Train Epoch: 1 [28800/ 529  (57%)] | Loss: 0.263190 | Elapsed: 10.96s | FPR 0.0003 -> TPR: 0.56 & F1: 0.72 | AUC: 0.9405
2023-04-03 19:55:33,528 WARNING  [*] 19:55:33: Train Epoch: 1 [38400/ 529  (76%)] | Loss: 0.205425 | Elapsed: 10.68s | FPR 0.0003 -> TPR: 0.70 & F1: 0.82 | AUC: 0.9780
2023-04-03 19:55:44,196 WARNING  [*] 19:55:44: Train Epoch: 1 [48000/ 529  (95%)] | Loss: 0.214820 | Elapsed: 10.65s | FPR 0.0003 -> TPR: 0.53 & F1: 0.69 | AUC: 0.9674
2023-04-03 19:55:48,427 WARNING  [*] Mon Apr  3 19:55:48 2023:    1    | Tr.loss: 0.326687 | Elapsed:   58.39  s | FPR 0.0003 -> TPR: 0.03 & F1: 0.05 | AUC: 0.9266
2023-04-03 19:55:48,427 WARNING  [*] Started epoch: 2
2023-04-03 19:55:48,551 WARNING  [*] 19:55:48: Train Epoch: 2 [  0  / 529  (0 %)] | Loss: 0.142960 | Elapsed: 0.11s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9899
2023-04-03 19:55:59,295 WARNING  [*] 19:55:59: Train Epoch: 2 [9600 / 529  (19%)] | Loss: 0.168133 | Elapsed: 10.74s | FPR 0.0003 -> TPR: 0.87 & F1: 0.93 | AUC: 0.9828
2023-04-03 19:56:10,071 WARNING  [*] 19:56:10: Train Epoch: 2 [19200/ 529  (38%)] | Loss: 0.172969 | Elapsed: 10.78s | FPR 0.0003 -> TPR: 0.60 & F1: 0.75 | AUC: 0.9757
2023-04-03 19:56:20,905 WARNING  [*] 19:56:20: Train Epoch: 2 [28800/ 529  (57%)] | Loss: 0.073021 | Elapsed: 10.82s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9990
2023-04-03 19:56:31,564 WARNING  [*] 19:56:31: Train Epoch: 2 [38400/ 529  (76%)] | Loss: 0.028870 | Elapsed: 10.66s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:56:42,211 WARNING  [*] 19:56:42: Train Epoch: 2 [48000/ 529  (95%)] | Loss: 0.123953 | Elapsed: 10.63s | FPR 0.0003 -> TPR: 0.95 & F1: 0.97 | AUC: 0.9946
2023-04-03 19:56:46,381 WARNING  [*] Mon Apr  3 19:56:46 2023:    2    | Tr.loss: 0.133462 | Elapsed:   57.95  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9877
2023-04-03 19:56:46,381 WARNING  [*] Started epoch: 3
2023-04-03 19:56:46,532 WARNING  [*] 19:56:46: Train Epoch: 3 [  0  / 529  (0 %)] | Loss: 0.108811 | Elapsed: 0.14s | FPR 0.0003 -> TPR: 0.95 & F1: 0.97 | AUC: 0.9931
2023-04-03 19:56:57,274 WARNING  [*] 19:56:57: Train Epoch: 3 [9600 / 529  (19%)] | Loss: 0.118732 | Elapsed: 10.73s | FPR 0.0003 -> TPR: 0.93 & F1: 0.96 | AUC: 0.9910
2023-04-03 19:57:07,992 WARNING  [*] 19:57:07: Train Epoch: 3 [19200/ 529  (38%)] | Loss: 0.037151 | Elapsed: 10.70s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:57:18,881 WARNING  [*] 19:57:18: Train Epoch: 3 [28800/ 529  (57%)] | Loss: 0.034410 | Elapsed: 10.89s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9991
2023-04-03 19:57:29,957 WARNING  [*] 19:57:29: Train Epoch: 3 [38400/ 529  (76%)] | Loss: 0.058262 | Elapsed: 11.06s | FPR 0.0003 -> TPR: 0.97 & F1: 0.98 | AUC: 0.9957
2023-04-03 19:57:40,900 WARNING  [*] 19:57:40: Train Epoch: 3 [48000/ 529  (95%)] | Loss: 0.143908 | Elapsed: 10.93s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9923
2023-04-03 19:57:45,132 WARNING  [*] Mon Apr  3 19:57:45 2023:    3    | Tr.loss: 0.080933 | Elapsed:   58.75  s | FPR 0.0003 -> TPR: 0.55 & F1: 0.71 | AUC: 0.9951
2023-04-03 19:57:45,132 WARNING  [*] Started epoch: 4
2023-04-03 19:57:45,268 WARNING  [*] 19:57:45: Train Epoch: 4 [  0  / 529  (0 %)] | Loss: 0.047481 | Elapsed: 0.12s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-03 19:57:55,971 WARNING  [*] 19:57:55: Train Epoch: 4 [9600 / 529  (19%)] | Loss: 0.016991 | Elapsed: 10.69s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:58:06,758 WARNING  [*] 19:58:06: Train Epoch: 4 [19200/ 529  (38%)] | Loss: 0.034517 | Elapsed: 10.79s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-03 19:58:17,347 WARNING  [*] 19:58:17: Train Epoch: 4 [28800/ 529  (57%)] | Loss: 0.067311 | Elapsed: 10.59s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9977
2023-04-03 19:58:27,953 WARNING  [*] 19:58:27: Train Epoch: 4 [38400/ 529  (76%)] | Loss: 0.082331 | Elapsed: 10.59s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9964
2023-04-03 19:58:38,565 WARNING  [*] 19:58:38: Train Epoch: 4 [48000/ 529  (95%)] | Loss: 0.024653 | Elapsed: 10.61s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:58:42,735 WARNING  [*] Mon Apr  3 19:58:42 2023:    4    | Tr.loss: 0.059456 | Elapsed:   57.60  s | FPR 0.0003 -> TPR: 0.74 & F1: 0.85 | AUC: 0.9973
2023-04-03 19:58:42,735 WARNING  [*] Started epoch: 5
2023-04-03 19:58:42,856 WARNING  [*] 19:58:42: Train Epoch: 5 [  0  / 529  (0 %)] | Loss: 0.046882 | Elapsed: 0.11s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9973
2023-04-03 19:58:53,566 WARNING  [*] 19:58:53: Train Epoch: 5 [9600 / 529  (19%)] | Loss: 0.032780 | Elapsed: 10.71s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9991
2023-04-03 19:59:04,361 WARNING  [*] 19:59:04: Train Epoch: 5 [19200/ 529  (38%)] | Loss: 0.027293 | Elapsed: 10.79s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9996
2023-04-03 19:59:15,020 WARNING  [*] 19:59:15: Train Epoch: 5 [28800/ 529  (57%)] | Loss: 0.102212 | Elapsed: 10.64s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:59:25,679 WARNING  [*] 19:59:25: Train Epoch: 5 [38400/ 529  (76%)] | Loss: 0.021935 | Elapsed: 10.64s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:59:36,286 WARNING  [*] 19:59:36: Train Epoch: 5 [48000/ 529  (95%)] | Loss: 0.013302 | Elapsed: 10.59s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-03 19:59:40,585 WARNING  [*] Mon Apr  3 19:59:40 2023:    5    | Tr.loss: 0.049062 | Elapsed:   57.85  s | FPR 0.0003 -> TPR: 0.76 & F1: 0.86 | AUC: 0.9981
2023-04-03 19:59:40,585 WARNING  [*] Started epoch: 6
2023-04-03 19:59:40,714 WARNING  [*] 19:59:40: Train Epoch: 6 [  0  / 529  (0 %)] | Loss: 0.090741 | Elapsed: 0.11s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9957
2023-04-03 19:59:49,994 WARNING  [!] Time budget exceeded, training stopped.
2023-04-03 19:59:50,049 WARNING  [!] Mon Apr  3 19:59:50 2023: Dumped results:
                model       : 1680544487-model.torch
		train time  : 1680544487-trainTime.npy
		train losses: 1680544487-trainLosses.npy
		train AUC   : 1680544487-auc.npy
		train F1s   : 1680544487-trainF1s.npy
		train TPRs  : 1680544487-trainTPRs.npy
2023-04-03 19:59:50,072 WARNING  [!] Evaluating model on training set...
2023-04-03 20:00:04,746 WARNING  [!] This fold metrics on training set:
2023-04-03 20:00:04,759 WARNING 	AUC: 0.9992
2023-04-03 20:00:04,766 WARNING 	FPR: 0.0001 | TPR: 0.7981 | F1: 0.8877
2023-04-03 20:00:04,793 WARNING 	FPR: 0.0003 | TPR: 0.9259 | F1: 0.9615
2023-04-03 20:00:04,798 WARNING 	FPR: 0.001 | TPR: 0.9605 | F1: 0.9796
2023-04-03 20:00:04,808 WARNING 	FPR: 0.003 | TPR: 0.9722 | F1: 0.9852
2023-04-03 20:00:04,831 WARNING 	FPR: 0.01 | TPR: 0.9864 | F1: 0.9908
2023-04-03 20:00:04,838 WARNING 	FPR: 0.03 | TPR: 0.9934 | F1: 0.9895
2023-04-03 20:00:04,847 WARNING 	FPR: 0.1 | TPR: 0.9980 | F1: 0.9757
2023-04-03 20:00:04,847 WARNING  [!] Evaluating model on validation set...
2023-04-03 20:00:12,105 WARNING  [!] This fold metrics on validation set:
2023-04-03 20:00:12,114 WARNING 	AUC: 0.9981
2023-04-03 20:00:12,117 WARNING 	FPR: 0.0001 | TPR: 0.8147 | F1: 0.8979
2023-04-03 20:00:12,124 WARNING 	FPR: 0.0003 | TPR: 0.8611 | F1: 0.9253
2023-04-03 20:00:12,138 WARNING 	FPR: 0.001 | TPR: 0.9095 | F1: 0.9524
2023-04-03 20:00:12,145 WARNING 	FPR: 0.003 | TPR: 0.9465 | F1: 0.9718
2023-04-03 20:00:12,149 WARNING 	FPR: 0.01 | TPR: 0.9757 | F1: 0.9853
2023-04-03 20:00:12,155 WARNING 	FPR: 0.03 | TPR: 0.9885 | F1: 0.9875
2023-04-03 20:00:12,164 WARNING 	FPR: 0.1 | TPR: 0.9949 | F1: 0.9751
2023-04-03 20:00:12,250 WARNING  [!] Metrics saved to out_tokenizer\cv_wordpunct_limNone_r1763_t5\wordpunct_metrics_validation.json
2023-04-03 20:00:12,250 WARNING  [!] Metrics saved to out_tokenizer\cv_wordpunct_limNone_r1763_t5\wordpunct_metrics_training.json
2023-04-03 20:00:12,266 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9979
	FPR: 0.0001 -- TPR: 0.4724 -- F1: 0.5500
	FPR: 0.0003 -- TPR: 0.5776 -- F1: 0.6189
	FPR:  0.001 -- TPR: 0.6124 -- F1: 0.6382
	FPR:  0.003 -- TPR: 0.9504 -- F1: 0.9738
	FPR:   0.01 -- TPR: 0.9748 -- F1: 0.9848
	FPR:   0.03 -- TPR: 0.9877 -- F1: 0.9869
	FPR:    0.1 -- TPR: 0.9952 -- F1: 0.9746

