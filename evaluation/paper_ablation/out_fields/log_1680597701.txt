2023-04-04 10:41:41,031 WARNING  [!] Working on api_file_network_registry!
2023-04-04 10:41:41,031 WARNING  [!] Skipping since exists: out_fields\api_file_network_registry_vocab_50000_seqlen_512\x_train_full.npy
2023-04-04 10:41:41,138 WARNING  [!] Skipping since exists: out_fields\api_file_network_registry_vocab_50000_seqlen_512\x_test_full.npy
2023-04-04 10:41:41,198 WARNING  [!!!] Starting CV over api_file_network_registry!
2023-04-04 10:41:41,331 WARNING  [!] CV output folder out_fields\cv_api_file_network_registry_limNone_r1763_t5 already exists, skipping!
2023-04-04 10:41:41,331 WARNING  [!] Working on api_only_name!
2023-04-04 10:41:41,331 WARNING  [!] Skipping since exists: out_fields\api_only_name_vocab_50000_seqlen_512\x_train_full.npy
2023-04-04 10:41:41,459 WARNING  [!] Skipping since exists: out_fields\api_only_name_vocab_50000_seqlen_512\x_test_full.npy
2023-04-04 10:41:41,517 WARNING  [!!!] Starting CV over api_only_name!
2023-04-04 10:41:41,666 WARNING  [!] CV output folder out_fields\cv_api_only_name_limNone_r1763_t5 already exists, skipping!
2023-04-04 10:41:41,667 WARNING  [!] Working on api_only_full!
2023-04-04 10:41:41,668 WARNING  [!] Skipping since exists: out_fields\api_only_full_vocab_50000_seqlen_512\x_train_full.npy
2023-04-04 10:41:41,822 WARNING  [!] Skipping since exists: out_fields\api_only_full_vocab_50000_seqlen_512\x_test_full.npy
2023-04-04 10:41:41,892 WARNING  [!!!] Starting CV over api_only_full!
2023-04-04 10:41:42,041 WARNING  [!] CV output folder out_fields\cv_api_only_full_limNone_r1763_t5 already exists, skipping!
2023-04-04 10:41:42,042 WARNING  [!] Working on file_network_registry!
2023-04-04 10:41:42,042 WARNING  [!] Skipping since exists: out_fields\file_network_registry_vocab_50000_seqlen_512\x_train_full.npy
2023-04-04 10:41:42,190 WARNING  [!] Skipping since exists: out_fields\file_network_registry_vocab_50000_seqlen_512\x_test_full.npy
2023-04-04 10:41:42,241 WARNING  [!!!] Starting CV over file_network_registry!
2023-04-04 10:41:42,422 WARNING  [!] CV output folder out_fields\cv_file_network_registry_limNone_r1763_t5 already exists, skipping!
2023-04-04 10:41:42,423 WARNING  [!] Working on all!
2023-04-04 10:41:42,424 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-04-04 10:43:12,995 WARNING Finished... Took: 90.57s
2023-04-04 10:43:12,996 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-04-04 10:46:18,919 WARNING Finished... Took: 185.92s
2023-04-04 10:46:18,919 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-04-04 10:47:32,554 WARNING Finished... Took: 73.64s
2023-04-04 10:47:32,555 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-04-04 10:48:43,831 WARNING Finished... Took: 71.28s
2023-04-04 10:48:43,831 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-04-04 10:49:19,728 WARNING Finished... Took: 35.90s
2023-04-04 10:49:19,728 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-04-04 10:50:39,595 WARNING Finished... Took: 79.87s
2023-04-04 10:50:39,596 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-04-04 10:51:00,063 WARNING Finished... Took: 20.47s
2023-04-04 10:51:00,064 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-04-04 10:52:15,037 WARNING Finished... Took: 74.97s
2023-04-04 10:52:15,037 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-04-04 10:52:16,390 WARNING Finished... Took: 1.35s
2023-04-04 10:52:16,395 WARNING  [!] Saved Y as out_fields\all_vocab_50000_seqlen_512\y_train_full.npy
2023-04-04 10:52:16,709 WARNING  [!] Saved Y names as out_fields\all_vocab_50000_seqlen_512\y_names_train_full.json
2023-04-04 10:52:16,709 WARNING  [!] Initialized tokenizer without pre-trained model.
	You need to train tokenizer with .train() or specify 'model_path=' during initialization!
2023-04-04 10:52:16,709 WARNING  [*] Initializing tokenizer training...
2023-04-04 10:52:16,709 WARNING  [*] Data preparation for SentencePiece tokenizer...
2023-04-04 10:58:01,509 WARNING  [*] Saving to disk...
2023-04-04 10:58:19,023 WARNING  [!] Training tokenizer with command: --input=out_fields\all_vocab_50000_seqlen_512\tokenizer_50000_trainset_1680598681.txt --model_prefix=out_fields\all_vocab_50000_seqlen_512\tokenizer_50000 --vocab_size=50000 --model_type=bpe --split_by_number=False --max_sentence_length=4192 --max_sentencepiece_length=64
2023-04-04 11:05:48,130 WARNING  [!] Loaded vocab with size 50001 from out_fields\all_vocab_50000_seqlen_512\tokenizer_50000.vocab
2023-04-04 11:05:48,959 WARNING  [*] Encoding and padding...
2023-04-04 11:19:11,784 WARNING  [!] Saved X as out_fields\all_vocab_50000_seqlen_512\x_train_full.npy
2023-04-04 11:19:16,733 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-04-04 11:20:07,479 WARNING Finished... Took: 50.75s
2023-04-04 11:20:07,479 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-04-04 11:22:24,859 WARNING Finished... Took: 137.38s
2023-04-04 11:22:24,866 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-04-04 11:23:01,502 WARNING Finished... Took: 36.64s
2023-04-04 11:23:01,503 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-04-04 11:23:05,006 WARNING Finished... Took: 3.50s
2023-04-04 11:23:05,006 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-04-04 11:23:18,645 WARNING Finished... Took: 13.64s
2023-04-04 11:23:18,645 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-04-04 11:26:05,614 WARNING Finished... Took: 166.97s
2023-04-04 11:26:05,614 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-04-04 11:26:28,524 WARNING Finished... Took: 22.91s
2023-04-04 11:26:28,524 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-04-04 11:26:44,679 WARNING Finished... Took: 16.16s
2023-04-04 11:26:44,679 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-04-04 11:26:45,437 WARNING Finished... Took: 0.76s
2023-04-04 11:26:45,442 WARNING  [!] Saved Y as out_fields\all_vocab_50000_seqlen_512\y_test_full.npy
2023-04-04 11:26:45,508 WARNING  [!] Saved Y names as out_fields\all_vocab_50000_seqlen_512\y_names_test_full.json
2023-04-04 11:26:45,563 WARNING  [!] Successfully loaded pre-trained tokenizer model!
2023-04-04 11:26:45,617 WARNING  [!] Loaded vocab with size 50001 from out_fields\all_vocab_50000_seqlen_512\tokenizer_50000_vocab.json
2023-04-04 11:26:45,619 WARNING  [*] Encoding and padding...
2023-04-04 11:28:27,247 WARNING  [!] Saved X as out_fields\all_vocab_50000_seqlen_512\x_test_full.npy
2023-04-04 11:28:28,244 WARNING  [!!!] Starting CV over all!
2023-04-04 11:28:28,399 WARNING  [!] Training time budget: 300min
2023-04-04 11:28:28,399 WARNING  [!] Model config: {'vocab_size': 50001, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-04-04 11:28:28,512 WARNING  [1/3] Train set size: 60730, Validation set size: 30366
2023-04-04 11:28:32,890 WARNING  [!] Saved dataset splits to dataset_splits_1680600508.npz
2023-04-04 11:28:33,256 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3973e6
2023-04-04 11:28:33,257 WARNING  [*] Training time budget set: 5.0 min
2023-04-04 11:28:33,300 WARNING  [*] Started epoch: 1
2023-04-04 11:28:38,337 WARNING  [*] 11:28:38: Train Epoch: 1 [  0  / 633  (0 %)] | Loss: 3.799706 | Elapsed: 5.02s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.5714
2023-04-04 11:28:50,224 WARNING  [*] 11:28:50: Train Epoch: 1 [9600 / 633  (16%)] | Loss: 0.308214 | Elapsed: 11.88s | FPR 0.0003 -> TPR: 0.74 & F1: 0.85 | AUC: 0.9381
2023-04-04 11:29:02,458 WARNING  [*] 11:29:02: Train Epoch: 1 [19200/ 633  (32%)] | Loss: 0.458198 | Elapsed: 12.22s | FPR 0.0003 -> TPR: 0.52 & F1: 0.68 | AUC: 0.8620
2023-04-04 11:29:15,506 WARNING  [*] 11:29:15: Train Epoch: 1 [28800/ 633  (47%)] | Loss: 0.204827 | Elapsed: 13.04s | FPR 0.0003 -> TPR: 0.62 & F1: 0.77 | AUC: 0.9662
2023-04-04 11:29:28,646 WARNING  [*] 11:29:28: Train Epoch: 1 [38400/ 633  (63%)] | Loss: 0.132102 | Elapsed: 13.13s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9857
2023-04-04 11:29:41,792 WARNING  [*] 11:29:41: Train Epoch: 1 [48000/ 633  (79%)] | Loss: 0.185268 | Elapsed: 13.14s | FPR 0.0003 -> TPR: 0.84 & F1: 0.91 | AUC: 0.9792
2023-04-04 11:29:54,976 WARNING  [*] 11:29:54: Train Epoch: 1 [57600/ 633  (95%)] | Loss: 0.141152 | Elapsed: 13.18s | FPR 0.0003 -> TPR: 0.77 & F1: 0.87 | AUC: 0.9852
2023-04-04 11:30:00,787 WARNING  [*] Tue Apr  4 11:30:00 2023:    1    | Tr.loss: 0.284928 | Elapsed:   87.49  s | FPR 0.0003 -> TPR: 0.04 & F1: 0.08 | AUC: 0.9374
2023-04-04 11:30:00,787 WARNING  [*] Started epoch: 2
2023-04-04 11:30:00,938 WARNING  [*] 11:30:00: Train Epoch: 2 [  0  / 633  (0 %)] | Loss: 0.111177 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.92 & F1: 0.96 | AUC: 0.9922
2023-04-04 11:30:14,434 WARNING  [*] 11:30:14: Train Epoch: 2 [9600 / 633  (16%)] | Loss: 0.176256 | Elapsed: 13.49s | FPR 0.0003 -> TPR: 0.69 & F1: 0.82 | AUC: 0.9812
2023-04-04 11:30:27,569 WARNING  [*] 11:30:27: Train Epoch: 2 [19200/ 633  (32%)] | Loss: 0.140865 | Elapsed: 13.13s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9860
2023-04-04 11:30:40,972 WARNING  [*] 11:30:40: Train Epoch: 2 [28800/ 633  (47%)] | Loss: 0.110999 | Elapsed: 13.40s | FPR 0.0003 -> TPR: 0.88 & F1: 0.94 | AUC: 0.9922
2023-04-04 11:30:54,272 WARNING  [*] 11:30:54: Train Epoch: 2 [38400/ 633  (63%)] | Loss: 0.094564 | Elapsed: 13.29s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9942
2023-04-04 11:31:07,458 WARNING  [*] 11:31:07: Train Epoch: 2 [48000/ 633  (79%)] | Loss: 0.057875 | Elapsed: 13.18s | FPR 0.0003 -> TPR: 0.94 & F1: 0.97 | AUC: 0.9959
2023-04-04 11:31:21,025 WARNING  [*] 11:31:21: Train Epoch: 2 [57600/ 633  (95%)] | Loss: 0.057816 | Elapsed: 13.55s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9970
2023-04-04 11:31:27,092 WARNING  [*] Tue Apr  4 11:31:27 2023:    2    | Tr.loss: 0.086880 | Elapsed:   86.31  s | FPR 0.0003 -> TPR: 0.63 & F1: 0.77 | AUC: 0.9942
2023-04-04 11:31:27,093 WARNING  [*] Started epoch: 3
2023-04-04 11:31:27,236 WARNING  [*] 11:31:27: Train Epoch: 3 [  0  / 633  (0 %)] | Loss: 0.052370 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9994
2023-04-04 11:31:40,815 WARNING  [*] 11:31:40: Train Epoch: 3 [9600 / 633  (16%)] | Loss: 0.063178 | Elapsed: 13.57s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9979
2023-04-04 11:31:54,366 WARNING  [*] 11:31:54: Train Epoch: 3 [19200/ 633  (32%)] | Loss: 0.042686 | Elapsed: 13.54s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9987
2023-04-04 11:32:07,972 WARNING  [*] 11:32:07: Train Epoch: 3 [28800/ 633  (47%)] | Loss: 0.005617 | Elapsed: 13.59s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:32:21,856 WARNING  [*] 11:32:21: Train Epoch: 3 [38400/ 633  (63%)] | Loss: 0.097661 | Elapsed: 13.87s | FPR 0.0003 -> TPR: 0.92 & F1: 0.96 | AUC: 0.9921
2023-04-04 11:32:35,893 WARNING  [*] 11:32:35: Train Epoch: 3 [48000/ 633  (79%)] | Loss: 0.008068 | Elapsed: 14.03s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:32:49,666 WARNING  [*] 11:32:49: Train Epoch: 3 [57600/ 633  (95%)] | Loss: 0.008460 | Elapsed: 13.76s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:32:55,572 WARNING  [*] Tue Apr  4 11:32:55 2023:    3    | Tr.loss: 0.044908 | Elapsed:   88.48  s | FPR 0.0003 -> TPR: 0.75 & F1: 0.86 | AUC: 0.9982
2023-04-04 11:32:55,573 WARNING  [*] Started epoch: 4
2023-04-04 11:32:55,732 WARNING  [*] 11:32:55: Train Epoch: 4 [  0  / 633  (0 %)] | Loss: 0.042534 | Elapsed: 0.14s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9992
2023-04-04 11:33:09,408 WARNING  [*] 11:33:09: Train Epoch: 4 [9600 / 633  (16%)] | Loss: 0.014050 | Elapsed: 13.67s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:33:23,134 WARNING  [*] 11:33:23: Train Epoch: 4 [19200/ 633  (32%)] | Loss: 0.005393 | Elapsed: 13.72s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:33:33,373 WARNING  [!] Time budget exceeded, training stopped.
2023-04-04 11:33:33,427 WARNING  [!] Tue Apr  4 11:33:33 2023: Dumped results:
                model       : 1680600508-model.torch
		train time  : 1680600508-trainTime.npy
		train losses: 1680600508-trainLosses.npy
		train AUC   : 1680600508-auc.npy
		train F1s   : 1680600508-trainF1s.npy
		train TPRs  : 1680600508-trainTPRs.npy
2023-04-04 11:33:33,472 WARNING  [!] Evaluating model on training set...
2023-04-04 11:33:57,244 WARNING  [!] This fold metrics on training set:
2023-04-04 11:33:57,254 WARNING 	AUC: 0.9995
2023-04-04 11:33:57,270 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:33:57,287 WARNING 	FPR: 0.0003 | TPR: 0.9311 | F1: 0.9643
2023-04-04 11:33:57,305 WARNING 	FPR: 0.001 | TPR: 0.9636 | F1: 0.9813
2023-04-04 11:33:57,322 WARNING 	FPR: 0.003 | TPR: 0.9862 | F1: 0.9925
2023-04-04 11:33:57,338 WARNING 	FPR: 0.01 | TPR: 0.9946 | F1: 0.9954
2023-04-04 11:33:57,355 WARNING 	FPR: 0.03 | TPR: 0.9974 | F1: 0.9930
2023-04-04 11:33:57,373 WARNING 	FPR: 0.1 | TPR: 0.9996 | F1: 0.9831
2023-04-04 11:33:57,374 WARNING  [!] Evaluating model on validation set...
2023-04-04 11:34:08,755 WARNING  [!] This fold metrics on validation set:
2023-04-04 11:34:08,761 WARNING 	AUC: 0.9980
2023-04-04 11:34:08,769 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:34:08,778 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:34:08,788 WARNING 	FPR: 0.001 | TPR: 0.9073 | F1: 0.9512
2023-04-04 11:34:08,796 WARNING 	FPR: 0.003 | TPR: 0.9628 | F1: 0.9804
2023-04-04 11:34:08,804 WARNING 	FPR: 0.01 | TPR: 0.9821 | F1: 0.9890
2023-04-04 11:34:08,814 WARNING 	FPR: 0.03 | TPR: 0.9888 | F1: 0.9886
2023-04-04 11:34:08,826 WARNING 	FPR: 0.1 | TPR: 0.9949 | F1: 0.9786
2023-04-04 11:34:09,057 WARNING  [2/3] Train set size: 60731, Validation set size: 30365
2023-04-04 11:34:13,656 WARNING  [!] Saved dataset splits to dataset_splits_1680600848.npz
2023-04-04 11:34:13,760 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3973e6
2023-04-04 11:34:13,761 WARNING  [*] Training time budget set: 5.0 min
2023-04-04 11:34:13,809 WARNING  [*] Started epoch: 1
2023-04-04 11:34:14,113 WARNING  [*] 11:34:14: Train Epoch: 1 [  0  / 633  (0 %)] | Loss: 3.268748 | Elapsed: 0.29s | FPR 0.0003 -> TPR: 0.01 & F1: 0.03 | AUC: 0.4107
2023-04-04 11:34:27,646 WARNING  [*] 11:34:27: Train Epoch: 1 [9600 / 633  (16%)] | Loss: 0.372357 | Elapsed: 13.52s | FPR 0.0003 -> TPR: 0.47 & F1: 0.64 | AUC: 0.8828
2023-04-04 11:34:41,378 WARNING  [*] 11:34:41: Train Epoch: 1 [19200/ 633  (32%)] | Loss: 0.267882 | Elapsed: 13.72s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9371
2023-04-04 11:34:55,203 WARNING  [*] 11:34:55: Train Epoch: 1 [28800/ 633  (47%)] | Loss: 0.199940 | Elapsed: 13.82s | FPR 0.0003 -> TPR: 0.75 & F1: 0.86 | AUC: 0.9746
2023-04-04 11:35:09,010 WARNING  [*] 11:35:09: Train Epoch: 1 [38400/ 633  (63%)] | Loss: 0.325445 | Elapsed: 13.80s | FPR 0.0003 -> TPR: 0.41 & F1: 0.58 | AUC: 0.9323
2023-04-04 11:35:22,836 WARNING  [*] 11:35:22: Train Epoch: 1 [48000/ 633  (79%)] | Loss: 0.137176 | Elapsed: 13.82s | FPR 0.0003 -> TPR: 0.86 & F1: 0.92 | AUC: 0.9869
2023-04-04 11:35:36,706 WARNING  [*] 11:35:36: Train Epoch: 1 [57600/ 633  (95%)] | Loss: 0.118032 | Elapsed: 13.86s | FPR 0.0003 -> TPR: 0.84 & F1: 0.91 | AUC: 0.9936
2023-04-04 11:35:42,735 WARNING  [*] Tue Apr  4 11:35:42 2023:    1    | Tr.loss: 0.298989 | Elapsed:   88.92  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9333
2023-04-04 11:35:42,735 WARNING  [*] Started epoch: 2
2023-04-04 11:35:42,901 WARNING  [*] 11:35:42: Train Epoch: 2 [  0  / 633  (0 %)] | Loss: 0.175410 | Elapsed: 0.15s | FPR 0.0003 -> TPR: 0.83 & F1: 0.91 | AUC: 0.9824
2023-04-04 11:35:56,917 WARNING  [*] 11:35:56: Train Epoch: 2 [9600 / 633  (16%)] | Loss: 0.051426 | Elapsed: 14.01s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:36:11,050 WARNING  [*] 11:36:11: Train Epoch: 2 [19200/ 633  (32%)] | Loss: 0.056267 | Elapsed: 14.12s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9986
2023-04-04 11:36:25,023 WARNING  [*] 11:36:25: Train Epoch: 2 [28800/ 633  (47%)] | Loss: 0.035280 | Elapsed: 13.96s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:36:39,306 WARNING  [*] 11:36:39: Train Epoch: 2 [38400/ 633  (63%)] | Loss: 0.056596 | Elapsed: 14.28s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9981
2023-04-04 11:36:53,294 WARNING  [*] 11:36:53: Train Epoch: 2 [48000/ 633  (79%)] | Loss: 0.023230 | Elapsed: 13.98s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:37:07,395 WARNING  [*] 11:37:07: Train Epoch: 2 [57600/ 633  (95%)] | Loss: 0.027444 | Elapsed: 14.09s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:37:13,551 WARNING  [*] Tue Apr  4 11:37:13 2023:    2    | Tr.loss: 0.082923 | Elapsed:   90.82  s | FPR 0.0003 -> TPR: 0.59 & F1: 0.74 | AUC: 0.9946
2023-04-04 11:37:13,551 WARNING  [*] Started epoch: 3
2023-04-04 11:37:13,702 WARNING  [*] 11:37:13: Train Epoch: 3 [  0  / 633  (0 %)] | Loss: 0.044968 | Elapsed: 0.14s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:37:27,445 WARNING  [*] 11:37:27: Train Epoch: 3 [9600 / 633  (16%)] | Loss: 0.041880 | Elapsed: 13.73s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9996
2023-04-04 11:37:41,497 WARNING  [*] 11:37:41: Train Epoch: 3 [19200/ 633  (32%)] | Loss: 0.050571 | Elapsed: 14.04s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9980
2023-04-04 11:37:55,455 WARNING  [*] 11:37:55: Train Epoch: 3 [28800/ 633  (47%)] | Loss: 0.012020 | Elapsed: 13.95s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:38:09,150 WARNING  [*] 11:38:09: Train Epoch: 3 [38400/ 633  (63%)] | Loss: 0.029510 | Elapsed: 13.69s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9989
2023-04-04 11:38:23,436 WARNING  [*] 11:38:23: Train Epoch: 3 [48000/ 633  (79%)] | Loss: 0.017004 | Elapsed: 14.28s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:38:37,398 WARNING  [*] 11:38:37: Train Epoch: 3 [57600/ 633  (95%)] | Loss: 0.049463 | Elapsed: 13.95s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9967
2023-04-04 11:38:43,446 WARNING  [*] Tue Apr  4 11:38:43 2023:    3    | Tr.loss: 0.044575 | Elapsed:   89.90  s | FPR 0.0003 -> TPR: 0.79 & F1: 0.88 | AUC: 0.9983
2023-04-04 11:38:43,446 WARNING  [*] Started epoch: 4
2023-04-04 11:38:43,597 WARNING  [*] 11:38:43: Train Epoch: 4 [  0  / 633  (0 %)] | Loss: 0.053650 | Elapsed: 0.14s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9994
2023-04-04 11:38:57,402 WARNING  [*] 11:38:57: Train Epoch: 4 [9600 / 633  (16%)] | Loss: 0.003436 | Elapsed: 13.79s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:39:11,391 WARNING  [*] 11:39:11: Train Epoch: 4 [19200/ 633  (32%)] | Loss: 0.010816 | Elapsed: 13.98s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:39:13,794 WARNING  [!] Time budget exceeded, training stopped.
2023-04-04 11:39:13,838 WARNING  [!] Tue Apr  4 11:39:13 2023: Dumped results:
                model       : 1680600848-model.torch
		train time  : 1680600848-trainTime.npy
		train losses: 1680600848-trainLosses.npy
		train AUC   : 1680600848-auc.npy
		train F1s   : 1680600848-trainF1s.npy
		train TPRs  : 1680600848-trainTPRs.npy
2023-04-04 11:39:13,876 WARNING  [!] Evaluating model on training set...
2023-04-04 11:39:37,889 WARNING  [!] This fold metrics on training set:
2023-04-04 11:39:37,899 WARNING 	AUC: 0.9994
2023-04-04 11:39:37,915 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:39:37,932 WARNING 	FPR: 0.0003 | TPR: 0.9098 | F1: 0.9527
2023-04-04 11:39:37,951 WARNING 	FPR: 0.001 | TPR: 0.9662 | F1: 0.9826
2023-04-04 11:39:37,967 WARNING 	FPR: 0.003 | TPR: 0.9822 | F1: 0.9904
2023-04-04 11:39:37,985 WARNING 	FPR: 0.01 | TPR: 0.9916 | F1: 0.9938
2023-04-04 11:39:38,006 WARNING 	FPR: 0.03 | TPR: 0.9968 | F1: 0.9926
2023-04-04 11:39:38,029 WARNING 	FPR: 0.1 | TPR: 0.9993 | F1: 0.9810
2023-04-04 11:39:38,029 WARNING  [!] Evaluating model on validation set...
2023-04-04 11:39:49,931 WARNING  [!] This fold metrics on validation set:
2023-04-04 11:39:49,937 WARNING 	AUC: 0.9981
2023-04-04 11:39:49,946 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:39:49,953 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:39:49,964 WARNING 	FPR: 0.001 | TPR: 0.9280 | F1: 0.9625
2023-04-04 11:39:49,974 WARNING 	FPR: 0.003 | TPR: 0.9585 | F1: 0.9782
2023-04-04 11:39:49,982 WARNING 	FPR: 0.01 | TPR: 0.9817 | F1: 0.9889
2023-04-04 11:39:49,989 WARNING 	FPR: 0.03 | TPR: 0.9890 | F1: 0.9888
2023-04-04 11:39:49,999 WARNING 	FPR: 0.1 | TPR: 0.9964 | F1: 0.9795
2023-04-04 11:39:50,210 WARNING  [3/3] Train set size: 60731, Validation set size: 30365
2023-04-04 11:39:54,532 WARNING  [!] Saved dataset splits to dataset_splits_1680601190.npz
2023-04-04 11:39:54,657 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3973e6
2023-04-04 11:39:54,657 WARNING  [*] Training time budget set: 5.0 min
2023-04-04 11:39:54,708 WARNING  [*] Started epoch: 1
2023-04-04 11:39:54,955 WARNING  [*] 11:39:54: Train Epoch: 1 [  0  / 633  (0 %)] | Loss: 4.888810 | Elapsed: 0.24s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.4670
2023-04-04 11:40:08,226 WARNING  [*] 11:40:08: Train Epoch: 1 [9600 / 633  (16%)] | Loss: 0.467353 | Elapsed: 13.26s | FPR 0.0003 -> TPR: 0.33 & F1: 0.49 | AUC: 0.8802
2023-04-04 11:40:21,950 WARNING  [*] 11:40:21: Train Epoch: 1 [19200/ 633  (32%)] | Loss: 0.278114 | Elapsed: 13.71s | FPR 0.0003 -> TPR: 0.59 & F1: 0.75 | AUC: 0.9416
2023-04-04 11:40:36,227 WARNING  [*] 11:40:36: Train Epoch: 1 [28800/ 633  (47%)] | Loss: 0.251364 | Elapsed: 14.27s | FPR 0.0003 -> TPR: 0.35 & F1: 0.52 | AUC: 0.9554
2023-04-04 11:40:50,220 WARNING  [*] 11:40:50: Train Epoch: 1 [38400/ 633  (63%)] | Loss: 0.257835 | Elapsed: 13.99s | FPR 0.0003 -> TPR: 0.72 & F1: 0.84 | AUC: 0.9523
2023-04-04 11:41:04,161 WARNING  [*] 11:41:04: Train Epoch: 1 [48000/ 633  (79%)] | Loss: 0.146273 | Elapsed: 13.93s | FPR 0.0003 -> TPR: 0.83 & F1: 0.91 | AUC: 0.9871
2023-04-04 11:41:18,712 WARNING  [*] 11:41:18: Train Epoch: 1 [57600/ 633  (95%)] | Loss: 0.129829 | Elapsed: 14.54s | FPR 0.0003 -> TPR: 0.82 & F1: 0.90 | AUC: 0.9903
2023-04-04 11:41:25,225 WARNING  [*] Tue Apr  4 11:41:25 2023:    1    | Tr.loss: 0.295323 | Elapsed:   90.52  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.9369
2023-04-04 11:41:25,226 WARNING  [*] Started epoch: 2
2023-04-04 11:41:25,405 WARNING  [*] 11:41:25: Train Epoch: 2 [  0  / 633  (0 %)] | Loss: 0.176205 | Elapsed: 0.17s | FPR 0.0003 -> TPR: 0.84 & F1: 0.91 | AUC: 0.9877
2023-04-04 11:41:39,928 WARNING  [*] 11:41:39: Train Epoch: 2 [9600 / 633  (16%)] | Loss: 0.069136 | Elapsed: 14.51s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9971
2023-04-04 11:41:54,500 WARNING  [*] 11:41:54: Train Epoch: 2 [19200/ 633  (32%)] | Loss: 0.161589 | Elapsed: 14.56s | FPR 0.0003 -> TPR: 0.91 & F1: 0.95 | AUC: 0.9943
2023-04-04 11:42:08,953 WARNING  [*] 11:42:08: Train Epoch: 2 [28800/ 633  (47%)] | Loss: 0.091165 | Elapsed: 14.44s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9961
2023-04-04 11:42:23,054 WARNING  [*] 11:42:23: Train Epoch: 2 [38400/ 633  (63%)] | Loss: 0.111343 | Elapsed: 14.09s | FPR 0.0003 -> TPR: 0.96 & F1: 0.98 | AUC: 0.9984
2023-04-04 11:42:37,524 WARNING  [*] 11:42:37: Train Epoch: 2 [48000/ 633  (79%)] | Loss: 0.046966 | Elapsed: 14.46s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-04 11:42:51,548 WARNING  [*] 11:42:51: Train Epoch: 2 [57600/ 633  (95%)] | Loss: 0.037200 | Elapsed: 14.02s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-04 11:42:57,786 WARNING  [*] Tue Apr  4 11:42:57 2023:    2    | Tr.loss: 0.078836 | Elapsed:   92.56  s | FPR 0.0003 -> TPR: 0.53 & F1: 0.69 | AUC: 0.9952
2023-04-04 11:42:57,787 WARNING  [*] Started epoch: 3
2023-04-04 11:42:57,932 WARNING  [*] 11:42:57: Train Epoch: 3 [  0  / 633  (0 %)] | Loss: 0.045822 | Elapsed: 0.13s | FPR 0.0003 -> TPR: 0.98 & F1: 0.99 | AUC: 0.9985
2023-04-04 11:43:12,339 WARNING  [*] 11:43:12: Train Epoch: 3 [9600 / 633  (16%)] | Loss: 0.053284 | Elapsed: 14.40s | FPR 0.0003 -> TPR: 0.97 & F1: 0.99 | AUC: 0.9990
2023-04-04 11:43:26,561 WARNING  [*] 11:43:26: Train Epoch: 3 [19200/ 633  (32%)] | Loss: 0.042182 | Elapsed: 14.21s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-04 11:43:41,102 WARNING  [*] 11:43:41: Train Epoch: 3 [28800/ 633  (47%)] | Loss: 0.023783 | Elapsed: 14.53s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:43:55,612 WARNING  [*] 11:43:55: Train Epoch: 3 [38400/ 633  (63%)] | Loss: 0.013205 | Elapsed: 14.50s | FPR 0.0003 -> TPR: 1.00 & F1: 1.00 | AUC: 1.0000
2023-04-04 11:44:10,180 WARNING  [*] 11:44:10: Train Epoch: 3 [48000/ 633  (79%)] | Loss: 0.114938 | Elapsed: 14.56s | FPR 0.0003 -> TPR: 0.78 & F1: 0.88 | AUC: 0.9931
2023-04-04 11:44:24,738 WARNING  [*] 11:44:24: Train Epoch: 3 [57600/ 633  (95%)] | Loss: 0.035826 | Elapsed: 14.55s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9984
2023-04-04 11:44:31,144 WARNING  [*] Tue Apr  4 11:44:31 2023:    3    | Tr.loss: 0.044196 | Elapsed:   93.36  s | FPR 0.0003 -> TPR: 0.67 & F1: 0.80 | AUC: 0.9982
2023-04-04 11:44:31,145 WARNING  [*] Started epoch: 4
2023-04-04 11:44:31,294 WARNING  [*] 11:44:31: Train Epoch: 4 [  0  / 633  (0 %)] | Loss: 0.057158 | Elapsed: 0.14s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9995
2023-04-04 11:44:45,930 WARNING  [*] 11:44:45: Train Epoch: 4 [9600 / 633  (16%)] | Loss: 0.056109 | Elapsed: 14.63s | FPR 0.0003 -> TPR: 0.99 & F1: 0.99 | AUC: 0.9971
2023-04-04 11:44:54,697 WARNING  [!] Time budget exceeded, training stopped.
2023-04-04 11:44:54,758 WARNING  [!] Tue Apr  4 11:44:54 2023: Dumped results:
                model       : 1680601190-model.torch
		train time  : 1680601190-trainTime.npy
		train losses: 1680601190-trainLosses.npy
		train AUC   : 1680601190-auc.npy
		train F1s   : 1680601190-trainF1s.npy
		train TPRs  : 1680601190-trainTPRs.npy
2023-04-04 11:44:54,794 WARNING  [!] Evaluating model on training set...
2023-04-04 11:45:19,001 WARNING  [!] This fold metrics on training set:
2023-04-04 11:45:19,011 WARNING 	AUC: 0.9996
2023-04-04 11:45:19,028 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:45:19,045 WARNING 	FPR: 0.0003 | TPR: 0.9142 | F1: 0.9551
2023-04-04 11:45:19,061 WARNING 	FPR: 0.001 | TPR: 0.9720 | F1: 0.9856
2023-04-04 11:45:19,080 WARNING 	FPR: 0.003 | TPR: 0.9877 | F1: 0.9932
2023-04-04 11:45:19,100 WARNING 	FPR: 0.01 | TPR: 0.9943 | F1: 0.9952
2023-04-04 11:45:19,112 WARNING 	FPR: 0.03 | TPR: 0.9979 | F1: 0.9932
2023-04-04 11:45:19,134 WARNING 	FPR: 0.1 | TPR: 0.9995 | F1: 0.9816
2023-04-04 11:45:19,134 WARNING  [!] Evaluating model on validation set...
2023-04-04 11:45:31,336 WARNING  [!] This fold metrics on validation set:
2023-04-04 11:45:31,342 WARNING 	AUC: 0.9977
2023-04-04 11:45:31,350 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:45:31,358 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:45:31,364 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-04-04 11:45:31,375 WARNING 	FPR: 0.003 | TPR: 0.9546 | F1: 0.9762
2023-04-04 11:45:31,384 WARNING 	FPR: 0.01 | TPR: 0.9806 | F1: 0.9883
2023-04-04 11:45:31,393 WARNING 	FPR: 0.03 | TPR: 0.9889 | F1: 0.9887
2023-04-04 11:45:31,402 WARNING 	FPR: 0.1 | TPR: 0.9962 | F1: 0.9789
2023-04-04 11:45:31,514 WARNING  [!] Metrics saved to out_fields\cv_all_limNone_r1763_t5\all_metrics_validation.json
2023-04-04 11:45:31,516 WARNING  [!] Metrics saved to out_fields\cv_all_limNone_r1763_t5\all_metrics_training.json
2023-04-04 11:45:31,516 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9979
	FPR: 0.0001 -- TPR: 0.0000 -- F1: 0.0000
	FPR: 0.0003 -- TPR: 0.0000 -- F1: 0.0000
	FPR:  0.001 -- TPR: 0.6118 -- F1: 0.6379
	FPR:  0.003 -- TPR: 0.9586 -- F1: 0.9783
	FPR:   0.01 -- TPR: 0.9815 -- F1: 0.9887
	FPR:   0.03 -- TPR: 0.9889 -- F1: 0.9887
	FPR:    0.1 -- TPR: 0.9958 -- F1: 0.9790

