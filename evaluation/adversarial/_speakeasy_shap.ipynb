{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from nebula.models.attention import TransformerEncoderModel, TransformerEncoderChunks\n",
    "from nebula import PEDynamicFeatureExtractor\n",
    "from nebula.preprocessing import JSONTokenizerNaive, JSONTokenizerBPE\n",
    "from nebula.misc import get_path, clear_cuda_cache, set_random_seed\n",
    "from nebula.constants import *\n",
    "from bertviz import model_view, head_view\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "SCRIPT_PATH = get_path(type=\"notebook\")\n",
    "ROOT = os.path.join(SCRIPT_PATH, \"..\", \"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root: [!] Successfully loaded pre-trained tokenizer model!\n",
      "WARNING:root: [!] Loaded vocab with size 50001 from C:\\Users\\dtrizna\\Code\\nebula\\evaluation\\explainability\\..\\..\\evaluation\\paper_sota\\out_speakeasy\\nebula_speakeasy_vocab_50000_seqlen_512\\tokenizer_50000_vocab.json\n"
     ]
    }
   ],
   "source": [
    "# done three folds on that run\n",
    "MODEL_IDX = 1\n",
    "\n",
    "# BPE\n",
    "datafolder = os.path.join(ROOT, r\"evaluation\\paper_sota\\out_speakeasy\\nebula_speakeasy_vocab_50000_seqlen_512\")\n",
    "model_folder = os.path.join(ROOT, r\"evaluation\\paper_sota\\out_speakeasy\\cv_nebula_limNone_r1763_t20\\training_files\")\n",
    "\n",
    "# Whitespace\n",
    "# datafolder = os.path.join(ROOT, r\"evaluation\\paper_ablation\\out_tokenizer\\nebula_whitespace_vocab_50000_seqlen_512\")\n",
    "# model_folder = os.path.join(ROOT, r\"evaluation\\paper_ablation\\out_tokenizer\\cv_whitespace_limNone_r1763_t5\\training_files\")\n",
    "\n",
    "# LOADING OBJECTS\n",
    "model_file = [x for x in os.listdir(model_folder) if x.endswith(\".torch\")][MODEL_IDX]\n",
    "model_file_fullpath = os.path.join(model_folder, model_file)\n",
    "state_dict = torch.load(model_file_fullpath)\n",
    "\n",
    "with open(os.path.join(datafolder, f\"tokenizer_50000_vocab.json\")) as f:\n",
    "    nebula_vocab = json.load(f)\n",
    "\n",
    "# BPE\n",
    "tokenizer = JSONTokenizerBPE(\n",
    "    vocab_size=len(nebula_vocab),\n",
    "    seq_len=512,\n",
    "    model_path=os.path.join(ROOT, datafolder, r\"tokenizer_50000.model\")\n",
    ")\n",
    "\n",
    "# Whitespace\n",
    "# tokenizer = JSONTokenizerNaive(\n",
    "#     vocab_size=len(nebula_vocab),\n",
    "#     seq_len=512,\n",
    "#     vocab=nebula_vocab\n",
    "# )\n",
    "\n",
    "model_config = {\n",
    "        \"vocab_size\": len(nebula_vocab),\n",
    "        \"maxlen\": 512,\n",
    "        \"chunk_size\": 64,\n",
    "        \"dModel\": 64,  # embedding & transformer dimension\n",
    "        \"nHeads\": 8,  # number of heads in nn.MultiheadAttention\n",
    "        \"dHidden\": 256,  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        \"nLayers\": 2,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        \"numClasses\": 1, # binary classification\n",
    "        \"hiddenNeurons\": [64],\n",
    "        \"layerNorm\": False,\n",
    "        \"dropout\": 0.3,\n",
    "        \"mean_over_sequence\": False,\n",
    "        \"norm_first\": True\n",
    "    }\n",
    "model = TransformerEncoderChunks(**model_config)\n",
    "model.load_state_dict(state_dict)\n",
    "_ = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "import shap\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "# build two models -- one for getting embed, other for SHAP (w/o embeddings)\n",
    "model = TransformerEncoderChunks(**model_config).to(DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model_no_embed = deepcopy(model)\n",
    "model_no_embed.encoder = Identity()\n",
    "\n",
    "\n",
    "# load sample\n",
    "report_root = os.path.join(ROOT, r\"data\\data_raw\\windows_emulation_trainset\\report_backdoor\")\n",
    "exampleFile = os.path.join(report_root, \"0009064322cdc719a82317553b805cbbc64230a9212d3b8aad1ea1b78d3bf10a.json\")\n",
    "with open(exampleFile) as f:\n",
    "    exampleFile = json.load(f)\n",
    "\n",
    "extractor = PEDynamicFeatureExtractor()\n",
    "exampleProcessed = extractor.filter_and_normalize_report(exampleFile)\n",
    "exampleTokenized = tokenizer.tokenize(exampleProcessed)\n",
    "exampleEncoded = tokenizer.encode(exampleProcessed)\n",
    "x = torch.Tensor(exampleEncoded).long().reshape(1,-1).to(DEVICE)\n",
    "x = x[:, :512]\n",
    "x_embed = model.encoder(x.long()).float()\n",
    "\n",
    "def analyze_feature_importance_shap(model, x):\n",
    "    explainer = shap.DeepExplainer(model, x)\n",
    "    shap_values = explainer.shap_values(x)\n",
    "    return shap_values\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_shap(shap_values, x):\n",
    "    # plot\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.bar(range(len(shap_values)), shap_values)\n",
    "\n",
    "shap_values = analyze_feature_importance_shap(model_no_embed, x_embed.requires_grad_())\n",
    "\n",
    "plot_shap(shap_values[0].mean(axis=1), x_embed.numpy()[0].mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = shap_values[0].mean(axis=1)\n",
    "# return elements in a that are not 0\n",
    "a[a != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
