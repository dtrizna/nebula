01/24/2023 06:19:31 PM  [!] Starting Masked Language Model evaluation over 2 splits!
01/24/2023 06:19:31 PM  [!] Loaded data and vocab. X train size: (76126, 512), X test size: (17407, 512), vocab size: 50002
01/24/2023 06:19:31 PM  [!] Initiating Self-Supervised Learning Pretraining
     Language Modeling MaskedLanguageModel
     Model TransformerEncoderLM with config:
	{'vocabSize': 50002, 'maxLen': 512, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'dropout': 0.3}

01/24/2023 06:19:31 PM  [!] Running pre-training split 1/2
01/24/2023 06:19:31 PM  [!] Pre-training model...
01/24/2023 06:19:32 PM  [*] Masking sequences...
01/24/2023 06:19:49 PM  [*] Started epoch: 1
01/24/2023 06:19:51 PM  [*] Tue Jan 24 18:19:51 2023: Train Epoch: 1 [  0  /60900 (0 %)]	Loss: 469.091492 | Not computing TPR and F1 | Elapsed: 2.28s
01/24/2023 06:20:03 PM  [*] Tue Jan 24 18:20:03 2023: Train Epoch: 1 [6400 /60900 (11%)]	Loss: 240.311493 | Not computing TPR and F1 | Elapsed: 11.73s
01/24/2023 06:20:15 PM  [*] Tue Jan 24 18:20:15 2023: Train Epoch: 1 [12800/60900 (21%)]	Loss: 208.109116 | Not computing TPR and F1 | Elapsed: 11.83s
01/24/2023 06:20:26 PM  [*] Tue Jan 24 18:20:26 2023: Train Epoch: 1 [19200/60900 (32%)]	Loss: 216.385132 | Not computing TPR and F1 | Elapsed: 11.85s
01/24/2023 06:20:38 PM  [*] Tue Jan 24 18:20:38 2023: Train Epoch: 1 [25600/60900 (42%)]	Loss: 216.906372 | Not computing TPR and F1 | Elapsed: 12.00s
01/24/2023 06:20:50 PM  [*] Tue Jan 24 18:20:50 2023: Train Epoch: 1 [32000/60900 (53%)]	Loss: 227.256714 | Not computing TPR and F1 | Elapsed: 12.07s
01/24/2023 06:21:03 PM  [*] Tue Jan 24 18:21:03 2023: Train Epoch: 1 [38400/60900 (63%)]	Loss: 188.191376 | Not computing TPR and F1 | Elapsed: 12.09s
01/24/2023 06:21:15 PM  [*] Tue Jan 24 18:21:15 2023: Train Epoch: 1 [44800/60900 (74%)]	Loss: 211.565796 | Not computing TPR and F1 | Elapsed: 12.18s
01/24/2023 06:21:27 PM  [*] Tue Jan 24 18:21:27 2023: Train Epoch: 1 [51200/60900 (84%)]	Loss: 179.027771 | Not computing TPR and F1 | Elapsed: 12.36s
01/24/2023 06:21:39 PM  [*] Tue Jan 24 18:21:39 2023: Train Epoch: 1 [57600/60900 (95%)]	Loss: 192.293289 | Not computing TPR and F1 | Elapsed: 12.38s
01/24/2023 06:21:46 PM  [*] Tue Jan 24 18:21:46 2023:    1    | Tr.loss: 208.109996 | Not computing TPR and F1 | Elapsed:  117.04  s
01/24/2023 06:21:46 PM  [*] Started epoch: 2
01/24/2023 06:21:46 PM  [*] Tue Jan 24 18:21:46 2023: Train Epoch: 2 [  0  /60900 (0 %)]	Loss: 195.270142 | Not computing TPR and F1 | Elapsed: 0.14s
01/24/2023 06:21:58 PM  [*] Tue Jan 24 18:21:58 2023: Train Epoch: 2 [6400 /60900 (11%)]	Loss: 189.820648 | Not computing TPR and F1 | Elapsed: 12.30s
01/24/2023 06:22:11 PM  [*] Tue Jan 24 18:22:11 2023: Train Epoch: 2 [12800/60900 (21%)]	Loss: 187.030502 | Not computing TPR and F1 | Elapsed: 12.39s
01/24/2023 06:22:23 PM  [*] Tue Jan 24 18:22:23 2023: Train Epoch: 2 [19200/60900 (32%)]	Loss: 196.225449 | Not computing TPR and F1 | Elapsed: 12.40s
01/24/2023 06:22:35 PM  [*] Tue Jan 24 18:22:35 2023: Train Epoch: 2 [25600/60900 (42%)]	Loss: 181.772034 | Not computing TPR and F1 | Elapsed: 12.38s
01/24/2023 06:22:48 PM  [*] Tue Jan 24 18:22:48 2023: Train Epoch: 2 [32000/60900 (53%)]	Loss: 211.198059 | Not computing TPR and F1 | Elapsed: 12.43s
01/24/2023 06:23:00 PM  [*] Tue Jan 24 18:23:00 2023: Train Epoch: 2 [38400/60900 (63%)]	Loss: 197.110931 | Not computing TPR and F1 | Elapsed: 12.41s
01/24/2023 06:23:13 PM  [*] Tue Jan 24 18:23:13 2023: Train Epoch: 2 [44800/60900 (74%)]	Loss: 190.048462 | Not computing TPR and F1 | Elapsed: 12.44s
01/24/2023 06:23:25 PM  [*] Tue Jan 24 18:23:25 2023: Train Epoch: 2 [51200/60900 (84%)]	Loss: 187.196930 | Not computing TPR and F1 | Elapsed: 12.55s
01/24/2023 06:23:38 PM  [*] Tue Jan 24 18:23:38 2023: Train Epoch: 2 [57600/60900 (95%)]	Loss: 195.892273 | Not computing TPR and F1 | Elapsed: 12.41s
01/24/2023 06:23:44 PM  [*] Tue Jan 24 18:23:44 2023:    2    | Tr.loss: 187.372663 | Not computing TPR and F1 | Elapsed:  118.23  s
01/24/2023 06:23:44 PM  [*] Started epoch: 3
01/24/2023 06:23:44 PM  [*] Tue Jan 24 18:23:44 2023: Train Epoch: 3 [  0  /60900 (0 %)]	Loss: 186.276321 | Not computing TPR and F1 | Elapsed: 0.13s
01/24/2023 06:23:57 PM  [*] Tue Jan 24 18:23:57 2023: Train Epoch: 3 [6400 /60900 (11%)]	Loss: 173.417297 | Not computing TPR and F1 | Elapsed: 12.44s
01/24/2023 06:24:09 PM  [*] Tue Jan 24 18:24:09 2023: Train Epoch: 3 [12800/60900 (21%)]	Loss: 171.723404 | Not computing TPR and F1 | Elapsed: 12.40s
01/24/2023 06:24:21 PM  [*] Tue Jan 24 18:24:21 2023: Train Epoch: 3 [19200/60900 (32%)]	Loss: 161.070694 | Not computing TPR and F1 | Elapsed: 12.57s
01/24/2023 06:24:34 PM  [*] Tue Jan 24 18:24:34 2023: Train Epoch: 3 [25600/60900 (42%)]	Loss: 182.869537 | Not computing TPR and F1 | Elapsed: 12.27s
01/24/2023 06:24:46 PM  [*] Tue Jan 24 18:24:46 2023: Train Epoch: 3 [32000/60900 (53%)]	Loss: 191.009338 | Not computing TPR and F1 | Elapsed: 12.33s
01/24/2023 06:24:59 PM  [*] Tue Jan 24 18:24:59 2023: Train Epoch: 3 [38400/60900 (63%)]	Loss: 192.998734 | Not computing TPR and F1 | Elapsed: 12.41s
01/24/2023 06:25:11 PM  [*] Tue Jan 24 18:25:11 2023: Train Epoch: 3 [44800/60900 (74%)]	Loss: 179.786407 | Not computing TPR and F1 | Elapsed: 12.43s
01/24/2023 06:25:23 PM  [*] Tue Jan 24 18:25:23 2023: Train Epoch: 3 [51200/60900 (84%)]	Loss: 198.028290 | Not computing TPR and F1 | Elapsed: 12.47s
01/24/2023 06:25:36 PM  [*] Tue Jan 24 18:25:36 2023: Train Epoch: 3 [57600/60900 (95%)]	Loss: 147.659546 | Not computing TPR and F1 | Elapsed: 12.40s
01/24/2023 06:25:42 PM  [*] Tue Jan 24 18:25:42 2023:    3    | Tr.loss: 182.375064 | Not computing TPR and F1 | Elapsed:  118.16  s
01/24/2023 06:25:42 PM  [*] Started epoch: 4
01/24/2023 06:25:42 PM  [*] Tue Jan 24 18:25:42 2023: Train Epoch: 4 [  0  /60900 (0 %)]	Loss: 184.838593 | Not computing TPR and F1 | Elapsed: 0.14s
01/24/2023 06:25:55 PM  [*] Tue Jan 24 18:25:55 2023: Train Epoch: 4 [6400 /60900 (11%)]	Loss: 195.980347 | Not computing TPR and F1 | Elapsed: 12.43s
01/24/2023 06:26:07 PM  [*] Tue Jan 24 18:26:07 2023: Train Epoch: 4 [12800/60900 (21%)]	Loss: 177.724396 | Not computing TPR and F1 | Elapsed: 12.43s
01/24/2023 06:26:20 PM  [*] Tue Jan 24 18:26:20 2023: Train Epoch: 4 [19200/60900 (32%)]	Loss: 189.582031 | Not computing TPR and F1 | Elapsed: 12.45s
01/24/2023 06:26:32 PM  [*] Tue Jan 24 18:26:32 2023: Train Epoch: 4 [25600/60900 (42%)]	Loss: 179.756805 | Not computing TPR and F1 | Elapsed: 12.49s
01/24/2023 06:26:45 PM  [*] Tue Jan 24 18:26:45 2023: Train Epoch: 4 [32000/60900 (53%)]	Loss: 178.603119 | Not computing TPR and F1 | Elapsed: 12.51s
01/24/2023 06:26:57 PM  [*] Tue Jan 24 18:26:57 2023: Train Epoch: 4 [38400/60900 (63%)]	Loss: 173.041931 | Not computing TPR and F1 | Elapsed: 12.43s
01/24/2023 06:27:09 PM  [*] Tue Jan 24 18:27:09 2023: Train Epoch: 4 [44800/60900 (74%)]	Loss: 168.397217 | Not computing TPR and F1 | Elapsed: 12.49s
01/24/2023 06:27:22 PM  [*] Tue Jan 24 18:27:22 2023: Train Epoch: 4 [51200/60900 (84%)]	Loss: 162.836578 | Not computing TPR and F1 | Elapsed: 12.48s
01/24/2023 06:27:34 PM  [*] Tue Jan 24 18:27:34 2023: Train Epoch: 4 [57600/60900 (95%)]	Loss: 199.898651 | Not computing TPR and F1 | Elapsed: 12.47s
01/24/2023 06:27:41 PM  [*] Tue Jan 24 18:27:41 2023:    4    | Tr.loss: 179.775187 | Not computing TPR and F1 | Elapsed:  118.71  s
01/24/2023 06:27:41 PM  [*] Started epoch: 5
01/24/2023 06:27:41 PM  [*] Tue Jan 24 18:27:41 2023: Train Epoch: 5 [  0  /60900 (0 %)]	Loss: 180.514435 | Not computing TPR and F1 | Elapsed: 0.13s
01/24/2023 06:27:53 PM  [*] Tue Jan 24 18:27:53 2023: Train Epoch: 5 [6400 /60900 (11%)]	Loss: 198.891266 | Not computing TPR and F1 | Elapsed: 12.44s
01/24/2023 06:28:06 PM  [*] Tue Jan 24 18:28:06 2023: Train Epoch: 5 [12800/60900 (21%)]	Loss: 160.997528 | Not computing TPR and F1 | Elapsed: 12.54s
01/24/2023 06:28:18 PM  [*] Tue Jan 24 18:28:18 2023: Train Epoch: 5 [19200/60900 (32%)]	Loss: 176.177505 | Not computing TPR and F1 | Elapsed: 12.52s
01/24/2023 06:28:31 PM  [*] Tue Jan 24 18:28:31 2023: Train Epoch: 5 [25600/60900 (42%)]	Loss: 175.327393 | Not computing TPR and F1 | Elapsed: 12.36s
01/24/2023 06:28:43 PM  [*] Tue Jan 24 18:28:43 2023: Train Epoch: 5 [32000/60900 (53%)]	Loss: 197.674377 | Not computing TPR and F1 | Elapsed: 12.56s
01/24/2023 06:28:56 PM  [*] Tue Jan 24 18:28:56 2023: Train Epoch: 5 [38400/60900 (63%)]	Loss: 191.540573 | Not computing TPR and F1 | Elapsed: 12.55s
01/24/2023 06:29:08 PM  [*] Tue Jan 24 18:29:08 2023: Train Epoch: 5 [44800/60900 (74%)]	Loss: 164.040741 | Not computing TPR and F1 | Elapsed: 12.39s
01/24/2023 06:29:21 PM  [*] Tue Jan 24 18:29:21 2023: Train Epoch: 5 [51200/60900 (84%)]	Loss: 166.148956 | Not computing TPR and F1 | Elapsed: 12.46s
01/24/2023 06:29:33 PM  [*] Tue Jan 24 18:29:33 2023: Train Epoch: 5 [57600/60900 (95%)]	Loss: 161.545715 | Not computing TPR and F1 | Elapsed: 12.57s
01/24/2023 06:29:40 PM  [*] Tue Jan 24 18:29:40 2023:    5    | Tr.loss: 178.153164 | Not computing TPR and F1 | Elapsed:  118.85  s
01/24/2023 06:29:40 PM [!] Tue Jan 24 18:29:40 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\preTraining\trainingFiles_1674581380-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\preTraining\trainingFiles_1674581380-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\preTraining\trainingFiles_1674581380-trainTime.npy
01/24/2023 06:29:41 PM  [!] Training pretrained model on downstream task...
01/24/2023 06:29:41 PM  [*] Started epoch: 1
01/24/2023 06:29:41 PM  [*] Tue Jan 24 18:29:41 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.878853 | FPR 0.0003 -- TPR 0.0000 | F1 0.0000 | Elapsed: 0.11s
01/24/2023 06:29:51 PM  [*] Tue Jan 24 18:29:51 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.488214 | FPR 0.0003 -- TPR 0.2328 | F1 0.3499 | Elapsed: 9.64s
01/24/2023 06:30:00 PM  [*] Tue Jan 24 18:30:00 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.368825 | FPR 0.0003 -- TPR 0.3490 | F1 0.4902 | Elapsed: 9.62s
01/24/2023 06:30:04 PM  [*] Tue Jan 24 18:30:04 2023:    1    | Tr.loss: 0.485448 | FPR 0.0003 -- TPR: 0.31 |  F1: 0.44 | Elapsed:   22.93  s
01/24/2023 06:30:04 PM  [*] Started epoch: 2
01/24/2023 06:30:04 PM  [*] Tue Jan 24 18:30:04 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.375587 | FPR 0.0003 -- TPR 0.5385 | F1 0.7000 | Elapsed: 0.09s
01/24/2023 06:30:14 PM  [*] Tue Jan 24 18:30:14 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.344704 | FPR 0.0003 -- TPR 0.5100 | F1 0.6555 | Elapsed: 9.62s
01/24/2023 06:30:23 PM  [*] Tue Jan 24 18:30:23 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.283493 | FPR 0.0003 -- TPR 0.6430 | F1 0.7694 | Elapsed: 9.65s
01/24/2023 06:30:27 PM  [*] Tue Jan 24 18:30:27 2023:    2    | Tr.loss: 0.305566 | FPR 0.0003 -- TPR: 0.59 |  F1: 0.72 | Elapsed:   22.90  s
01/24/2023 06:30:27 PM  [*] Started epoch: 3
01/24/2023 06:30:27 PM  [*] Tue Jan 24 18:30:27 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.223554 | FPR 0.0003 -- TPR 0.5952 | F1 0.7463 | Elapsed: 0.10s
01/24/2023 06:30:36 PM  [*] Tue Jan 24 18:30:36 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.197518 | FPR 0.0003 -- TPR 0.7542 | F1 0.8488 | Elapsed: 9.63s
01/24/2023 06:30:46 PM  [*] Tue Jan 24 18:30:46 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.125319 | FPR 0.0003 -- TPR 0.7749 | F1 0.8631 | Elapsed: 9.64s
01/24/2023 06:30:50 PM  [*] Tue Jan 24 18:30:50 2023:    3    | Tr.loss: 0.216910 | FPR 0.0003 -- TPR: 0.76 |  F1: 0.86 | Elapsed:   22.95  s
01/24/2023 06:30:50 PM [!] Tue Jan 24 18:30:50 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674581450-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674581450-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674581450-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674581450-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674581450-trainTPRs.npy
01/24/2023 06:30:50 PM  [!] Training non_pretrained model on downstream task...
01/24/2023 06:30:51 PM  [*] Started epoch: 1
01/24/2023 06:30:51 PM  [*] Tue Jan 24 18:30:51 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 1.681726 | FPR 0.0003 -- TPR 0.0465 | F1 0.0889 | Elapsed: 0.08s
01/24/2023 06:30:57 PM  [*] Tue Jan 24 18:30:57 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.320574 | FPR 0.0003 -- TPR 0.2319 | F1 0.3448 | Elapsed: 6.75s
01/24/2023 06:31:04 PM  [*] Tue Jan 24 18:31:04 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.312146 | FPR 0.0003 -- TPR 0.4556 | F1 0.6051 | Elapsed: 6.80s
01/24/2023 06:31:07 PM  [*] Tue Jan 24 18:31:07 2023:    1    | Tr.loss: 0.468067 | FPR 0.0003 -- TPR: 0.37 |  F1: 0.51 | Elapsed:   16.12  s
01/24/2023 06:31:07 PM  [*] Started epoch: 2
01/24/2023 06:31:07 PM  [*] Tue Jan 24 18:31:07 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.237130 | FPR 0.0003 -- TPR 0.8542 | F1 0.9213 | Elapsed: 0.08s
01/24/2023 06:31:14 PM  [*] Tue Jan 24 18:31:14 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.255054 | FPR 0.0003 -- TPR 0.5866 | F1 0.7275 | Elapsed: 6.75s
01/24/2023 06:31:20 PM  [*] Tue Jan 24 18:31:20 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.291402 | FPR 0.0003 -- TPR 0.6300 | F1 0.7620 | Elapsed: 6.81s
01/24/2023 06:31:23 PM  [*] Tue Jan 24 18:31:23 2023:    2    | Tr.loss: 0.305558 | FPR 0.0003 -- TPR: 0.62 |  F1: 0.75 | Elapsed:   16.13  s
01/24/2023 06:31:23 PM  [*] Started epoch: 3
01/24/2023 06:31:23 PM  [*] Tue Jan 24 18:31:23 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.257999 | FPR 0.0003 -- TPR 0.7660 | F1 0.8675 | Elapsed: 0.08s
01/24/2023 06:31:30 PM  [*] Tue Jan 24 18:31:30 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.188953 | FPR 0.0003 -- TPR 0.7115 | F1 0.8192 | Elapsed: 6.74s
01/24/2023 06:31:36 PM  [*] Tue Jan 24 18:31:36 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.119997 | FPR 0.0003 -- TPR 0.7403 | F1 0.8402 | Elapsed: 6.77s
01/24/2023 06:31:39 PM  [*] Tue Jan 24 18:31:39 2023:    3    | Tr.loss: 0.231819 | FPR 0.0003 -- TPR: 0.72 |  F1: 0.83 | Elapsed:   16.09  s
01/24/2023 06:31:39 PM [!] Tue Jan 24 18:31:39 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674581499-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674581499-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674581499-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674581499-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674581499-trainTPRs.npy
01/24/2023 06:31:39 PM  [!] Training full_data model on downstream task...
01/24/2023 06:31:40 PM  [*] Started epoch: 1
01/24/2023 06:31:40 PM  [*] Tue Jan 24 18:31:40 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 2.511333 | FPR 0.0003 -- TPR 0.1087 | F1 0.1961 | Elapsed: 0.08s
01/24/2023 06:31:47 PM  [*] Tue Jan 24 18:31:47 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.374580 | FPR 0.0003 -- TPR 0.2271 | F1 0.3455 | Elapsed: 6.76s
01/24/2023 06:31:54 PM  [*] Tue Jan 24 18:31:54 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.304426 | FPR 0.0003 -- TPR 0.4399 | F1 0.5942 | Elapsed: 6.75s
01/24/2023 06:32:00 PM  [*] Tue Jan 24 18:32:00 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.302289 | FPR 0.0003 -- TPR 0.4894 | F1 0.6361 | Elapsed: 6.74s
01/24/2023 06:32:07 PM  [*] Tue Jan 24 18:32:07 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.356035 | FPR 0.0003 -- TPR 0.5705 | F1 0.7145 | Elapsed: 6.76s
01/24/2023 06:32:14 PM  [*] Tue Jan 24 18:32:14 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.287871 | FPR 0.0003 -- TPR 0.6284 | F1 0.7639 | Elapsed: 6.76s
01/24/2023 06:32:21 PM  [*] Tue Jan 24 18:32:21 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.155288 | FPR 0.0003 -- TPR 0.6473 | F1 0.7747 | Elapsed: 6.78s
01/24/2023 06:32:27 PM  [*] Tue Jan 24 18:32:27 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.163097 | FPR 0.0003 -- TPR 0.7038 | F1 0.8161 | Elapsed: 6.72s
01/24/2023 06:32:34 PM  [*] Tue Jan 24 18:32:34 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.218810 | FPR 0.0003 -- TPR 0.7343 | F1 0.8356 | Elapsed: 6.77s
01/24/2023 06:32:41 PM  [*] Tue Jan 24 18:32:41 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.302235 | FPR 0.0003 -- TPR 0.7586 | F1 0.8532 | Elapsed: 6.76s
01/24/2023 06:32:48 PM  [*] Tue Jan 24 18:32:48 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.183999 | FPR 0.0003 -- TPR 0.7983 | F1 0.8818 | Elapsed: 6.77s
01/24/2023 06:32:54 PM  [*] Tue Jan 24 18:32:54 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.218771 | FPR 0.0003 -- TPR 0.7839 | F1 0.8702 | Elapsed: 6.78s
01/24/2023 06:33:00 PM  [*] Tue Jan 24 18:33:00 2023:    1    | Tr.loss: 0.303323 | FPR 0.0003 -- TPR: 0.63 |  F1: 0.75 | Elapsed:   80.43  s
01/24/2023 06:33:00 PM  [*] Started epoch: 2
01/24/2023 06:33:00 PM  [*] Tue Jan 24 18:33:00 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.233799 | FPR 0.0003 -- TPR 0.8409 | F1 0.9136 | Elapsed: 0.08s
01/24/2023 06:33:07 PM  [*] Tue Jan 24 18:33:07 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.185197 | FPR 0.0003 -- TPR 0.8113 | F1 0.8883 | Elapsed: 6.76s
01/24/2023 06:33:14 PM  [*] Tue Jan 24 18:33:14 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.132777 | FPR 0.0003 -- TPR 0.8678 | F1 0.9260 | Elapsed: 6.78s
01/24/2023 06:33:21 PM  [*] Tue Jan 24 18:33:21 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.109134 | FPR 0.0003 -- TPR 0.8807 | F1 0.9333 | Elapsed: 6.83s
01/24/2023 06:33:28 PM  [*] Tue Jan 24 18:33:28 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.130935 | FPR 0.0003 -- TPR 0.8692 | F1 0.9246 | Elapsed: 6.75s
01/24/2023 06:33:34 PM  [*] Tue Jan 24 18:33:34 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.070034 | FPR 0.0003 -- TPR 0.8840 | F1 0.9361 | Elapsed: 6.79s
01/24/2023 06:33:41 PM  [*] Tue Jan 24 18:33:41 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.055493 | FPR 0.0003 -- TPR 0.8901 | F1 0.9398 | Elapsed: 6.76s
01/24/2023 06:33:48 PM  [*] Tue Jan 24 18:33:48 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.145311 | FPR 0.0003 -- TPR 0.8870 | F1 0.9353 | Elapsed: 6.76s
01/24/2023 06:33:55 PM  [*] Tue Jan 24 18:33:55 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.189047 | FPR 0.0003 -- TPR 0.8839 | F1 0.9348 | Elapsed: 6.77s
01/24/2023 06:33:55 PM [!] Learning rate: 2.5e-05
01/24/2023 06:34:01 PM  [*] Tue Jan 24 18:34:01 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.121947 | FPR 0.0003 -- TPR 0.9146 | F1 0.9526 | Elapsed: 6.78s
01/24/2023 06:34:08 PM  [*] Tue Jan 24 18:34:08 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.072496 | FPR 0.0003 -- TPR 0.9082 | F1 0.9490 | Elapsed: 6.75s
01/24/2023 06:34:15 PM  [*] Tue Jan 24 18:34:15 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.114761 | FPR 0.0003 -- TPR 0.9110 | F1 0.9514 | Elapsed: 6.79s
01/24/2023 06:34:21 PM  [*] Tue Jan 24 18:34:21 2023:    2    | Tr.loss: 0.137861 | FPR 0.0003 -- TPR: 0.88 |  F1: 0.93 | Elapsed:   80.60  s
01/24/2023 06:34:21 PM  [*] Started epoch: 3
01/24/2023 06:34:21 PM  [*] Tue Jan 24 18:34:21 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.068287 | FPR 0.0003 -- TPR 1.0000 | F1 1.0000 | Elapsed: 0.09s
01/24/2023 06:34:28 PM  [*] Tue Jan 24 18:34:28 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.073995 | FPR 0.0003 -- TPR 0.9386 | F1 0.9674 | Elapsed: 6.77s
01/24/2023 06:34:35 PM  [*] Tue Jan 24 18:34:35 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.136989 | FPR 0.0003 -- TPR 0.9251 | F1 0.9594 | Elapsed: 6.77s
01/24/2023 06:34:41 PM  [*] Tue Jan 24 18:34:41 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.095232 | FPR 0.0003 -- TPR 0.9410 | F1 0.9684 | Elapsed: 6.77s
01/24/2023 06:34:48 PM  [*] Tue Jan 24 18:34:48 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.136683 | FPR 0.0003 -- TPR 0.9280 | F1 0.9612 | Elapsed: 6.78s
01/24/2023 06:34:55 PM  [*] Tue Jan 24 18:34:55 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.125689 | FPR 0.0003 -- TPR 0.9375 | F1 0.9664 | Elapsed: 6.75s
01/24/2023 06:35:02 PM  [*] Tue Jan 24 18:35:02 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.104087 | FPR 0.0003 -- TPR 0.9271 | F1 0.9602 | Elapsed: 6.77s
01/24/2023 06:35:08 PM  [*] Tue Jan 24 18:35:08 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.103414 | FPR 0.0003 -- TPR 0.9409 | F1 0.9682 | Elapsed: 6.76s
01/24/2023 06:35:15 PM  [*] Tue Jan 24 18:35:15 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.046177 | FPR 0.0003 -- TPR 0.9353 | F1 0.9651 | Elapsed: 6.75s
01/24/2023 06:35:22 PM  [*] Tue Jan 24 18:35:22 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.022201 | FPR 0.0003 -- TPR 0.9240 | F1 0.9569 | Elapsed: 6.82s
01/24/2023 06:35:29 PM  [*] Tue Jan 24 18:35:29 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.112539 | FPR 0.0003 -- TPR 0.9363 | F1 0.9660 | Elapsed: 6.74s
01/24/2023 06:35:36 PM  [*] Tue Jan 24 18:35:36 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.069735 | FPR 0.0003 -- TPR 0.9311 | F1 0.9625 | Elapsed: 6.83s
01/24/2023 06:35:42 PM  [*] Tue Jan 24 18:35:42 2023:    3    | Tr.loss: 0.101638 | FPR 0.0003 -- TPR: 0.93 |  F1: 0.96 | Elapsed:   80.63  s
01/24/2023 06:35:42 PM [!] Tue Jan 24 18:35:42 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674581742-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674581742-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674581742-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674581742-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674581742-trainTPRs.npy
01/24/2023 06:35:42 PM  [*] Evaluating pretrained model on test set...
01/24/2023 06:35:47 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0908 | F1: 0.1665
01/24/2023 06:35:47 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1167 | F1: 0.2089
01/24/2023 06:35:47 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2572 | F1: 0.4088
01/24/2023 06:35:47 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3323 | F1: 0.4979
01/24/2023 06:35:47 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4348 | F1: 0.6025
01/24/2023 06:35:47 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5788 | F1: 0.7215
01/24/2023 06:35:47 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7838 | F1: 0.8388
01/24/2023 06:35:47 PM  [*] Evaluating non_pretrained model on test set...
01/24/2023 06:35:52 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0623 | F1: 0.1173
01/24/2023 06:35:52 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2364 | F1: 0.3824
01/24/2023 06:35:52 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2607 | F1: 0.4133
01/24/2023 06:35:52 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.2782 | F1: 0.4345
01/24/2023 06:35:52 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.3323 | F1: 0.4957
01/24/2023 06:35:52 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.4626 | F1: 0.6217
01/24/2023 06:35:52 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7107 | F1: 0.7916
01/24/2023 06:35:52 PM  [*] Evaluating full_data model on test set...
01/24/2023 06:35:57 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.2101 | F1: 0.3473
01/24/2023 06:35:57 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.3609 | F1: 0.5304
01/24/2023 06:35:57 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3944 | F1: 0.5654
01/24/2023 06:35:57 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.4450 | F1: 0.6148
01/24/2023 06:35:57 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.5847 | F1: 0.7340
01/24/2023 06:35:57 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.7071 | F1: 0.8162
01/24/2023 06:35:57 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8445 | F1: 0.8754
01/24/2023 06:35:57 PM  [!] Running pre-training split 2/2
01/24/2023 06:35:57 PM  [!] Pre-training model...
01/24/2023 06:35:58 PM  [*] Masking sequences...
01/24/2023 06:36:17 PM  [*] Started epoch: 1
01/24/2023 06:36:17 PM  [*] Tue Jan 24 18:36:17 2023: Train Epoch: 1 [  0  /60900 (0 %)]	Loss: 400.999451 | Not computing TPR and F1 | Elapsed: 0.84s
01/24/2023 06:36:30 PM  [*] Tue Jan 24 18:36:30 2023: Train Epoch: 1 [6400 /60900 (11%)]	Loss: 198.305908 | Not computing TPR and F1 | Elapsed: 12.43s
01/24/2023 06:36:42 PM  [*] Tue Jan 24 18:36:42 2023: Train Epoch: 1 [12800/60900 (21%)]	Loss: 227.860718 | Not computing TPR and F1 | Elapsed: 12.46s
01/24/2023 06:36:55 PM  [*] Tue Jan 24 18:36:55 2023: Train Epoch: 1 [19200/60900 (32%)]	Loss: 208.249344 | Not computing TPR and F1 | Elapsed: 12.50s
01/24/2023 06:37:07 PM  [*] Tue Jan 24 18:37:07 2023: Train Epoch: 1 [25600/60900 (42%)]	Loss: 204.303696 | Not computing TPR and F1 | Elapsed: 12.51s
01/24/2023 06:37:20 PM  [*] Tue Jan 24 18:37:20 2023: Train Epoch: 1 [32000/60900 (53%)]	Loss: 184.870743 | Not computing TPR and F1 | Elapsed: 12.61s
01/24/2023 06:37:32 PM  [*] Tue Jan 24 18:37:32 2023: Train Epoch: 1 [38400/60900 (63%)]	Loss: 204.861053 | Not computing TPR and F1 | Elapsed: 12.43s
01/24/2023 06:37:45 PM  [*] Tue Jan 24 18:37:45 2023: Train Epoch: 1 [44800/60900 (74%)]	Loss: 184.701721 | Not computing TPR and F1 | Elapsed: 12.56s
01/24/2023 06:37:57 PM  [*] Tue Jan 24 18:37:57 2023: Train Epoch: 1 [51200/60900 (84%)]	Loss: 203.658417 | Not computing TPR and F1 | Elapsed: 12.51s
01/24/2023 06:38:10 PM  [*] Tue Jan 24 18:38:10 2023: Train Epoch: 1 [57600/60900 (95%)]	Loss: 221.076752 | Not computing TPR and F1 | Elapsed: 12.46s
01/24/2023 06:38:16 PM  [*] Tue Jan 24 18:38:16 2023:    1    | Tr.loss: 206.818734 | Not computing TPR and F1 | Elapsed:  119.74  s
01/24/2023 06:38:16 PM  [*] Started epoch: 2
01/24/2023 06:38:16 PM  [*] Tue Jan 24 18:38:16 2023: Train Epoch: 2 [  0  /60900 (0 %)]	Loss: 192.787598 | Not computing TPR and F1 | Elapsed: 0.12s
01/24/2023 06:38:29 PM  [*] Tue Jan 24 18:38:29 2023: Train Epoch: 2 [6400 /60900 (11%)]	Loss: 206.999176 | Not computing TPR and F1 | Elapsed: 12.45s
01/24/2023 06:38:41 PM  [*] Tue Jan 24 18:38:41 2023: Train Epoch: 2 [12800/60900 (21%)]	Loss: 176.396973 | Not computing TPR and F1 | Elapsed: 12.53s
01/24/2023 06:38:54 PM  [*] Tue Jan 24 18:38:54 2023: Train Epoch: 2 [19200/60900 (32%)]	Loss: 206.475891 | Not computing TPR and F1 | Elapsed: 12.55s
01/24/2023 06:39:07 PM  [*] Tue Jan 24 18:39:07 2023: Train Epoch: 2 [25600/60900 (42%)]	Loss: 167.705978 | Not computing TPR and F1 | Elapsed: 12.59s
01/24/2023 06:39:19 PM  [*] Tue Jan 24 18:39:19 2023: Train Epoch: 2 [32000/60900 (53%)]	Loss: 188.679413 | Not computing TPR and F1 | Elapsed: 12.61s
01/24/2023 06:39:32 PM  [*] Tue Jan 24 18:39:32 2023: Train Epoch: 2 [38400/60900 (63%)]	Loss: 179.785126 | Not computing TPR and F1 | Elapsed: 12.50s
01/24/2023 06:39:44 PM  [*] Tue Jan 24 18:39:44 2023: Train Epoch: 2 [44800/60900 (74%)]	Loss: 169.963486 | Not computing TPR and F1 | Elapsed: 12.50s
01/24/2023 06:39:57 PM  [*] Tue Jan 24 18:39:57 2023: Train Epoch: 2 [51200/60900 (84%)]	Loss: 194.877777 | Not computing TPR and F1 | Elapsed: 12.46s
01/24/2023 06:40:09 PM  [*] Tue Jan 24 18:40:09 2023: Train Epoch: 2 [57600/60900 (95%)]	Loss: 165.557800 | Not computing TPR and F1 | Elapsed: 12.45s
01/24/2023 06:40:16 PM  [*] Tue Jan 24 18:40:16 2023:    2    | Tr.loss: 185.640259 | Not computing TPR and F1 | Elapsed:  119.25  s
01/24/2023 06:40:16 PM  [*] Started epoch: 3
01/24/2023 06:40:16 PM  [*] Tue Jan 24 18:40:16 2023: Train Epoch: 3 [  0  /60900 (0 %)]	Loss: 164.294891 | Not computing TPR and F1 | Elapsed: 0.13s
01/24/2023 06:40:28 PM  [*] Tue Jan 24 18:40:28 2023: Train Epoch: 3 [6400 /60900 (11%)]	Loss: 172.433014 | Not computing TPR and F1 | Elapsed: 12.51s
01/24/2023 06:40:41 PM  [*] Tue Jan 24 18:40:41 2023: Train Epoch: 3 [12800/60900 (21%)]	Loss: 175.508484 | Not computing TPR and F1 | Elapsed: 12.58s
01/24/2023 06:40:53 PM  [*] Tue Jan 24 18:40:53 2023: Train Epoch: 3 [19200/60900 (32%)]	Loss: 201.959045 | Not computing TPR and F1 | Elapsed: 12.48s
01/24/2023 06:41:06 PM  [*] Tue Jan 24 18:41:06 2023: Train Epoch: 3 [25600/60900 (42%)]	Loss: 171.704437 | Not computing TPR and F1 | Elapsed: 12.49s
01/24/2023 06:41:18 PM  [*] Tue Jan 24 18:41:18 2023: Train Epoch: 3 [32000/60900 (53%)]	Loss: 168.463272 | Not computing TPR and F1 | Elapsed: 12.56s
01/24/2023 06:41:31 PM  [*] Tue Jan 24 18:41:31 2023: Train Epoch: 3 [38400/60900 (63%)]	Loss: 170.524628 | Not computing TPR and F1 | Elapsed: 12.56s
01/24/2023 06:41:43 PM  [*] Tue Jan 24 18:41:43 2023: Train Epoch: 3 [44800/60900 (74%)]	Loss: 174.490967 | Not computing TPR and F1 | Elapsed: 12.50s
01/24/2023 06:41:56 PM  [*] Tue Jan 24 18:41:56 2023: Train Epoch: 3 [51200/60900 (84%)]	Loss: 177.239212 | Not computing TPR and F1 | Elapsed: 12.52s
01/24/2023 06:42:08 PM  [*] Tue Jan 24 18:42:08 2023: Train Epoch: 3 [57600/60900 (95%)]	Loss: 194.120682 | Not computing TPR and F1 | Elapsed: 12.58s
01/24/2023 06:42:15 PM  [*] Tue Jan 24 18:42:15 2023:    3    | Tr.loss: 180.567934 | Not computing TPR and F1 | Elapsed:  119.35  s
01/24/2023 06:42:15 PM  [*] Started epoch: 4
01/24/2023 06:42:15 PM  [*] Tue Jan 24 18:42:15 2023: Train Epoch: 4 [  0  /60900 (0 %)]	Loss: 172.269897 | Not computing TPR and F1 | Elapsed: 0.15s
01/24/2023 06:42:28 PM  [*] Tue Jan 24 18:42:28 2023: Train Epoch: 4 [6400 /60900 (11%)]	Loss: 172.483246 | Not computing TPR and F1 | Elapsed: 12.58s
01/24/2023 06:42:40 PM  [*] Tue Jan 24 18:42:40 2023: Train Epoch: 4 [12800/60900 (21%)]	Loss: 169.112854 | Not computing TPR and F1 | Elapsed: 12.46s
01/24/2023 06:42:53 PM  [*] Tue Jan 24 18:42:53 2023: Train Epoch: 4 [19200/60900 (32%)]	Loss: 195.648285 | Not computing TPR and F1 | Elapsed: 12.48s
01/24/2023 06:43:05 PM  [*] Tue Jan 24 18:43:05 2023: Train Epoch: 4 [25600/60900 (42%)]	Loss: 174.922363 | Not computing TPR and F1 | Elapsed: 12.50s
01/24/2023 06:43:18 PM  [*] Tue Jan 24 18:43:18 2023: Train Epoch: 4 [32000/60900 (53%)]	Loss: 178.226898 | Not computing TPR and F1 | Elapsed: 12.58s
01/24/2023 06:43:30 PM  [*] Tue Jan 24 18:43:30 2023: Train Epoch: 4 [38400/60900 (63%)]	Loss: 168.572861 | Not computing TPR and F1 | Elapsed: 12.62s
01/24/2023 06:43:43 PM  [*] Tue Jan 24 18:43:43 2023: Train Epoch: 4 [44800/60900 (74%)]	Loss: 185.163330 | Not computing TPR and F1 | Elapsed: 12.59s
01/24/2023 06:43:55 PM  [*] Tue Jan 24 18:43:55 2023: Train Epoch: 4 [51200/60900 (84%)]	Loss: 185.261078 | Not computing TPR and F1 | Elapsed: 12.42s
01/24/2023 06:44:08 PM  [*] Tue Jan 24 18:44:08 2023: Train Epoch: 4 [57600/60900 (95%)]	Loss: 162.657471 | Not computing TPR and F1 | Elapsed: 12.56s
01/24/2023 06:44:14 PM  [*] Tue Jan 24 18:44:14 2023:    4    | Tr.loss: 177.896196 | Not computing TPR and F1 | Elapsed:  119.32  s
01/24/2023 06:44:14 PM  [*] Started epoch: 5
01/24/2023 06:44:14 PM  [*] Tue Jan 24 18:44:14 2023: Train Epoch: 5 [  0  /60900 (0 %)]	Loss: 195.282059 | Not computing TPR and F1 | Elapsed: 0.12s
01/24/2023 06:44:27 PM  [*] Tue Jan 24 18:44:27 2023: Train Epoch: 5 [6400 /60900 (11%)]	Loss: 185.940979 | Not computing TPR and F1 | Elapsed: 12.56s
01/24/2023 06:44:39 PM  [*] Tue Jan 24 18:44:39 2023: Train Epoch: 5 [12800/60900 (21%)]	Loss: 200.584366 | Not computing TPR and F1 | Elapsed: 12.49s
01/24/2023 06:44:52 PM  [*] Tue Jan 24 18:44:52 2023: Train Epoch: 5 [19200/60900 (32%)]	Loss: 158.139999 | Not computing TPR and F1 | Elapsed: 12.57s
01/24/2023 06:45:05 PM  [*] Tue Jan 24 18:45:05 2023: Train Epoch: 5 [25600/60900 (42%)]	Loss: 176.706696 | Not computing TPR and F1 | Elapsed: 12.59s
01/24/2023 06:45:17 PM  [*] Tue Jan 24 18:45:17 2023: Train Epoch: 5 [32000/60900 (53%)]	Loss: 191.470001 | Not computing TPR and F1 | Elapsed: 12.54s
01/24/2023 06:45:30 PM  [*] Tue Jan 24 18:45:30 2023: Train Epoch: 5 [38400/60900 (63%)]	Loss: 177.466461 | Not computing TPR and F1 | Elapsed: 12.49s
01/24/2023 06:45:42 PM  [*] Tue Jan 24 18:45:42 2023: Train Epoch: 5 [44800/60900 (74%)]	Loss: 157.892151 | Not computing TPR and F1 | Elapsed: 12.59s
01/24/2023 06:45:55 PM  [*] Tue Jan 24 18:45:55 2023: Train Epoch: 5 [51200/60900 (84%)]	Loss: 171.853668 | Not computing TPR and F1 | Elapsed: 12.47s
01/24/2023 06:46:07 PM  [*] Tue Jan 24 18:46:07 2023: Train Epoch: 5 [57600/60900 (95%)]	Loss: 196.004211 | Not computing TPR and F1 | Elapsed: 12.57s
01/24/2023 06:46:14 PM  [*] Tue Jan 24 18:46:14 2023:    5    | Tr.loss: 176.060508 | Not computing TPR and F1 | Elapsed:  119.41  s
01/24/2023 06:46:14 PM [!] Tue Jan 24 18:46:14 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\preTraining\trainingFiles_1674582374-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\preTraining\trainingFiles_1674582374-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\preTraining\trainingFiles_1674582374-trainTime.npy
01/24/2023 06:46:15 PM  [!] Training pretrained model on downstream task...
01/24/2023 06:46:15 PM  [*] Started epoch: 1
01/24/2023 06:46:15 PM  [*] Tue Jan 24 18:46:15 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.841881 | FPR 0.0003 -- TPR 0.1429 | F1 0.2500 | Elapsed: 0.10s
01/24/2023 06:46:25 PM  [*] Tue Jan 24 18:46:25 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.267113 | FPR 0.0003 -- TPR 0.2910 | F1 0.4212 | Elapsed: 9.66s
01/24/2023 06:46:34 PM  [*] Tue Jan 24 18:46:34 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.311683 | FPR 0.0003 -- TPR 0.4641 | F1 0.6155 | Elapsed: 9.60s
01/24/2023 06:46:38 PM  [*] Tue Jan 24 18:46:38 2023:    1    | Tr.loss: 0.442745 | FPR 0.0003 -- TPR: 0.40 |  F1: 0.54 | Elapsed:   22.91  s
01/24/2023 06:46:38 PM  [*] Started epoch: 2
01/24/2023 06:46:38 PM  [*] Tue Jan 24 18:46:38 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.346300 | FPR 0.0003 -- TPR 0.4524 | F1 0.6230 | Elapsed: 0.10s
01/24/2023 06:46:47 PM  [*] Tue Jan 24 18:46:47 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.142482 | FPR 0.0003 -- TPR 0.5986 | F1 0.7384 | Elapsed: 9.66s
01/24/2023 06:46:57 PM  [*] Tue Jan 24 18:46:57 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.232714 | FPR 0.0003 -- TPR 0.6765 | F1 0.7989 | Elapsed: 9.60s
01/24/2023 06:47:01 PM  [*] Tue Jan 24 18:47:01 2023:    2    | Tr.loss: 0.282625 | FPR 0.0003 -- TPR: 0.65 |  F1: 0.77 | Elapsed:   22.92  s
01/24/2023 06:47:01 PM  [*] Started epoch: 3
01/24/2023 06:47:01 PM  [*] Tue Jan 24 18:47:01 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.293252 | FPR 0.0003 -- TPR 0.7805 | F1 0.8767 | Elapsed: 0.10s
01/24/2023 06:47:10 PM  [*] Tue Jan 24 18:47:10 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.358739 | FPR 0.0003 -- TPR 0.7707 | F1 0.8650 | Elapsed: 9.63s
01/24/2023 06:47:20 PM  [*] Tue Jan 24 18:47:20 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.207077 | FPR 0.0003 -- TPR 0.8008 | F1 0.8835 | Elapsed: 9.61s
01/24/2023 06:47:24 PM  [*] Tue Jan 24 18:47:24 2023:    3    | Tr.loss: 0.203353 | FPR 0.0003 -- TPR: 0.80 |  F1: 0.88 | Elapsed:   22.91  s
01/24/2023 06:47:24 PM [!] Tue Jan 24 18:47:24 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674582444-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674582444-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674582444-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674582444-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_pretrained\trainingFiles_1674582444-trainTPRs.npy
01/24/2023 06:47:24 PM  [!] Training non_pretrained model on downstream task...
01/24/2023 06:47:24 PM  [*] Started epoch: 1
01/24/2023 06:47:25 PM  [*] Tue Jan 24 18:47:25 2023: Train Epoch: 1 [  0  /15226 (0 %)]	Loss: 2.568446 | FPR 0.0003 -- TPR 0.0000 | F1 0.0000 | Elapsed: 0.08s
01/24/2023 06:47:31 PM  [*] Tue Jan 24 18:47:31 2023: Train Epoch: 1 [6400 /15226 (42%)]	Loss: 0.504511 | FPR 0.0003 -- TPR 0.2018 | F1 0.3117 | Elapsed: 6.74s
01/24/2023 06:47:38 PM  [*] Tue Jan 24 18:47:38 2023: Train Epoch: 1 [12800/15226 (84%)]	Loss: 0.308442 | FPR 0.0003 -- TPR 0.4120 | F1 0.5654 | Elapsed: 6.77s
01/24/2023 06:47:41 PM  [*] Tue Jan 24 18:47:41 2023:    1    | Tr.loss: 0.516702 | FPR 0.0003 -- TPR: 0.34 |  F1: 0.47 | Elapsed:   16.10  s
01/24/2023 06:47:41 PM  [*] Started epoch: 2
01/24/2023 06:47:41 PM  [*] Tue Jan 24 18:47:41 2023: Train Epoch: 2 [  0  /15226 (0 %)]	Loss: 0.324965 | FPR 0.0003 -- TPR 0.7209 | F1 0.8378 | Elapsed: 0.07s
01/24/2023 06:47:47 PM  [*] Tue Jan 24 18:47:47 2023: Train Epoch: 2 [6400 /15226 (42%)]	Loss: 0.480190 | FPR 0.0003 -- TPR 0.5204 | F1 0.6727 | Elapsed: 6.78s
01/24/2023 06:47:54 PM  [*] Tue Jan 24 18:47:54 2023: Train Epoch: 2 [12800/15226 (84%)]	Loss: 0.280989 | FPR 0.0003 -- TPR 0.6043 | F1 0.7412 | Elapsed: 6.75s
01/24/2023 06:47:57 PM  [*] Tue Jan 24 18:47:57 2023:    2    | Tr.loss: 0.327947 | FPR 0.0003 -- TPR: 0.57 |  F1: 0.72 | Elapsed:   16.10  s
01/24/2023 06:47:57 PM  [*] Started epoch: 3
01/24/2023 06:47:57 PM  [*] Tue Jan 24 18:47:57 2023: Train Epoch: 3 [  0  /15226 (0 %)]	Loss: 0.227286 | FPR 0.0003 -- TPR 0.8077 | F1 0.8936 | Elapsed: 0.08s
01/24/2023 06:48:03 PM  [*] Tue Jan 24 18:48:03 2023: Train Epoch: 3 [6400 /15226 (42%)]	Loss: 0.233676 | FPR 0.0003 -- TPR 0.6763 | F1 0.7965 | Elapsed: 6.74s
01/24/2023 06:48:10 PM  [*] Tue Jan 24 18:48:10 2023: Train Epoch: 3 [12800/15226 (84%)]	Loss: 0.350864 | FPR 0.0003 -- TPR 0.7082 | F1 0.8198 | Elapsed: 6.77s
01/24/2023 06:48:13 PM  [*] Tue Jan 24 18:48:13 2023:    3    | Tr.loss: 0.256778 | FPR 0.0003 -- TPR: 0.70 |  F1: 0.81 | Elapsed:   16.12  s
01/24/2023 06:48:13 PM [!] Tue Jan 24 18:48:13 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674582493-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674582493-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674582493-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674582493-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_non_pretrained\trainingFiles_1674582493-trainTPRs.npy
01/24/2023 06:48:13 PM  [!] Training full_data model on downstream task...
01/24/2023 06:48:14 PM  [*] Started epoch: 1
01/24/2023 06:48:14 PM  [*] Tue Jan 24 18:48:14 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 2.231585 | FPR 0.0003 -- TPR 0.0294 | F1 0.0571 | Elapsed: 0.08s
01/24/2023 06:48:21 PM  [*] Tue Jan 24 18:48:21 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.437082 | FPR 0.0003 -- TPR 0.2024 | F1 0.3073 | Elapsed: 6.83s
01/24/2023 06:48:27 PM  [*] Tue Jan 24 18:48:27 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.397175 | FPR 0.0003 -- TPR 0.4153 | F1 0.5701 | Elapsed: 6.78s
01/24/2023 06:48:34 PM  [*] Tue Jan 24 18:48:34 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.273015 | FPR 0.0003 -- TPR 0.4933 | F1 0.6458 | Elapsed: 6.77s
01/24/2023 06:48:41 PM  [*] Tue Jan 24 18:48:41 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.264778 | FPR 0.0003 -- TPR 0.5882 | F1 0.7276 | Elapsed: 6.78s
01/24/2023 06:48:48 PM  [*] Tue Jan 24 18:48:48 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.208175 | FPR 0.0003 -- TPR 0.6029 | F1 0.7395 | Elapsed: 6.80s
01/24/2023 06:48:55 PM  [*] Tue Jan 24 18:48:55 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.363504 | FPR 0.0003 -- TPR 0.6672 | F1 0.7928 | Elapsed: 6.82s
01/24/2023 06:49:01 PM  [*] Tue Jan 24 18:49:01 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.191757 | FPR 0.0003 -- TPR 0.7300 | F1 0.8343 | Elapsed: 6.77s
01/24/2023 06:49:08 PM  [*] Tue Jan 24 18:49:08 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.156591 | FPR 0.0003 -- TPR 0.7249 | F1 0.8309 | Elapsed: 6.79s
01/24/2023 06:49:15 PM  [*] Tue Jan 24 18:49:15 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.202877 | FPR 0.0003 -- TPR 0.7596 | F1 0.8550 | Elapsed: 6.80s
01/24/2023 06:49:22 PM  [*] Tue Jan 24 18:49:22 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.166988 | FPR 0.0003 -- TPR 0.7727 | F1 0.8615 | Elapsed: 6.80s
01/24/2023 06:49:29 PM  [*] Tue Jan 24 18:49:29 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.358180 | FPR 0.0003 -- TPR 0.7802 | F1 0.8687 | Elapsed: 6.78s
01/24/2023 06:49:35 PM  [*] Tue Jan 24 18:49:35 2023:    1    | Tr.loss: 0.311073 | FPR 0.0003 -- TPR: 0.63 |  F1: 0.74 | Elapsed:   80.85  s
01/24/2023 06:49:35 PM  [*] Started epoch: 2
01/24/2023 06:49:35 PM  [*] Tue Jan 24 18:49:35 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.097147 | FPR 0.0003 -- TPR 0.9348 | F1 0.9663 | Elapsed: 0.08s
01/24/2023 06:49:41 PM  [*] Tue Jan 24 18:49:41 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.225723 | FPR 0.0003 -- TPR 0.8480 | F1 0.9129 | Elapsed: 6.81s
01/24/2023 06:49:48 PM  [*] Tue Jan 24 18:49:48 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.155904 | FPR 0.0003 -- TPR 0.8240 | F1 0.8974 | Elapsed: 6.79s
01/24/2023 06:49:55 PM  [*] Tue Jan 24 18:49:55 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.207507 | FPR 0.0003 -- TPR 0.8570 | F1 0.9153 | Elapsed: 6.80s
01/24/2023 06:50:02 PM  [*] Tue Jan 24 18:50:02 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.100853 | FPR 0.0003 -- TPR 0.8668 | F1 0.9251 | Elapsed: 6.77s
01/24/2023 06:50:09 PM  [*] Tue Jan 24 18:50:09 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.076381 | FPR 0.0003 -- TPR 0.8796 | F1 0.9303 | Elapsed: 6.80s
01/24/2023 06:50:15 PM  [*] Tue Jan 24 18:50:15 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.113464 | FPR 0.0003 -- TPR 0.8578 | F1 0.9181 | Elapsed: 6.82s
01/24/2023 06:50:22 PM  [*] Tue Jan 24 18:50:22 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.154839 | FPR 0.0003 -- TPR 0.8906 | F1 0.9385 | Elapsed: 6.80s
01/24/2023 06:50:29 PM  [*] Tue Jan 24 18:50:29 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.156456 | FPR 0.0003 -- TPR 0.9035 | F1 0.9469 | Elapsed: 6.80s
01/24/2023 06:50:30 PM [!] Learning rate: 2.5e-05
01/24/2023 06:50:36 PM  [*] Tue Jan 24 18:50:36 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.179791 | FPR 0.0003 -- TPR 0.9022 | F1 0.9464 | Elapsed: 6.79s
01/24/2023 06:50:43 PM  [*] Tue Jan 24 18:50:43 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.201596 | FPR 0.0003 -- TPR 0.9308 | F1 0.9619 | Elapsed: 6.84s
01/24/2023 06:50:49 PM  [*] Tue Jan 24 18:50:49 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.101320 | FPR 0.0003 -- TPR 0.8999 | F1 0.9447 | Elapsed: 6.79s
01/24/2023 06:50:56 PM  [*] Tue Jan 24 18:50:56 2023:    2    | Tr.loss: 0.141399 | FPR 0.0003 -- TPR: 0.88 |  F1: 0.93 | Elapsed:   80.92  s
01/24/2023 06:50:56 PM  [*] Started epoch: 3
01/24/2023 06:50:56 PM  [*] Tue Jan 24 18:50:56 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.085499 | FPR 0.0003 -- TPR 0.9744 | F1 0.9870 | Elapsed: 0.08s
01/24/2023 06:51:02 PM  [*] Tue Jan 24 18:51:02 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.053886 | FPR 0.0003 -- TPR 0.9170 | F1 0.9540 | Elapsed: 6.79s
01/24/2023 06:51:09 PM  [*] Tue Jan 24 18:51:09 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.166795 | FPR 0.0003 -- TPR 0.9183 | F1 0.9540 | Elapsed: 6.78s
01/24/2023 06:51:16 PM  [*] Tue Jan 24 18:51:16 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.020002 | FPR 0.0003 -- TPR 0.9244 | F1 0.9594 | Elapsed: 6.77s
01/24/2023 06:51:23 PM  [*] Tue Jan 24 18:51:23 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.145015 | FPR 0.0003 -- TPR 0.9157 | F1 0.9538 | Elapsed: 6.80s
01/24/2023 06:51:30 PM  [*] Tue Jan 24 18:51:30 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.083603 | FPR 0.0003 -- TPR 0.9310 | F1 0.9624 | Elapsed: 6.78s
01/24/2023 06:51:36 PM  [*] Tue Jan 24 18:51:36 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.159139 | FPR 0.0003 -- TPR 0.9181 | F1 0.9547 | Elapsed: 6.79s
01/24/2023 06:51:43 PM  [*] Tue Jan 24 18:51:43 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.084551 | FPR 0.0003 -- TPR 0.9226 | F1 0.9583 | Elapsed: 6.79s
01/24/2023 06:51:50 PM  [*] Tue Jan 24 18:51:50 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.082024 | FPR 0.0003 -- TPR 0.9299 | F1 0.9625 | Elapsed: 6.79s
01/24/2023 06:51:57 PM  [*] Tue Jan 24 18:51:57 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.203123 | FPR 0.0003 -- TPR 0.9299 | F1 0.9626 | Elapsed: 6.77s
01/24/2023 06:52:03 PM  [*] Tue Jan 24 18:52:03 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.090847 | FPR 0.0003 -- TPR 0.9374 | F1 0.9663 | Elapsed: 6.76s
01/24/2023 06:52:10 PM  [*] Tue Jan 24 18:52:10 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.187045 | FPR 0.0003 -- TPR 0.9239 | F1 0.9585 | Elapsed: 6.79s
01/24/2023 06:52:16 PM  [*] Tue Jan 24 18:52:16 2023:    3    | Tr.loss: 0.107051 | FPR 0.0003 -- TPR: 0.92 |  F1: 0.96 | Elapsed:   80.74  s
01/24/2023 06:52:17 PM [!] Tue Jan 24 18:52:17 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674582736-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674582736-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674582736-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674582736-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771\downstreamTask_full_data\trainingFiles_1674582736-trainTPRs.npy
01/24/2023 06:52:17 PM  [*] Evaluating pretrained model on test set...
01/24/2023 06:52:22 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0676 | F1: 0.1267
01/24/2023 06:52:22 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2283 | F1: 0.3717
01/24/2023 06:52:22 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2782 | F1: 0.4350
01/24/2023 06:52:22 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3377 | F1: 0.5039
01/24/2023 06:52:22 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4201 | F1: 0.5881
01/24/2023 06:52:22 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5189 | F1: 0.6719
01/24/2023 06:52:22 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7829 | F1: 0.8383
01/24/2023 06:52:22 PM  [*] Evaluating non_pretrained model on test set...
01/24/2023 06:52:27 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0017 | F1: 0.0034
01/24/2023 06:52:27 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.0938 | F1: 0.1715
01/24/2023 06:52:27 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2057 | F1: 0.3409
01/24/2023 06:52:27 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.2718 | F1: 0.4266
01/24/2023 06:52:27 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.3215 | F1: 0.4834
01/24/2023 06:52:27 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.4121 | F1: 0.5734
01/24/2023 06:52:27 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7415 | F1: 0.8119
01/24/2023 06:52:27 PM  [*] Evaluating full_data model on test set...
01/24/2023 06:52:32 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.2375 | F1: 0.3838
01/24/2023 06:52:32 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.3759 | F1: 0.5464
01/24/2023 06:52:32 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.4240 | F1: 0.5952
01/24/2023 06:52:32 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.4767 | F1: 0.6445
01/24/2023 06:52:32 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.5523 | F1: 0.7077
01/24/2023 06:52:32 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.6633 | F1: 0.7855
01/24/2023 06:52:32 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8490 | F1: 0.8782
01/24/2023 06:52:32 PM  [!] Finished pre-training evaluation over 2 splits! Saved metrics to:
	C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\TransformerLM_MetricsImprove_unlabeledDataSize_0.8_preTrain_5_downStream_3_nSplits_2_1674580771/metrics_MaskedLanguageModel_nSplits_2_limit_None.json
