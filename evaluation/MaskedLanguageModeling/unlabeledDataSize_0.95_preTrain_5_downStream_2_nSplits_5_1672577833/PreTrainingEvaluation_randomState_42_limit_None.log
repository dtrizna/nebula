WARNING:root: [!] Starting Masked Language Model evaluation over 5 splits!
WARNING:root: [!] Loaded data and vocab. X train size: (76126, 2048), X test size: (17407, 2048), vocab size: 10000
WARNING:root: [!] Initiating Self-Supervised Learning Pretraining
     Language Modeling MaskedLanguageModel
     Model Cnn1DLinearLM with config:
	{'vocabSize': 10000, 'hiddenNeurons': [512, 256, 128]}

WARNING:root: [!] Running pre-training split 1/5
WARNING:root: [!] Pre-training model...
WARNING:root: [*] Masking sequences...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 13:57:43 2023: Train Epoch: 1 [  0  /72319 (0 %)]	Loss: 494.657288 | Not computing TPR and F1 | Elapsed: 3.86s
WARNING:root: [*] Sun Jan  1 13:57:59 2023: Train Epoch: 1 [12800/72319 (18%)]	Loss: 356.611389 | Not computing TPR and F1 | Elapsed: 15.51s
WARNING:root: [*] Sun Jan  1 13:58:14 2023: Train Epoch: 1 [25600/72319 (35%)]	Loss: 340.665527 | Not computing TPR and F1 | Elapsed: 15.60s
WARNING:root: [*] Sun Jan  1 13:58:30 2023: Train Epoch: 1 [38400/72319 (53%)]	Loss: 345.098816 | Not computing TPR and F1 | Elapsed: 15.91s
WARNING:root: [*] Sun Jan  1 13:58:46 2023: Train Epoch: 1 [51200/72319 (71%)]	Loss: 343.783630 | Not computing TPR and F1 | Elapsed: 15.60s
WARNING:root: [*] Sun Jan  1 13:59:02 2023: Train Epoch: 1 [64000/72319 (88%)]	Loss: 332.681122 | Not computing TPR and F1 | Elapsed: 16.18s
WARNING:root: [*] Sun Jan  1 13:59:12 2023:    1    | Tr.loss: 347.265680 | Not computing TPR and F1 | Elapsed:   92.38  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 13:59:12 2023: Train Epoch: 2 [  0  /72319 (0 %)]	Loss: 311.974945 | Not computing TPR and F1 | Elapsed: 0.34s
WARNING:root: [*] Sun Jan  1 13:59:28 2023: Train Epoch: 2 [12800/72319 (18%)]	Loss: 323.487335 | Not computing TPR and F1 | Elapsed: 15.90s
WARNING:root: [*] Sun Jan  1 13:59:44 2023: Train Epoch: 2 [25600/72319 (35%)]	Loss: 317.442505 | Not computing TPR and F1 | Elapsed: 16.14s
WARNING:root: [*] Sun Jan  1 14:00:00 2023: Train Epoch: 2 [38400/72319 (53%)]	Loss: 305.235474 | Not computing TPR and F1 | Elapsed: 15.79s
WARNING:root: [*] Sun Jan  1 14:00:16 2023: Train Epoch: 2 [51200/72319 (71%)]	Loss: 325.074219 | Not computing TPR and F1 | Elapsed: 15.61s
WARNING:root: [*] Sun Jan  1 14:00:31 2023: Train Epoch: 2 [64000/72319 (88%)]	Loss: 305.729309 | Not computing TPR and F1 | Elapsed: 15.71s
WARNING:root: [*] Sun Jan  1 14:00:41 2023:    2    | Tr.loss: 304.906992 | Not computing TPR and F1 | Elapsed:   89.64  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sun Jan  1 14:00:42 2023: Train Epoch: 3 [  0  /72319 (0 %)]	Loss: 297.457581 | Not computing TPR and F1 | Elapsed: 0.35s
WARNING:root: [*] Sun Jan  1 14:00:58 2023: Train Epoch: 3 [12800/72319 (18%)]	Loss: 322.867371 | Not computing TPR and F1 | Elapsed: 16.07s
WARNING:root: [*] Sun Jan  1 14:01:14 2023: Train Epoch: 3 [25600/72319 (35%)]	Loss: 294.030670 | Not computing TPR and F1 | Elapsed: 16.43s
WARNING:root: [*] Sun Jan  1 14:01:30 2023: Train Epoch: 3 [38400/72319 (53%)]	Loss: 273.121094 | Not computing TPR and F1 | Elapsed: 15.81s
WARNING:root: [*] Sun Jan  1 14:01:46 2023: Train Epoch: 3 [51200/72319 (71%)]	Loss: 269.586975 | Not computing TPR and F1 | Elapsed: 16.42s
WARNING:root: [*] Sun Jan  1 14:02:03 2023: Train Epoch: 3 [64000/72319 (88%)]	Loss: 305.003296 | Not computing TPR and F1 | Elapsed: 16.51s
WARNING:root: [*] Sun Jan  1 14:02:13 2023:    3    | Tr.loss: 295.002892 | Not computing TPR and F1 | Elapsed:   91.68  s
WARNING:root: [*] Started epoch: 4
WARNING:root: [*] Sun Jan  1 14:02:13 2023: Train Epoch: 4 [  0  /72319 (0 %)]	Loss: 291.272461 | Not computing TPR and F1 | Elapsed: 0.35s
WARNING:root: [*] Sun Jan  1 14:02:30 2023: Train Epoch: 4 [12800/72319 (18%)]	Loss: 309.728729 | Not computing TPR and F1 | Elapsed: 16.29s
WARNING:root: [*] Sun Jan  1 14:02:46 2023: Train Epoch: 4 [25600/72319 (35%)]	Loss: 280.642761 | Not computing TPR and F1 | Elapsed: 15.83s
WARNING:root: [*] Sun Jan  1 14:03:02 2023: Train Epoch: 4 [38400/72319 (53%)]	Loss: 309.973450 | Not computing TPR and F1 | Elapsed: 16.45s
WARNING:root: [*] Sun Jan  1 14:03:18 2023: Train Epoch: 4 [51200/72319 (71%)]	Loss: 278.040039 | Not computing TPR and F1 | Elapsed: 16.07s
WARNING:root: [*] Sun Jan  1 14:03:34 2023: Train Epoch: 4 [64000/72319 (88%)]	Loss: 296.247009 | Not computing TPR and F1 | Elapsed: 15.96s
WARNING:root: [*] Sun Jan  1 14:03:44 2023:    4    | Tr.loss: 291.159055 | Not computing TPR and F1 | Elapsed:   91.32  s
WARNING:root: [*] Started epoch: 5
WARNING:root: [*] Sun Jan  1 14:03:45 2023: Train Epoch: 5 [  0  /72319 (0 %)]	Loss: 299.342285 | Not computing TPR and F1 | Elapsed: 0.33s
WARNING:root: [*] Sun Jan  1 14:04:01 2023: Train Epoch: 5 [12800/72319 (18%)]	Loss: 297.927612 | Not computing TPR and F1 | Elapsed: 15.93s
WARNING:root: [*] Sun Jan  1 14:04:17 2023: Train Epoch: 5 [25600/72319 (35%)]	Loss: 297.342896 | Not computing TPR and F1 | Elapsed: 15.97s
WARNING:root: [*] Sun Jan  1 14:04:33 2023: Train Epoch: 5 [38400/72319 (53%)]	Loss: 312.446075 | Not computing TPR and F1 | Elapsed: 16.74s
WARNING:root: [*] Sun Jan  1 14:04:49 2023: Train Epoch: 5 [51200/72319 (71%)]	Loss: 301.257263 | Not computing TPR and F1 | Elapsed: 15.39s
WARNING:root: [*] Sun Jan  1 14:05:05 2023: Train Epoch: 5 [64000/72319 (88%)]	Loss: 298.940033 | Not computing TPR and F1 | Elapsed: 16.28s
WARNING:root: [*] Sun Jan  1 14:05:15 2023:    5    | Tr.loss: 288.976059 | Not computing TPR and F1 | Elapsed:   90.89  s
WARNING:root:[!] Sun Jan  1 14:05:16 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672578315-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672578315-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672578315-trainTime.npy
WARNING:root: [!] Training pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:05:16 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.707334 | FPR 0.01 -- TPR 0.0106 | F1 0.0211 | Elapsed: 0.30s
WARNING:root: [*] Sun Jan  1 14:05:21 2023:    1    | Tr.loss: 0.527917 | FPR 0.01 -- TPR: 0.12 |  F1: 0.21 | Elapsed:   4.61   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:05:21 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.389652 | FPR 0.01 -- TPR 0.1099 | F1 0.1980 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:05:25 2023:    2    | Tr.loss: 0.369600 | FPR 0.01 -- TPR: 0.24 |  F1: 0.38 | Elapsed:   4.60   s
WARNING:root:[!] Sun Jan  1 14:05:25 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672578325-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672578325-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672578325-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672578325-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672578325-trainTPRs.npy
WARNING:root: [!] Training non_pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:05:26 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.673901 | FPR 0.01 -- TPR 0.0000 | F1 0.0000 | Elapsed: 0.32s
WARNING:root: [*] Sun Jan  1 14:05:30 2023:    1    | Tr.loss: 0.635105 | FPR 0.01 -- TPR: 0.03 |  F1: 0.06 | Elapsed:   4.43   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:05:30 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.566401 | FPR 0.01 -- TPR 0.0585 | F1 0.1106 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:05:34 2023:    2    | Tr.loss: 0.507488 | FPR 0.01 -- TPR: 0.18 |  F1: 0.29 | Elapsed:   4.40   s
WARNING:root:[!] Sun Jan  1 14:05:34 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672578334-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672578334-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672578334-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672578334-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672578334-trainTPRs.npy
WARNING:root: [!] Training full_data model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:05:35 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 0.703547 | FPR 0.01 -- TPR 0.0237 | F1 0.0462 | Elapsed: 0.34s
WARNING:root: [*] Sun Jan  1 14:05:50 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.188269 | FPR 0.01 -- TPR 0.6744 | F1 0.8056 | Elapsed: 14.56s
WARNING:root: [*] Sun Jan  1 14:06:04 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.152243 | FPR 0.01 -- TPR 0.9368 | F1 0.9674 | Elapsed: 14.23s
WARNING:root: [*] Sun Jan  1 14:06:18 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.078577 | FPR 0.01 -- TPR 0.9298 | F1 0.9636 | Elapsed: 14.28s
WARNING:root: [*] Sun Jan  1 14:06:33 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.098547 | FPR 0.01 -- TPR 0.9471 | F1 0.9728 | Elapsed: 14.59s
WARNING:root: [*] Sun Jan  1 14:06:47 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.064264 | FPR 0.01 -- TPR 0.9107 | F1 0.9533 | Elapsed: 14.54s
WARNING:root: [*] Sun Jan  1 14:07:01 2023:    1    | Tr.loss: 0.154907 | FPR 0.01 -- TPR: 0.79 |  F1: 0.85 | Elapsed:   86.06  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:07:01 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.076709 | FPR 0.01 -- TPR 0.9207 | F1 0.9587 | Elapsed: 0.30s
WARNING:root: [*] Sun Jan  1 14:07:16 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.043618 | FPR 0.01 -- TPR 0.9824 | F1 0.9911 | Elapsed: 14.71s
WARNING:root: [*] Sun Jan  1 14:07:30 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.096910 | FPR 0.01 -- TPR 0.8488 | F1 0.9182 | Elapsed: 14.59s
WARNING:root: [*] Sun Jan  1 14:07:45 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.030799 | FPR 0.01 -- TPR 0.9890 | F1 0.9944 | Elapsed: 14.62s
WARNING:root: [*] Sun Jan  1 14:08:00 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.061309 | FPR 0.01 -- TPR 0.9503 | F1 0.9745 | Elapsed: 14.56s
WARNING:root: [*] Sun Jan  1 14:08:14 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.034009 | FPR 0.01 -- TPR 0.9887 | F1 0.9943 | Elapsed: 14.74s
WARNING:root: [*] Sun Jan  1 14:08:28 2023:    2    | Tr.loss: 0.059670 | FPR 0.01 -- TPR: 0.93 |  F1: 0.96 | Elapsed:   87.09  s
WARNING:root:[!] Sun Jan  1 14:08:28 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672578508-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672578508-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672578508-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672578508-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672578508-trainTPRs.npy
WARNING:root: [*] Evaluating pretrained model on test set...
WARNING:root: [!] Test F1 score for pretrained model at 0.01 FPR : 0.4770
WARNING:root: [!] Test TPR score for pretrained model at 0.01 FPR: 0.3196
WARNING:root: [*] Evaluating non_pretrained model on test set...
WARNING:root: [!] Test F1 score for non_pretrained model at 0.01 FPR : 0.4017
WARNING:root: [!] Test TPR score for non_pretrained model at 0.01 FPR: 0.2632
WARNING:root: [*] Evaluating full_data model on test set...
WARNING:root: [!] Test F1 score for full_data model at 0.01 FPR : 0.8894
WARNING:root: [!] Test TPR score for full_data model at 0.01 FPR: 0.8087
WARNING:root: [!] Running pre-training split 2/5
WARNING:root: [!] Pre-training model...
WARNING:root: [*] Masking sequences...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:09:47 2023: Train Epoch: 1 [  0  /72319 (0 %)]	Loss: 505.275879 | Not computing TPR and F1 | Elapsed: 1.22s
WARNING:root: [*] Sun Jan  1 14:10:02 2023: Train Epoch: 1 [12800/72319 (18%)]	Loss: 392.706024 | Not computing TPR and F1 | Elapsed: 15.58s
WARNING:root: [*] Sun Jan  1 14:10:18 2023: Train Epoch: 1 [25600/72319 (35%)]	Loss: 346.061523 | Not computing TPR and F1 | Elapsed: 15.95s
WARNING:root: [*] Sun Jan  1 14:10:34 2023: Train Epoch: 1 [38400/72319 (53%)]	Loss: 339.170349 | Not computing TPR and F1 | Elapsed: 15.89s
WARNING:root: [*] Sun Jan  1 14:10:50 2023: Train Epoch: 1 [51200/72319 (71%)]	Loss: 327.046143 | Not computing TPR and F1 | Elapsed: 15.99s
WARNING:root: [*] Sun Jan  1 14:11:06 2023: Train Epoch: 1 [64000/72319 (88%)]	Loss: 301.597443 | Not computing TPR and F1 | Elapsed: 16.04s
WARNING:root: [*] Sun Jan  1 14:11:17 2023:    1    | Tr.loss: 345.279601 | Not computing TPR and F1 | Elapsed:   90.89  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:11:17 2023: Train Epoch: 2 [  0  /72319 (0 %)]	Loss: 301.377991 | Not computing TPR and F1 | Elapsed: 0.34s
WARNING:root: [*] Sun Jan  1 14:11:33 2023: Train Epoch: 2 [12800/72319 (18%)]	Loss: 312.230408 | Not computing TPR and F1 | Elapsed: 16.40s
WARNING:root: [*] Sun Jan  1 14:11:50 2023: Train Epoch: 2 [25600/72319 (35%)]	Loss: 300.434967 | Not computing TPR and F1 | Elapsed: 16.43s
WARNING:root: [*] Sun Jan  1 14:12:06 2023: Train Epoch: 2 [38400/72319 (53%)]	Loss: 300.626678 | Not computing TPR and F1 | Elapsed: 16.35s
WARNING:root: [*] Sun Jan  1 14:12:22 2023: Train Epoch: 2 [51200/72319 (71%)]	Loss: 303.854462 | Not computing TPR and F1 | Elapsed: 15.71s
WARNING:root: [*] Sun Jan  1 14:12:38 2023: Train Epoch: 2 [64000/72319 (88%)]	Loss: 274.678680 | Not computing TPR and F1 | Elapsed: 16.02s
WARNING:root: [*] Sun Jan  1 14:12:48 2023:    2    | Tr.loss: 301.509492 | Not computing TPR and F1 | Elapsed:   91.52  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sun Jan  1 14:12:48 2023: Train Epoch: 3 [  0  /72319 (0 %)]	Loss: 307.597778 | Not computing TPR and F1 | Elapsed: 0.34s
WARNING:root: [*] Sun Jan  1 14:13:05 2023: Train Epoch: 3 [12800/72319 (18%)]	Loss: 303.677490 | Not computing TPR and F1 | Elapsed: 16.15s
WARNING:root: [*] Sun Jan  1 14:13:20 2023: Train Epoch: 3 [25600/72319 (35%)]	Loss: 291.785645 | Not computing TPR and F1 | Elapsed: 15.69s
WARNING:root: [*] Sun Jan  1 14:13:36 2023: Train Epoch: 3 [38400/72319 (53%)]	Loss: 310.837372 | Not computing TPR and F1 | Elapsed: 15.91s
WARNING:root: [*] Sun Jan  1 14:13:53 2023: Train Epoch: 3 [51200/72319 (71%)]	Loss: 292.867004 | Not computing TPR and F1 | Elapsed: 16.43s
WARNING:root: [*] Sun Jan  1 14:14:08 2023: Train Epoch: 3 [64000/72319 (88%)]	Loss: 266.667389 | Not computing TPR and F1 | Elapsed: 15.41s
WARNING:root: [*] Sun Jan  1 14:14:18 2023:    3    | Tr.loss: 292.380386 | Not computing TPR and F1 | Elapsed:   90.01  s
WARNING:root: [*] Started epoch: 4
WARNING:root: [*] Sun Jan  1 14:14:18 2023: Train Epoch: 4 [  0  /72319 (0 %)]	Loss: 286.721283 | Not computing TPR and F1 | Elapsed: 0.35s
WARNING:root: [*] Sun Jan  1 14:14:34 2023: Train Epoch: 4 [12800/72319 (18%)]	Loss: 285.517578 | Not computing TPR and F1 | Elapsed: 15.84s
WARNING:root: [*] Sun Jan  1 14:14:50 2023: Train Epoch: 4 [25600/72319 (35%)]	Loss: 315.891571 | Not computing TPR and F1 | Elapsed: 15.98s
WARNING:root: [*] Sun Jan  1 14:15:06 2023: Train Epoch: 4 [38400/72319 (53%)]	Loss: 275.989227 | Not computing TPR and F1 | Elapsed: 15.44s
WARNING:root: [*] Sun Jan  1 14:15:21 2023: Train Epoch: 4 [51200/72319 (71%)]	Loss: 250.393219 | Not computing TPR and F1 | Elapsed: 15.52s
WARNING:root: [*] Sun Jan  1 14:15:37 2023: Train Epoch: 4 [64000/72319 (88%)]	Loss: 304.254944 | Not computing TPR and F1 | Elapsed: 15.54s
WARNING:root: [*] Sun Jan  1 14:15:47 2023:    4    | Tr.loss: 287.120865 | Not computing TPR and F1 | Elapsed:   88.55  s
WARNING:root: [*] Started epoch: 5
WARNING:root: [*] Sun Jan  1 14:15:47 2023: Train Epoch: 5 [  0  /72319 (0 %)]	Loss: 293.687805 | Not computing TPR and F1 | Elapsed: 0.35s
WARNING:root: [*] Sun Jan  1 14:16:03 2023: Train Epoch: 5 [12800/72319 (18%)]	Loss: 287.905701 | Not computing TPR and F1 | Elapsed: 15.76s
WARNING:root: [*] Sun Jan  1 14:16:18 2023: Train Epoch: 5 [25600/72319 (35%)]	Loss: 274.037292 | Not computing TPR and F1 | Elapsed: 15.72s
WARNING:root: [*] Sun Jan  1 14:16:34 2023: Train Epoch: 5 [38400/72319 (53%)]	Loss: 285.714844 | Not computing TPR and F1 | Elapsed: 15.66s
WARNING:root: [*] Sun Jan  1 14:16:50 2023: Train Epoch: 5 [51200/72319 (71%)]	Loss: 276.760498 | Not computing TPR and F1 | Elapsed: 15.76s
WARNING:root: [*] Sun Jan  1 14:17:05 2023: Train Epoch: 5 [64000/72319 (88%)]	Loss: 272.737427 | Not computing TPR and F1 | Elapsed: 15.64s
WARNING:root: [*] Sun Jan  1 14:17:15 2023:    5    | Tr.loss: 283.850078 | Not computing TPR and F1 | Elapsed:   88.77  s
WARNING:root:[!] Sun Jan  1 14:17:15 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672579035-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672579035-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672579035-trainTime.npy
WARNING:root: [!] Training pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:17:16 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.672753 | FPR 0.01 -- TPR 0.2022 | F1 0.3364 | Elapsed: 0.33s
WARNING:root: [*] Sun Jan  1 14:17:20 2023:    1    | Tr.loss: 0.464393 | FPR 0.01 -- TPR: 0.21 |  F1: 0.33 | Elapsed:   4.49   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:17:21 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.366115 | FPR 0.01 -- TPR 0.4850 | F1 0.6532 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:17:25 2023:    2    | Tr.loss: 0.298529 | FPR 0.01 -- TPR: 0.36 |  F1: 0.53 | Elapsed:   4.42   s
WARNING:root:[!] Sun Jan  1 14:17:25 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579045-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579045-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579045-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579045-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579045-trainTPRs.npy
WARNING:root: [!] Training non_pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:17:25 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.698591 | FPR 0.01 -- TPR 0.0057 | F1 0.0113 | Elapsed: 0.33s
WARNING:root: [*] Sun Jan  1 14:17:29 2023:    1    | Tr.loss: 0.621079 | FPR 0.01 -- TPR: 0.04 |  F1: 0.07 | Elapsed:   4.28   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:17:30 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.581311 | FPR 0.01 -- TPR 0.0289 | F1 0.0562 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:17:33 2023:    2    | Tr.loss: 0.472977 | FPR 0.01 -- TPR: 0.19 |  F1: 0.30 | Elapsed:   4.24   s
WARNING:root:[!] Sun Jan  1 14:17:34 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579053-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579053-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579053-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579053-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579053-trainTPRs.npy
WARNING:root: [!] Training full_data model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:17:34 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 0.676479 | FPR 0.01 -- TPR 0.0116 | F1 0.0229 | Elapsed: 0.33s
WARNING:root: [*] Sun Jan  1 14:17:48 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.171340 | FPR 0.01 -- TPR 0.6747 | F1 0.8058 | Elapsed: 14.19s
WARNING:root: [*] Sun Jan  1 14:18:03 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.135944 | FPR 0.01 -- TPR 0.7771 | F1 0.8746 | Elapsed: 14.19s
WARNING:root: [*] Sun Jan  1 14:18:17 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.083592 | FPR 0.01 -- TPR 0.9588 | F1 0.9790 | Elapsed: 14.28s
WARNING:root: [*] Sun Jan  1 14:18:31 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.055001 | FPR 0.01 -- TPR 0.9789 | F1 0.9894 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:18:45 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.060200 | FPR 0.01 -- TPR 0.9708 | F1 0.9852 | Elapsed: 14.19s
WARNING:root: [*] Sun Jan  1 14:18:58 2023:    1    | Tr.loss: 0.156173 | FPR 0.01 -- TPR: 0.79 |  F1: 0.84 | Elapsed:   84.57  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:18:59 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.058151 | FPR 0.01 -- TPR 0.9231 | F1 0.9600 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:19:13 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.066870 | FPR 0.01 -- TPR 0.8402 | F1 0.9132 | Elapsed: 14.30s
WARNING:root: [*] Sun Jan  1 14:19:27 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.044531 | FPR 0.01 -- TPR 0.9731 | F1 0.9864 | Elapsed: 14.37s
WARNING:root: [*] Sun Jan  1 14:19:42 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.080243 | FPR 0.01 -- TPR 0.9205 | F1 0.9586 | Elapsed: 14.19s
WARNING:root: [*] Sun Jan  1 14:19:56 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.034343 | FPR 0.01 -- TPR 0.9942 | F1 0.9971 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:20:10 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.040618 | FPR 0.01 -- TPR 0.9828 | F1 0.9913 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:20:23 2023:    2    | Tr.loss: 0.061247 | FPR 0.01 -- TPR: 0.94 |  F1: 0.97 | Elapsed:   84.75  s
WARNING:root:[!] Sun Jan  1 14:20:23 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579223-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579223-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579223-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579223-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579223-trainTPRs.npy
WARNING:root: [*] Evaluating pretrained model on test set...
WARNING:root: [!] Test F1 score for pretrained model at 0.01 FPR : 0.5732
WARNING:root: [!] Test TPR score for pretrained model at 0.01 FPR: 0.4107
WARNING:root: [*] Evaluating non_pretrained model on test set...
WARNING:root: [!] Test F1 score for non_pretrained model at 0.01 FPR : 0.4497
WARNING:root: [!] Test TPR score for non_pretrained model at 0.01 FPR: 0.3154
WARNING:root: [*] Evaluating full_data model on test set...
WARNING:root: [!] Test F1 score for full_data model at 0.01 FPR : 0.8734
WARNING:root: [!] Test TPR score for full_data model at 0.01 FPR: 0.7824
WARNING:root: [!] Running pre-training split 3/5
WARNING:root: [!] Pre-training model...
WARNING:root: [*] Masking sequences...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:21:34 2023: Train Epoch: 1 [  0  /72319 (0 %)]	Loss: 461.049377 | Not computing TPR and F1 | Elapsed: 1.05s
WARNING:root: [*] Sun Jan  1 14:21:50 2023: Train Epoch: 1 [12800/72319 (18%)]	Loss: 371.693054 | Not computing TPR and F1 | Elapsed: 15.74s
WARNING:root: [*] Sun Jan  1 14:22:06 2023: Train Epoch: 1 [25600/72319 (35%)]	Loss: 335.775299 | Not computing TPR and F1 | Elapsed: 15.49s
WARNING:root: [*] Sun Jan  1 14:22:21 2023: Train Epoch: 1 [38400/72319 (53%)]	Loss: 327.141388 | Not computing TPR and F1 | Elapsed: 15.24s
WARNING:root: [*] Sun Jan  1 14:22:36 2023: Train Epoch: 1 [51200/72319 (71%)]	Loss: 330.392731 | Not computing TPR and F1 | Elapsed: 15.23s
WARNING:root: [*] Sun Jan  1 14:22:52 2023: Train Epoch: 1 [64000/72319 (88%)]	Loss: 325.726196 | Not computing TPR and F1 | Elapsed: 15.63s
WARNING:root: [*] Sun Jan  1 14:23:01 2023:    1    | Tr.loss: 347.956363 | Not computing TPR and F1 | Elapsed:   88.06  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:23:02 2023: Train Epoch: 2 [  0  /72319 (0 %)]	Loss: 310.068848 | Not computing TPR and F1 | Elapsed: 0.33s
WARNING:root: [*] Sun Jan  1 14:23:17 2023: Train Epoch: 2 [12800/72319 (18%)]	Loss: 324.874908 | Not computing TPR and F1 | Elapsed: 15.59s
WARNING:root: [*] Sun Jan  1 14:23:33 2023: Train Epoch: 2 [25600/72319 (35%)]	Loss: 335.565247 | Not computing TPR and F1 | Elapsed: 15.63s
WARNING:root: [*] Sun Jan  1 14:23:49 2023: Train Epoch: 2 [38400/72319 (53%)]	Loss: 292.497253 | Not computing TPR and F1 | Elapsed: 15.91s
WARNING:root: [*] Sun Jan  1 14:24:04 2023: Train Epoch: 2 [51200/72319 (71%)]	Loss: 286.855103 | Not computing TPR and F1 | Elapsed: 15.50s
WARNING:root: [*] Sun Jan  1 14:24:20 2023: Train Epoch: 2 [64000/72319 (88%)]	Loss: 291.431458 | Not computing TPR and F1 | Elapsed: 15.55s
WARNING:root: [*] Sun Jan  1 14:24:30 2023:    2    | Tr.loss: 306.117567 | Not computing TPR and F1 | Elapsed:   88.24  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sun Jan  1 14:24:30 2023: Train Epoch: 3 [  0  /72319 (0 %)]	Loss: 304.444733 | Not computing TPR and F1 | Elapsed: 0.32s
WARNING:root: [*] Sun Jan  1 14:24:45 2023: Train Epoch: 3 [12800/72319 (18%)]	Loss: 296.919098 | Not computing TPR and F1 | Elapsed: 15.47s
WARNING:root: [*] Sun Jan  1 14:25:01 2023: Train Epoch: 3 [25600/72319 (35%)]	Loss: 312.450073 | Not computing TPR and F1 | Elapsed: 15.95s
WARNING:root: [*] Sun Jan  1 14:25:17 2023: Train Epoch: 3 [38400/72319 (53%)]	Loss: 303.151306 | Not computing TPR and F1 | Elapsed: 15.89s
WARNING:root: [*] Sun Jan  1 14:25:33 2023: Train Epoch: 3 [51200/72319 (71%)]	Loss: 286.624390 | Not computing TPR and F1 | Elapsed: 15.87s
WARNING:root: [*] Sun Jan  1 14:25:49 2023: Train Epoch: 3 [64000/72319 (88%)]	Loss: 304.531647 | Not computing TPR and F1 | Elapsed: 15.59s
WARNING:root: [*] Sun Jan  1 14:25:59 2023:    3    | Tr.loss: 296.461363 | Not computing TPR and F1 | Elapsed:   89.00  s
WARNING:root: [*] Started epoch: 4
WARNING:root: [*] Sun Jan  1 14:25:59 2023: Train Epoch: 4 [  0  /72319 (0 %)]	Loss: 289.218842 | Not computing TPR and F1 | Elapsed: 0.34s
WARNING:root: [*] Sun Jan  1 14:26:15 2023: Train Epoch: 4 [12800/72319 (18%)]	Loss: 310.776215 | Not computing TPR and F1 | Elapsed: 15.85s
WARNING:root: [*] Sun Jan  1 14:26:30 2023: Train Epoch: 4 [25600/72319 (35%)]	Loss: 317.386536 | Not computing TPR and F1 | Elapsed: 15.70s
WARNING:root: [*] Sun Jan  1 14:26:46 2023: Train Epoch: 4 [38400/72319 (53%)]	Loss: 277.357666 | Not computing TPR and F1 | Elapsed: 15.83s
WARNING:root: [*] Sun Jan  1 14:27:02 2023: Train Epoch: 4 [51200/72319 (71%)]	Loss: 270.149536 | Not computing TPR and F1 | Elapsed: 16.04s
WARNING:root: [*] Sun Jan  1 14:27:18 2023: Train Epoch: 4 [64000/72319 (88%)]	Loss: 305.465088 | Not computing TPR and F1 | Elapsed: 15.83s
WARNING:root: [*] Sun Jan  1 14:27:28 2023:    4    | Tr.loss: 292.029188 | Not computing TPR and F1 | Elapsed:   89.56  s
WARNING:root: [*] Started epoch: 5
WARNING:root: [*] Sun Jan  1 14:27:29 2023: Train Epoch: 5 [  0  /72319 (0 %)]	Loss: 271.951813 | Not computing TPR and F1 | Elapsed: 0.35s
WARNING:root: [*] Sun Jan  1 14:27:44 2023: Train Epoch: 5 [12800/72319 (18%)]	Loss: 287.735107 | Not computing TPR and F1 | Elapsed: 15.62s
WARNING:root: [*] Sun Jan  1 14:28:00 2023: Train Epoch: 5 [25600/72319 (35%)]	Loss: 287.349548 | Not computing TPR and F1 | Elapsed: 15.85s
WARNING:root: [*] Sun Jan  1 14:28:16 2023: Train Epoch: 5 [38400/72319 (53%)]	Loss: 285.813568 | Not computing TPR and F1 | Elapsed: 15.82s
WARNING:root: [*] Sun Jan  1 14:28:31 2023: Train Epoch: 5 [51200/72319 (71%)]	Loss: 288.278595 | Not computing TPR and F1 | Elapsed: 15.67s
WARNING:root: [*] Sun Jan  1 14:28:47 2023: Train Epoch: 5 [64000/72319 (88%)]	Loss: 281.476135 | Not computing TPR and F1 | Elapsed: 15.54s
WARNING:root: [*] Sun Jan  1 14:28:57 2023:    5    | Tr.loss: 289.065007 | Not computing TPR and F1 | Elapsed:   88.76  s
WARNING:root:[!] Sun Jan  1 14:28:57 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672579737-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672579737-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672579737-trainTime.npy
WARNING:root: [!] Training pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:28:58 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.690246 | FPR 0.01 -- TPR 0.1222 | F1 0.2178 | Elapsed: 0.32s
WARNING:root: [*] Sun Jan  1 14:29:02 2023:    1    | Tr.loss: 0.496114 | FPR 0.01 -- TPR: 0.20 |  F1: 0.33 | Elapsed:   4.53   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:29:02 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.405648 | FPR 0.01 -- TPR 0.2733 | F1 0.4292 | Elapsed: 0.32s
WARNING:root: [*] Sun Jan  1 14:29:07 2023:    2    | Tr.loss: 0.346198 | FPR 0.01 -- TPR: 0.26 |  F1: 0.40 | Elapsed:   4.60   s
WARNING:root:[!] Sun Jan  1 14:29:07 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579747-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579747-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579747-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579747-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672579747-trainTPRs.npy
WARNING:root: [!] Training non_pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:29:07 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.690819 | FPR 0.01 -- TPR 0.0055 | F1 0.0110 | Elapsed: 0.28s
WARNING:root: [*] Sun Jan  1 14:29:11 2023:    1    | Tr.loss: 0.632186 | FPR 0.01 -- TPR: 0.05 |  F1: 0.10 | Elapsed:   4.23   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:29:11 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.579031 | FPR 0.01 -- TPR 0.1919 | F1 0.3220 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:29:15 2023:    2    | Tr.loss: 0.476928 | FPR 0.01 -- TPR: 0.18 |  F1: 0.29 | Elapsed:   4.24   s
WARNING:root:[!] Sun Jan  1 14:29:15 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579755-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579755-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579755-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579755-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672579755-trainTPRs.npy
WARNING:root: [!] Training full_data model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:29:16 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 0.686979 | FPR 0.01 -- TPR 0.0179 | F1 0.0351 | Elapsed: 0.32s
WARNING:root: [*] Sun Jan  1 14:29:30 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.173951 | FPR 0.01 -- TPR 0.6784 | F1 0.8084 | Elapsed: 14.32s
WARNING:root: [*] Sun Jan  1 14:29:45 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.114707 | FPR 0.01 -- TPR 0.9301 | F1 0.9638 | Elapsed: 14.32s
WARNING:root: [*] Sun Jan  1 14:29:59 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.121879 | FPR 0.01 -- TPR 0.9489 | F1 0.9738 | Elapsed: 14.36s
WARNING:root: [*] Sun Jan  1 14:30:13 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.079621 | FPR 0.01 -- TPR 0.8772 | F1 0.9346 | Elapsed: 14.37s
WARNING:root: [*] Sun Jan  1 14:30:28 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.034476 | FPR 0.01 -- TPR 0.9942 | F1 0.9971 | Elapsed: 14.30s
WARNING:root: [*] Sun Jan  1 14:30:41 2023:    1    | Tr.loss: 0.148367 | FPR 0.01 -- TPR: 0.80 |  F1: 0.85 | Elapsed:   85.16  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:30:41 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.050040 | FPR 0.01 -- TPR 0.8757 | F1 0.9337 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:30:55 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.081080 | FPR 0.01 -- TPR 0.8706 | F1 0.9308 | Elapsed: 14.19s
WARNING:root: [*] Sun Jan  1 14:31:09 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.060192 | FPR 0.01 -- TPR 0.9396 | F1 0.9688 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:31:24 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.058419 | FPR 0.01 -- TPR 0.9392 | F1 0.9687 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:31:38 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.058758 | FPR 0.01 -- TPR 0.9565 | F1 0.9778 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:31:52 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.050803 | FPR 0.01 -- TPR 0.9613 | F1 0.9803 | Elapsed: 14.22s
WARNING:root: [*] Sun Jan  1 14:32:05 2023:    2    | Tr.loss: 0.060788 | FPR 0.01 -- TPR: 0.94 |  F1: 0.97 | Elapsed:   84.51  s
WARNING:root:[!] Sun Jan  1 14:32:05 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579925-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579925-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579925-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579925-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672579925-trainTPRs.npy
WARNING:root: [*] Evaluating pretrained model on test set...
WARNING:root: [!] Test F1 score for pretrained model at 0.01 FPR : 0.4474
WARNING:root: [!] Test TPR score for pretrained model at 0.01 FPR: 0.2955
WARNING:root: [*] Evaluating non_pretrained model on test set...
WARNING:root: [!] Test F1 score for non_pretrained model at 0.01 FPR : 0.5200
WARNING:root: [!] Test TPR score for non_pretrained model at 0.01 FPR: 0.3656
WARNING:root: [*] Evaluating full_data model on test set...
WARNING:root: [!] Test F1 score for full_data model at 0.01 FPR : 0.8801
WARNING:root: [!] Test TPR score for full_data model at 0.01 FPR: 0.7936
WARNING:root: [!] Running pre-training split 4/5
WARNING:root: [!] Pre-training model...
WARNING:root: [*] Masking sequences...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:33:18 2023: Train Epoch: 1 [  0  /72319 (0 %)]	Loss: 483.163635 | Not computing TPR and F1 | Elapsed: 1.04s
WARNING:root: [*] Sun Jan  1 14:33:34 2023: Train Epoch: 1 [12800/72319 (18%)]	Loss: 364.778320 | Not computing TPR and F1 | Elapsed: 15.71s
WARNING:root: [*] Sun Jan  1 14:33:49 2023: Train Epoch: 1 [25600/72319 (35%)]	Loss: 361.907990 | Not computing TPR and F1 | Elapsed: 15.39s
WARNING:root: [*] Sun Jan  1 14:34:05 2023: Train Epoch: 1 [38400/72319 (53%)]	Loss: 339.752991 | Not computing TPR and F1 | Elapsed: 15.32s
WARNING:root: [*] Sun Jan  1 14:34:20 2023: Train Epoch: 1 [51200/72319 (71%)]	Loss: 295.183533 | Not computing TPR and F1 | Elapsed: 15.31s
WARNING:root: [*] Sun Jan  1 14:34:35 2023: Train Epoch: 1 [64000/72319 (88%)]	Loss: 297.309998 | Not computing TPR and F1 | Elapsed: 15.39s
WARNING:root: [*] Sun Jan  1 14:34:45 2023:    1    | Tr.loss: 344.965996 | Not computing TPR and F1 | Elapsed:   87.98  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:34:45 2023: Train Epoch: 2 [  0  /72319 (0 %)]	Loss: 320.216736 | Not computing TPR and F1 | Elapsed: 0.34s
WARNING:root: [*] Sun Jan  1 14:35:01 2023: Train Epoch: 2 [12800/72319 (18%)]	Loss: 302.543152 | Not computing TPR and F1 | Elapsed: 15.63s
WARNING:root: [*] Sun Jan  1 14:35:17 2023: Train Epoch: 2 [25600/72319 (35%)]	Loss: 315.476288 | Not computing TPR and F1 | Elapsed: 15.76s
WARNING:root: [*] Sun Jan  1 14:35:33 2023: Train Epoch: 2 [38400/72319 (53%)]	Loss: 296.160614 | Not computing TPR and F1 | Elapsed: 16.07s
WARNING:root: [*] Sun Jan  1 14:35:49 2023: Train Epoch: 2 [51200/72319 (71%)]	Loss: 303.629547 | Not computing TPR and F1 | Elapsed: 15.73s
WARNING:root: [*] Sun Jan  1 14:36:04 2023: Train Epoch: 2 [64000/72319 (88%)]	Loss: 287.099884 | Not computing TPR and F1 | Elapsed: 15.30s
WARNING:root: [*] Sun Jan  1 14:36:14 2023:    2    | Tr.loss: 299.924797 | Not computing TPR and F1 | Elapsed:   88.70  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sun Jan  1 14:36:14 2023: Train Epoch: 3 [  0  /72319 (0 %)]	Loss: 294.726562 | Not computing TPR and F1 | Elapsed: 0.35s
WARNING:root: [*] Sun Jan  1 14:36:30 2023: Train Epoch: 3 [12800/72319 (18%)]	Loss: 305.294983 | Not computing TPR and F1 | Elapsed: 15.46s
WARNING:root: [*] Sun Jan  1 14:36:45 2023: Train Epoch: 3 [25600/72319 (35%)]	Loss: 291.958771 | Not computing TPR and F1 | Elapsed: 15.66s
WARNING:root: [*] Sun Jan  1 14:37:01 2023: Train Epoch: 3 [38400/72319 (53%)]	Loss: 292.807739 | Not computing TPR and F1 | Elapsed: 15.65s
WARNING:root: [*] Sun Jan  1 14:37:16 2023: Train Epoch: 3 [51200/72319 (71%)]	Loss: 289.866577 | Not computing TPR and F1 | Elapsed: 15.55s
WARNING:root: [*] Sun Jan  1 14:37:32 2023: Train Epoch: 3 [64000/72319 (88%)]	Loss: 292.957947 | Not computing TPR and F1 | Elapsed: 15.71s
WARNING:root: [*] Sun Jan  1 14:37:42 2023:    3    | Tr.loss: 291.788250 | Not computing TPR and F1 | Elapsed:   88.44  s
WARNING:root: [*] Started epoch: 4
WARNING:root: [*] Sun Jan  1 14:37:43 2023: Train Epoch: 4 [  0  /72319 (0 %)]	Loss: 293.107971 | Not computing TPR and F1 | Elapsed: 0.35s
WARNING:root: [*] Sun Jan  1 14:37:58 2023: Train Epoch: 4 [12800/72319 (18%)]	Loss: 285.050079 | Not computing TPR and F1 | Elapsed: 15.82s
WARNING:root: [*] Sun Jan  1 14:38:14 2023: Train Epoch: 4 [25600/72319 (35%)]	Loss: 257.046082 | Not computing TPR and F1 | Elapsed: 15.79s
WARNING:root: [*] Sun Jan  1 14:38:30 2023: Train Epoch: 4 [38400/72319 (53%)]	Loss: 263.994202 | Not computing TPR and F1 | Elapsed: 15.63s
WARNING:root: [*] Sun Jan  1 14:38:46 2023: Train Epoch: 4 [51200/72319 (71%)]	Loss: 291.838013 | Not computing TPR and F1 | Elapsed: 15.70s
WARNING:root: [*] Sun Jan  1 14:39:01 2023: Train Epoch: 4 [64000/72319 (88%)]	Loss: 277.516113 | Not computing TPR and F1 | Elapsed: 15.69s
WARNING:root: [*] Sun Jan  1 14:39:11 2023:    4    | Tr.loss: 287.386394 | Not computing TPR and F1 | Elapsed:   88.82  s
WARNING:root: [*] Started epoch: 5
WARNING:root: [*] Sun Jan  1 14:39:11 2023: Train Epoch: 5 [  0  /72319 (0 %)]	Loss: 288.675262 | Not computing TPR and F1 | Elapsed: 0.39s
WARNING:root: [*] Sun Jan  1 14:39:27 2023: Train Epoch: 5 [12800/72319 (18%)]	Loss: 281.077881 | Not computing TPR and F1 | Elapsed: 15.71s
WARNING:root: [*] Sun Jan  1 14:39:43 2023: Train Epoch: 5 [25600/72319 (35%)]	Loss: 301.770935 | Not computing TPR and F1 | Elapsed: 15.79s
WARNING:root: [*] Sun Jan  1 14:39:59 2023: Train Epoch: 5 [38400/72319 (53%)]	Loss: 285.064697 | Not computing TPR and F1 | Elapsed: 15.73s
WARNING:root: [*] Sun Jan  1 14:40:14 2023: Train Epoch: 5 [51200/72319 (71%)]	Loss: 304.724213 | Not computing TPR and F1 | Elapsed: 15.76s
WARNING:root: [*] Sun Jan  1 14:40:30 2023: Train Epoch: 5 [64000/72319 (88%)]	Loss: 270.443451 | Not computing TPR and F1 | Elapsed: 15.75s
WARNING:root: [*] Sun Jan  1 14:40:40 2023:    5    | Tr.loss: 284.567882 | Not computing TPR and F1 | Elapsed:   89.14  s
WARNING:root:[!] Sun Jan  1 14:40:40 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672580440-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672580440-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672580440-trainTime.npy
WARNING:root: [!] Training pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:40:41 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.695242 | FPR 0.01 -- TPR 0.0756 | F1 0.1405 | Elapsed: 0.33s
WARNING:root: [*] Sun Jan  1 14:40:45 2023:    1    | Tr.loss: 0.496385 | FPR 0.01 -- TPR: 0.18 |  F1: 0.30 | Elapsed:   4.55   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:40:46 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.343028 | FPR 0.01 -- TPR 0.3855 | F1 0.5565 | Elapsed: 0.32s
WARNING:root: [*] Sun Jan  1 14:40:50 2023:    2    | Tr.loss: 0.336015 | FPR 0.01 -- TPR: 0.31 |  F1: 0.47 | Elapsed:   4.65   s
WARNING:root:[!] Sun Jan  1 14:40:50 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672580450-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672580450-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672580450-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672580450-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672580450-trainTPRs.npy
WARNING:root: [!] Training non_pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:40:50 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.698427 | FPR 0.01 -- TPR 0.0000 | F1 0.0000 | Elapsed: 0.30s
WARNING:root: [*] Sun Jan  1 14:40:54 2023:    1    | Tr.loss: 0.627614 | FPR 0.01 -- TPR: 0.03 |  F1: 0.05 | Elapsed:   4.27   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:40:55 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.571529 | FPR 0.01 -- TPR 0.1299 | F1 0.2300 | Elapsed: 0.30s
WARNING:root: [*] Sun Jan  1 14:40:59 2023:    2    | Tr.loss: 0.505839 | FPR 0.01 -- TPR: 0.19 |  F1: 0.30 | Elapsed:   4.24   s
WARNING:root:[!] Sun Jan  1 14:40:59 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672580459-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672580459-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672580459-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672580459-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672580459-trainTPRs.npy
WARNING:root: [!] Training full_data model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:40:59 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 0.702702 | FPR 0.01 -- TPR 0.0000 | F1 0.0000 | Elapsed: 0.30s
WARNING:root: [*] Sun Jan  1 14:41:14 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.212907 | FPR 0.01 -- TPR 0.7329 | F1 0.8459 | Elapsed: 14.22s
WARNING:root: [*] Sun Jan  1 14:41:28 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.116112 | FPR 0.01 -- TPR 0.7440 | F1 0.8532 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:41:42 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.058741 | FPR 0.01 -- TPR 0.9699 | F1 0.9847 | Elapsed: 14.30s
WARNING:root: [*] Sun Jan  1 14:41:56 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.060557 | FPR 0.01 -- TPR 0.9730 | F1 0.9863 | Elapsed: 14.21s
WARNING:root: [*] Sun Jan  1 14:42:10 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.033136 | FPR 0.01 -- TPR 0.9709 | F1 0.9853 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:42:24 2023:    1    | Tr.loss: 0.160747 | FPR 0.01 -- TPR: 0.78 |  F1: 0.84 | Elapsed:   84.61  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:42:24 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.085048 | FPR 0.01 -- TPR 0.8639 | F1 0.9270 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:42:38 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.065363 | FPR 0.01 -- TPR 0.9602 | F1 0.9797 | Elapsed: 14.19s
WARNING:root: [*] Sun Jan  1 14:42:52 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.051632 | FPR 0.01 -- TPR 0.9721 | F1 0.9858 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:43:06 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.052399 | FPR 0.01 -- TPR 0.9598 | F1 0.9795 | Elapsed: 14.21s
WARNING:root: [*] Sun Jan  1 14:43:21 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.043634 | FPR 0.01 -- TPR 0.9239 | F1 0.9605 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:43:35 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.028422 | FPR 0.01 -- TPR 0.9884 | F1 0.9942 | Elapsed: 14.19s
WARNING:root: [*] Sun Jan  1 14:43:48 2023:    2    | Tr.loss: 0.061204 | FPR 0.01 -- TPR: 0.93 |  F1: 0.96 | Elapsed:   84.46  s
WARNING:root:[!] Sun Jan  1 14:43:48 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672580628-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672580628-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672580628-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672580628-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672580628-trainTPRs.npy
WARNING:root: [*] Evaluating pretrained model on test set...
WARNING:root: [!] Test F1 score for pretrained model at 0.01 FPR : 0.5446
WARNING:root: [!] Test TPR score for pretrained model at 0.01 FPR: 0.3852
WARNING:root: [*] Evaluating non_pretrained model on test set...
WARNING:root: [!] Test F1 score for non_pretrained model at 0.01 FPR : 0.4513
WARNING:root: [!] Test TPR score for non_pretrained model at 0.01 FPR: 0.3040
WARNING:root: [*] Evaluating full_data model on test set...
WARNING:root: [!] Test F1 score for full_data model at 0.01 FPR : 0.8605
WARNING:root: [!] Test TPR score for full_data model at 0.01 FPR: 0.7628
WARNING:root: [!] Running pre-training split 5/5
WARNING:root: [!] Pre-training model...
WARNING:root: [*] Masking sequences...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:45:01 2023: Train Epoch: 1 [  0  /72319 (0 %)]	Loss: 497.767731 | Not computing TPR and F1 | Elapsed: 1.05s
WARNING:root: [*] Sun Jan  1 14:45:17 2023: Train Epoch: 1 [12800/72319 (18%)]	Loss: 394.124115 | Not computing TPR and F1 | Elapsed: 15.58s
WARNING:root: [*] Sun Jan  1 14:45:32 2023: Train Epoch: 1 [25600/72319 (35%)]	Loss: 338.273865 | Not computing TPR and F1 | Elapsed: 15.49s
WARNING:root: [*] Sun Jan  1 14:45:47 2023: Train Epoch: 1 [38400/72319 (53%)]	Loss: 324.059357 | Not computing TPR and F1 | Elapsed: 15.30s
WARNING:root: [*] Sun Jan  1 14:46:03 2023: Train Epoch: 1 [51200/72319 (71%)]	Loss: 323.423431 | Not computing TPR and F1 | Elapsed: 15.23s
WARNING:root: [*] Sun Jan  1 14:46:18 2023: Train Epoch: 1 [64000/72319 (88%)]	Loss: 326.266937 | Not computing TPR and F1 | Elapsed: 15.25s
WARNING:root: [*] Sun Jan  1 14:46:28 2023:    1    | Tr.loss: 348.851515 | Not computing TPR and F1 | Elapsed:   87.57  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:46:28 2023: Train Epoch: 2 [  0  /72319 (0 %)]	Loss: 315.125458 | Not computing TPR and F1 | Elapsed: 0.34s
WARNING:root: [*] Sun Jan  1 14:46:43 2023: Train Epoch: 2 [12800/72319 (18%)]	Loss: 301.255951 | Not computing TPR and F1 | Elapsed: 15.58s
WARNING:root: [*] Sun Jan  1 14:46:59 2023: Train Epoch: 2 [25600/72319 (35%)]	Loss: 297.554596 | Not computing TPR and F1 | Elapsed: 15.55s
WARNING:root: [*] Sun Jan  1 14:47:14 2023: Train Epoch: 2 [38400/72319 (53%)]	Loss: 303.626923 | Not computing TPR and F1 | Elapsed: 15.40s
WARNING:root: [*] Sun Jan  1 14:47:30 2023: Train Epoch: 2 [51200/72319 (71%)]	Loss: 304.718628 | Not computing TPR and F1 | Elapsed: 15.36s
WARNING:root: [*] Sun Jan  1 14:47:46 2023: Train Epoch: 2 [64000/72319 (88%)]	Loss: 276.420624 | Not computing TPR and F1 | Elapsed: 15.95s
WARNING:root: [*] Sun Jan  1 14:47:56 2023:    2    | Tr.loss: 302.123331 | Not computing TPR and F1 | Elapsed:   87.97  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sun Jan  1 14:47:56 2023: Train Epoch: 3 [  0  /72319 (0 %)]	Loss: 268.560303 | Not computing TPR and F1 | Elapsed: 0.31s
WARNING:root: [*] Sun Jan  1 14:48:11 2023: Train Epoch: 3 [12800/72319 (18%)]	Loss: 317.243652 | Not computing TPR and F1 | Elapsed: 15.64s
WARNING:root: [*] Sun Jan  1 14:48:27 2023: Train Epoch: 3 [25600/72319 (35%)]	Loss: 297.411224 | Not computing TPR and F1 | Elapsed: 15.44s
WARNING:root: [*] Sun Jan  1 14:48:43 2023: Train Epoch: 3 [38400/72319 (53%)]	Loss: 288.593689 | Not computing TPR and F1 | Elapsed: 15.69s
WARNING:root: [*] Sun Jan  1 14:48:58 2023: Train Epoch: 3 [51200/72319 (71%)]	Loss: 293.233582 | Not computing TPR and F1 | Elapsed: 15.50s
WARNING:root: [*] Sun Jan  1 14:49:13 2023: Train Epoch: 3 [64000/72319 (88%)]	Loss: 295.236755 | Not computing TPR and F1 | Elapsed: 15.33s
WARNING:root: [*] Sun Jan  1 14:49:23 2023:    3    | Tr.loss: 293.057162 | Not computing TPR and F1 | Elapsed:   87.57  s
WARNING:root: [*] Started epoch: 4
WARNING:root: [*] Sun Jan  1 14:49:23 2023: Train Epoch: 4 [  0  /72319 (0 %)]	Loss: 289.602722 | Not computing TPR and F1 | Elapsed: 0.32s
WARNING:root: [*] Sun Jan  1 14:49:39 2023: Train Epoch: 4 [12800/72319 (18%)]	Loss: 271.287323 | Not computing TPR and F1 | Elapsed: 15.22s
WARNING:root: [*] Sun Jan  1 14:49:54 2023: Train Epoch: 4 [25600/72319 (35%)]	Loss: 293.159424 | Not computing TPR and F1 | Elapsed: 15.47s
WARNING:root: [*] Sun Jan  1 14:50:10 2023: Train Epoch: 4 [38400/72319 (53%)]	Loss: 299.099182 | Not computing TPR and F1 | Elapsed: 15.57s
WARNING:root: [*] Sun Jan  1 14:50:25 2023: Train Epoch: 4 [51200/72319 (71%)]	Loss: 286.329926 | Not computing TPR and F1 | Elapsed: 15.73s
WARNING:root: [*] Sun Jan  1 14:50:41 2023: Train Epoch: 4 [64000/72319 (88%)]	Loss: 284.010864 | Not computing TPR and F1 | Elapsed: 15.58s
WARNING:root: [*] Sun Jan  1 14:50:51 2023:    4    | Tr.loss: 288.782006 | Not computing TPR and F1 | Elapsed:   87.96  s
WARNING:root: [*] Started epoch: 5
WARNING:root: [*] Sun Jan  1 14:50:51 2023: Train Epoch: 5 [  0  /72319 (0 %)]	Loss: 287.885254 | Not computing TPR and F1 | Elapsed: 0.34s
WARNING:root: [*] Sun Jan  1 14:51:07 2023: Train Epoch: 5 [12800/72319 (18%)]	Loss: 279.503662 | Not computing TPR and F1 | Elapsed: 15.81s
WARNING:root: [*] Sun Jan  1 14:51:23 2023: Train Epoch: 5 [25600/72319 (35%)]	Loss: 287.593140 | Not computing TPR and F1 | Elapsed: 15.69s
WARNING:root: [*] Sun Jan  1 14:51:39 2023: Train Epoch: 5 [38400/72319 (53%)]	Loss: 286.036865 | Not computing TPR and F1 | Elapsed: 15.69s
WARNING:root: [*] Sun Jan  1 14:51:54 2023: Train Epoch: 5 [51200/72319 (71%)]	Loss: 283.852631 | Not computing TPR and F1 | Elapsed: 15.74s
WARNING:root: [*] Sun Jan  1 14:52:10 2023: Train Epoch: 5 [64000/72319 (88%)]	Loss: 284.802734 | Not computing TPR and F1 | Elapsed: 15.58s
WARNING:root: [*] Sun Jan  1 14:52:20 2023:    5    | Tr.loss: 285.952431 | Not computing TPR and F1 | Elapsed:   88.99  s
WARNING:root:[!] Sun Jan  1 14:52:20 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672581140-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672581140-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\preTraining\trainingFiles_1672581140-trainTime.npy
WARNING:root: [!] Training pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:52:21 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.680773 | FPR 0.01 -- TPR 0.0651 | F1 0.1222 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:52:25 2023:    1    | Tr.loss: 0.497463 | FPR 0.01 -- TPR: 0.13 |  F1: 0.21 | Elapsed:   4.50   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:52:25 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.392163 | FPR 0.01 -- TPR 0.2500 | F1 0.4000 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:52:30 2023:    2    | Tr.loss: 0.344870 | FPR 0.01 -- TPR: 0.24 |  F1: 0.38 | Elapsed:   4.47   s
WARNING:root:[!] Sun Jan  1 14:52:30 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672581150-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672581150-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672581150-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672581150-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_pretrained\trainingFiles_1672581150-trainTPRs.npy
WARNING:root: [!] Training non_pretrained model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:52:30 2023: Train Epoch: 1 [  0  /3807  (0 %)]	Loss: 0.668487 | FPR 0.01 -- TPR 0.0058 | F1 0.0116 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:52:34 2023:    1    | Tr.loss: 0.633202 | FPR 0.01 -- TPR: 0.04 |  F1: 0.08 | Elapsed:   4.23   s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:52:34 2023: Train Epoch: 2 [  0  /3807  (0 %)]	Loss: 0.607278 | FPR 0.01 -- TPR 0.0175 | F1 0.0345 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:52:38 2023:    2    | Tr.loss: 0.502528 | FPR 0.01 -- TPR: 0.15 |  F1: 0.25 | Elapsed:   4.28   s
WARNING:root:[!] Sun Jan  1 14:52:38 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672581158-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672581158-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672581158-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672581158-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_non_pretrained\trainingFiles_1672581158-trainTPRs.npy
WARNING:root: [!] Training full_data model on downstream task...
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  1 14:52:39 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 0.695958 | FPR 0.01 -- TPR 0.0125 | F1 0.0247 | Elapsed: 0.32s
WARNING:root: [*] Sun Jan  1 14:52:53 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.137982 | FPR 0.01 -- TPR 0.8343 | F1 0.9097 | Elapsed: 14.32s
WARNING:root: [*] Sun Jan  1 14:53:07 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.110106 | FPR 0.01 -- TPR 0.9515 | F1 0.9752 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:53:22 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.100499 | FPR 0.01 -- TPR 0.8218 | F1 0.9022 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:53:36 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.087463 | FPR 0.01 -- TPR 0.9177 | F1 0.9571 | Elapsed: 14.23s
WARNING:root: [*] Sun Jan  1 14:53:50 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.085559 | FPR 0.01 -- TPR 0.9383 | F1 0.9682 | Elapsed: 14.28s
WARNING:root: [*] Sun Jan  1 14:54:03 2023:    1    | Tr.loss: 0.155813 | FPR 0.01 -- TPR: 0.78 |  F1: 0.84 | Elapsed:   84.84  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  1 14:54:04 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.076629 | FPR 0.01 -- TPR 0.9783 | F1 0.9890 | Elapsed: 0.29s
WARNING:root: [*] Sun Jan  1 14:54:18 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.056221 | FPR 0.01 -- TPR 0.9758 | F1 0.9877 | Elapsed: 14.19s
WARNING:root: [*] Sun Jan  1 14:54:32 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.031398 | FPR 0.01 -- TPR 0.9890 | F1 0.9945 | Elapsed: 14.21s
WARNING:root: [*] Sun Jan  1 14:54:46 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.036847 | FPR 0.01 -- TPR 0.9885 | F1 0.9942 | Elapsed: 14.29s
WARNING:root: [*] Sun Jan  1 14:55:01 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.060943 | FPR 0.01 -- TPR 0.8721 | F1 0.9317 | Elapsed: 14.20s
WARNING:root: [*] Sun Jan  1 14:55:15 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.099114 | FPR 0.01 -- TPR 0.8274 | F1 0.9055 | Elapsed: 14.38s
WARNING:root: [*] Sun Jan  1 14:55:28 2023:    2    | Tr.loss: 0.060875 | FPR 0.01 -- TPR: 0.94 |  F1: 0.96 | Elapsed:   84.83  s
WARNING:root:[!] Sun Jan  1 14:55:28 2023: Dumped results:
                model     : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672581328-model.torch
                losses    : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672581328-train_losses.npy
                duration  : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672581328-trainTime.npy
		train F1s : evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672581328-trainF1s.npy
		train TPRs: evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833\downstreamTask_full_data\trainingFiles_1672581328-trainTPRs.npy
WARNING:root: [*] Evaluating pretrained model on test set...
WARNING:root: [!] Test F1 score for pretrained model at 0.01 FPR : 0.5186
WARNING:root: [!] Test TPR score for pretrained model at 0.01 FPR: 0.3533
WARNING:root: [*] Evaluating non_pretrained model on test set...
WARNING:root: [!] Test F1 score for non_pretrained model at 0.01 FPR : 0.5037
WARNING:root: [!] Test TPR score for non_pretrained model at 0.01 FPR: 0.3521
WARNING:root: [*] Evaluating full_data model on test set...
WARNING:root: [!] Test F1 score for full_data model at 0.01 FPR : 0.8720
WARNING:root: [!] Test TPR score for full_data model at 0.01 FPR: 0.7803
WARNING:root: [!] Finished pre-training evaluation over 5 splits! Saved metrics to:
	evaluation\MaskedLanguageModeling\unlabeledDataSize_0.95_preTrain_5_downStream_2_nSplits_5_1672577833/metrics_MaskedLanguageModel_nSplits_5_limit_None.json
