01/28/2023 10:54:13 AM  [!] Starting Masked Language Model evaluation over 3 splits!
01/28/2023 10:54:13 AM  [!] Loaded data and vocab. X train size: (76126, 512), X test size: (17407, 512), vocab size: 50002
01/28/2023 10:54:13 AM  [!] Initiating Self-Supervised Learning Pretraining
     Language Modeling MaskedLanguageModel
     Model TransformerEncoderLM with config:
	{'vocabSize': 50002, 'maxLen': 512, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'dropout': 0.3}

01/28/2023 10:54:13 AM  [!] Running pre-training split 1/3
01/28/2023 10:54:16 AM  [!] Pre-training model...
01/28/2023 10:54:16 AM  [*] Masking sequences...
01/28/2023 10:54:32 AM  [*] Started epoch: 1
01/28/2023 10:54:34 AM  [*] Sat Jan 28 10:54:34 2023: Train Epoch: 1 [  0  /53288 (0 %)]	Loss: 412.843567 | Elapsed: 2.28s
01/28/2023 10:54:46 AM  [*] Sat Jan 28 10:54:46 2023: Train Epoch: 1 [6400 /53288 (12%)]	Loss: 212.443954 | Elapsed: 11.66s
01/28/2023 10:54:58 AM  [*] Sat Jan 28 10:54:58 2023: Train Epoch: 1 [12800/53288 (24%)]	Loss: 223.312286 | Elapsed: 12.17s
01/28/2023 10:55:11 AM  [*] Sat Jan 28 10:55:11 2023: Train Epoch: 1 [19200/53288 (36%)]	Loss: 210.177887 | Elapsed: 12.52s
01/28/2023 10:55:23 AM  [*] Sat Jan 28 10:55:23 2023: Train Epoch: 1 [25600/53288 (48%)]	Loss: 196.672455 | Elapsed: 12.61s
01/28/2023 10:55:36 AM  [*] Sat Jan 28 10:55:36 2023: Train Epoch: 1 [32000/53288 (60%)]	Loss: 182.938309 | Elapsed: 12.61s
01/28/2023 10:55:48 AM  [*] Sat Jan 28 10:55:48 2023: Train Epoch: 1 [38400/53288 (72%)]	Loss: 194.855072 | Elapsed: 12.63s
01/28/2023 10:56:01 AM  [*] Sat Jan 28 10:56:01 2023: Train Epoch: 1 [44800/53288 (84%)]	Loss: 187.755493 | Elapsed: 12.63s
01/28/2023 10:56:14 AM  [*] Sat Jan 28 10:56:14 2023: Train Epoch: 1 [51200/53288 (96%)]	Loss: 186.955383 | Elapsed: 12.71s
01/28/2023 10:56:19 AM  [*] Sat Jan 28 10:56:19 2023:    1    | Tr.loss: 210.963897 | Elapsed:  107.31  s
01/28/2023 10:56:19 AM  [*] Started epoch: 2
01/28/2023 10:56:19 AM  [*] Sat Jan 28 10:56:19 2023: Train Epoch: 2 [  0  /53288 (0 %)]	Loss: 187.930847 | Elapsed: 0.14s
01/28/2023 10:56:32 AM  [*] Sat Jan 28 10:56:32 2023: Train Epoch: 2 [6400 /53288 (12%)]	Loss: 199.672012 | Elapsed: 12.45s
01/28/2023 10:56:44 AM  [*] Sat Jan 28 10:56:44 2023: Train Epoch: 2 [12800/53288 (24%)]	Loss: 185.355194 | Elapsed: 12.36s
01/28/2023 10:56:57 AM  [*] Sat Jan 28 10:56:57 2023: Train Epoch: 2 [19200/53288 (36%)]	Loss: 202.900452 | Elapsed: 12.51s
01/28/2023 10:57:09 AM  [*] Sat Jan 28 10:57:09 2023: Train Epoch: 2 [25600/53288 (48%)]	Loss: 199.501892 | Elapsed: 12.42s
01/28/2023 10:57:22 AM  [*] Sat Jan 28 10:57:22 2023: Train Epoch: 2 [32000/53288 (60%)]	Loss: 182.359314 | Elapsed: 12.43s
01/28/2023 10:57:34 AM  [*] Sat Jan 28 10:57:34 2023: Train Epoch: 2 [38400/53288 (72%)]	Loss: 180.278702 | Elapsed: 12.55s
01/28/2023 10:57:47 AM  [*] Sat Jan 28 10:57:47 2023: Train Epoch: 2 [44800/53288 (84%)]	Loss: 193.087006 | Elapsed: 12.41s
01/28/2023 10:57:59 AM  [*] Sat Jan 28 10:57:59 2023: Train Epoch: 2 [51200/53288 (96%)]	Loss: 188.444946 | Elapsed: 12.48s
01/28/2023 10:58:05 AM  [*] Sat Jan 28 10:58:05 2023:    2    | Tr.loss: 190.469572 | Elapsed:  105.35  s
01/28/2023 10:58:05 AM  [*] Started epoch: 3
01/28/2023 10:58:05 AM  [*] Sat Jan 28 10:58:05 2023: Train Epoch: 3 [  0  /53288 (0 %)]	Loss: 193.641022 | Elapsed: 0.14s
01/28/2023 10:58:17 AM  [*] Sat Jan 28 10:58:17 2023: Train Epoch: 3 [6400 /53288 (12%)]	Loss: 199.730133 | Elapsed: 12.47s
01/28/2023 10:58:30 AM  [*] Sat Jan 28 10:58:30 2023: Train Epoch: 3 [12800/53288 (24%)]	Loss: 182.574295 | Elapsed: 12.46s
01/28/2023 10:58:42 AM  [*] Sat Jan 28 10:58:42 2023: Train Epoch: 3 [19200/53288 (36%)]	Loss: 173.859634 | Elapsed: 12.42s
01/28/2023 10:58:55 AM  [*] Sat Jan 28 10:58:55 2023: Train Epoch: 3 [25600/53288 (48%)]	Loss: 188.676208 | Elapsed: 12.50s
01/28/2023 10:59:07 AM  [*] Sat Jan 28 10:59:07 2023: Train Epoch: 3 [32000/53288 (60%)]	Loss: 194.819229 | Elapsed: 12.47s
01/28/2023 10:59:20 AM  [*] Sat Jan 28 10:59:20 2023: Train Epoch: 3 [38400/53288 (72%)]	Loss: 202.258087 | Elapsed: 12.49s
01/28/2023 10:59:32 AM  [*] Sat Jan 28 10:59:32 2023: Train Epoch: 3 [44800/53288 (84%)]	Loss: 163.892303 | Elapsed: 12.46s
01/28/2023 10:59:45 AM  [*] Sat Jan 28 10:59:45 2023: Train Epoch: 3 [51200/53288 (96%)]	Loss: 181.195602 | Elapsed: 12.55s
01/28/2023 10:59:50 AM  [*] Sat Jan 28 10:59:50 2023:    3    | Tr.loss: 185.250428 | Elapsed:  105.32  s
01/28/2023 10:59:50 AM  [*] Started epoch: 4
01/28/2023 10:59:50 AM  [*] Sat Jan 28 10:59:50 2023: Train Epoch: 4 [  0  /53288 (0 %)]	Loss: 176.674286 | Elapsed: 0.12s
01/28/2023 11:00:03 AM  [*] Sat Jan 28 11:00:03 2023: Train Epoch: 4 [6400 /53288 (12%)]	Loss: 185.875549 | Elapsed: 12.54s
01/28/2023 11:00:15 AM  [*] Sat Jan 28 11:00:15 2023: Train Epoch: 4 [12800/53288 (24%)]	Loss: 177.442184 | Elapsed: 12.67s
01/28/2023 11:00:28 AM  [*] Sat Jan 28 11:00:28 2023: Train Epoch: 4 [19200/53288 (36%)]	Loss: 173.568680 | Elapsed: 12.61s
01/28/2023 11:00:40 AM  [*] Sat Jan 28 11:00:40 2023: Train Epoch: 4 [25600/53288 (48%)]	Loss: 171.012207 | Elapsed: 12.42s
01/28/2023 11:00:53 AM  [*] Sat Jan 28 11:00:53 2023: Train Epoch: 4 [32000/53288 (60%)]	Loss: 177.297455 | Elapsed: 12.41s
01/28/2023 11:01:05 AM  [*] Sat Jan 28 11:01:05 2023: Train Epoch: 4 [38400/53288 (72%)]	Loss: 177.860168 | Elapsed: 12.41s
01/28/2023 11:01:18 AM  [*] Sat Jan 28 11:01:18 2023: Train Epoch: 4 [44800/53288 (84%)]	Loss: 176.514328 | Elapsed: 12.44s
01/28/2023 11:01:30 AM  [*] Sat Jan 28 11:01:30 2023: Train Epoch: 4 [51200/53288 (96%)]	Loss: 188.428329 | Elapsed: 12.47s
01/28/2023 11:01:35 AM  [*] Sat Jan 28 11:01:35 2023:    4    | Tr.loss: 182.500657 | Elapsed:  105.45  s
01/28/2023 11:01:35 AM  [*] Started epoch: 5
01/28/2023 11:01:36 AM  [*] Sat Jan 28 11:01:36 2023: Train Epoch: 5 [  0  /53288 (0 %)]	Loss: 191.630920 | Elapsed: 0.14s
01/28/2023 11:01:48 AM  [*] Sat Jan 28 11:01:48 2023: Train Epoch: 5 [6400 /53288 (12%)]	Loss: 185.934296 | Elapsed: 12.45s
01/28/2023 11:02:01 AM  [*] Sat Jan 28 11:02:01 2023: Train Epoch: 5 [12800/53288 (24%)]	Loss: 171.665985 | Elapsed: 12.49s
01/28/2023 11:02:13 AM  [*] Sat Jan 28 11:02:13 2023: Train Epoch: 5 [19200/53288 (36%)]	Loss: 156.562775 | Elapsed: 12.43s
01/28/2023 11:02:25 AM  [*] Sat Jan 28 11:02:25 2023: Train Epoch: 5 [25600/53288 (48%)]	Loss: 183.864868 | Elapsed: 12.44s
01/28/2023 11:02:38 AM  [*] Sat Jan 28 11:02:38 2023: Train Epoch: 5 [32000/53288 (60%)]	Loss: 175.563507 | Elapsed: 12.56s
01/28/2023 11:02:50 AM  [*] Sat Jan 28 11:02:50 2023: Train Epoch: 5 [38400/53288 (72%)]	Loss: 188.660034 | Elapsed: 12.46s
01/28/2023 11:03:03 AM  [*] Sat Jan 28 11:03:03 2023: Train Epoch: 5 [44800/53288 (84%)]	Loss: 162.419983 | Elapsed: 12.48s
01/28/2023 11:03:15 AM  [*] Sat Jan 28 11:03:15 2023: Train Epoch: 5 [51200/53288 (96%)]	Loss: 155.496643 | Elapsed: 12.47s
01/28/2023 11:03:21 AM  [*] Sat Jan 28 11:03:21 2023:    5    | Tr.loss: 180.748849 | Elapsed:  105.29  s
01/28/2023 11:03:21 AM  [*] Started epoch: 6
01/28/2023 11:03:21 AM  [*] Sat Jan 28 11:03:21 2023: Train Epoch: 6 [  0  /53288 (0 %)]	Loss: 181.143829 | Elapsed: 0.13s
01/28/2023 11:03:33 AM  [*] Sat Jan 28 11:03:33 2023: Train Epoch: 6 [6400 /53288 (12%)]	Loss: 186.335144 | Elapsed: 12.42s
01/28/2023 11:03:46 AM  [*] Sat Jan 28 11:03:46 2023: Train Epoch: 6 [12800/53288 (24%)]	Loss: 176.000931 | Elapsed: 12.42s
01/28/2023 11:03:58 AM  [*] Sat Jan 28 11:03:58 2023: Train Epoch: 6 [19200/53288 (36%)]	Loss: 196.432587 | Elapsed: 12.49s
01/28/2023 11:04:11 AM  [*] Sat Jan 28 11:04:11 2023: Train Epoch: 6 [25600/53288 (48%)]	Loss: 204.476227 | Elapsed: 12.45s
01/28/2023 11:04:23 AM  [*] Sat Jan 28 11:04:23 2023: Train Epoch: 6 [32000/53288 (60%)]	Loss: 183.572983 | Elapsed: 12.36s
01/28/2023 11:04:35 AM  [*] Sat Jan 28 11:04:35 2023: Train Epoch: 6 [38400/53288 (72%)]	Loss: 161.945892 | Elapsed: 12.41s
01/28/2023 11:04:48 AM  [*] Sat Jan 28 11:04:48 2023: Train Epoch: 6 [44800/53288 (84%)]	Loss: 181.132812 | Elapsed: 12.37s
01/28/2023 11:05:00 AM  [*] Sat Jan 28 11:05:00 2023: Train Epoch: 6 [51200/53288 (96%)]	Loss: 181.787872 | Elapsed: 12.37s
01/28/2023 11:05:06 AM  [*] Sat Jan 28 11:05:06 2023:    6    | Tr.loss: 179.379774 | Elapsed:  104.89  s
01/28/2023 11:05:06 AM  [*] Started epoch: 7
01/28/2023 11:05:06 AM  [*] Sat Jan 28 11:05:06 2023: Train Epoch: 7 [  0  /53288 (0 %)]	Loss: 191.112808 | Elapsed: 0.13s
01/28/2023 11:05:06 AM [!] Learning rate: 2.5e-05
01/28/2023 11:05:18 AM  [*] Sat Jan 28 11:05:18 2023: Train Epoch: 7 [6400 /53288 (12%)]	Loss: 174.714096 | Elapsed: 12.40s
01/28/2023 11:05:31 AM  [*] Sat Jan 28 11:05:31 2023: Train Epoch: 7 [12800/53288 (24%)]	Loss: 180.093643 | Elapsed: 12.46s
01/28/2023 11:05:43 AM  [*] Sat Jan 28 11:05:43 2023: Train Epoch: 7 [19200/53288 (36%)]	Loss: 168.638794 | Elapsed: 12.40s
01/28/2023 11:05:55 AM  [*] Sat Jan 28 11:05:55 2023: Train Epoch: 7 [25600/53288 (48%)]	Loss: 176.063446 | Elapsed: 12.36s
01/28/2023 11:06:08 AM  [*] Sat Jan 28 11:06:08 2023: Train Epoch: 7 [32000/53288 (60%)]	Loss: 168.175369 | Elapsed: 12.37s
01/28/2023 11:06:20 AM  [*] Sat Jan 28 11:06:20 2023: Train Epoch: 7 [38400/53288 (72%)]	Loss: 193.828827 | Elapsed: 12.40s
01/28/2023 11:06:32 AM  [*] Sat Jan 28 11:06:32 2023: Train Epoch: 7 [44800/53288 (84%)]	Loss: 195.927673 | Elapsed: 12.35s
01/28/2023 11:06:45 AM  [*] Sat Jan 28 11:06:45 2023: Train Epoch: 7 [51200/53288 (96%)]	Loss: 173.175537 | Elapsed: 12.38s
01/28/2023 11:06:50 AM  [*] Sat Jan 28 11:06:50 2023:    7    | Tr.loss: 177.966910 | Elapsed:  104.68  s
01/28/2023 11:06:50 AM  [*] Started epoch: 8
01/28/2023 11:06:50 AM  [*] Sat Jan 28 11:06:50 2023: Train Epoch: 8 [  0  /53288 (0 %)]	Loss: 169.216675 | Elapsed: 0.13s
01/28/2023 11:07:03 AM  [*] Sat Jan 28 11:07:03 2023: Train Epoch: 8 [6400 /53288 (12%)]	Loss: 174.809540 | Elapsed: 12.43s
01/28/2023 11:07:15 AM  [*] Sat Jan 28 11:07:15 2023: Train Epoch: 8 [12800/53288 (24%)]	Loss: 191.904694 | Elapsed: 12.39s
01/28/2023 11:07:28 AM  [*] Sat Jan 28 11:07:28 2023: Train Epoch: 8 [19200/53288 (36%)]	Loss: 175.944885 | Elapsed: 12.35s
01/28/2023 11:07:40 AM  [*] Sat Jan 28 11:07:40 2023: Train Epoch: 8 [25600/53288 (48%)]	Loss: 179.758270 | Elapsed: 12.40s
01/28/2023 11:07:53 AM  [*] Sat Jan 28 11:07:53 2023: Train Epoch: 8 [32000/53288 (60%)]	Loss: 180.633759 | Elapsed: 12.52s
01/28/2023 11:08:05 AM  [*] Sat Jan 28 11:08:05 2023: Train Epoch: 8 [38400/53288 (72%)]	Loss: 178.517731 | Elapsed: 12.39s
01/28/2023 11:08:17 AM  [*] Sat Jan 28 11:08:17 2023: Train Epoch: 8 [44800/53288 (84%)]	Loss: 179.619110 | Elapsed: 12.38s
01/28/2023 11:08:30 AM  [*] Sat Jan 28 11:08:30 2023: Train Epoch: 8 [51200/53288 (96%)]	Loss: 163.444473 | Elapsed: 12.32s
01/28/2023 11:08:35 AM  [*] Sat Jan 28 11:08:35 2023:    8    | Tr.loss: 177.655334 | Elapsed:  104.79  s
01/28/2023 11:08:35 AM  [*] Started epoch: 9
01/28/2023 11:08:35 AM  [*] Sat Jan 28 11:08:35 2023: Train Epoch: 9 [  0  /53288 (0 %)]	Loss: 173.285110 | Elapsed: 0.13s
01/28/2023 11:08:48 AM  [*] Sat Jan 28 11:08:48 2023: Train Epoch: 9 [6400 /53288 (12%)]	Loss: 160.153778 | Elapsed: 12.40s
01/28/2023 11:09:00 AM  [*] Sat Jan 28 11:09:00 2023: Train Epoch: 9 [12800/53288 (24%)]	Loss: 177.774567 | Elapsed: 12.35s
01/28/2023 11:09:12 AM  [*] Sat Jan 28 11:09:12 2023: Train Epoch: 9 [19200/53288 (36%)]	Loss: 175.444153 | Elapsed: 12.40s
01/28/2023 11:09:25 AM  [*] Sat Jan 28 11:09:25 2023: Train Epoch: 9 [25600/53288 (48%)]	Loss: 199.133392 | Elapsed: 12.40s
01/28/2023 11:09:37 AM  [*] Sat Jan 28 11:09:37 2023: Train Epoch: 9 [32000/53288 (60%)]	Loss: 179.570312 | Elapsed: 12.44s
01/28/2023 11:09:50 AM  [*] Sat Jan 28 11:09:50 2023: Train Epoch: 9 [38400/53288 (72%)]	Loss: 179.984222 | Elapsed: 12.41s
01/28/2023 11:10:02 AM  [*] Sat Jan 28 11:10:02 2023: Train Epoch: 9 [44800/53288 (84%)]	Loss: 187.093750 | Elapsed: 12.31s
01/28/2023 11:10:14 AM  [*] Sat Jan 28 11:10:14 2023: Train Epoch: 9 [51200/53288 (96%)]	Loss: 175.015350 | Elapsed: 12.37s
01/28/2023 11:10:20 AM  [*] Sat Jan 28 11:10:20 2023:    9    | Tr.loss: 177.451887 | Elapsed:  104.73  s
01/28/2023 11:10:20 AM  [*] Started epoch: 10
01/28/2023 11:10:20 AM  [*] Sat Jan 28 11:10:20 2023: Train Epoch: 10 [  0  /53288 (0 %)]	Loss: 162.054352 | Elapsed: 0.14s
01/28/2023 11:10:32 AM  [*] Sat Jan 28 11:10:32 2023: Train Epoch: 10 [6400 /53288 (12%)]	Loss: 171.980194 | Elapsed: 12.47s
01/28/2023 11:10:45 AM  [*] Sat Jan 28 11:10:45 2023: Train Epoch: 10 [12800/53288 (24%)]	Loss: 172.635315 | Elapsed: 12.49s
01/28/2023 11:10:57 AM  [*] Sat Jan 28 11:10:57 2023: Train Epoch: 10 [19200/53288 (36%)]	Loss: 186.875153 | Elapsed: 12.51s
01/28/2023 11:11:10 AM  [*] Sat Jan 28 11:11:10 2023: Train Epoch: 10 [25600/53288 (48%)]	Loss: 171.848846 | Elapsed: 12.46s
01/28/2023 11:11:22 AM  [*] Sat Jan 28 11:11:22 2023: Train Epoch: 10 [32000/53288 (60%)]	Loss: 195.455566 | Elapsed: 12.48s
01/28/2023 11:11:35 AM  [*] Sat Jan 28 11:11:35 2023: Train Epoch: 10 [38400/53288 (72%)]	Loss: 180.449570 | Elapsed: 12.42s
01/28/2023 11:11:47 AM  [*] Sat Jan 28 11:11:47 2023: Train Epoch: 10 [44800/53288 (84%)]	Loss: 174.516602 | Elapsed: 12.36s
01/28/2023 11:12:00 AM  [*] Sat Jan 28 11:12:00 2023: Train Epoch: 10 [51200/53288 (96%)]	Loss: 184.850677 | Elapsed: 12.53s
01/28/2023 11:12:05 AM  [*] Sat Jan 28 11:12:05 2023:   10    | Tr.loss: 177.268106 | Elapsed:  105.38  s
01/28/2023 11:12:06 AM [!] Sat Jan 28 11:12:06 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674900725-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674900725-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674900725-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674900725-auc.npy
01/28/2023 11:12:06 AM  [!] Training pretrained model on downstream task...
01/28/2023 11:12:06 AM  [*] Started epoch: 1
01/28/2023 11:12:07 AM  [*] Sat Jan 28 11:12:07 2023: Train Epoch: 1 [  0  /22838 (0 %)]	Loss: 1.481398 | Elapsed: 0.33s | FPR 0.0003 -> TPR 0.2128 & F1 0.3509
01/28/2023 11:12:16 AM  [*] Sat Jan 28 11:12:16 2023: Train Epoch: 1 [6400 /22838 (28%)]	Loss: 0.436644 | Elapsed: 9.13s | FPR 0.0003 -> TPR 0.3924 & F1 0.5636
01/28/2023 11:12:25 AM  [*] Sat Jan 28 11:12:25 2023: Train Epoch: 1 [12800/22838 (56%)]	Loss: 0.404452 | Elapsed: 9.11s | FPR 0.0003 -> TPR 0.5694 & F1 0.7257
01/28/2023 11:12:34 AM  [*] Sat Jan 28 11:12:34 2023: Train Epoch: 1 [19200/22838 (84%)]	Loss: 0.304873 | Elapsed: 9.10s | FPR 0.0003 -> TPR 0.5714 & F1 0.7273
01/28/2023 11:12:40 AM  [*] Sat Jan 28 11:12:40 2023:    1    | Tr.loss: 0.431670 | Elapsed:   33.34  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8500
01/28/2023 11:12:40 AM  [*] Started epoch: 2
01/28/2023 11:12:40 AM  [*] Sat Jan 28 11:12:40 2023: Train Epoch: 2 [  0  /22838 (0 %)]	Loss: 0.280210 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.5400 & F1 0.7013
01/28/2023 11:12:49 AM  [*] Sat Jan 28 11:12:49 2023: Train Epoch: 2 [6400 /22838 (28%)]	Loss: 0.228705 | Elapsed: 9.11s | FPR 0.0003 -> TPR 0.6757 & F1 0.8065
01/28/2023 11:12:58 AM  [*] Sat Jan 28 11:12:58 2023: Train Epoch: 2 [12800/22838 (56%)]	Loss: 0.157608 | Elapsed: 9.13s | FPR 0.0003 -> TPR 0.8615 & F1 0.9256
01/28/2023 11:13:07 AM  [*] Sat Jan 28 11:13:07 2023: Train Epoch: 2 [19200/22838 (84%)]	Loss: 0.131699 | Elapsed: 9.18s | FPR 0.0003 -> TPR 0.9143 & F1 0.9552
01/28/2023 11:13:13 AM  [*] Sat Jan 28 11:13:13 2023:    2    | Tr.loss: 0.234221 | Elapsed:   33.23  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.45 | AUC: 0.9607
01/28/2023 11:13:13 AM  [*] Started epoch: 3
01/28/2023 11:13:13 AM  [*] Sat Jan 28 11:13:13 2023: Train Epoch: 3 [  0  /22838 (0 %)]	Loss: 0.126000 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.9787 & F1 0.9892
01/28/2023 11:13:22 AM  [*] Sat Jan 28 11:13:22 2023: Train Epoch: 3 [6400 /22838 (28%)]	Loss: 0.193998 | Elapsed: 9.12s | FPR 0.0003 -> TPR 0.7429 & F1 0.8525
01/28/2023 11:13:31 AM  [*] Sat Jan 28 11:13:31 2023: Train Epoch: 3 [12800/22838 (56%)]	Loss: 0.109639 | Elapsed: 9.10s | FPR 0.0003 -> TPR 0.9062 & F1 0.9508
01/28/2023 11:13:41 AM  [*] Sat Jan 28 11:13:41 2023: Train Epoch: 3 [19200/22838 (84%)]	Loss: 0.160614 | Elapsed: 9.09s | FPR 0.0003 -> TPR 0.6875 & F1 0.8148
01/28/2023 11:13:46 AM  [*] Sat Jan 28 11:13:46 2023:    3    | Tr.loss: 0.154231 | Elapsed:   33.07  s | FPR 0.0003 -> TPR: 0.48 & F1: 0.64 | AUC: 0.9837
01/28/2023 11:13:47 AM [!] Sat Jan 28 11:13:47 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674900826-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674900826-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674900826-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674900826-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674900826-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674900826-trainTPRs.npy
01/28/2023 11:13:47 AM  [!] Training non_pretrained model on downstream task...
01/28/2023 11:13:47 AM  [*] Started epoch: 1
01/28/2023 11:13:47 AM  [*] Sat Jan 28 11:13:47 2023: Train Epoch: 1 [  0  /22838 (0 %)]	Loss: 1.491862 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0488 & F1 0.0930
01/28/2023 11:13:53 AM  [*] Sat Jan 28 11:13:53 2023: Train Epoch: 1 [6400 /22838 (28%)]	Loss: 0.521046 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.1912 & F1 0.3210
01/28/2023 11:14:00 AM  [*] Sat Jan 28 11:14:00 2023: Train Epoch: 1 [12800/22838 (56%)]	Loss: 0.266541 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.6000 & F1 0.7500
01/28/2023 11:14:06 AM  [*] Sat Jan 28 11:14:06 2023: Train Epoch: 1 [19200/22838 (84%)]	Loss: 0.282322 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.4627 & F1 0.6327
01/28/2023 11:14:10 AM  [*] Sat Jan 28 11:14:10 2023:    1    | Tr.loss: 0.420051 | Elapsed:   22.81  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.8618
01/28/2023 11:14:10 AM  [*] Started epoch: 2
01/28/2023 11:14:10 AM  [*] Sat Jan 28 11:14:10 2023: Train Epoch: 2 [  0  /22838 (0 %)]	Loss: 0.192368 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7826 & F1 0.8780
01/28/2023 11:14:16 AM  [*] Sat Jan 28 11:14:16 2023: Train Epoch: 2 [6400 /22838 (28%)]	Loss: 0.255298 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7200 & F1 0.8372
01/28/2023 11:14:22 AM  [*] Sat Jan 28 11:14:22 2023: Train Epoch: 2 [12800/22838 (56%)]	Loss: 0.282976 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000
01/28/2023 11:14:29 AM  [*] Sat Jan 28 11:14:29 2023: Train Epoch: 2 [19200/22838 (84%)]	Loss: 0.357899 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.7639 & F1 0.8661
01/28/2023 11:14:33 AM  [*] Sat Jan 28 11:14:33 2023:    2    | Tr.loss: 0.255161 | Elapsed:   22.69  s | FPR 0.0003 -> TPR: 0.27 & F1: 0.42 | AUC: 0.9527
01/28/2023 11:14:33 AM  [*] Started epoch: 3
01/28/2023 11:14:33 AM  [*] Sat Jan 28 11:14:33 2023: Train Epoch: 3 [  0  /22838 (0 %)]	Loss: 0.231795 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.6486 & F1 0.7869
01/28/2023 11:14:39 AM  [*] Sat Jan 28 11:14:39 2023: Train Epoch: 3 [6400 /22838 (28%)]	Loss: 0.206928 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.8889 & F1 0.9412
01/28/2023 11:14:45 AM  [*] Sat Jan 28 11:14:45 2023: Train Epoch: 3 [12800/22838 (56%)]	Loss: 0.153286 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.8308 & F1 0.9076
01/28/2023 11:14:50 AM [!] Learning rate: 2.5e-05
01/28/2023 11:14:51 AM  [*] Sat Jan 28 11:14:51 2023: Train Epoch: 3 [19200/22838 (84%)]	Loss: 0.150052 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.8889 & F1 0.9412
01/28/2023 11:14:55 AM  [*] Sat Jan 28 11:14:55 2023:    3    | Tr.loss: 0.180856 | Elapsed:   22.74  s | FPR 0.0003 -> TPR: 0.32 & F1: 0.48 | AUC: 0.9769
01/28/2023 11:14:56 AM [!] Sat Jan 28 11:14:56 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674900895-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674900895-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674900895-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674900895-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674900895-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674900895-trainTPRs.npy
01/28/2023 11:14:56 AM  [!] Training full_data model on downstream task...
01/28/2023 11:14:56 AM  [*] Started epoch: 1
01/28/2023 11:14:56 AM  [*] Sat Jan 28 11:14:56 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 3.610206 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000
01/28/2023 11:15:03 AM  [*] Sat Jan 28 11:15:03 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.552404 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.2759 & F1 0.4324
01/28/2023 11:15:09 AM  [*] Sat Jan 28 11:15:09 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.404754 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.4306 & F1 0.6019
01/28/2023 11:15:15 AM  [*] Sat Jan 28 11:15:15 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.337533 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.4545 & F1 0.6250
01/28/2023 11:15:21 AM  [*] Sat Jan 28 11:15:21 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.282945 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.5161 & F1 0.6809
01/28/2023 11:15:28 AM  [*] Sat Jan 28 11:15:28 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.200557 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.6571 & F1 0.7931
01/28/2023 11:15:34 AM  [*] Sat Jan 28 11:15:34 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.221706 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.8154 & F1 0.8983
01/28/2023 11:15:40 AM  [*] Sat Jan 28 11:15:40 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.210340 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.7222 & F1 0.8387
01/28/2023 11:15:46 AM  [*] Sat Jan 28 11:15:46 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.264241 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.6500 & F1 0.7879
01/28/2023 11:15:52 AM  [*] Sat Jan 28 11:15:52 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.104771 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.8382 & F1 0.9120
01/28/2023 11:15:59 AM [!] Learning rate: 2.5e-05
01/28/2023 11:15:59 AM  [*] Sat Jan 28 11:15:59 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.136184 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.7656 & F1 0.8673
01/28/2023 11:16:05 AM  [*] Sat Jan 28 11:16:05 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.140870 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.7826 & F1 0.8780
01/28/2023 11:16:12 AM  [*] Sat Jan 28 11:16:12 2023:    1    | Tr.loss: 0.300045 | Elapsed:   76.11  s | FPR 0.0003 -> TPR: 0.04 & F1: 0.07 | AUC: 0.9347
01/28/2023 11:16:12 AM  [*] Started epoch: 2
01/28/2023 11:16:12 AM  [*] Sat Jan 28 11:16:12 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.157157 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.9302 & F1 0.9639
01/28/2023 11:16:19 AM  [*] Sat Jan 28 11:16:19 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.292527 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.1930 & F1 0.3235
01/28/2023 11:16:25 AM  [*] Sat Jan 28 11:16:25 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.148306 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.9062 & F1 0.9508
01/28/2023 11:16:31 AM  [*] Sat Jan 28 11:16:31 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.094815 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.9545 & F1 0.9767
01/28/2023 11:16:37 AM  [*] Sat Jan 28 11:16:37 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.105342 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.9394 & F1 0.9688
01/28/2023 11:16:44 AM  [*] Sat Jan 28 11:16:44 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.169877 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.7846 & F1 0.8793
01/28/2023 11:16:50 AM  [*] Sat Jan 28 11:16:50 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.213008 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.6825 & F1 0.8113
01/28/2023 11:16:56 AM  [*] Sat Jan 28 11:16:56 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.198967 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.8769 & F1 0.9344
01/28/2023 11:17:02 AM  [*] Sat Jan 28 11:17:02 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.138932 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.8286 & F1 0.9062
01/28/2023 11:17:03 AM [!] Learning rate: 2.5e-06
01/28/2023 11:17:09 AM  [*] Sat Jan 28 11:17:09 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.123435 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.7385 & F1 0.8496
01/28/2023 11:17:15 AM  [*] Sat Jan 28 11:17:15 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.109684 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.8857 & F1 0.9394
01/28/2023 11:17:21 AM  [*] Sat Jan 28 11:17:21 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.142210 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.9167 & F1 0.9565
01/28/2023 11:17:28 AM  [*] Sat Jan 28 11:17:28 2023:    2    | Tr.loss: 0.165885 | Elapsed:   76.06  s | FPR 0.0003 -> TPR: 0.47 & F1: 0.64 | AUC: 0.9810
01/28/2023 11:17:28 AM  [*] Started epoch: 3
01/28/2023 11:17:29 AM  [*] Sat Jan 28 11:17:29 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.122842 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.9375 & F1 0.9677
01/28/2023 11:17:35 AM  [*] Sat Jan 28 11:17:35 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.119600 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.8824 & F1 0.9375
01/28/2023 11:17:41 AM  [*] Sat Jan 28 11:17:41 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.107604 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.9508 & F1 0.9748
01/28/2023 11:17:47 AM  [*] Sat Jan 28 11:17:47 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.091252 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.9437 & F1 0.9710
01/28/2023 11:17:54 AM  [*] Sat Jan 28 11:17:54 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.122900 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.7612 & F1 0.8644
01/28/2023 11:18:00 AM  [*] Sat Jan 28 11:18:00 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.071000 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.7164 & F1 0.8348
01/28/2023 11:18:06 AM  [*] Sat Jan 28 11:18:06 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.147315 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.6774 & F1 0.8077
01/28/2023 11:18:07 AM [!] Learning rate: 2.5000000000000004e-07
01/28/2023 11:18:12 AM  [*] Sat Jan 28 11:18:12 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.151856 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.8793 & F1 0.9358
01/28/2023 11:18:18 AM  [*] Sat Jan 28 11:18:18 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.297773 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.7432 & F1 0.8527
01/28/2023 11:18:25 AM  [*] Sat Jan 28 11:18:25 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.218472 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.6714 & F1 0.8034
01/28/2023 11:18:31 AM  [*] Sat Jan 28 11:18:31 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.144404 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.9067 & F1 0.9510
01/28/2023 11:18:37 AM  [*] Sat Jan 28 11:18:37 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.145208 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.7903 & F1 0.8829
01/28/2023 11:18:45 AM  [*] Sat Jan 28 11:18:45 2023:    3    | Tr.loss: 0.158044 | Elapsed:   76.09  s | FPR 0.0003 -> TPR: 0.46 & F1: 0.63 | AUC: 0.9828
01/28/2023 11:18:45 AM [!] Sat Jan 28 11:18:45 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674901125-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674901125-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674901125-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674901125-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674901125-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674901125-trainTPRs.npy
01/28/2023 11:18:45 AM  [*] Evaluating pretrained model on test set...
01/28/2023 11:18:50 AM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0731 | F1: 0.1362
01/28/2023 11:18:50 AM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2942 | F1: 0.4545
01/28/2023 11:18:50 AM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.4277 | F1: 0.5988
01/28/2023 11:18:50 AM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.4614 | F1: 0.6304
01/28/2023 11:18:50 AM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.5571 | F1: 0.7116
01/28/2023 11:18:50 AM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.6641 | F1: 0.7861
01/28/2023 11:18:50 AM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7968 | F1: 0.8468
01/28/2023 11:18:50 AM  [*] Evaluating non_pretrained model on test set...
01/28/2023 11:18:55 AM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0893 | F1: 0.1640
01/28/2023 11:18:55 AM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1939 | F1: 0.3247
01/28/2023 11:18:55 AM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2965 | F1: 0.4571
01/28/2023 11:18:55 AM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3306 | F1: 0.4960
01/28/2023 11:18:55 AM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.3698 | F1: 0.5366
01/28/2023 11:18:55 AM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5153 | F1: 0.6688
01/28/2023 11:18:55 AM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7037 | F1: 0.7869
01/28/2023 11:18:55 AM  [*] Evaluating full_data model on test set...
01/28/2023 11:19:00 AM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0281 | F1: 0.0546
01/28/2023 11:19:00 AM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2737 | F1: 0.4298
01/28/2023 11:19:00 AM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3426 | F1: 0.5100
01/28/2023 11:19:00 AM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.4143 | F1: 0.5849
01/28/2023 11:19:00 AM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4890 | F1: 0.6531
01/28/2023 11:19:00 AM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5900 | F1: 0.7304
01/28/2023 11:19:00 AM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7944 | F1: 0.8454
01/28/2023 11:19:00 AM  [!] Running pre-training split 2/3
01/28/2023 11:19:03 AM  [!] Pre-training model...
01/28/2023 11:19:04 AM  [*] Masking sequences...
01/28/2023 11:19:21 AM  [*] Started epoch: 1
01/28/2023 11:19:22 AM  [*] Sat Jan 28 11:19:22 2023: Train Epoch: 1 [  0  /53288 (0 %)]	Loss: 414.126770 | Elapsed: 0.85s
01/28/2023 11:19:35 AM  [*] Sat Jan 28 11:19:35 2023: Train Epoch: 1 [6400 /53288 (12%)]	Loss: 200.671234 | Elapsed: 12.28s
01/28/2023 11:19:47 AM  [*] Sat Jan 28 11:19:47 2023: Train Epoch: 1 [12800/53288 (24%)]	Loss: 225.539368 | Elapsed: 12.30s
01/28/2023 11:19:59 AM  [*] Sat Jan 28 11:19:59 2023: Train Epoch: 1 [19200/53288 (36%)]	Loss: 236.964188 | Elapsed: 12.42s
01/28/2023 11:20:12 AM  [*] Sat Jan 28 11:20:12 2023: Train Epoch: 1 [25600/53288 (48%)]	Loss: 208.999008 | Elapsed: 12.43s
01/28/2023 11:20:24 AM  [*] Sat Jan 28 11:20:24 2023: Train Epoch: 1 [32000/53288 (60%)]	Loss: 189.723175 | Elapsed: 12.35s
01/28/2023 11:20:36 AM  [*] Sat Jan 28 11:20:36 2023: Train Epoch: 1 [38400/53288 (72%)]	Loss: 193.701904 | Elapsed: 12.42s
01/28/2023 11:20:49 AM  [*] Sat Jan 28 11:20:49 2023: Train Epoch: 1 [44800/53288 (84%)]	Loss: 194.880310 | Elapsed: 12.37s
01/28/2023 11:21:01 AM  [*] Sat Jan 28 11:21:01 2023: Train Epoch: 1 [51200/53288 (96%)]	Loss: 204.468292 | Elapsed: 12.40s
01/28/2023 11:21:07 AM  [*] Sat Jan 28 11:21:07 2023:    1    | Tr.loss: 215.109078 | Elapsed:  105.12  s
01/28/2023 11:21:07 AM  [*] Started epoch: 2
01/28/2023 11:21:07 AM  [*] Sat Jan 28 11:21:07 2023: Train Epoch: 2 [  0  /53288 (0 %)]	Loss: 206.584091 | Elapsed: 0.12s
01/28/2023 11:21:19 AM  [*] Sat Jan 28 11:21:19 2023: Train Epoch: 2 [6400 /53288 (12%)]	Loss: 211.576172 | Elapsed: 12.45s
01/28/2023 11:21:32 AM  [*] Sat Jan 28 11:21:32 2023: Train Epoch: 2 [12800/53288 (24%)]	Loss: 194.830750 | Elapsed: 12.50s
01/28/2023 11:21:44 AM  [*] Sat Jan 28 11:21:44 2023: Train Epoch: 2 [19200/53288 (36%)]	Loss: 206.314392 | Elapsed: 12.49s
01/28/2023 11:21:56 AM  [*] Sat Jan 28 11:21:56 2023: Train Epoch: 2 [25600/53288 (48%)]	Loss: 216.577194 | Elapsed: 12.39s
01/28/2023 11:22:09 AM  [*] Sat Jan 28 11:22:09 2023: Train Epoch: 2 [32000/53288 (60%)]	Loss: 189.320404 | Elapsed: 12.38s
01/28/2023 11:22:21 AM  [*] Sat Jan 28 11:22:21 2023: Train Epoch: 2 [38400/53288 (72%)]	Loss: 187.483337 | Elapsed: 12.37s
01/28/2023 11:22:34 AM  [*] Sat Jan 28 11:22:34 2023: Train Epoch: 2 [44800/53288 (84%)]	Loss: 193.726807 | Elapsed: 12.40s
01/28/2023 11:22:46 AM  [*] Sat Jan 28 11:22:46 2023: Train Epoch: 2 [51200/53288 (96%)]	Loss: 187.796829 | Elapsed: 12.37s
01/28/2023 11:22:51 AM  [*] Sat Jan 28 11:22:51 2023:    2    | Tr.loss: 196.648983 | Elapsed:  104.74  s
01/28/2023 11:22:51 AM  [*] Started epoch: 3
01/28/2023 11:22:51 AM  [*] Sat Jan 28 11:22:51 2023: Train Epoch: 3 [  0  /53288 (0 %)]	Loss: 194.620148 | Elapsed: 0.14s
01/28/2023 11:23:04 AM  [*] Sat Jan 28 11:23:04 2023: Train Epoch: 3 [6400 /53288 (12%)]	Loss: 197.234253 | Elapsed: 12.39s
01/28/2023 11:23:16 AM  [*] Sat Jan 28 11:23:16 2023: Train Epoch: 3 [12800/53288 (24%)]	Loss: 214.343338 | Elapsed: 12.50s
01/28/2023 11:23:29 AM  [*] Sat Jan 28 11:23:29 2023: Train Epoch: 3 [19200/53288 (36%)]	Loss: 207.132172 | Elapsed: 12.42s
01/28/2023 11:23:41 AM  [*] Sat Jan 28 11:23:41 2023: Train Epoch: 3 [25600/53288 (48%)]	Loss: 182.967316 | Elapsed: 12.35s
01/28/2023 11:23:54 AM  [*] Sat Jan 28 11:23:54 2023: Train Epoch: 3 [32000/53288 (60%)]	Loss: 171.320572 | Elapsed: 13.11s
01/28/2023 11:24:08 AM  [*] Sat Jan 28 11:24:08 2023: Train Epoch: 3 [38400/53288 (72%)]	Loss: 175.604416 | Elapsed: 13.60s
01/28/2023 11:24:21 AM  [*] Sat Jan 28 11:24:21 2023: Train Epoch: 3 [44800/53288 (84%)]	Loss: 213.732697 | Elapsed: 12.97s
01/28/2023 11:24:34 AM  [*] Sat Jan 28 11:24:34 2023: Train Epoch: 3 [51200/53288 (96%)]	Loss: 192.196869 | Elapsed: 13.04s
01/28/2023 11:24:39 AM  [*] Sat Jan 28 11:24:39 2023:    3    | Tr.loss: 190.062548 | Elapsed:  108.16  s
01/28/2023 11:24:39 AM  [*] Started epoch: 4
01/28/2023 11:24:40 AM  [*] Sat Jan 28 11:24:40 2023: Train Epoch: 4 [  0  /53288 (0 %)]	Loss: 236.839188 | Elapsed: 0.14s
01/28/2023 11:24:52 AM  [*] Sat Jan 28 11:24:52 2023: Train Epoch: 4 [6400 /53288 (12%)]	Loss: 183.435486 | Elapsed: 12.56s
01/28/2023 11:25:05 AM  [*] Sat Jan 28 11:25:05 2023: Train Epoch: 4 [12800/53288 (24%)]	Loss: 194.282684 | Elapsed: 12.43s
01/28/2023 11:25:17 AM  [*] Sat Jan 28 11:25:17 2023: Train Epoch: 4 [19200/53288 (36%)]	Loss: 190.695374 | Elapsed: 12.45s
01/28/2023 11:25:29 AM  [*] Sat Jan 28 11:25:29 2023: Train Epoch: 4 [25600/53288 (48%)]	Loss: 162.590729 | Elapsed: 12.43s
01/28/2023 11:25:42 AM  [*] Sat Jan 28 11:25:42 2023: Train Epoch: 4 [32000/53288 (60%)]	Loss: 179.147369 | Elapsed: 12.43s
01/28/2023 11:25:54 AM  [*] Sat Jan 28 11:25:54 2023: Train Epoch: 4 [38400/53288 (72%)]	Loss: 182.272766 | Elapsed: 12.40s
01/28/2023 11:26:07 AM  [*] Sat Jan 28 11:26:07 2023: Train Epoch: 4 [44800/53288 (84%)]	Loss: 177.724152 | Elapsed: 12.42s
01/28/2023 11:26:19 AM  [*] Sat Jan 28 11:26:19 2023: Train Epoch: 4 [51200/53288 (96%)]	Loss: 202.781158 | Elapsed: 12.43s
01/28/2023 11:26:24 AM  [*] Sat Jan 28 11:26:24 2023:    4    | Tr.loss: 186.255413 | Elapsed:  105.04  s
01/28/2023 11:26:24 AM  [*] Started epoch: 5
01/28/2023 11:26:25 AM  [*] Sat Jan 28 11:26:25 2023: Train Epoch: 5 [  0  /53288 (0 %)]	Loss: 188.865280 | Elapsed: 0.13s
01/28/2023 11:26:37 AM  [*] Sat Jan 28 11:26:37 2023: Train Epoch: 5 [6400 /53288 (12%)]	Loss: 195.425308 | Elapsed: 12.41s
01/28/2023 11:26:49 AM  [*] Sat Jan 28 11:26:49 2023: Train Epoch: 5 [12800/53288 (24%)]	Loss: 187.681580 | Elapsed: 12.41s
01/28/2023 11:27:02 AM  [*] Sat Jan 28 11:27:02 2023: Train Epoch: 5 [19200/53288 (36%)]	Loss: 185.629059 | Elapsed: 12.45s
01/28/2023 11:27:14 AM  [*] Sat Jan 28 11:27:14 2023: Train Epoch: 5 [25600/53288 (48%)]	Loss: 166.336624 | Elapsed: 12.38s
01/28/2023 11:27:27 AM  [*] Sat Jan 28 11:27:27 2023: Train Epoch: 5 [32000/53288 (60%)]	Loss: 187.386093 | Elapsed: 12.41s
01/28/2023 11:27:39 AM  [*] Sat Jan 28 11:27:39 2023: Train Epoch: 5 [38400/53288 (72%)]	Loss: 182.549774 | Elapsed: 12.40s
01/28/2023 11:27:51 AM  [*] Sat Jan 28 11:27:51 2023: Train Epoch: 5 [44800/53288 (84%)]	Loss: 201.463791 | Elapsed: 12.40s
01/28/2023 11:28:04 AM  [*] Sat Jan 28 11:28:04 2023: Train Epoch: 5 [51200/53288 (96%)]	Loss: 187.382034 | Elapsed: 12.38s
01/28/2023 11:28:09 AM  [*] Sat Jan 28 11:28:09 2023:    5    | Tr.loss: 183.592727 | Elapsed:  104.70  s
01/28/2023 11:28:09 AM  [*] Started epoch: 6
01/28/2023 11:28:09 AM  [*] Sat Jan 28 11:28:09 2023: Train Epoch: 6 [  0  /53288 (0 %)]	Loss: 200.013336 | Elapsed: 0.13s
01/28/2023 11:28:22 AM  [*] Sat Jan 28 11:28:22 2023: Train Epoch: 6 [6400 /53288 (12%)]	Loss: 178.669174 | Elapsed: 12.39s
01/28/2023 11:28:34 AM  [*] Sat Jan 28 11:28:34 2023: Train Epoch: 6 [12800/53288 (24%)]	Loss: 189.053513 | Elapsed: 12.33s
01/28/2023 11:28:46 AM  [*] Sat Jan 28 11:28:46 2023: Train Epoch: 6 [19200/53288 (36%)]	Loss: 182.434647 | Elapsed: 12.40s
01/28/2023 11:28:59 AM  [*] Sat Jan 28 11:28:59 2023: Train Epoch: 6 [25600/53288 (48%)]	Loss: 191.748169 | Elapsed: 12.36s
01/28/2023 11:29:11 AM  [*] Sat Jan 28 11:29:11 2023: Train Epoch: 6 [32000/53288 (60%)]	Loss: 189.641693 | Elapsed: 12.42s
01/28/2023 11:29:24 AM  [*] Sat Jan 28 11:29:24 2023: Train Epoch: 6 [38400/53288 (72%)]	Loss: 195.827484 | Elapsed: 12.46s
01/28/2023 11:29:36 AM  [*] Sat Jan 28 11:29:36 2023: Train Epoch: 6 [44800/53288 (84%)]	Loss: 207.073090 | Elapsed: 12.40s
01/28/2023 11:29:48 AM  [*] Sat Jan 28 11:29:48 2023: Train Epoch: 6 [51200/53288 (96%)]	Loss: 170.652039 | Elapsed: 12.36s
01/28/2023 11:29:54 AM  [*] Sat Jan 28 11:29:54 2023:    6    | Tr.loss: 181.940007 | Elapsed:  104.59  s
01/28/2023 11:29:54 AM  [*] Started epoch: 7
01/28/2023 11:29:54 AM  [*] Sat Jan 28 11:29:54 2023: Train Epoch: 7 [  0  /53288 (0 %)]	Loss: 194.803818 | Elapsed: 0.13s
01/28/2023 11:29:54 AM [!] Learning rate: 2.5e-05
01/28/2023 11:30:06 AM  [*] Sat Jan 28 11:30:06 2023: Train Epoch: 7 [6400 /53288 (12%)]	Loss: 179.550507 | Elapsed: 12.38s
01/28/2023 11:30:19 AM  [*] Sat Jan 28 11:30:19 2023: Train Epoch: 7 [12800/53288 (24%)]	Loss: 171.677429 | Elapsed: 12.45s
01/28/2023 11:30:31 AM  [*] Sat Jan 28 11:30:31 2023: Train Epoch: 7 [19200/53288 (36%)]	Loss: 175.573059 | Elapsed: 12.45s
01/28/2023 11:30:44 AM  [*] Sat Jan 28 11:30:44 2023: Train Epoch: 7 [25600/53288 (48%)]	Loss: 175.850891 | Elapsed: 12.38s
01/28/2023 11:30:56 AM  [*] Sat Jan 28 11:30:56 2023: Train Epoch: 7 [32000/53288 (60%)]	Loss: 188.045898 | Elapsed: 12.37s
01/28/2023 11:31:08 AM  [*] Sat Jan 28 11:31:08 2023: Train Epoch: 7 [38400/53288 (72%)]	Loss: 199.015381 | Elapsed: 12.36s
01/28/2023 11:31:21 AM  [*] Sat Jan 28 11:31:21 2023: Train Epoch: 7 [44800/53288 (84%)]	Loss: 179.488617 | Elapsed: 12.44s
01/28/2023 11:31:33 AM  [*] Sat Jan 28 11:31:33 2023: Train Epoch: 7 [51200/53288 (96%)]	Loss: 187.289429 | Elapsed: 12.39s
01/28/2023 11:31:38 AM  [*] Sat Jan 28 11:31:38 2023:    7    | Tr.loss: 180.183717 | Elapsed:  104.56  s
01/28/2023 11:31:38 AM  [*] Started epoch: 8
01/28/2023 11:31:38 AM  [*] Sat Jan 28 11:31:38 2023: Train Epoch: 8 [  0  /53288 (0 %)]	Loss: 179.921448 | Elapsed: 0.14s
01/28/2023 11:31:51 AM  [*] Sat Jan 28 11:31:51 2023: Train Epoch: 8 [6400 /53288 (12%)]	Loss: 163.165588 | Elapsed: 12.53s
01/28/2023 11:32:03 AM  [*] Sat Jan 28 11:32:03 2023: Train Epoch: 8 [12800/53288 (24%)]	Loss: 175.105438 | Elapsed: 12.46s
01/28/2023 11:32:16 AM  [*] Sat Jan 28 11:32:16 2023: Train Epoch: 8 [19200/53288 (36%)]	Loss: 182.684631 | Elapsed: 12.42s
01/28/2023 11:32:28 AM  [*] Sat Jan 28 11:32:28 2023: Train Epoch: 8 [25600/53288 (48%)]	Loss: 190.588226 | Elapsed: 12.38s
01/28/2023 11:32:41 AM  [*] Sat Jan 28 11:32:41 2023: Train Epoch: 8 [32000/53288 (60%)]	Loss: 174.895828 | Elapsed: 12.35s
01/28/2023 11:32:53 AM  [*] Sat Jan 28 11:32:53 2023: Train Epoch: 8 [38400/53288 (72%)]	Loss: 197.328949 | Elapsed: 12.42s
01/28/2023 11:33:05 AM  [*] Sat Jan 28 11:33:05 2023: Train Epoch: 8 [44800/53288 (84%)]	Loss: 189.453384 | Elapsed: 12.41s
01/28/2023 11:33:18 AM  [*] Sat Jan 28 11:33:18 2023: Train Epoch: 8 [51200/53288 (96%)]	Loss: 194.081558 | Elapsed: 12.44s
01/28/2023 11:33:23 AM  [*] Sat Jan 28 11:33:23 2023:    8    | Tr.loss: 180.027594 | Elapsed:  104.87  s
01/28/2023 11:33:23 AM  [*] Started epoch: 9
01/28/2023 11:33:23 AM  [*] Sat Jan 28 11:33:23 2023: Train Epoch: 9 [  0  /53288 (0 %)]	Loss: 177.382629 | Elapsed: 0.14s
01/28/2023 11:33:36 AM  [*] Sat Jan 28 11:33:36 2023: Train Epoch: 9 [6400 /53288 (12%)]	Loss: 172.226456 | Elapsed: 12.43s
01/28/2023 11:33:48 AM  [*] Sat Jan 28 11:33:48 2023: Train Epoch: 9 [12800/53288 (24%)]	Loss: 180.325302 | Elapsed: 12.42s
01/28/2023 11:34:01 AM  [*] Sat Jan 28 11:34:01 2023: Train Epoch: 9 [19200/53288 (36%)]	Loss: 169.003357 | Elapsed: 12.46s
01/28/2023 11:34:13 AM  [*] Sat Jan 28 11:34:13 2023: Train Epoch: 9 [25600/53288 (48%)]	Loss: 165.300262 | Elapsed: 12.39s
01/28/2023 11:34:25 AM  [*] Sat Jan 28 11:34:25 2023: Train Epoch: 9 [32000/53288 (60%)]	Loss: 206.026215 | Elapsed: 12.36s
01/28/2023 11:34:38 AM  [*] Sat Jan 28 11:34:38 2023: Train Epoch: 9 [38400/53288 (72%)]	Loss: 196.225281 | Elapsed: 12.36s
01/28/2023 11:34:50 AM  [*] Sat Jan 28 11:34:50 2023: Train Epoch: 9 [44800/53288 (84%)]	Loss: 181.993927 | Elapsed: 12.35s
01/28/2023 11:35:02 AM  [*] Sat Jan 28 11:35:02 2023: Train Epoch: 9 [51200/53288 (96%)]	Loss: 171.808075 | Elapsed: 12.36s
01/28/2023 11:35:08 AM  [*] Sat Jan 28 11:35:08 2023:    9    | Tr.loss: 179.757865 | Elapsed:  104.64  s
01/28/2023 11:35:08 AM  [*] Started epoch: 10
01/28/2023 11:35:08 AM  [*] Sat Jan 28 11:35:08 2023: Train Epoch: 10 [  0  /53288 (0 %)]	Loss: 173.564774 | Elapsed: 0.13s
01/28/2023 11:35:20 AM  [*] Sat Jan 28 11:35:20 2023: Train Epoch: 10 [6400 /53288 (12%)]	Loss: 189.356567 | Elapsed: 12.51s
01/28/2023 11:35:33 AM  [*] Sat Jan 28 11:35:33 2023: Train Epoch: 10 [12800/53288 (24%)]	Loss: 181.405273 | Elapsed: 12.44s
01/28/2023 11:35:45 AM  [*] Sat Jan 28 11:35:45 2023: Train Epoch: 10 [19200/53288 (36%)]	Loss: 194.289124 | Elapsed: 12.33s
01/28/2023 11:35:58 AM  [*] Sat Jan 28 11:35:58 2023: Train Epoch: 10 [25600/53288 (48%)]	Loss: 179.402908 | Elapsed: 12.41s
01/28/2023 11:36:10 AM  [*] Sat Jan 28 11:36:10 2023: Train Epoch: 10 [32000/53288 (60%)]	Loss: 180.790146 | Elapsed: 12.40s
01/28/2023 11:36:23 AM  [*] Sat Jan 28 11:36:23 2023: Train Epoch: 10 [38400/53288 (72%)]	Loss: 188.266388 | Elapsed: 12.49s
01/28/2023 11:36:35 AM  [*] Sat Jan 28 11:36:35 2023: Train Epoch: 10 [44800/53288 (84%)]	Loss: 176.646164 | Elapsed: 12.38s
01/28/2023 11:36:47 AM  [*] Sat Jan 28 11:36:47 2023: Train Epoch: 10 [51200/53288 (96%)]	Loss: 186.639435 | Elapsed: 12.32s
01/28/2023 11:36:52 AM  [*] Sat Jan 28 11:36:52 2023:   10    | Tr.loss: 179.696627 | Elapsed:  104.67  s
01/28/2023 11:36:53 AM [!] Sat Jan 28 11:36:53 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674902212-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674902212-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674902212-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674902212-auc.npy
01/28/2023 11:36:54 AM  [!] Training pretrained model on downstream task...
01/28/2023 11:36:54 AM  [*] Started epoch: 1
01/28/2023 11:36:54 AM  [*] Sat Jan 28 11:36:54 2023: Train Epoch: 1 [  0  /22838 (0 %)]	Loss: 2.809530 | Elapsed: 0.36s | FPR 0.0003 -> TPR 0.1020 & F1 0.1852
01/28/2023 11:37:03 AM  [*] Sat Jan 28 11:37:03 2023: Train Epoch: 1 [6400 /22838 (28%)]	Loss: 0.498429 | Elapsed: 9.11s | FPR 0.0003 -> TPR 0.4028 & F1 0.5743
01/28/2023 11:37:12 AM  [*] Sat Jan 28 11:37:12 2023: Train Epoch: 1 [12800/22838 (56%)]	Loss: 0.373982 | Elapsed: 9.10s | FPR 0.0003 -> TPR 0.3906 & F1 0.5618
01/28/2023 11:37:22 AM  [*] Sat Jan 28 11:37:22 2023: Train Epoch: 1 [19200/22838 (84%)]	Loss: 0.388746 | Elapsed: 9.11s | FPR 0.0003 -> TPR 0.2931 & F1 0.4533
01/28/2023 11:37:27 AM  [*] Sat Jan 28 11:37:27 2023:    1    | Tr.loss: 0.508791 | Elapsed:   33.33  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8025
01/28/2023 11:37:27 AM  [*] Started epoch: 2
01/28/2023 11:37:27 AM  [*] Sat Jan 28 11:37:27 2023: Train Epoch: 2 [  0  /22838 (0 %)]	Loss: 0.386587 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.2174 & F1 0.3571
01/28/2023 11:37:36 AM  [*] Sat Jan 28 11:37:36 2023: Train Epoch: 2 [6400 /22838 (28%)]	Loss: 0.398477 | Elapsed: 9.09s | FPR 0.0003 -> TPR 0.6094 & F1 0.7573
01/28/2023 11:37:45 AM  [*] Sat Jan 28 11:37:45 2023: Train Epoch: 2 [12800/22838 (56%)]	Loss: 0.362650 | Elapsed: 9.09s | FPR 0.0003 -> TPR 0.4333 & F1 0.6047
01/28/2023 11:37:55 AM  [*] Sat Jan 28 11:37:55 2023: Train Epoch: 2 [19200/22838 (84%)]	Loss: 0.307140 | Elapsed: 9.10s | FPR 0.0003 -> TPR 0.6491 & F1 0.7872
01/28/2023 11:38:00 AM  [*] Sat Jan 28 11:38:00 2023:    2    | Tr.loss: 0.305358 | Elapsed:   33.01  s | FPR 0.0003 -> TPR: 0.19 & F1: 0.32 | AUC: 0.9271
01/28/2023 11:38:00 AM  [*] Started epoch: 3
01/28/2023 11:38:00 AM  [*] Sat Jan 28 11:38:00 2023: Train Epoch: 3 [  0  /22838 (0 %)]	Loss: 0.250132 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.7083 & F1 0.8293
01/28/2023 11:38:09 AM  [*] Sat Jan 28 11:38:09 2023: Train Epoch: 3 [6400 /22838 (28%)]	Loss: 0.179051 | Elapsed: 9.08s | FPR 0.0003 -> TPR 0.8356 & F1 0.9104
01/28/2023 11:38:18 AM  [*] Sat Jan 28 11:38:18 2023: Train Epoch: 3 [12800/22838 (56%)]	Loss: 0.249117 | Elapsed: 9.11s | FPR 0.0003 -> TPR 0.7681 & F1 0.8689
01/28/2023 11:38:28 AM  [*] Sat Jan 28 11:38:28 2023: Train Epoch: 3 [19200/22838 (84%)]	Loss: 0.249816 | Elapsed: 9.11s | FPR 0.0003 -> TPR 0.7922 & F1 0.8841
01/28/2023 11:38:33 AM  [*] Sat Jan 28 11:38:33 2023:    3    | Tr.loss: 0.204477 | Elapsed:   33.03  s | FPR 0.0003 -> TPR: 0.45 & F1: 0.62 | AUC: 0.9705
01/28/2023 11:38:34 AM [!] Sat Jan 28 11:38:34 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674902313-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674902313-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674902313-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674902313-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674902313-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674902313-trainTPRs.npy
01/28/2023 11:38:34 AM  [!] Training non_pretrained model on downstream task...
01/28/2023 11:38:34 AM  [*] Started epoch: 1
01/28/2023 11:38:34 AM  [*] Sat Jan 28 11:38:34 2023: Train Epoch: 1 [  0  /22838 (0 %)]	Loss: 1.968449 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0256 & F1 0.0500
01/28/2023 11:38:40 AM  [*] Sat Jan 28 11:38:40 2023: Train Epoch: 1 [6400 /22838 (28%)]	Loss: 0.521723 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.2424 & F1 0.3902
01/28/2023 11:38:47 AM  [*] Sat Jan 28 11:38:47 2023: Train Epoch: 1 [12800/22838 (56%)]	Loss: 0.367563 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.3810 & F1 0.5517
01/28/2023 11:38:53 AM  [*] Sat Jan 28 11:38:53 2023: Train Epoch: 1 [19200/22838 (84%)]	Loss: 0.373733 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.4306 & F1 0.6019
01/28/2023 11:38:57 AM  [*] Sat Jan 28 11:38:57 2023:    1    | Tr.loss: 0.428112 | Elapsed:   22.75  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.8574
01/28/2023 11:38:57 AM  [*] Started epoch: 2
01/28/2023 11:38:57 AM  [*] Sat Jan 28 11:38:57 2023: Train Epoch: 2 [  0  /22838 (0 %)]	Loss: 0.304296 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.7619 & F1 0.8649
01/28/2023 11:39:03 AM  [*] Sat Jan 28 11:39:03 2023: Train Epoch: 2 [6400 /22838 (28%)]	Loss: 0.237697 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667
01/28/2023 11:39:09 AM  [*] Sat Jan 28 11:39:09 2023: Train Epoch: 2 [12800/22838 (56%)]	Loss: 0.295483 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.5424 & F1 0.7033
01/28/2023 11:39:16 AM  [*] Sat Jan 28 11:39:16 2023: Train Epoch: 2 [19200/22838 (84%)]	Loss: 0.242612 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7467 & F1 0.8550
01/28/2023 11:39:20 AM  [*] Sat Jan 28 11:39:20 2023:    2    | Tr.loss: 0.260677 | Elapsed:   22.82  s | FPR 0.0003 -> TPR: 0.40 & F1: 0.57 | AUC: 0.9508
01/28/2023 11:39:20 AM  [*] Started epoch: 3
01/28/2023 11:39:20 AM  [*] Sat Jan 28 11:39:20 2023: Train Epoch: 3 [  0  /22838 (0 %)]	Loss: 0.171555 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.8140 & F1 0.8974
01/28/2023 11:39:26 AM  [*] Sat Jan 28 11:39:26 2023: Train Epoch: 3 [6400 /22838 (28%)]	Loss: 0.216214 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7200 & F1 0.8372
01/28/2023 11:39:32 AM  [*] Sat Jan 28 11:39:32 2023: Train Epoch: 3 [12800/22838 (56%)]	Loss: 0.238451 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.4559 & F1 0.6263
01/28/2023 11:39:37 AM [!] Learning rate: 2.5e-05
01/28/2023 11:39:38 AM  [*] Sat Jan 28 11:39:38 2023: Train Epoch: 3 [19200/22838 (84%)]	Loss: 0.238115 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.5867 & F1 0.7395
01/28/2023 11:39:42 AM  [*] Sat Jan 28 11:39:42 2023:    3    | Tr.loss: 0.187325 | Elapsed:   22.74  s | FPR 0.0003 -> TPR: 0.38 & F1: 0.55 | AUC: 0.9754
01/28/2023 11:39:43 AM [!] Sat Jan 28 11:39:43 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674902382-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674902382-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674902382-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674902382-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674902382-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674902382-trainTPRs.npy
01/28/2023 11:39:43 AM  [!] Training full_data model on downstream task...
01/28/2023 11:39:43 AM  [*] Started epoch: 1
01/28/2023 11:39:43 AM  [*] Sat Jan 28 11:39:43 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 2.889488 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0488 & F1 0.0930
01/28/2023 11:39:50 AM  [*] Sat Jan 28 11:39:50 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.431735 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.3077 & F1 0.4706
01/28/2023 11:39:56 AM  [*] Sat Jan 28 11:39:56 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.470656 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.3636 & F1 0.5333
01/28/2023 11:40:02 AM  [*] Sat Jan 28 11:40:02 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.312541 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.4058 & F1 0.5773
01/28/2023 11:40:08 AM  [*] Sat Jan 28 11:40:08 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.370312 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.5286 & F1 0.6916
01/28/2023 11:40:14 AM  [*] Sat Jan 28 11:40:14 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.336713 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.7681 & F1 0.8689
01/28/2023 11:40:21 AM  [*] Sat Jan 28 11:40:21 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.143939 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.9306 & F1 0.9640
01/28/2023 11:40:27 AM  [*] Sat Jan 28 11:40:27 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.375644 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.6351 & F1 0.7769
01/28/2023 11:40:33 AM  [*] Sat Jan 28 11:40:33 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.271087 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.4923 & F1 0.6598
01/28/2023 11:40:39 AM  [*] Sat Jan 28 11:40:39 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.180238 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.8611 & F1 0.9254
01/28/2023 11:40:46 AM [!] Learning rate: 2.5e-05
01/28/2023 11:40:46 AM  [*] Sat Jan 28 11:40:46 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.276911 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.4925 & F1 0.6600
01/28/2023 11:40:52 AM  [*] Sat Jan 28 11:40:52 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.245910 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.6571 & F1 0.7931
01/28/2023 11:40:59 AM  [*] Sat Jan 28 11:40:59 2023:    1    | Tr.loss: 0.303772 | Elapsed:   76.04  s | FPR 0.0003 -> TPR: 0.04 & F1: 0.08 | AUC: 0.9327
01/28/2023 11:40:59 AM  [*] Started epoch: 2
01/28/2023 11:40:59 AM  [*] Sat Jan 28 11:40:59 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.144908 | Elapsed: 0.12s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000
01/28/2023 11:41:06 AM  [*] Sat Jan 28 11:41:06 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.164411 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.8125 & F1 0.8966
01/28/2023 11:41:12 AM  [*] Sat Jan 28 11:41:12 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.140727 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8475 & F1 0.9174
01/28/2023 11:41:18 AM  [*] Sat Jan 28 11:41:18 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.128249 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.5970 & F1 0.7477
01/28/2023 11:41:24 AM  [*] Sat Jan 28 11:41:24 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.103543 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.9104 & F1 0.9531
01/28/2023 11:41:31 AM  [*] Sat Jan 28 11:41:31 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.166172 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.7910 & F1 0.8833
01/28/2023 11:41:37 AM  [*] Sat Jan 28 11:41:37 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.180287 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.8611 & F1 0.9254
01/28/2023 11:41:43 AM  [*] Sat Jan 28 11:41:43 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.178108 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.6970 & F1 0.8214
01/28/2023 11:41:49 AM  [*] Sat Jan 28 11:41:49 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.111258 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8676 & F1 0.9291
01/28/2023 11:41:50 AM [!] Learning rate: 2.5e-06
01/28/2023 11:41:56 AM  [*] Sat Jan 28 11:41:56 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.187369 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.6769 & F1 0.8073
01/28/2023 11:42:02 AM  [*] Sat Jan 28 11:42:02 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.222857 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.7576 & F1 0.8621
01/28/2023 11:42:08 AM  [*] Sat Jan 28 11:42:08 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.175056 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.7808 & F1 0.8769
01/28/2023 11:42:15 AM  [*] Sat Jan 28 11:42:15 2023:    2    | Tr.loss: 0.172457 | Elapsed:   76.09  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9792
01/28/2023 11:42:15 AM  [*] Started epoch: 3
01/28/2023 11:42:15 AM  [*] Sat Jan 28 11:42:15 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.191794 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.8421 & F1 0.9143
01/28/2023 11:42:22 AM  [*] Sat Jan 28 11:42:22 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.254575 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.7612 & F1 0.8644
01/28/2023 11:42:28 AM  [*] Sat Jan 28 11:42:28 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.103341 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.8462 & F1 0.9167
01/28/2023 11:42:34 AM  [*] Sat Jan 28 11:42:34 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.153644 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.8413 & F1 0.9138
01/28/2023 11:42:40 AM  [*] Sat Jan 28 11:42:40 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.138977 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.9254 & F1 0.9612
01/28/2023 11:42:47 AM  [*] Sat Jan 28 11:42:47 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.156479 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.5385 & F1 0.7000
01/28/2023 11:42:53 AM  [*] Sat Jan 28 11:42:53 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.092896 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.9342 & F1 0.9660
01/28/2023 11:42:54 AM [!] Learning rate: 2.5000000000000004e-07
01/28/2023 11:42:59 AM  [*] Sat Jan 28 11:42:59 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.098656 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.9531 & F1 0.9760
01/28/2023 11:43:05 AM  [*] Sat Jan 28 11:43:05 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.181050 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.7534 & F1 0.8594
01/28/2023 11:43:12 AM  [*] Sat Jan 28 11:43:12 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.108847 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.8873 & F1 0.9403
01/28/2023 11:43:18 AM  [*] Sat Jan 28 11:43:18 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.137318 | Elapsed: 6.22s | FPR 0.0003 -> TPR 0.6769 & F1 0.8073
01/28/2023 11:43:24 AM  [*] Sat Jan 28 11:43:24 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.163366 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.8209 & F1 0.9016
01/28/2023 11:43:31 AM  [*] Sat Jan 28 11:43:31 2023:    3    | Tr.loss: 0.165322 | Elapsed:   76.08  s | FPR 0.0003 -> TPR: 0.48 & F1: 0.65 | AUC: 0.9810
01/28/2023 11:43:32 AM [!] Sat Jan 28 11:43:32 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674902611-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674902611-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674902611-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674902611-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674902611-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674902611-trainTPRs.npy
01/28/2023 11:43:32 AM  [*] Evaluating pretrained model on test set...
01/28/2023 11:43:37 AM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.1580 | F1: 0.2729
01/28/2023 11:43:37 AM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2803 | F1: 0.4378
01/28/2023 11:43:37 AM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3108 | F1: 0.4739
01/28/2023 11:43:37 AM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3740 | F1: 0.5434
01/28/2023 11:43:37 AM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4458 | F1: 0.6131
01/28/2023 11:43:37 AM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5801 | F1: 0.7226
01/28/2023 11:43:37 AM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7471 | F1: 0.8155
01/28/2023 11:43:37 AM  [*] Evaluating non_pretrained model on test set...
01/28/2023 11:43:42 AM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.1197 | F1: 0.2139
01/28/2023 11:43:42 AM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2190 | F1: 0.3592
01/28/2023 11:43:42 AM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2926 | F1: 0.4524
01/28/2023 11:43:42 AM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3608 | F1: 0.5293
01/28/2023 11:43:42 AM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4153 | F1: 0.5834
01/28/2023 11:43:42 AM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5325 | F1: 0.6835
01/28/2023 11:43:42 AM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8077 | F1: 0.8535
01/28/2023 11:43:42 AM  [*] Evaluating full_data model on test set...
01/28/2023 11:43:47 AM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0287 | F1: 0.0558
01/28/2023 11:43:47 AM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2007 | F1: 0.3342
01/28/2023 11:43:47 AM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3272 | F1: 0.4928
01/28/2023 11:43:47 AM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3656 | F1: 0.5345
01/28/2023 11:43:47 AM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4382 | F1: 0.6057
01/28/2023 11:43:47 AM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5576 | F1: 0.7044
01/28/2023 11:43:47 AM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7903 | F1: 0.8429
01/28/2023 11:43:47 AM  [!] Running pre-training split 3/3
01/28/2023 11:43:50 AM  [!] Pre-training model...
01/28/2023 11:43:51 AM  [*] Masking sequences...
01/28/2023 11:44:08 AM  [*] Started epoch: 1
01/28/2023 11:44:09 AM  [*] Sat Jan 28 11:44:09 2023: Train Epoch: 1 [  0  /53288 (0 %)]	Loss: 450.779633 | Elapsed: 0.82s
01/28/2023 11:44:22 AM  [*] Sat Jan 28 11:44:22 2023: Train Epoch: 1 [6400 /53288 (12%)]	Loss: 244.692200 | Elapsed: 12.34s
01/28/2023 11:44:34 AM  [*] Sat Jan 28 11:44:34 2023: Train Epoch: 1 [12800/53288 (24%)]	Loss: 225.739304 | Elapsed: 12.34s
01/28/2023 11:44:46 AM  [*] Sat Jan 28 11:44:46 2023: Train Epoch: 1 [19200/53288 (36%)]	Loss: 195.300537 | Elapsed: 12.37s
01/28/2023 11:44:59 AM  [*] Sat Jan 28 11:44:59 2023: Train Epoch: 1 [25600/53288 (48%)]	Loss: 215.400345 | Elapsed: 12.46s
01/28/2023 11:45:11 AM  [*] Sat Jan 28 11:45:11 2023: Train Epoch: 1 [32000/53288 (60%)]	Loss: 206.178238 | Elapsed: 12.45s
01/28/2023 11:45:24 AM  [*] Sat Jan 28 11:45:24 2023: Train Epoch: 1 [38400/53288 (72%)]	Loss: 210.721405 | Elapsed: 12.47s
01/28/2023 11:45:36 AM  [*] Sat Jan 28 11:45:36 2023: Train Epoch: 1 [44800/53288 (84%)]	Loss: 207.406738 | Elapsed: 12.58s
01/28/2023 11:45:49 AM  [*] Sat Jan 28 11:45:49 2023: Train Epoch: 1 [51200/53288 (96%)]	Loss: 180.476440 | Elapsed: 12.49s
01/28/2023 11:45:54 AM  [*] Sat Jan 28 11:45:54 2023:    1    | Tr.loss: 211.287931 | Elapsed:  105.64  s
01/28/2023 11:45:54 AM  [*] Started epoch: 2
01/28/2023 11:45:54 AM  [*] Sat Jan 28 11:45:54 2023: Train Epoch: 2 [  0  /53288 (0 %)]	Loss: 205.208527 | Elapsed: 0.13s
01/28/2023 11:46:07 AM  [*] Sat Jan 28 11:46:07 2023: Train Epoch: 2 [6400 /53288 (12%)]	Loss: 181.988113 | Elapsed: 12.43s
01/28/2023 11:46:19 AM  [*] Sat Jan 28 11:46:19 2023: Train Epoch: 2 [12800/53288 (24%)]	Loss: 217.278687 | Elapsed: 12.39s
01/28/2023 11:46:31 AM  [*] Sat Jan 28 11:46:31 2023: Train Epoch: 2 [19200/53288 (36%)]	Loss: 192.755829 | Elapsed: 12.49s
01/28/2023 11:46:44 AM  [*] Sat Jan 28 11:46:44 2023: Train Epoch: 2 [25600/53288 (48%)]	Loss: 175.081009 | Elapsed: 12.42s
01/28/2023 11:46:56 AM  [*] Sat Jan 28 11:46:56 2023: Train Epoch: 2 [32000/53288 (60%)]	Loss: 175.980530 | Elapsed: 12.48s
01/28/2023 11:47:09 AM  [*] Sat Jan 28 11:47:09 2023: Train Epoch: 2 [38400/53288 (72%)]	Loss: 199.049530 | Elapsed: 12.41s
01/28/2023 11:47:21 AM  [*] Sat Jan 28 11:47:21 2023: Train Epoch: 2 [44800/53288 (84%)]	Loss: 181.015579 | Elapsed: 12.44s
01/28/2023 11:47:34 AM  [*] Sat Jan 28 11:47:34 2023: Train Epoch: 2 [51200/53288 (96%)]	Loss: 188.517395 | Elapsed: 12.40s
01/28/2023 11:47:39 AM  [*] Sat Jan 28 11:47:39 2023:    2    | Tr.loss: 189.990941 | Elapsed:  104.94  s
01/28/2023 11:47:39 AM  [*] Started epoch: 3
01/28/2023 11:47:39 AM  [*] Sat Jan 28 11:47:39 2023: Train Epoch: 3 [  0  /53288 (0 %)]	Loss: 176.173645 | Elapsed: 0.14s
01/28/2023 11:47:52 AM  [*] Sat Jan 28 11:47:52 2023: Train Epoch: 3 [6400 /53288 (12%)]	Loss: 178.588165 | Elapsed: 12.48s
01/28/2023 11:48:04 AM  [*] Sat Jan 28 11:48:04 2023: Train Epoch: 3 [12800/53288 (24%)]	Loss: 171.456726 | Elapsed: 12.38s
01/28/2023 11:48:16 AM  [*] Sat Jan 28 11:48:16 2023: Train Epoch: 3 [19200/53288 (36%)]	Loss: 179.670288 | Elapsed: 12.42s
01/28/2023 11:48:29 AM  [*] Sat Jan 28 11:48:29 2023: Train Epoch: 3 [25600/53288 (48%)]	Loss: 174.205353 | Elapsed: 12.38s
01/28/2023 11:48:41 AM  [*] Sat Jan 28 11:48:41 2023: Train Epoch: 3 [32000/53288 (60%)]	Loss: 181.726761 | Elapsed: 12.41s
01/28/2023 11:48:54 AM  [*] Sat Jan 28 11:48:54 2023: Train Epoch: 3 [38400/53288 (72%)]	Loss: 194.292740 | Elapsed: 12.48s
01/28/2023 11:49:06 AM  [*] Sat Jan 28 11:49:06 2023: Train Epoch: 3 [44800/53288 (84%)]	Loss: 191.617249 | Elapsed: 12.40s
01/28/2023 11:49:18 AM  [*] Sat Jan 28 11:49:18 2023: Train Epoch: 3 [51200/53288 (96%)]	Loss: 166.474731 | Elapsed: 12.42s
01/28/2023 11:49:24 AM  [*] Sat Jan 28 11:49:24 2023:    3    | Tr.loss: 184.219615 | Elapsed:  104.83  s
01/28/2023 11:49:24 AM  [*] Started epoch: 4
01/28/2023 11:49:24 AM  [*] Sat Jan 28 11:49:24 2023: Train Epoch: 4 [  0  /53288 (0 %)]	Loss: 175.078064 | Elapsed: 0.13s
01/28/2023 11:49:36 AM  [*] Sat Jan 28 11:49:36 2023: Train Epoch: 4 [6400 /53288 (12%)]	Loss: 177.877457 | Elapsed: 12.38s
01/28/2023 11:49:49 AM  [*] Sat Jan 28 11:49:49 2023: Train Epoch: 4 [12800/53288 (24%)]	Loss: 169.197540 | Elapsed: 12.46s
01/28/2023 11:50:01 AM  [*] Sat Jan 28 11:50:01 2023: Train Epoch: 4 [19200/53288 (36%)]	Loss: 197.549774 | Elapsed: 12.37s
01/28/2023 11:50:14 AM  [*] Sat Jan 28 11:50:14 2023: Train Epoch: 4 [25600/53288 (48%)]	Loss: 179.151978 | Elapsed: 12.47s
01/28/2023 11:50:26 AM  [*] Sat Jan 28 11:50:26 2023: Train Epoch: 4 [32000/53288 (60%)]	Loss: 160.748901 | Elapsed: 12.44s
01/28/2023 11:50:38 AM  [*] Sat Jan 28 11:50:38 2023: Train Epoch: 4 [38400/53288 (72%)]	Loss: 192.383362 | Elapsed: 12.40s
01/28/2023 11:50:51 AM  [*] Sat Jan 28 11:50:51 2023: Train Epoch: 4 [44800/53288 (84%)]	Loss: 192.808685 | Elapsed: 12.45s
01/28/2023 11:51:03 AM  [*] Sat Jan 28 11:51:03 2023: Train Epoch: 4 [51200/53288 (96%)]	Loss: 187.840271 | Elapsed: 12.42s
01/28/2023 11:51:09 AM  [*] Sat Jan 28 11:51:09 2023:    4    | Tr.loss: 180.901925 | Elapsed:  104.91  s
01/28/2023 11:51:09 AM  [*] Started epoch: 5
01/28/2023 11:51:09 AM  [*] Sat Jan 28 11:51:09 2023: Train Epoch: 5 [  0  /53288 (0 %)]	Loss: 183.548340 | Elapsed: 0.13s
01/28/2023 11:51:21 AM  [*] Sat Jan 28 11:51:21 2023: Train Epoch: 5 [6400 /53288 (12%)]	Loss: 182.940231 | Elapsed: 12.64s
01/28/2023 11:51:34 AM  [*] Sat Jan 28 11:51:34 2023: Train Epoch: 5 [12800/53288 (24%)]	Loss: 179.311783 | Elapsed: 12.85s
01/28/2023 11:51:47 AM  [*] Sat Jan 28 11:51:47 2023: Train Epoch: 5 [19200/53288 (36%)]	Loss: 195.058563 | Elapsed: 12.73s
01/28/2023 11:52:00 AM  [*] Sat Jan 28 11:52:00 2023: Train Epoch: 5 [25600/53288 (48%)]	Loss: 180.651840 | Elapsed: 12.49s
01/28/2023 11:52:12 AM  [*] Sat Jan 28 11:52:12 2023: Train Epoch: 5 [32000/53288 (60%)]	Loss: 200.733292 | Elapsed: 12.36s
01/28/2023 11:52:24 AM  [*] Sat Jan 28 11:52:24 2023: Train Epoch: 5 [38400/53288 (72%)]	Loss: 182.011703 | Elapsed: 12.40s
01/28/2023 11:52:37 AM  [*] Sat Jan 28 11:52:37 2023: Train Epoch: 5 [44800/53288 (84%)]	Loss: 162.852524 | Elapsed: 12.38s
01/28/2023 11:52:49 AM  [*] Sat Jan 28 11:52:49 2023: Train Epoch: 5 [51200/53288 (96%)]	Loss: 172.863129 | Elapsed: 12.40s
01/28/2023 11:52:54 AM  [*] Sat Jan 28 11:52:54 2023:    5    | Tr.loss: 178.645072 | Elapsed:  105.72  s
01/28/2023 11:52:54 AM  [*] Started epoch: 6
01/28/2023 11:52:55 AM  [*] Sat Jan 28 11:52:55 2023: Train Epoch: 6 [  0  /53288 (0 %)]	Loss: 182.508331 | Elapsed: 0.12s
01/28/2023 11:53:07 AM  [*] Sat Jan 28 11:53:07 2023: Train Epoch: 6 [6400 /53288 (12%)]	Loss: 188.855713 | Elapsed: 12.46s
01/28/2023 11:53:20 AM  [*] Sat Jan 28 11:53:20 2023: Train Epoch: 6 [12800/53288 (24%)]	Loss: 180.318542 | Elapsed: 12.64s
01/28/2023 11:53:32 AM  [*] Sat Jan 28 11:53:32 2023: Train Epoch: 6 [19200/53288 (36%)]	Loss: 202.611191 | Elapsed: 12.55s
01/28/2023 11:53:45 AM  [*] Sat Jan 28 11:53:45 2023: Train Epoch: 6 [25600/53288 (48%)]	Loss: 177.585678 | Elapsed: 12.50s
01/28/2023 11:53:57 AM  [*] Sat Jan 28 11:53:57 2023: Train Epoch: 6 [32000/53288 (60%)]	Loss: 179.735321 | Elapsed: 12.53s
01/28/2023 11:54:10 AM  [*] Sat Jan 28 11:54:10 2023: Train Epoch: 6 [38400/53288 (72%)]	Loss: 179.891418 | Elapsed: 12.48s
01/28/2023 11:54:22 AM  [*] Sat Jan 28 11:54:22 2023: Train Epoch: 6 [44800/53288 (84%)]	Loss: 163.239258 | Elapsed: 12.34s
01/28/2023 11:54:34 AM  [*] Sat Jan 28 11:54:34 2023: Train Epoch: 6 [51200/53288 (96%)]	Loss: 171.295532 | Elapsed: 12.44s
01/28/2023 11:54:40 AM  [*] Sat Jan 28 11:54:40 2023:    6    | Tr.loss: 177.340000 | Elapsed:  105.45  s
01/28/2023 11:54:40 AM  [*] Started epoch: 7
01/28/2023 11:54:40 AM  [*] Sat Jan 28 11:54:40 2023: Train Epoch: 7 [  0  /53288 (0 %)]	Loss: 168.384583 | Elapsed: 0.13s
01/28/2023 11:54:40 AM [!] Learning rate: 2.5e-05
01/28/2023 11:54:52 AM  [*] Sat Jan 28 11:54:52 2023: Train Epoch: 7 [6400 /53288 (12%)]	Loss: 198.526245 | Elapsed: 12.38s
01/28/2023 11:55:05 AM  [*] Sat Jan 28 11:55:05 2023: Train Epoch: 7 [12800/53288 (24%)]	Loss: 188.732529 | Elapsed: 12.27s
01/28/2023 11:55:17 AM  [*] Sat Jan 28 11:55:17 2023: Train Epoch: 7 [19200/53288 (36%)]	Loss: 159.140625 | Elapsed: 12.26s
01/28/2023 11:55:29 AM  [*] Sat Jan 28 11:55:29 2023: Train Epoch: 7 [25600/53288 (48%)]	Loss: 162.469330 | Elapsed: 12.24s
01/28/2023 11:55:41 AM  [*] Sat Jan 28 11:55:41 2023: Train Epoch: 7 [32000/53288 (60%)]	Loss: 164.753113 | Elapsed: 12.30s
01/28/2023 11:55:54 AM  [*] Sat Jan 28 11:55:54 2023: Train Epoch: 7 [38400/53288 (72%)]	Loss: 167.707870 | Elapsed: 12.52s
01/28/2023 11:56:06 AM  [*] Sat Jan 28 11:56:06 2023: Train Epoch: 7 [44800/53288 (84%)]	Loss: 175.885071 | Elapsed: 12.30s
01/28/2023 11:56:18 AM  [*] Sat Jan 28 11:56:18 2023: Train Epoch: 7 [51200/53288 (96%)]	Loss: 200.114380 | Elapsed: 12.23s
01/28/2023 11:56:24 AM  [*] Sat Jan 28 11:56:24 2023:    7    | Tr.loss: 175.871893 | Elapsed:  103.77  s
01/28/2023 11:56:24 AM  [*] Started epoch: 8
01/28/2023 11:56:24 AM  [*] Sat Jan 28 11:56:24 2023: Train Epoch: 8 [  0  /53288 (0 %)]	Loss: 169.229279 | Elapsed: 0.14s
01/28/2023 11:56:36 AM  [*] Sat Jan 28 11:56:36 2023: Train Epoch: 8 [6400 /53288 (12%)]	Loss: 163.572662 | Elapsed: 12.38s
01/28/2023 11:56:48 AM  [*] Sat Jan 28 11:56:48 2023: Train Epoch: 8 [12800/53288 (24%)]	Loss: 174.036255 | Elapsed: 12.26s
01/28/2023 11:57:01 AM  [*] Sat Jan 28 11:57:01 2023: Train Epoch: 8 [19200/53288 (36%)]	Loss: 170.230774 | Elapsed: 12.23s
01/28/2023 11:57:13 AM  [*] Sat Jan 28 11:57:13 2023: Train Epoch: 8 [25600/53288 (48%)]	Loss: 197.265854 | Elapsed: 12.24s
01/28/2023 11:57:25 AM  [*] Sat Jan 28 11:57:25 2023: Train Epoch: 8 [32000/53288 (60%)]	Loss: 171.032410 | Elapsed: 12.25s
01/28/2023 11:57:37 AM  [*] Sat Jan 28 11:57:37 2023: Train Epoch: 8 [38400/53288 (72%)]	Loss: 168.759201 | Elapsed: 12.24s
01/28/2023 11:57:50 AM  [*] Sat Jan 28 11:57:50 2023: Train Epoch: 8 [44800/53288 (84%)]	Loss: 180.289566 | Elapsed: 12.18s
01/28/2023 11:58:02 AM  [*] Sat Jan 28 11:58:02 2023: Train Epoch: 8 [51200/53288 (96%)]	Loss: 188.618774 | Elapsed: 12.21s
01/28/2023 11:58:07 AM  [*] Sat Jan 28 11:58:07 2023:    8    | Tr.loss: 175.577463 | Elapsed:  103.31  s
01/28/2023 11:58:07 AM  [*] Started epoch: 9
01/28/2023 11:58:07 AM  [*] Sat Jan 28 11:58:07 2023: Train Epoch: 9 [  0  /53288 (0 %)]	Loss: 174.079010 | Elapsed: 0.14s
01/28/2023 11:58:19 AM  [*] Sat Jan 28 11:58:19 2023: Train Epoch: 9 [6400 /53288 (12%)]	Loss: 179.773361 | Elapsed: 12.31s
01/28/2023 11:58:32 AM  [*] Sat Jan 28 11:58:32 2023: Train Epoch: 9 [12800/53288 (24%)]	Loss: 178.839447 | Elapsed: 12.26s
01/28/2023 11:58:44 AM  [*] Sat Jan 28 11:58:44 2023: Train Epoch: 9 [19200/53288 (36%)]	Loss: 176.043121 | Elapsed: 12.33s
01/28/2023 11:58:56 AM  [*] Sat Jan 28 11:58:56 2023: Train Epoch: 9 [25600/53288 (48%)]	Loss: 181.421021 | Elapsed: 12.22s
01/28/2023 11:59:08 AM  [*] Sat Jan 28 11:59:08 2023: Train Epoch: 9 [32000/53288 (60%)]	Loss: 159.347229 | Elapsed: 12.27s
01/28/2023 11:59:21 AM  [*] Sat Jan 28 11:59:21 2023: Train Epoch: 9 [38400/53288 (72%)]	Loss: 198.224335 | Elapsed: 12.18s
01/28/2023 11:59:33 AM  [*] Sat Jan 28 11:59:33 2023: Train Epoch: 9 [44800/53288 (84%)]	Loss: 172.975449 | Elapsed: 12.23s
01/28/2023 11:59:45 AM  [*] Sat Jan 28 11:59:45 2023: Train Epoch: 9 [51200/53288 (96%)]	Loss: 194.777237 | Elapsed: 12.22s
01/28/2023 11:59:50 AM  [*] Sat Jan 28 11:59:50 2023:    9    | Tr.loss: 175.413676 | Elapsed:  103.41  s
01/28/2023 11:59:50 AM  [*] Started epoch: 10
01/28/2023 11:59:50 AM  [*] Sat Jan 28 11:59:50 2023: Train Epoch: 10 [  0  /53288 (0 %)]	Loss: 165.481812 | Elapsed: 0.13s
01/28/2023 12:00:03 PM  [*] Sat Jan 28 12:00:03 2023: Train Epoch: 10 [6400 /53288 (12%)]	Loss: 181.285919 | Elapsed: 12.28s
01/28/2023 12:00:15 PM  [*] Sat Jan 28 12:00:15 2023: Train Epoch: 10 [12800/53288 (24%)]	Loss: 169.865936 | Elapsed: 12.34s
01/28/2023 12:00:28 PM  [*] Sat Jan 28 12:00:28 2023: Train Epoch: 10 [19200/53288 (36%)]	Loss: 182.968292 | Elapsed: 12.41s
01/28/2023 12:00:40 PM  [*] Sat Jan 28 12:00:40 2023: Train Epoch: 10 [25600/53288 (48%)]	Loss: 183.834381 | Elapsed: 12.26s
01/28/2023 12:00:52 PM  [*] Sat Jan 28 12:00:52 2023: Train Epoch: 10 [32000/53288 (60%)]	Loss: 185.223907 | Elapsed: 12.24s
01/28/2023 12:01:04 PM  [*] Sat Jan 28 12:01:04 2023: Train Epoch: 10 [38400/53288 (72%)]	Loss: 186.750641 | Elapsed: 12.33s
01/28/2023 12:01:17 PM  [*] Sat Jan 28 12:01:17 2023: Train Epoch: 10 [44800/53288 (84%)]	Loss: 178.589844 | Elapsed: 12.23s
01/28/2023 12:01:29 PM  [*] Sat Jan 28 12:01:29 2023: Train Epoch: 10 [51200/53288 (96%)]	Loss: 183.985107 | Elapsed: 12.26s
01/28/2023 12:01:34 PM  [*] Sat Jan 28 12:01:34 2023:   10    | Tr.loss: 175.151651 | Elapsed:  103.84  s
01/28/2023 12:01:35 PM [!] Sat Jan 28 12:01:35 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674903694-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674903694-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674903694-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\preTraining\training_files\1674903694-auc.npy
01/28/2023 12:01:35 PM  [!] Training pretrained model on downstream task...
01/28/2023 12:01:35 PM  [*] Started epoch: 1
01/28/2023 12:01:36 PM  [*] Sat Jan 28 12:01:36 2023: Train Epoch: 1 [  0  /22838 (0 %)]	Loss: 3.877881 | Elapsed: 0.35s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000
01/28/2023 12:01:45 PM  [*] Sat Jan 28 12:01:45 2023: Train Epoch: 1 [6400 /22838 (28%)]	Loss: 0.341615 | Elapsed: 9.09s | FPR 0.0003 -> TPR 0.4625 & F1 0.6325
01/28/2023 12:01:54 PM  [*] Sat Jan 28 12:01:54 2023: Train Epoch: 1 [12800/22838 (56%)]	Loss: 0.383633 | Elapsed: 9.07s | FPR 0.0003 -> TPR 0.1406 & F1 0.2466
01/28/2023 12:02:03 PM  [*] Sat Jan 28 12:02:03 2023: Train Epoch: 1 [19200/22838 (84%)]	Loss: 0.411885 | Elapsed: 9.08s | FPR 0.0003 -> TPR 0.4030 & F1 0.5745
01/28/2023 12:02:09 PM  [*] Sat Jan 28 12:02:09 2023:    1    | Tr.loss: 0.456360 | Elapsed:   33.18  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8507
01/28/2023 12:02:09 PM  [*] Started epoch: 2
01/28/2023 12:02:09 PM  [*] Sat Jan 28 12:02:09 2023: Train Epoch: 2 [  0  /22838 (0 %)]	Loss: 0.331621 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.7209 & F1 0.8378
01/28/2023 12:02:18 PM  [*] Sat Jan 28 12:02:18 2023: Train Epoch: 2 [6400 /22838 (28%)]	Loss: 0.202780 | Elapsed: 9.06s | FPR 0.0003 -> TPR 0.7000 & F1 0.8235
01/28/2023 12:02:27 PM  [*] Sat Jan 28 12:02:27 2023: Train Epoch: 2 [12800/22838 (56%)]	Loss: 0.184082 | Elapsed: 9.07s | FPR 0.0003 -> TPR 0.7424 & F1 0.8522
01/28/2023 12:02:36 PM  [*] Sat Jan 28 12:02:36 2023: Train Epoch: 2 [19200/22838 (84%)]	Loss: 0.213074 | Elapsed: 9.07s | FPR 0.0003 -> TPR 0.8000 & F1 0.8889
01/28/2023 12:02:41 PM  [*] Sat Jan 28 12:02:41 2023:    2    | Tr.loss: 0.253411 | Elapsed:   32.87  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9531
01/28/2023 12:02:41 PM  [*] Started epoch: 3
01/28/2023 12:02:42 PM  [*] Sat Jan 28 12:02:42 2023: Train Epoch: 3 [  0  /22838 (0 %)]	Loss: 0.214455 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.8043 & F1 0.8916
01/28/2023 12:02:51 PM  [*] Sat Jan 28 12:02:51 2023: Train Epoch: 3 [6400 /22838 (28%)]	Loss: 0.284164 | Elapsed: 9.06s | FPR 0.0003 -> TPR 0.5938 & F1 0.7451
01/28/2023 12:03:00 PM  [*] Sat Jan 28 12:03:00 2023: Train Epoch: 3 [12800/22838 (56%)]	Loss: 0.205150 | Elapsed: 9.06s | FPR 0.0003 -> TPR 0.6462 & F1 0.7850
01/28/2023 12:03:09 PM  [*] Sat Jan 28 12:03:09 2023: Train Epoch: 3 [19200/22838 (84%)]	Loss: 0.159748 | Elapsed: 9.07s | FPR 0.0003 -> TPR 0.8548 & F1 0.9217
01/28/2023 12:03:14 PM  [*] Sat Jan 28 12:03:14 2023:    3    | Tr.loss: 0.185907 | Elapsed:   32.89  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9763
01/28/2023 12:03:15 PM [!] Sat Jan 28 12:03:15 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674903794-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674903794-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674903794-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674903794-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674903794-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_pretrained\training_files\1674903794-trainTPRs.npy
01/28/2023 12:03:15 PM  [!] Training non_pretrained model on downstream task...
01/28/2023 12:03:15 PM  [*] Started epoch: 1
01/28/2023 12:03:15 PM  [*] Sat Jan 28 12:03:15 2023: Train Epoch: 1 [  0  /22838 (0 %)]	Loss: 2.055013 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0435 & F1 0.0833
01/28/2023 12:03:21 PM  [*] Sat Jan 28 12:03:21 2023: Train Epoch: 1 [6400 /22838 (28%)]	Loss: 0.483711 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.3281 & F1 0.4941
01/28/2023 12:03:28 PM  [*] Sat Jan 28 12:03:28 2023: Train Epoch: 1 [12800/22838 (56%)]	Loss: 0.335528 | Elapsed: 6.17s | FPR 0.0003 -> TPR 0.4853 & F1 0.6535
01/28/2023 12:03:34 PM  [*] Sat Jan 28 12:03:34 2023: Train Epoch: 1 [19200/22838 (84%)]	Loss: 0.296662 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.4000 & F1 0.5714
01/28/2023 12:03:38 PM  [*] Sat Jan 28 12:03:38 2023:    1    | Tr.loss: 0.423576 | Elapsed:   22.64  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8639
01/28/2023 12:03:38 PM  [*] Started epoch: 2
01/28/2023 12:03:38 PM  [*] Sat Jan 28 12:03:38 2023: Train Epoch: 2 [  0  /22838 (0 %)]	Loss: 0.312943 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667
01/28/2023 12:03:44 PM  [*] Sat Jan 28 12:03:44 2023: Train Epoch: 2 [6400 /22838 (28%)]	Loss: 0.266334 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.8276 & F1 0.9057
01/28/2023 12:03:50 PM  [*] Sat Jan 28 12:03:50 2023: Train Epoch: 2 [12800/22838 (56%)]	Loss: 0.283033 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.6129 & F1 0.7600
01/28/2023 12:03:57 PM  [*] Sat Jan 28 12:03:57 2023: Train Epoch: 2 [19200/22838 (84%)]	Loss: 0.238018 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.5278 & F1 0.6909
01/28/2023 12:04:00 PM  [*] Sat Jan 28 12:04:00 2023:    2    | Tr.loss: 0.243832 | Elapsed:   22.67  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9577
01/28/2023 12:04:00 PM  [*] Started epoch: 3
01/28/2023 12:04:01 PM  [*] Sat Jan 28 12:04:01 2023: Train Epoch: 3 [  0  /22838 (0 %)]	Loss: 0.105051 | Elapsed: 0.05s | FPR 0.0003 -> TPR 0.9000 & F1 0.9474
01/28/2023 12:04:07 PM  [*] Sat Jan 28 12:04:07 2023: Train Epoch: 3 [6400 /22838 (28%)]	Loss: 0.146472 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.9265 & F1 0.9618
01/28/2023 12:04:13 PM  [*] Sat Jan 28 12:04:13 2023: Train Epoch: 3 [12800/22838 (56%)]	Loss: 0.270863 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.5312 & F1 0.6939
01/28/2023 12:04:18 PM [!] Learning rate: 2.5e-05
01/28/2023 12:04:19 PM  [*] Sat Jan 28 12:04:19 2023: Train Epoch: 3 [19200/22838 (84%)]	Loss: 0.161333 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.7119 & F1 0.8317
01/28/2023 12:04:23 PM  [*] Sat Jan 28 12:04:23 2023:    3    | Tr.loss: 0.183146 | Elapsed:   22.63  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9765
01/28/2023 12:04:24 PM [!] Sat Jan 28 12:04:23 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674903863-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674903863-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674903863-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674903863-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674903863-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_non_pretrained\training_files\1674903863-trainTPRs.npy
01/28/2023 12:04:24 PM  [!] Training full_data model on downstream task...
01/28/2023 12:04:24 PM  [*] Started epoch: 1
01/28/2023 12:04:24 PM  [*] Sat Jan 28 12:04:24 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 1.474212 | Elapsed: 0.08s | FPR 0.0003 -> TPR 0.0435 & F1 0.0833
01/28/2023 12:04:30 PM  [*] Sat Jan 28 12:04:30 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.552806 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.1000 & F1 0.1818
01/28/2023 12:04:36 PM  [*] Sat Jan 28 12:04:36 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.412145 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.5286 & F1 0.6916
01/28/2023 12:04:43 PM  [*] Sat Jan 28 12:04:43 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.364121 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.5625 & F1 0.7200
01/28/2023 12:04:49 PM  [*] Sat Jan 28 12:04:49 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.353891 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.4237 & F1 0.5952
01/28/2023 12:04:55 PM  [*] Sat Jan 28 12:04:55 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.230535 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.6883 & F1 0.8154
01/28/2023 12:05:01 PM  [*] Sat Jan 28 12:05:01 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.246980 | Elapsed: 6.21s | FPR 0.0003 -> TPR 0.6400 & F1 0.7805
01/28/2023 12:05:08 PM  [*] Sat Jan 28 12:05:08 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.240528 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.6462 & F1 0.7850
01/28/2023 12:05:14 PM  [*] Sat Jan 28 12:05:14 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.123213 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.7031 & F1 0.8257
01/28/2023 12:05:20 PM  [*] Sat Jan 28 12:05:20 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.238410 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7941 & F1 0.8852
01/28/2023 12:05:26 PM [!] Learning rate: 2.5e-05
01/28/2023 12:05:26 PM  [*] Sat Jan 28 12:05:26 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.189011 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8154 & F1 0.8983
01/28/2023 12:05:32 PM  [*] Sat Jan 28 12:05:32 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.199960 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8730 & F1 0.9322
01/28/2023 12:05:40 PM  [*] Sat Jan 28 12:05:40 2023:    1    | Tr.loss: 0.290426 | Elapsed:   75.67  s | FPR 0.0003 -> TPR: 0.20 & F1: 0.33 | AUC: 0.9379
01/28/2023 12:05:40 PM  [*] Started epoch: 2
01/28/2023 12:05:40 PM  [*] Sat Jan 28 12:05:40 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.183339 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.7500 & F1 0.8571
01/28/2023 12:05:46 PM  [*] Sat Jan 28 12:05:46 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.179241 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8986 & F1 0.9466
01/28/2023 12:05:52 PM  [*] Sat Jan 28 12:05:52 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.188758 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8824 & F1 0.9375
01/28/2023 12:05:58 PM  [*] Sat Jan 28 12:05:58 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.125310 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8060 & F1 0.8926
01/28/2023 12:06:05 PM  [*] Sat Jan 28 12:06:05 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.200555 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8939 & F1 0.9440
01/28/2023 12:06:11 PM  [*] Sat Jan 28 12:06:11 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.085873 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8615 & F1 0.9256
01/28/2023 12:06:17 PM  [*] Sat Jan 28 12:06:17 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.181523 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.8451 & F1 0.9160
01/28/2023 12:06:23 PM  [*] Sat Jan 28 12:06:23 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.154955 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.9254 & F1 0.9612
01/28/2023 12:06:29 PM  [*] Sat Jan 28 12:06:29 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.227819 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.7973 & F1 0.8872
01/28/2023 12:06:30 PM [!] Learning rate: 2.5e-06
01/28/2023 12:06:36 PM  [*] Sat Jan 28 12:06:36 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.194182 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.8243 & F1 0.9037
01/28/2023 12:06:42 PM  [*] Sat Jan 28 12:06:42 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.143220 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.6923 & F1 0.8182
01/28/2023 12:06:48 PM  [*] Sat Jan 28 12:06:48 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.150833 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.9211 & F1 0.9589
01/28/2023 12:06:55 PM  [*] Sat Jan 28 12:06:55 2023:    2    | Tr.loss: 0.160535 | Elapsed:   75.53  s | FPR 0.0003 -> TPR: 0.46 & F1: 0.63 | AUC: 0.9821
01/28/2023 12:06:55 PM  [*] Started epoch: 3
01/28/2023 12:06:55 PM  [*] Sat Jan 28 12:06:55 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.119374 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.8958 & F1 0.9451
01/28/2023 12:07:01 PM  [*] Sat Jan 28 12:07:01 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.114680 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.9014 & F1 0.9481
01/28/2023 12:07:08 PM  [*] Sat Jan 28 12:07:08 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.256644 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.8169 & F1 0.8992
01/28/2023 12:07:14 PM  [*] Sat Jan 28 12:07:14 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.126306 | Elapsed: 6.18s | FPR 0.0003 -> TPR 0.8806 & F1 0.9365
01/28/2023 12:07:20 PM  [*] Sat Jan 28 12:07:20 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.108089 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8636 & F1 0.9268
01/28/2023 12:07:26 PM  [*] Sat Jan 28 12:07:26 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.217365 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.4928 & F1 0.6602
01/28/2023 12:07:33 PM  [*] Sat Jan 28 12:07:33 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.125107 | Elapsed: 6.20s | FPR 0.0003 -> TPR 0.7692 & F1 0.8696
01/28/2023 12:07:34 PM [!] Learning rate: 2.5000000000000004e-07
01/28/2023 12:07:39 PM  [*] Sat Jan 28 12:07:39 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.118509 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.9028 & F1 0.9489
01/28/2023 12:07:45 PM  [*] Sat Jan 28 12:07:45 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.162888 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8154 & F1 0.8983
01/28/2023 12:07:51 PM  [*] Sat Jan 28 12:07:51 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.155822 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.7465 & F1 0.8548
01/28/2023 12:07:57 PM  [*] Sat Jan 28 12:07:57 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.181735 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.8475 & F1 0.9174
01/28/2023 12:08:03 PM  [*] Sat Jan 28 12:08:03 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.197022 | Elapsed: 6.19s | FPR 0.0003 -> TPR 0.7465 & F1 0.8548
01/28/2023 12:08:11 PM  [*] Sat Jan 28 12:08:11 2023:    3    | Tr.loss: 0.154981 | Elapsed:   75.53  s | FPR 0.0003 -> TPR: 0.48 & F1: 0.65 | AUC: 0.9833
01/28/2023 12:08:11 PM [!] Sat Jan 28 12:08:11 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674904091-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674904091-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674904091-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674904091-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674904091-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653\downstreamTask_full_data\training_files\1674904091-trainTPRs.npy
01/28/2023 12:08:11 PM  [*] Evaluating pretrained model on test set...
01/28/2023 12:08:16 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0914 | F1: 0.1674
01/28/2023 12:08:16 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1380 | F1: 0.2425
01/28/2023 12:08:16 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3678 | F1: 0.5374
01/28/2023 12:08:16 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.4436 | F1: 0.6135
01/28/2023 12:08:16 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.5178 | F1: 0.6785
01/28/2023 12:08:16 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5953 | F1: 0.7346
01/28/2023 12:08:16 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7789 | F1: 0.8359
01/28/2023 12:08:16 PM  [*] Evaluating non_pretrained model on test set...
01/28/2023 12:08:21 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0945 | F1: 0.1728
01/28/2023 12:08:21 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1359 | F1: 0.2393
01/28/2023 12:08:21 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2995 | F1: 0.4606
01/28/2023 12:08:21 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3437 | F1: 0.5106
01/28/2023 12:08:21 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.3986 | F1: 0.5665
01/28/2023 12:08:21 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5085 | F1: 0.6630
01/28/2023 12:08:21 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7753 | F1: 0.8335
01/28/2023 12:08:21 PM  [*] Evaluating full_data model on test set...
01/28/2023 12:08:26 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.1236 | F1: 0.2200
01/28/2023 12:08:26 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.3273 | F1: 0.4932
01/28/2023 12:08:26 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3500 | F1: 0.5182
01/28/2023 12:08:26 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3939 | F1: 0.5642
01/28/2023 12:08:26 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4498 | F1: 0.6168
01/28/2023 12:08:26 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5642 | F1: 0.7098
01/28/2023 12:08:26 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8241 | F1: 0.8633
01/28/2023 12:08:26 PM  [!] Finished pre-training evaluation over 3 splits! Saved metrics to:
	C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.7_1674899653/metrics_MaskedLanguageModel_nSplits_3_limit_None.json
01/28/2023 12:08:27 PM  [!] Starting Masked Language Model evaluation over 3 splits!
01/28/2023 12:08:27 PM  [!] Loaded data and vocab. X train size: (76126, 512), X test size: (17407, 512), vocab size: 50002
01/28/2023 12:08:27 PM  [!] Initiating Self-Supervised Learning Pretraining
     Language Modeling MaskedLanguageModel
     Model TransformerEncoderLM with config:
	{'vocabSize': 50002, 'maxLen': 512, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'dropout': 0.3}

01/28/2023 12:08:27 PM  [!] Running pre-training split 1/3
01/28/2023 12:08:30 PM  [!] Pre-training model...
01/28/2023 12:08:31 PM  [*] Masking sequences...
01/28/2023 12:08:47 PM  [*] Started epoch: 1
01/28/2023 12:08:48 PM  [*] Sat Jan 28 12:08:48 2023: Train Epoch: 1 [  0  /57094 (0 %)]	Loss: 457.050629 | Elapsed: 0.84s
01/28/2023 12:09:01 PM  [*] Sat Jan 28 12:09:01 2023: Train Epoch: 1 [6400 /57094 (11%)]	Loss: 246.710449 | Elapsed: 12.22s
01/28/2023 12:09:13 PM  [*] Sat Jan 28 12:09:13 2023: Train Epoch: 1 [12800/57094 (22%)]	Loss: 237.526978 | Elapsed: 12.29s
01/28/2023 12:09:25 PM  [*] Sat Jan 28 12:09:25 2023: Train Epoch: 1 [19200/57094 (34%)]	Loss: 221.280716 | Elapsed: 12.27s
01/28/2023 12:09:37 PM  [*] Sat Jan 28 12:09:37 2023: Train Epoch: 1 [25600/57094 (45%)]	Loss: 195.528320 | Elapsed: 12.26s
01/28/2023 12:09:50 PM  [*] Sat Jan 28 12:09:50 2023: Train Epoch: 1 [32000/57094 (56%)]	Loss: 232.711258 | Elapsed: 12.23s
01/28/2023 12:10:02 PM  [*] Sat Jan 28 12:10:02 2023: Train Epoch: 1 [38400/57094 (67%)]	Loss: 213.084442 | Elapsed: 12.27s
01/28/2023 12:10:14 PM  [*] Sat Jan 28 12:10:14 2023: Train Epoch: 1 [44800/57094 (78%)]	Loss: 206.083221 | Elapsed: 12.29s
01/28/2023 12:10:26 PM  [*] Sat Jan 28 12:10:26 2023: Train Epoch: 1 [51200/57094 (90%)]	Loss: 200.400558 | Elapsed: 12.32s
01/28/2023 12:10:39 PM  [*] Sat Jan 28 12:10:39 2023:    1    | Tr.loss: 213.955269 | Elapsed:  111.80  s
01/28/2023 12:10:39 PM  [*] Started epoch: 2
01/28/2023 12:10:39 PM  [*] Sat Jan 28 12:10:39 2023: Train Epoch: 2 [  0  /57094 (0 %)]	Loss: 165.150284 | Elapsed: 0.14s
01/28/2023 12:10:52 PM  [*] Sat Jan 28 12:10:52 2023: Train Epoch: 2 [6400 /57094 (11%)]	Loss: 194.292831 | Elapsed: 12.33s
01/28/2023 12:11:04 PM  [*] Sat Jan 28 12:11:04 2023: Train Epoch: 2 [12800/57094 (22%)]	Loss: 184.928192 | Elapsed: 12.28s
01/28/2023 12:11:16 PM  [*] Sat Jan 28 12:11:16 2023: Train Epoch: 2 [19200/57094 (34%)]	Loss: 182.717010 | Elapsed: 12.32s
01/28/2023 12:11:29 PM  [*] Sat Jan 28 12:11:29 2023: Train Epoch: 2 [25600/57094 (45%)]	Loss: 186.507706 | Elapsed: 12.25s
01/28/2023 12:11:41 PM  [*] Sat Jan 28 12:11:41 2023: Train Epoch: 2 [32000/57094 (56%)]	Loss: 166.652313 | Elapsed: 12.35s
01/28/2023 12:11:53 PM  [*] Sat Jan 28 12:11:53 2023: Train Epoch: 2 [38400/57094 (67%)]	Loss: 180.609665 | Elapsed: 12.24s
01/28/2023 12:12:05 PM  [*] Sat Jan 28 12:12:05 2023: Train Epoch: 2 [44800/57094 (78%)]	Loss: 196.966492 | Elapsed: 12.28s
01/28/2023 12:12:18 PM  [*] Sat Jan 28 12:12:18 2023: Train Epoch: 2 [51200/57094 (90%)]	Loss: 215.116943 | Elapsed: 12.32s
01/28/2023 12:12:30 PM  [*] Sat Jan 28 12:12:30 2023:    2    | Tr.loss: 193.522560 | Elapsed:  111.11  s
01/28/2023 12:12:30 PM  [*] Started epoch: 3
01/28/2023 12:12:31 PM  [*] Sat Jan 28 12:12:31 2023: Train Epoch: 3 [  0  /57094 (0 %)]	Loss: 180.675095 | Elapsed: 0.13s
01/28/2023 12:12:43 PM  [*] Sat Jan 28 12:12:43 2023: Train Epoch: 3 [6400 /57094 (11%)]	Loss: 200.587860 | Elapsed: 12.33s
01/28/2023 12:12:55 PM  [*] Sat Jan 28 12:12:55 2023: Train Epoch: 3 [12800/57094 (22%)]	Loss: 201.703781 | Elapsed: 12.25s
01/28/2023 12:13:07 PM  [*] Sat Jan 28 12:13:07 2023: Train Epoch: 3 [19200/57094 (34%)]	Loss: 183.402069 | Elapsed: 12.32s
01/28/2023 12:13:20 PM  [*] Sat Jan 28 12:13:20 2023: Train Epoch: 3 [25600/57094 (45%)]	Loss: 173.134857 | Elapsed: 12.29s
01/28/2023 12:13:32 PM  [*] Sat Jan 28 12:13:32 2023: Train Epoch: 3 [32000/57094 (56%)]	Loss: 181.321991 | Elapsed: 12.31s
01/28/2023 12:13:44 PM  [*] Sat Jan 28 12:13:44 2023: Train Epoch: 3 [38400/57094 (67%)]	Loss: 183.629440 | Elapsed: 12.29s
01/28/2023 12:13:57 PM  [*] Sat Jan 28 12:13:57 2023: Train Epoch: 3 [44800/57094 (78%)]	Loss: 223.664993 | Elapsed: 12.23s
01/28/2023 12:14:09 PM  [*] Sat Jan 28 12:14:09 2023: Train Epoch: 3 [51200/57094 (90%)]	Loss: 181.836624 | Elapsed: 12.26s
01/28/2023 12:14:21 PM  [*] Sat Jan 28 12:14:21 2023:    3    | Tr.loss: 187.234365 | Elapsed:  111.03  s
01/28/2023 12:14:21 PM  [*] Started epoch: 4
01/28/2023 12:14:22 PM  [*] Sat Jan 28 12:14:22 2023: Train Epoch: 4 [  0  /57094 (0 %)]	Loss: 187.153458 | Elapsed: 0.14s
01/28/2023 12:14:34 PM  [*] Sat Jan 28 12:14:34 2023: Train Epoch: 4 [6400 /57094 (11%)]	Loss: 175.185028 | Elapsed: 12.32s
01/28/2023 12:14:46 PM  [*] Sat Jan 28 12:14:46 2023: Train Epoch: 4 [12800/57094 (22%)]	Loss: 179.692627 | Elapsed: 12.27s
01/28/2023 12:14:58 PM  [*] Sat Jan 28 12:14:58 2023: Train Epoch: 4 [19200/57094 (34%)]	Loss: 191.012451 | Elapsed: 12.27s
01/28/2023 12:15:11 PM  [*] Sat Jan 28 12:15:11 2023: Train Epoch: 4 [25600/57094 (45%)]	Loss: 183.181580 | Elapsed: 12.29s
01/28/2023 12:15:23 PM  [*] Sat Jan 28 12:15:23 2023: Train Epoch: 4 [32000/57094 (56%)]	Loss: 203.930878 | Elapsed: 12.26s
01/28/2023 12:15:35 PM  [*] Sat Jan 28 12:15:35 2023: Train Epoch: 4 [38400/57094 (67%)]	Loss: 173.646576 | Elapsed: 12.20s
01/28/2023 12:15:47 PM  [*] Sat Jan 28 12:15:47 2023: Train Epoch: 4 [44800/57094 (78%)]	Loss: 180.219833 | Elapsed: 12.26s
01/28/2023 12:16:00 PM  [*] Sat Jan 28 12:16:00 2023: Train Epoch: 4 [51200/57094 (90%)]	Loss: 166.084518 | Elapsed: 12.24s
01/28/2023 12:16:12 PM  [*] Sat Jan 28 12:16:12 2023:    4    | Tr.loss: 184.222909 | Elapsed:  111.00  s
01/28/2023 12:16:12 PM  [*] Started epoch: 5
01/28/2023 12:16:13 PM  [*] Sat Jan 28 12:16:13 2023: Train Epoch: 5 [  0  /57094 (0 %)]	Loss: 184.723328 | Elapsed: 0.14s
01/28/2023 12:16:25 PM  [*] Sat Jan 28 12:16:25 2023: Train Epoch: 5 [6400 /57094 (11%)]	Loss: 197.157166 | Elapsed: 12.28s
01/28/2023 12:16:37 PM  [*] Sat Jan 28 12:16:37 2023: Train Epoch: 5 [12800/57094 (22%)]	Loss: 198.554138 | Elapsed: 12.26s
01/28/2023 12:16:49 PM  [*] Sat Jan 28 12:16:49 2023: Train Epoch: 5 [19200/57094 (34%)]	Loss: 203.338043 | Elapsed: 12.28s
01/28/2023 12:17:02 PM  [*] Sat Jan 28 12:17:02 2023: Train Epoch: 5 [25600/57094 (45%)]	Loss: 183.921783 | Elapsed: 12.25s
01/28/2023 12:17:14 PM  [*] Sat Jan 28 12:17:14 2023: Train Epoch: 5 [32000/57094 (56%)]	Loss: 186.554413 | Elapsed: 12.22s
01/28/2023 12:17:26 PM  [*] Sat Jan 28 12:17:26 2023: Train Epoch: 5 [38400/57094 (67%)]	Loss: 166.534836 | Elapsed: 12.19s
01/28/2023 12:17:38 PM  [*] Sat Jan 28 12:17:38 2023: Train Epoch: 5 [44800/57094 (78%)]	Loss: 202.150421 | Elapsed: 12.24s
01/28/2023 12:17:51 PM  [*] Sat Jan 28 12:17:51 2023: Train Epoch: 5 [51200/57094 (90%)]	Loss: 184.407349 | Elapsed: 12.21s
01/28/2023 12:18:03 PM  [*] Sat Jan 28 12:18:03 2023:    5    | Tr.loss: 182.497677 | Elapsed:  110.76  s
01/28/2023 12:18:03 PM  [*] Started epoch: 6
01/28/2023 12:18:03 PM  [*] Sat Jan 28 12:18:03 2023: Train Epoch: 6 [  0  /57094 (0 %)]	Loss: 177.015869 | Elapsed: 0.13s
01/28/2023 12:18:16 PM  [*] Sat Jan 28 12:18:16 2023: Train Epoch: 6 [6400 /57094 (11%)]	Loss: 194.381866 | Elapsed: 12.24s
01/28/2023 12:18:28 PM  [*] Sat Jan 28 12:18:28 2023: Train Epoch: 6 [12800/57094 (22%)]	Loss: 186.781433 | Elapsed: 12.26s
01/28/2023 12:18:40 PM  [*] Sat Jan 28 12:18:40 2023: Train Epoch: 6 [19200/57094 (34%)]	Loss: 167.801636 | Elapsed: 12.21s
01/28/2023 12:18:52 PM  [*] Sat Jan 28 12:18:52 2023: Train Epoch: 6 [25600/57094 (45%)]	Loss: 185.228180 | Elapsed: 12.29s
01/28/2023 12:19:05 PM  [*] Sat Jan 28 12:19:05 2023: Train Epoch: 6 [32000/57094 (56%)]	Loss: 174.817719 | Elapsed: 12.25s
01/28/2023 12:19:09 PM [!] Learning rate: 2.5e-05
01/28/2023 12:19:17 PM  [*] Sat Jan 28 12:19:17 2023: Train Epoch: 6 [38400/57094 (67%)]	Loss: 161.264725 | Elapsed: 12.23s
01/28/2023 12:19:29 PM  [*] Sat Jan 28 12:19:29 2023: Train Epoch: 6 [44800/57094 (78%)]	Loss: 176.687195 | Elapsed: 12.18s
01/28/2023 12:19:41 PM  [*] Sat Jan 28 12:19:41 2023: Train Epoch: 6 [51200/57094 (90%)]	Loss: 183.546722 | Elapsed: 12.26s
01/28/2023 12:19:54 PM  [*] Sat Jan 28 12:19:54 2023:    6    | Tr.loss: 181.173223 | Elapsed:  110.66  s
01/28/2023 12:19:54 PM  [*] Started epoch: 7
01/28/2023 12:19:54 PM  [*] Sat Jan 28 12:19:54 2023: Train Epoch: 7 [  0  /57094 (0 %)]	Loss: 158.977478 | Elapsed: 0.12s
01/28/2023 12:20:06 PM  [*] Sat Jan 28 12:20:06 2023: Train Epoch: 7 [6400 /57094 (11%)]	Loss: 174.994568 | Elapsed: 12.28s
01/28/2023 12:20:19 PM  [*] Sat Jan 28 12:20:19 2023: Train Epoch: 7 [12800/57094 (22%)]	Loss: 197.377365 | Elapsed: 12.30s
01/28/2023 12:20:31 PM  [*] Sat Jan 28 12:20:31 2023: Train Epoch: 7 [19200/57094 (34%)]	Loss: 162.222290 | Elapsed: 12.30s
01/28/2023 12:20:43 PM  [*] Sat Jan 28 12:20:43 2023: Train Epoch: 7 [25600/57094 (45%)]	Loss: 173.902267 | Elapsed: 12.28s
01/28/2023 12:20:55 PM  [*] Sat Jan 28 12:20:55 2023: Train Epoch: 7 [32000/57094 (56%)]	Loss: 193.751877 | Elapsed: 12.22s
01/28/2023 12:21:08 PM  [*] Sat Jan 28 12:21:08 2023: Train Epoch: 7 [38400/57094 (67%)]	Loss: 161.987167 | Elapsed: 12.21s
01/28/2023 12:21:20 PM  [*] Sat Jan 28 12:21:20 2023: Train Epoch: 7 [44800/57094 (78%)]	Loss: 175.694275 | Elapsed: 12.26s
01/28/2023 12:21:32 PM  [*] Sat Jan 28 12:21:32 2023: Train Epoch: 7 [51200/57094 (90%)]	Loss: 174.176300 | Elapsed: 12.41s
01/28/2023 12:21:45 PM  [*] Sat Jan 28 12:21:45 2023:    7    | Tr.loss: 180.149738 | Elapsed:  111.07  s
01/28/2023 12:21:45 PM  [*] Started epoch: 8
01/28/2023 12:21:45 PM  [*] Sat Jan 28 12:21:45 2023: Train Epoch: 8 [  0  /57094 (0 %)]	Loss: 177.933655 | Elapsed: 0.13s
01/28/2023 12:21:57 PM  [*] Sat Jan 28 12:21:57 2023: Train Epoch: 8 [6400 /57094 (11%)]	Loss: 190.310852 | Elapsed: 12.31s
01/28/2023 12:22:10 PM  [*] Sat Jan 28 12:22:10 2023: Train Epoch: 8 [12800/57094 (22%)]	Loss: 157.093506 | Elapsed: 12.29s
01/28/2023 12:22:22 PM  [*] Sat Jan 28 12:22:22 2023: Train Epoch: 8 [19200/57094 (34%)]	Loss: 182.761948 | Elapsed: 12.28s
01/28/2023 12:22:34 PM  [*] Sat Jan 28 12:22:34 2023: Train Epoch: 8 [25600/57094 (45%)]	Loss: 191.752731 | Elapsed: 12.20s
01/28/2023 12:22:46 PM  [*] Sat Jan 28 12:22:46 2023: Train Epoch: 8 [32000/57094 (56%)]	Loss: 183.822495 | Elapsed: 12.25s
01/28/2023 12:22:59 PM  [*] Sat Jan 28 12:22:59 2023: Train Epoch: 8 [38400/57094 (67%)]	Loss: 181.028656 | Elapsed: 12.27s
01/28/2023 12:23:11 PM  [*] Sat Jan 28 12:23:11 2023: Train Epoch: 8 [44800/57094 (78%)]	Loss: 180.962173 | Elapsed: 12.26s
01/28/2023 12:23:23 PM  [*] Sat Jan 28 12:23:23 2023: Train Epoch: 8 [51200/57094 (90%)]	Loss: 171.059540 | Elapsed: 12.28s
01/28/2023 12:23:36 PM  [*] Sat Jan 28 12:23:36 2023:    8    | Tr.loss: 179.874298 | Elapsed:  110.96  s
01/28/2023 12:23:36 PM  [*] Started epoch: 9
01/28/2023 12:23:36 PM  [*] Sat Jan 28 12:23:36 2023: Train Epoch: 9 [  0  /57094 (0 %)]	Loss: 182.998291 | Elapsed: 0.13s
01/28/2023 12:23:48 PM  [*] Sat Jan 28 12:23:48 2023: Train Epoch: 9 [6400 /57094 (11%)]	Loss: 197.076630 | Elapsed: 12.26s
01/28/2023 12:24:01 PM  [*] Sat Jan 28 12:24:01 2023: Train Epoch: 9 [12800/57094 (22%)]	Loss: 179.208694 | Elapsed: 12.22s
01/28/2023 12:24:13 PM  [*] Sat Jan 28 12:24:13 2023: Train Epoch: 9 [19200/57094 (34%)]	Loss: 165.316772 | Elapsed: 12.40s
01/28/2023 12:24:25 PM  [*] Sat Jan 28 12:24:25 2023: Train Epoch: 9 [25600/57094 (45%)]	Loss: 179.629227 | Elapsed: 12.21s
01/28/2023 12:24:37 PM  [*] Sat Jan 28 12:24:37 2023: Train Epoch: 9 [32000/57094 (56%)]	Loss: 165.972290 | Elapsed: 12.23s
01/28/2023 12:24:50 PM  [*] Sat Jan 28 12:24:50 2023: Train Epoch: 9 [38400/57094 (67%)]	Loss: 172.987915 | Elapsed: 12.25s
01/28/2023 12:25:02 PM  [*] Sat Jan 28 12:25:02 2023: Train Epoch: 9 [44800/57094 (78%)]	Loss: 171.631958 | Elapsed: 12.21s
01/28/2023 12:25:14 PM  [*] Sat Jan 28 12:25:14 2023: Train Epoch: 9 [51200/57094 (90%)]	Loss: 191.210907 | Elapsed: 12.29s
01/28/2023 12:25:27 PM  [*] Sat Jan 28 12:25:27 2023:    9    | Tr.loss: 179.527891 | Elapsed:  110.86  s
01/28/2023 12:25:27 PM  [*] Started epoch: 10
01/28/2023 12:25:27 PM  [*] Sat Jan 28 12:25:27 2023: Train Epoch: 10 [  0  /57094 (0 %)]	Loss: 185.113922 | Elapsed: 0.12s
01/28/2023 12:25:39 PM  [*] Sat Jan 28 12:25:39 2023: Train Epoch: 10 [6400 /57094 (11%)]	Loss: 165.704025 | Elapsed: 12.34s
01/28/2023 12:25:51 PM  [*] Sat Jan 28 12:25:51 2023: Train Epoch: 10 [12800/57094 (22%)]	Loss: 165.693451 | Elapsed: 12.27s
01/28/2023 12:26:04 PM  [*] Sat Jan 28 12:26:04 2023: Train Epoch: 10 [19200/57094 (34%)]	Loss: 181.310532 | Elapsed: 12.21s
01/28/2023 12:26:16 PM  [*] Sat Jan 28 12:26:16 2023: Train Epoch: 10 [25600/57094 (45%)]	Loss: 167.471832 | Elapsed: 12.28s
01/28/2023 12:26:28 PM  [*] Sat Jan 28 12:26:28 2023: Train Epoch: 10 [32000/57094 (56%)]	Loss: 175.596832 | Elapsed: 12.27s
01/28/2023 12:26:41 PM  [*] Sat Jan 28 12:26:41 2023: Train Epoch: 10 [38400/57094 (67%)]	Loss: 180.296844 | Elapsed: 12.32s
01/28/2023 12:26:53 PM  [*] Sat Jan 28 12:26:53 2023: Train Epoch: 10 [44800/57094 (78%)]	Loss: 165.985596 | Elapsed: 12.24s
01/28/2023 12:27:06 PM  [*] Sat Jan 28 12:27:06 2023: Train Epoch: 10 [51200/57094 (90%)]	Loss: 183.682831 | Elapsed: 13.41s
01/28/2023 12:27:19 PM  [*] Sat Jan 28 12:27:19 2023:   10    | Tr.loss: 179.357013 | Elapsed:  112.60  s
01/28/2023 12:27:20 PM [!] Sat Jan 28 12:27:20 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674905239-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674905239-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674905239-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674905239-auc.npy
01/28/2023 12:27:21 PM  [!] Training pretrained model on downstream task...
01/28/2023 12:27:21 PM  [*] Started epoch: 1
01/28/2023 12:27:21 PM  [*] Sat Jan 28 12:27:21 2023: Train Epoch: 1 [  0  /19032 (0 %)]	Loss: 3.921295 | Elapsed: 0.35s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000
01/28/2023 12:27:30 PM  [*] Sat Jan 28 12:27:30 2023: Train Epoch: 1 [6400 /19032 (34%)]	Loss: 0.436540 | Elapsed: 9.18s | FPR 0.0003 -> TPR 0.2464 & F1 0.3953
01/28/2023 12:27:40 PM  [*] Sat Jan 28 12:27:40 2023: Train Epoch: 1 [12800/19032 (67%)]	Loss: 0.452691 | Elapsed: 9.16s | FPR 0.0003 -> TPR 0.1667 & F1 0.2857
01/28/2023 12:27:49 PM  [*] Sat Jan 28 12:27:49 2023:    1    | Tr.loss: 0.580739 | Elapsed:   28.03  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.7870
01/28/2023 12:27:49 PM  [*] Started epoch: 2
01/28/2023 12:27:49 PM  [*] Sat Jan 28 12:27:49 2023: Train Epoch: 2 [  0  /19032 (0 %)]	Loss: 0.277510 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667
01/28/2023 12:27:58 PM  [*] Sat Jan 28 12:27:58 2023: Train Epoch: 2 [6400 /19032 (34%)]	Loss: 0.372526 | Elapsed: 9.16s | FPR 0.0003 -> TPR 0.3175 & F1 0.4819
01/28/2023 12:28:07 PM  [*] Sat Jan 28 12:28:07 2023: Train Epoch: 2 [12800/19032 (67%)]	Loss: 0.281330 | Elapsed: 9.17s | FPR 0.0003 -> TPR 0.6429 & F1 0.7826
01/28/2023 12:28:17 PM  [*] Sat Jan 28 12:28:17 2023:    2    | Tr.loss: 0.345463 | Elapsed:   27.76  s | FPR 0.0003 -> TPR: 0.07 & F1: 0.14 | AUC: 0.9094
01/28/2023 12:28:17 PM  [*] Started epoch: 3
01/28/2023 12:28:17 PM  [*] Sat Jan 28 12:28:17 2023: Train Epoch: 3 [  0  /19032 (0 %)]	Loss: 0.247500 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.8298 & F1 0.9070
01/28/2023 12:28:26 PM  [*] Sat Jan 28 12:28:26 2023: Train Epoch: 3 [6400 /19032 (34%)]	Loss: 0.281957 | Elapsed: 9.18s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091
01/28/2023 12:28:35 PM  [*] Sat Jan 28 12:28:35 2023: Train Epoch: 3 [12800/19032 (67%)]	Loss: 0.139691 | Elapsed: 9.16s | FPR 0.0003 -> TPR 0.8281 & F1 0.9060
01/28/2023 12:28:44 PM  [*] Sat Jan 28 12:28:44 2023:    3    | Tr.loss: 0.245594 | Elapsed:   27.74  s | FPR 0.0003 -> TPR: 0.17 & F1: 0.29 | AUC: 0.9566
01/28/2023 12:28:45 PM [!] Sat Jan 28 12:28:45 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674905324-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674905324-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674905324-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674905324-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674905324-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674905324-trainTPRs.npy
01/28/2023 12:28:45 PM  [!] Training non_pretrained model on downstream task...
01/28/2023 12:28:45 PM  [*] Started epoch: 1
01/28/2023 12:28:45 PM  [*] Sat Jan 28 12:28:45 2023: Train Epoch: 1 [  0  /19032 (0 %)]	Loss: 1.467192 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0256 & F1 0.0500
01/28/2023 12:28:52 PM  [*] Sat Jan 28 12:28:52 2023: Train Epoch: 1 [6400 /19032 (34%)]	Loss: 0.590549 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.3333 & F1 0.5000
01/28/2023 12:28:58 PM  [*] Sat Jan 28 12:28:58 2023: Train Epoch: 1 [12800/19032 (67%)]	Loss: 0.294993 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.1940 & F1 0.3250
01/28/2023 12:29:04 PM  [*] Sat Jan 28 12:29:04 2023:    1    | Tr.loss: 0.452233 | Elapsed:   19.19  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.8365
01/28/2023 12:29:04 PM  [*] Started epoch: 2
01/28/2023 12:29:05 PM  [*] Sat Jan 28 12:29:05 2023: Train Epoch: 2 [  0  /19032 (0 %)]	Loss: 0.378257 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.5556 & F1 0.7143
01/28/2023 12:29:11 PM  [*] Sat Jan 28 12:29:11 2023: Train Epoch: 2 [6400 /19032 (34%)]	Loss: 0.413631 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.5312 & F1 0.6939
01/28/2023 12:29:17 PM  [*] Sat Jan 28 12:29:17 2023: Train Epoch: 2 [12800/19032 (67%)]	Loss: 0.300310 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.3158 & F1 0.4800
01/28/2023 12:29:24 PM  [*] Sat Jan 28 12:29:24 2023:    2    | Tr.loss: 0.279980 | Elapsed:   19.20  s | FPR 0.0003 -> TPR: 0.25 & F1: 0.40 | AUC: 0.9416
01/28/2023 12:29:24 PM  [*] Started epoch: 3
01/28/2023 12:29:24 PM  [*] Sat Jan 28 12:29:24 2023: Train Epoch: 3 [  0  /19032 (0 %)]	Loss: 0.284141 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.6000 & F1 0.7500
01/28/2023 12:29:30 PM  [*] Sat Jan 28 12:29:30 2023: Train Epoch: 3 [6400 /19032 (34%)]	Loss: 0.190615 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.8000 & F1 0.8889
01/28/2023 12:29:36 PM  [*] Sat Jan 28 12:29:36 2023: Train Epoch: 3 [12800/19032 (67%)]	Loss: 0.296180 | Elapsed: 6.35s | FPR 0.0003 -> TPR 0.6714 & F1 0.8034
01/28/2023 12:29:43 PM  [*] Sat Jan 28 12:29:43 2023:    3    | Tr.loss: 0.207382 | Elapsed:   19.33  s | FPR 0.0003 -> TPR: 0.27 & F1: 0.42 | AUC: 0.9694
01/28/2023 12:29:43 PM [!] Sat Jan 28 12:29:43 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674905383-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674905383-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674905383-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674905383-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674905383-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674905383-trainTPRs.npy
01/28/2023 12:29:43 PM  [!] Training full_data model on downstream task...
01/28/2023 12:29:44 PM  [*] Started epoch: 1
01/28/2023 12:29:44 PM  [*] Sat Jan 28 12:29:44 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 3.629414 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000
01/28/2023 12:29:50 PM  [*] Sat Jan 28 12:29:50 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.480572 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.4483 & F1 0.6190
01/28/2023 12:29:57 PM  [*] Sat Jan 28 12:29:57 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.537402 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.3056 & F1 0.4681
01/28/2023 12:30:03 PM  [*] Sat Jan 28 12:30:03 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.323060 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.4091 & F1 0.5806
01/28/2023 12:30:09 PM  [*] Sat Jan 28 12:30:09 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.316081 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.6452 & F1 0.7843
01/28/2023 12:30:16 PM  [*] Sat Jan 28 12:30:16 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.244386 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.5857 & F1 0.7387
01/28/2023 12:30:22 PM  [*] Sat Jan 28 12:30:22 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.252179 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.7385 & F1 0.8496
01/28/2023 12:30:28 PM  [*] Sat Jan 28 12:30:28 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.147773 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.8889 & F1 0.9412
01/28/2023 12:30:34 PM  [*] Sat Jan 28 12:30:34 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.259148 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.7000 & F1 0.8235
01/28/2023 12:30:41 PM  [*] Sat Jan 28 12:30:41 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.117260 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.8382 & F1 0.9120
01/28/2023 12:30:47 PM [!] Learning rate: 2.5e-05
01/28/2023 12:30:47 PM  [*] Sat Jan 28 12:30:47 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.096545 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.8594 & F1 0.9244
01/28/2023 12:30:53 PM  [*] Sat Jan 28 12:30:53 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.133729 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.8696 & F1 0.9302
01/28/2023 12:31:01 PM  [*] Sat Jan 28 12:31:01 2023:    1    | Tr.loss: 0.300014 | Elapsed:   76.70  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.9345
01/28/2023 12:31:01 PM  [*] Started epoch: 2
01/28/2023 12:31:01 PM  [*] Sat Jan 28 12:31:01 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.159672 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.7442 & F1 0.8533
01/28/2023 12:31:07 PM  [*] Sat Jan 28 12:31:07 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.242194 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.5614 & F1 0.7191
01/28/2023 12:31:13 PM  [*] Sat Jan 28 12:31:13 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.164831 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.8906 & F1 0.9421
01/28/2023 12:31:20 PM  [*] Sat Jan 28 12:31:20 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.167425 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091
01/28/2023 12:31:26 PM  [*] Sat Jan 28 12:31:26 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.088926 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.9242 & F1 0.9606
01/28/2023 12:31:32 PM  [*] Sat Jan 28 12:31:32 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.163629 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.2923 & F1 0.4524
01/28/2023 12:31:38 PM  [*] Sat Jan 28 12:31:38 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.163938 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.7778 & F1 0.8750
01/28/2023 12:31:45 PM  [*] Sat Jan 28 12:31:45 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.133349 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.8769 & F1 0.9344
01/28/2023 12:31:51 PM  [*] Sat Jan 28 12:31:51 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.101127 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.9429 & F1 0.9706
01/28/2023 12:31:51 PM [!] Learning rate: 2.5e-06
01/28/2023 12:31:57 PM  [*] Sat Jan 28 12:31:57 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.136525 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.7231 & F1 0.8393
01/28/2023 12:32:03 PM  [*] Sat Jan 28 12:32:03 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.088872 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.7571 & F1 0.8618
01/28/2023 12:32:10 PM  [*] Sat Jan 28 12:32:10 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.147187 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.7778 & F1 0.8750
01/28/2023 12:32:17 PM  [*] Sat Jan 28 12:32:17 2023:    2    | Tr.loss: 0.164945 | Elapsed:   76.35  s | FPR 0.0003 -> TPR: 0.47 & F1: 0.64 | AUC: 0.9812
01/28/2023 12:32:17 PM  [*] Started epoch: 3
01/28/2023 12:32:17 PM  [*] Sat Jan 28 12:32:17 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.180970 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.9375 & F1 0.9677
01/28/2023 12:32:23 PM  [*] Sat Jan 28 12:32:23 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.171813 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.7647 & F1 0.8667
01/28/2023 12:32:30 PM  [*] Sat Jan 28 12:32:30 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.143194 | Elapsed: 6.23s | FPR 0.0003 -> TPR 0.8689 & F1 0.9298
01/28/2023 12:32:36 PM  [*] Sat Jan 28 12:32:36 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.104833 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.9577 & F1 0.9784
01/28/2023 12:32:42 PM  [*] Sat Jan 28 12:32:42 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.099067 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.6269 & F1 0.7706
01/28/2023 12:32:48 PM  [*] Sat Jan 28 12:32:48 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.092341 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.6119 & F1 0.7593
01/28/2023 12:32:55 PM  [*] Sat Jan 28 12:32:55 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.171951 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.8387 & F1 0.9123
01/28/2023 12:32:56 PM [!] Learning rate: 2.5000000000000004e-07
01/28/2023 12:33:01 PM  [*] Sat Jan 28 12:33:01 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.124623 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.9310 & F1 0.9643
01/28/2023 12:33:07 PM  [*] Sat Jan 28 12:33:07 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.266995 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.7162 & F1 0.8346
01/28/2023 12:33:13 PM  [*] Sat Jan 28 12:33:13 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.225824 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.8143 & F1 0.8976
01/28/2023 12:33:20 PM  [*] Sat Jan 28 12:33:20 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.163652 | Elapsed: 6.24s | FPR 0.0003 -> TPR 0.9067 & F1 0.9510
01/28/2023 12:33:26 PM  [*] Sat Jan 28 12:33:26 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.153596 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.8548 & F1 0.9217
01/28/2023 12:33:33 PM  [*] Sat Jan 28 12:33:33 2023:    3    | Tr.loss: 0.157919 | Elapsed:   76.29  s | FPR 0.0003 -> TPR: 0.45 & F1: 0.62 | AUC: 0.9827
01/28/2023 12:33:34 PM [!] Sat Jan 28 12:33:34 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674905613-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674905613-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674905613-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674905613-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674905613-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674905613-trainTPRs.npy
01/28/2023 12:33:34 PM  [*] Evaluating pretrained model on test set...
01/28/2023 12:33:39 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0145 | F1: 0.0285
01/28/2023 12:33:39 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.0715 | F1: 0.1334
01/28/2023 12:33:39 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2943 | F1: 0.4544
01/28/2023 12:33:39 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3766 | F1: 0.5461
01/28/2023 12:33:39 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4141 | F1: 0.5822
01/28/2023 12:33:39 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.4920 | F1: 0.6484
01/28/2023 12:33:39 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7030 | F1: 0.7863
01/28/2023 12:33:39 PM  [*] Evaluating non_pretrained model on test set...
01/28/2023 12:33:44 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0552 | F1: 0.1046
01/28/2023 12:33:44 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1928 | F1: 0.3232
01/28/2023 12:33:44 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2802 | F1: 0.4375
01/28/2023 12:33:44 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3120 | F1: 0.4747
01/28/2023 12:33:44 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.3810 | F1: 0.5484
01/28/2023 12:33:44 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5030 | F1: 0.6582
01/28/2023 12:33:44 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.6987 | F1: 0.7834
01/28/2023 12:33:44 PM  [*] Evaluating full_data model on test set...
01/28/2023 12:33:49 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0620 | F1: 0.1168
01/28/2023 12:33:49 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2669 | F1: 0.4213
01/28/2023 12:33:49 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3399 | F1: 0.5070
01/28/2023 12:33:49 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.4007 | F1: 0.5711
01/28/2023 12:33:49 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4806 | F1: 0.6455
01/28/2023 12:33:49 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5853 | F1: 0.7267
01/28/2023 12:33:49 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8059 | F1: 0.8524
01/28/2023 12:33:49 PM  [!] Running pre-training split 2/3
01/28/2023 12:33:52 PM  [!] Pre-training model...
01/28/2023 12:33:52 PM  [*] Masking sequences...
01/28/2023 12:34:12 PM  [*] Started epoch: 1
01/28/2023 12:34:12 PM  [*] Sat Jan 28 12:34:12 2023: Train Epoch: 1 [  0  /57094 (0 %)]	Loss: 406.298767 | Elapsed: 0.42s
01/28/2023 12:34:25 PM  [*] Sat Jan 28 12:34:25 2023: Train Epoch: 1 [6400 /57094 (11%)]	Loss: 216.414520 | Elapsed: 12.42s
01/28/2023 12:34:37 PM  [*] Sat Jan 28 12:34:37 2023: Train Epoch: 1 [12800/57094 (22%)]	Loss: 206.720520 | Elapsed: 12.49s
01/28/2023 12:34:50 PM  [*] Sat Jan 28 12:34:50 2023: Train Epoch: 1 [19200/57094 (34%)]	Loss: 199.904663 | Elapsed: 12.51s
01/28/2023 12:35:02 PM  [*] Sat Jan 28 12:35:02 2023: Train Epoch: 1 [25600/57094 (45%)]	Loss: 194.705322 | Elapsed: 12.55s
01/28/2023 12:35:15 PM  [*] Sat Jan 28 12:35:15 2023: Train Epoch: 1 [32000/57094 (56%)]	Loss: 210.669464 | Elapsed: 12.56s
01/28/2023 12:35:27 PM  [*] Sat Jan 28 12:35:27 2023: Train Epoch: 1 [38400/57094 (67%)]	Loss: 226.335556 | Elapsed: 12.44s
01/28/2023 12:35:40 PM  [*] Sat Jan 28 12:35:40 2023: Train Epoch: 1 [44800/57094 (78%)]	Loss: 186.519348 | Elapsed: 12.41s
01/28/2023 12:35:52 PM  [*] Sat Jan 28 12:35:52 2023: Train Epoch: 1 [51200/57094 (90%)]	Loss: 204.320435 | Elapsed: 12.41s
01/28/2023 12:36:05 PM  [*] Sat Jan 28 12:36:05 2023:    1    | Tr.loss: 210.912386 | Elapsed:  113.15  s
01/28/2023 12:36:05 PM  [*] Started epoch: 2
01/28/2023 12:36:05 PM  [*] Sat Jan 28 12:36:05 2023: Train Epoch: 2 [  0  /57094 (0 %)]	Loss: 197.445862 | Elapsed: 0.15s
01/28/2023 12:36:18 PM  [*] Sat Jan 28 12:36:18 2023: Train Epoch: 2 [6400 /57094 (11%)]	Loss: 189.630768 | Elapsed: 12.59s
01/28/2023 12:36:30 PM  [*] Sat Jan 28 12:36:30 2023: Train Epoch: 2 [12800/57094 (22%)]	Loss: 181.014297 | Elapsed: 12.48s
01/28/2023 12:36:43 PM  [*] Sat Jan 28 12:36:43 2023: Train Epoch: 2 [19200/57094 (34%)]	Loss: 172.449615 | Elapsed: 12.68s
01/28/2023 12:36:55 PM  [*] Sat Jan 28 12:36:55 2023: Train Epoch: 2 [25600/57094 (45%)]	Loss: 216.752991 | Elapsed: 12.61s
01/28/2023 12:37:08 PM  [*] Sat Jan 28 12:37:08 2023: Train Epoch: 2 [32000/57094 (56%)]	Loss: 200.048340 | Elapsed: 12.54s
01/28/2023 12:37:20 PM  [*] Sat Jan 28 12:37:20 2023: Train Epoch: 2 [38400/57094 (67%)]	Loss: 188.437012 | Elapsed: 12.50s
01/28/2023 12:37:33 PM  [*] Sat Jan 28 12:37:33 2023: Train Epoch: 2 [44800/57094 (78%)]	Loss: 185.667755 | Elapsed: 12.58s
01/28/2023 12:37:46 PM  [*] Sat Jan 28 12:37:46 2023: Train Epoch: 2 [51200/57094 (90%)]	Loss: 184.085846 | Elapsed: 12.53s
01/28/2023 12:37:59 PM  [*] Sat Jan 28 12:37:59 2023:    2    | Tr.loss: 191.392191 | Elapsed:  113.68  s
01/28/2023 12:37:59 PM  [*] Started epoch: 3
01/28/2023 12:37:59 PM  [*] Sat Jan 28 12:37:59 2023: Train Epoch: 3 [  0  /57094 (0 %)]	Loss: 205.268250 | Elapsed: 0.13s
01/28/2023 12:38:11 PM  [*] Sat Jan 28 12:38:11 2023: Train Epoch: 3 [6400 /57094 (11%)]	Loss: 167.633377 | Elapsed: 12.63s
01/28/2023 12:38:24 PM  [*] Sat Jan 28 12:38:24 2023: Train Epoch: 3 [12800/57094 (22%)]	Loss: 203.635132 | Elapsed: 12.59s
01/28/2023 12:38:36 PM  [*] Sat Jan 28 12:38:36 2023: Train Epoch: 3 [19200/57094 (34%)]	Loss: 195.998245 | Elapsed: 12.53s
01/28/2023 12:38:49 PM  [*] Sat Jan 28 12:38:49 2023: Train Epoch: 3 [25600/57094 (45%)]	Loss: 200.823898 | Elapsed: 12.52s
01/28/2023 12:39:02 PM  [*] Sat Jan 28 12:39:02 2023: Train Epoch: 3 [32000/57094 (56%)]	Loss: 198.743011 | Elapsed: 12.60s
01/28/2023 12:39:14 PM  [*] Sat Jan 28 12:39:14 2023: Train Epoch: 3 [38400/57094 (67%)]	Loss: 172.188629 | Elapsed: 12.58s
01/28/2023 12:39:27 PM  [*] Sat Jan 28 12:39:27 2023: Train Epoch: 3 [44800/57094 (78%)]	Loss: 189.607727 | Elapsed: 12.45s
01/28/2023 12:39:39 PM  [*] Sat Jan 28 12:39:39 2023: Train Epoch: 3 [51200/57094 (90%)]	Loss: 165.738846 | Elapsed: 12.52s
01/28/2023 12:39:52 PM  [*] Sat Jan 28 12:39:52 2023:    3    | Tr.loss: 185.616054 | Elapsed:  113.40  s
01/28/2023 12:39:52 PM  [*] Started epoch: 4
01/28/2023 12:39:52 PM  [*] Sat Jan 28 12:39:52 2023: Train Epoch: 4 [  0  /57094 (0 %)]	Loss: 176.011475 | Elapsed: 0.13s
01/28/2023 12:40:05 PM  [*] Sat Jan 28 12:40:05 2023: Train Epoch: 4 [6400 /57094 (11%)]	Loss: 166.323761 | Elapsed: 12.49s
01/28/2023 12:40:17 PM  [*] Sat Jan 28 12:40:17 2023: Train Epoch: 4 [12800/57094 (22%)]	Loss: 162.371506 | Elapsed: 12.49s
01/28/2023 12:40:29 PM  [*] Sat Jan 28 12:40:29 2023: Train Epoch: 4 [19200/57094 (34%)]	Loss: 178.132385 | Elapsed: 12.42s
01/28/2023 12:40:42 PM  [*] Sat Jan 28 12:40:42 2023: Train Epoch: 4 [25600/57094 (45%)]	Loss: 187.259888 | Elapsed: 12.47s
01/28/2023 12:40:54 PM  [*] Sat Jan 28 12:40:54 2023: Train Epoch: 4 [32000/57094 (56%)]	Loss: 186.919769 | Elapsed: 12.46s
01/28/2023 12:41:07 PM  [*] Sat Jan 28 12:41:07 2023: Train Epoch: 4 [38400/57094 (67%)]	Loss: 194.973328 | Elapsed: 12.42s
01/28/2023 12:41:19 PM  [*] Sat Jan 28 12:41:19 2023: Train Epoch: 4 [44800/57094 (78%)]	Loss: 190.075714 | Elapsed: 12.44s
01/28/2023 12:41:32 PM  [*] Sat Jan 28 12:41:32 2023: Train Epoch: 4 [51200/57094 (90%)]	Loss: 193.873871 | Elapsed: 12.56s
01/28/2023 12:41:45 PM  [*] Sat Jan 28 12:41:45 2023:    4    | Tr.loss: 182.637248 | Elapsed:  112.83  s
01/28/2023 12:41:45 PM  [*] Started epoch: 5
01/28/2023 12:41:45 PM  [*] Sat Jan 28 12:41:45 2023: Train Epoch: 5 [  0  /57094 (0 %)]	Loss: 191.645782 | Elapsed: 0.12s
01/28/2023 12:41:57 PM  [*] Sat Jan 28 12:41:57 2023: Train Epoch: 5 [6400 /57094 (11%)]	Loss: 188.225586 | Elapsed: 12.62s
01/28/2023 12:42:10 PM  [*] Sat Jan 28 12:42:10 2023: Train Epoch: 5 [12800/57094 (22%)]	Loss: 199.544769 | Elapsed: 12.61s
01/28/2023 12:42:23 PM  [*] Sat Jan 28 12:42:23 2023: Train Epoch: 5 [19200/57094 (34%)]	Loss: 183.176376 | Elapsed: 12.56s
01/28/2023 12:42:35 PM  [*] Sat Jan 28 12:42:35 2023: Train Epoch: 5 [25600/57094 (45%)]	Loss: 168.464508 | Elapsed: 12.58s
01/28/2023 12:42:48 PM  [*] Sat Jan 28 12:42:48 2023: Train Epoch: 5 [32000/57094 (56%)]	Loss: 184.815216 | Elapsed: 12.57s
01/28/2023 12:43:00 PM  [*] Sat Jan 28 12:43:00 2023: Train Epoch: 5 [38400/57094 (67%)]	Loss: 184.612045 | Elapsed: 12.45s
01/28/2023 12:43:13 PM  [*] Sat Jan 28 12:43:13 2023: Train Epoch: 5 [44800/57094 (78%)]	Loss: 190.763123 | Elapsed: 12.51s
01/28/2023 12:43:25 PM  [*] Sat Jan 28 12:43:25 2023: Train Epoch: 5 [51200/57094 (90%)]	Loss: 172.571960 | Elapsed: 12.48s
01/28/2023 12:43:38 PM  [*] Sat Jan 28 12:43:38 2023:    5    | Tr.loss: 180.926216 | Elapsed:  113.31  s
01/28/2023 12:43:38 PM  [*] Started epoch: 6
01/28/2023 12:43:38 PM  [*] Sat Jan 28 12:43:38 2023: Train Epoch: 6 [  0  /57094 (0 %)]	Loss: 185.063492 | Elapsed: 0.14s
01/28/2023 12:43:51 PM  [*] Sat Jan 28 12:43:51 2023: Train Epoch: 6 [6400 /57094 (11%)]	Loss: 173.931763 | Elapsed: 12.49s
01/28/2023 12:44:03 PM  [*] Sat Jan 28 12:44:03 2023: Train Epoch: 6 [12800/57094 (22%)]	Loss: 153.549316 | Elapsed: 12.42s
01/28/2023 12:44:16 PM  [*] Sat Jan 28 12:44:16 2023: Train Epoch: 6 [19200/57094 (34%)]	Loss: 180.041016 | Elapsed: 12.44s
01/28/2023 12:44:28 PM  [*] Sat Jan 28 12:44:28 2023: Train Epoch: 6 [25600/57094 (45%)]	Loss: 169.491348 | Elapsed: 12.50s
01/28/2023 12:44:40 PM  [*] Sat Jan 28 12:44:40 2023: Train Epoch: 6 [32000/57094 (56%)]	Loss: 181.406723 | Elapsed: 12.43s
01/28/2023 12:44:45 PM [!] Learning rate: 2.5e-05
01/28/2023 12:44:53 PM  [*] Sat Jan 28 12:44:53 2023: Train Epoch: 6 [38400/57094 (67%)]	Loss: 191.021500 | Elapsed: 12.42s
01/28/2023 12:45:05 PM  [*] Sat Jan 28 12:45:05 2023: Train Epoch: 6 [44800/57094 (78%)]	Loss: 161.064590 | Elapsed: 12.41s
01/28/2023 12:45:18 PM  [*] Sat Jan 28 12:45:18 2023: Train Epoch: 6 [51200/57094 (90%)]	Loss: 167.476776 | Elapsed: 12.47s
01/28/2023 12:45:31 PM  [*] Sat Jan 28 12:45:31 2023:    6    | Tr.loss: 179.400447 | Elapsed:  112.65  s
01/28/2023 12:45:31 PM  [*] Started epoch: 7
01/28/2023 12:45:31 PM  [*] Sat Jan 28 12:45:31 2023: Train Epoch: 7 [  0  /57094 (0 %)]	Loss: 180.226151 | Elapsed: 0.14s
01/28/2023 12:45:43 PM  [*] Sat Jan 28 12:45:43 2023: Train Epoch: 7 [6400 /57094 (11%)]	Loss: 180.884811 | Elapsed: 12.56s
01/28/2023 12:45:56 PM  [*] Sat Jan 28 12:45:56 2023: Train Epoch: 7 [12800/57094 (22%)]	Loss: 182.751205 | Elapsed: 12.60s
01/28/2023 12:46:09 PM  [*] Sat Jan 28 12:46:09 2023: Train Epoch: 7 [19200/57094 (34%)]	Loss: 186.298584 | Elapsed: 12.53s
01/28/2023 12:46:21 PM  [*] Sat Jan 28 12:46:21 2023: Train Epoch: 7 [25600/57094 (45%)]	Loss: 169.627243 | Elapsed: 12.54s
01/28/2023 12:46:34 PM  [*] Sat Jan 28 12:46:34 2023: Train Epoch: 7 [32000/57094 (56%)]	Loss: 181.141571 | Elapsed: 12.55s
01/28/2023 12:46:46 PM  [*] Sat Jan 28 12:46:46 2023: Train Epoch: 7 [38400/57094 (67%)]	Loss: 178.587860 | Elapsed: 12.49s
01/28/2023 12:46:59 PM  [*] Sat Jan 28 12:46:59 2023: Train Epoch: 7 [44800/57094 (78%)]	Loss: 158.946869 | Elapsed: 12.56s
01/28/2023 12:47:11 PM  [*] Sat Jan 28 12:47:11 2023: Train Epoch: 7 [51200/57094 (90%)]	Loss: 174.978439 | Elapsed: 12.62s
01/28/2023 12:47:24 PM  [*] Sat Jan 28 12:47:24 2023:    7    | Tr.loss: 178.635407 | Elapsed:  113.56  s
01/28/2023 12:47:24 PM  [*] Started epoch: 8
01/28/2023 12:47:24 PM  [*] Sat Jan 28 12:47:24 2023: Train Epoch: 8 [  0  /57094 (0 %)]	Loss: 164.544922 | Elapsed: 0.15s
01/28/2023 12:47:37 PM  [*] Sat Jan 28 12:47:37 2023: Train Epoch: 8 [6400 /57094 (11%)]	Loss: 174.581421 | Elapsed: 12.48s
01/28/2023 12:47:49 PM  [*] Sat Jan 28 12:47:49 2023: Train Epoch: 8 [12800/57094 (22%)]	Loss: 155.196152 | Elapsed: 12.45s
01/28/2023 12:48:02 PM  [*] Sat Jan 28 12:48:02 2023: Train Epoch: 8 [19200/57094 (34%)]	Loss: 161.111511 | Elapsed: 12.46s
01/28/2023 12:48:14 PM  [*] Sat Jan 28 12:48:14 2023: Train Epoch: 8 [25600/57094 (45%)]	Loss: 184.190018 | Elapsed: 12.55s
01/28/2023 12:48:27 PM  [*] Sat Jan 28 12:48:27 2023: Train Epoch: 8 [32000/57094 (56%)]	Loss: 164.691864 | Elapsed: 12.44s
01/28/2023 12:48:39 PM  [*] Sat Jan 28 12:48:39 2023: Train Epoch: 8 [38400/57094 (67%)]	Loss: 179.999359 | Elapsed: 12.45s
01/28/2023 12:48:52 PM  [*] Sat Jan 28 12:48:52 2023: Train Epoch: 8 [44800/57094 (78%)]	Loss: 192.878540 | Elapsed: 12.51s
01/28/2023 12:49:04 PM  [*] Sat Jan 28 12:49:04 2023: Train Epoch: 8 [51200/57094 (90%)]	Loss: 195.721283 | Elapsed: 12.43s
01/28/2023 12:49:17 PM  [*] Sat Jan 28 12:49:17 2023:    8    | Tr.loss: 178.341253 | Elapsed:  112.88  s
01/28/2023 12:49:17 PM  [*] Started epoch: 9
01/28/2023 12:49:17 PM  [*] Sat Jan 28 12:49:17 2023: Train Epoch: 9 [  0  /57094 (0 %)]	Loss: 165.110916 | Elapsed: 0.13s
01/28/2023 12:49:30 PM  [*] Sat Jan 28 12:49:30 2023: Train Epoch: 9 [6400 /57094 (11%)]	Loss: 184.600159 | Elapsed: 12.48s
01/28/2023 12:49:42 PM  [*] Sat Jan 28 12:49:42 2023: Train Epoch: 9 [12800/57094 (22%)]	Loss: 184.961349 | Elapsed: 12.37s
01/28/2023 12:49:55 PM  [*] Sat Jan 28 12:49:55 2023: Train Epoch: 9 [19200/57094 (34%)]	Loss: 186.734497 | Elapsed: 12.53s
01/28/2023 12:50:07 PM  [*] Sat Jan 28 12:50:07 2023: Train Epoch: 9 [25600/57094 (45%)]	Loss: 200.275253 | Elapsed: 12.41s
01/28/2023 12:50:19 PM  [*] Sat Jan 28 12:50:19 2023: Train Epoch: 9 [32000/57094 (56%)]	Loss: 168.125977 | Elapsed: 12.39s
01/28/2023 12:50:32 PM  [*] Sat Jan 28 12:50:32 2023: Train Epoch: 9 [38400/57094 (67%)]	Loss: 184.544312 | Elapsed: 12.45s
01/28/2023 12:50:44 PM  [*] Sat Jan 28 12:50:44 2023: Train Epoch: 9 [44800/57094 (78%)]	Loss: 191.005234 | Elapsed: 12.38s
01/28/2023 12:50:57 PM  [*] Sat Jan 28 12:50:57 2023: Train Epoch: 9 [51200/57094 (90%)]	Loss: 191.335037 | Elapsed: 12.47s
01/28/2023 12:51:10 PM  [*] Sat Jan 28 12:51:10 2023:    9    | Tr.loss: 178.155806 | Elapsed:  112.60  s
01/28/2023 12:51:10 PM  [*] Started epoch: 10
01/28/2023 12:51:10 PM  [*] Sat Jan 28 12:51:10 2023: Train Epoch: 10 [  0  /57094 (0 %)]	Loss: 166.078766 | Elapsed: 0.14s
01/28/2023 12:51:22 PM  [*] Sat Jan 28 12:51:22 2023: Train Epoch: 10 [6400 /57094 (11%)]	Loss: 190.945663 | Elapsed: 12.58s
01/28/2023 12:51:35 PM  [*] Sat Jan 28 12:51:35 2023: Train Epoch: 10 [12800/57094 (22%)]	Loss: 180.954193 | Elapsed: 12.56s
01/28/2023 12:51:48 PM  [*] Sat Jan 28 12:51:48 2023: Train Epoch: 10 [19200/57094 (34%)]	Loss: 193.619537 | Elapsed: 12.59s
01/28/2023 12:52:01 PM  [*] Sat Jan 28 12:52:01 2023: Train Epoch: 10 [25600/57094 (45%)]	Loss: 162.808716 | Elapsed: 12.98s
01/28/2023 12:52:13 PM  [*] Sat Jan 28 12:52:13 2023: Train Epoch: 10 [32000/57094 (56%)]	Loss: 177.202606 | Elapsed: 12.59s
01/28/2023 12:52:26 PM  [*] Sat Jan 28 12:52:26 2023: Train Epoch: 10 [38400/57094 (67%)]	Loss: 172.148880 | Elapsed: 12.50s
01/28/2023 12:52:38 PM  [*] Sat Jan 28 12:52:38 2023: Train Epoch: 10 [44800/57094 (78%)]	Loss: 169.865952 | Elapsed: 12.62s
01/28/2023 12:52:51 PM  [*] Sat Jan 28 12:52:51 2023: Train Epoch: 10 [51200/57094 (90%)]	Loss: 185.564178 | Elapsed: 12.55s
01/28/2023 12:53:05 PM  [*] Sat Jan 28 12:53:05 2023:   10    | Tr.loss: 178.009110 | Elapsed:  114.75  s
01/28/2023 12:53:05 PM [!] Sat Jan 28 12:53:05 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674906785-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674906785-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674906785-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674906785-auc.npy
01/28/2023 12:53:06 PM  [!] Training pretrained model on downstream task...
01/28/2023 12:53:06 PM  [*] Started epoch: 1
01/28/2023 12:53:06 PM  [*] Sat Jan 28 12:53:06 2023: Train Epoch: 1 [  0  /19032 (0 %)]	Loss: 1.290760 | Elapsed: 0.28s | FPR 0.0003 -> TPR 0.1111 & F1 0.2000
01/28/2023 12:53:16 PM  [*] Sat Jan 28 12:53:16 2023: Train Epoch: 1 [6400 /19032 (34%)]	Loss: 0.515092 | Elapsed: 9.29s | FPR 0.0003 -> TPR 0.2254 & F1 0.3678
01/28/2023 12:53:25 PM  [*] Sat Jan 28 12:53:25 2023: Train Epoch: 1 [12800/19032 (67%)]	Loss: 0.539883 | Elapsed: 9.20s | FPR 0.0003 -> TPR 0.2540 & F1 0.4051
01/28/2023 12:53:34 PM  [*] Sat Jan 28 12:53:34 2023:    1    | Tr.loss: 0.455166 | Elapsed:   28.09  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.01 | AUC: 0.8423
01/28/2023 12:53:34 PM  [*] Started epoch: 2
01/28/2023 12:53:34 PM  [*] Sat Jan 28 12:53:34 2023: Train Epoch: 2 [  0  /19032 (0 %)]	Loss: 0.307566 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.4186 & F1 0.5902
01/28/2023 12:53:43 PM  [*] Sat Jan 28 12:53:43 2023: Train Epoch: 2 [6400 /19032 (34%)]	Loss: 0.205892 | Elapsed: 9.14s | FPR 0.0003 -> TPR 0.7231 & F1 0.8393
01/28/2023 12:53:53 PM  [*] Sat Jan 28 12:53:53 2023: Train Epoch: 2 [12800/19032 (67%)]	Loss: 0.255215 | Elapsed: 9.15s | FPR 0.0003 -> TPR 0.7059 & F1 0.8276
01/28/2023 12:54:02 PM  [*] Sat Jan 28 12:54:02 2023:    2    | Tr.loss: 0.259275 | Elapsed:   27.89  s | FPR 0.0003 -> TPR: 0.35 & F1: 0.52 | AUC: 0.9513
01/28/2023 12:54:02 PM  [*] Started epoch: 3
01/28/2023 12:54:02 PM  [*] Sat Jan 28 12:54:02 2023: Train Epoch: 3 [  0  /19032 (0 %)]	Loss: 0.140441 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.8864 & F1 0.9398
01/28/2023 12:54:11 PM  [*] Sat Jan 28 12:54:11 2023: Train Epoch: 3 [6400 /19032 (34%)]	Loss: 0.219954 | Elapsed: 9.18s | FPR 0.0003 -> TPR 0.7258 & F1 0.8411
01/28/2023 12:54:21 PM  [*] Sat Jan 28 12:54:21 2023: Train Epoch: 3 [12800/19032 (67%)]	Loss: 0.129980 | Elapsed: 9.19s | FPR 0.0003 -> TPR 0.9315 & F1 0.9645
01/28/2023 12:54:30 PM  [*] Sat Jan 28 12:54:30 2023:    3    | Tr.loss: 0.171085 | Elapsed:   27.95  s | FPR 0.0003 -> TPR: 0.36 & F1: 0.53 | AUC: 0.9799
01/28/2023 12:54:30 PM [!] Sat Jan 28 12:54:30 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674906870-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674906870-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674906870-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674906870-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674906870-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674906870-trainTPRs.npy
01/28/2023 12:54:30 PM  [!] Training non_pretrained model on downstream task...
01/28/2023 12:54:31 PM  [*] Started epoch: 1
01/28/2023 12:54:31 PM  [*] Sat Jan 28 12:54:31 2023: Train Epoch: 1 [  0  /19032 (0 %)]	Loss: 2.549270 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000
01/28/2023 12:54:37 PM  [*] Sat Jan 28 12:54:37 2023: Train Epoch: 1 [6400 /19032 (34%)]	Loss: 0.485021 | Elapsed: 6.37s | FPR 0.0003 -> TPR 0.2500 & F1 0.4000
01/28/2023 12:54:44 PM  [*] Sat Jan 28 12:54:44 2023: Train Epoch: 1 [12800/19032 (67%)]	Loss: 0.400275 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.3594 & F1 0.5287
01/28/2023 12:54:50 PM  [*] Sat Jan 28 12:54:50 2023:    1    | Tr.loss: 0.459849 | Elapsed:   19.22  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.8355
01/28/2023 12:54:50 PM  [*] Started epoch: 2
01/28/2023 12:54:50 PM  [*] Sat Jan 28 12:54:50 2023: Train Epoch: 2 [  0  /19032 (0 %)]	Loss: 0.318967 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000
01/28/2023 12:54:57 PM  [*] Sat Jan 28 12:54:57 2023: Train Epoch: 2 [6400 /19032 (34%)]	Loss: 0.307924 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.6197 & F1 0.7652
01/28/2023 12:55:03 PM  [*] Sat Jan 28 12:55:03 2023: Train Epoch: 2 [12800/19032 (67%)]	Loss: 0.396903 | Elapsed: 6.25s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667
01/28/2023 12:55:09 PM  [*] Sat Jan 28 12:55:09 2023:    2    | Tr.loss: 0.279473 | Elapsed:   19.07  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.51 | AUC: 0.9419
01/28/2023 12:55:09 PM  [*] Started epoch: 3
01/28/2023 12:55:09 PM  [*] Sat Jan 28 12:55:09 2023: Train Epoch: 3 [  0  /19032 (0 %)]	Loss: 0.252728 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.4400 & F1 0.6111
01/28/2023 12:55:16 PM  [*] Sat Jan 28 12:55:16 2023: Train Epoch: 3 [6400 /19032 (34%)]	Loss: 0.186056 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.7733 & F1 0.8722
01/28/2023 12:55:22 PM  [*] Sat Jan 28 12:55:22 2023: Train Epoch: 3 [12800/19032 (67%)]	Loss: 0.197180 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.7838 & F1 0.8788
01/28/2023 12:55:28 PM  [*] Sat Jan 28 12:55:28 2023:    3    | Tr.loss: 0.203284 | Elapsed:   19.20  s | FPR 0.0003 -> TPR: 0.26 & F1: 0.42 | AUC: 0.9709
01/28/2023 12:55:29 PM [!] Sat Jan 28 12:55:29 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674906928-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674906928-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674906928-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674906928-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674906928-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674906928-trainTPRs.npy
01/28/2023 12:55:29 PM  [!] Training full_data model on downstream task...
01/28/2023 12:55:29 PM  [*] Started epoch: 1
01/28/2023 12:55:30 PM  [*] Sat Jan 28 12:55:30 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 2.903152 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0976 & F1 0.1778
01/28/2023 12:55:36 PM  [*] Sat Jan 28 12:55:36 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.512971 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.0462 & F1 0.0882
01/28/2023 12:55:42 PM  [*] Sat Jan 28 12:55:42 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.503650 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.2879 & F1 0.4471
01/28/2023 12:55:49 PM  [*] Sat Jan 28 12:55:49 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.316120 | Elapsed: 6.33s | FPR 0.0003 -> TPR 0.3913 & F1 0.5625
01/28/2023 12:55:55 PM  [*] Sat Jan 28 12:55:55 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.383074 | Elapsed: 6.35s | FPR 0.0003 -> TPR 0.5143 & F1 0.6792
01/28/2023 12:56:01 PM  [*] Sat Jan 28 12:56:01 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.280736 | Elapsed: 6.44s | FPR 0.0003 -> TPR 0.7971 & F1 0.8871
01/28/2023 12:56:08 PM  [*] Sat Jan 28 12:56:08 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.174147 | Elapsed: 6.54s | FPR 0.0003 -> TPR 0.8056 & F1 0.8923
01/28/2023 12:56:14 PM  [*] Sat Jan 28 12:56:14 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.287259 | Elapsed: 6.37s | FPR 0.0003 -> TPR 0.6351 & F1 0.7769
01/28/2023 12:56:21 PM  [*] Sat Jan 28 12:56:21 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.213000 | Elapsed: 6.41s | FPR 0.0003 -> TPR 0.6615 & F1 0.7963
01/28/2023 12:56:27 PM  [*] Sat Jan 28 12:56:27 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.174149 | Elapsed: 6.32s | FPR 0.0003 -> TPR 0.7639 & F1 0.8661
01/28/2023 12:56:33 PM [!] Learning rate: 2.5e-05
01/28/2023 12:56:33 PM  [*] Sat Jan 28 12:56:33 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.208167 | Elapsed: 6.37s | FPR 0.0003 -> TPR 0.7164 & F1 0.8348
01/28/2023 12:56:40 PM  [*] Sat Jan 28 12:56:40 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.187303 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.9286 & F1 0.9630
01/28/2023 12:56:47 PM  [*] Sat Jan 28 12:56:47 2023:    1    | Tr.loss: 0.307635 | Elapsed:   77.87  s | FPR 0.0003 -> TPR: 0.08 & F1: 0.15 | AUC: 0.9313
01/28/2023 12:56:47 PM  [*] Started epoch: 2
01/28/2023 12:56:47 PM  [*] Sat Jan 28 12:56:47 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.087626 | Elapsed: 0.13s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000
01/28/2023 12:56:54 PM  [*] Sat Jan 28 12:56:54 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.148517 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.8750 & F1 0.9333
01/28/2023 12:57:00 PM  [*] Sat Jan 28 12:57:00 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.120532 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.7458 & F1 0.8544
01/28/2023 12:57:06 PM  [*] Sat Jan 28 12:57:06 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.102120 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.7761 & F1 0.8739
01/28/2023 12:57:13 PM  [*] Sat Jan 28 12:57:13 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.101756 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.9254 & F1 0.9612
01/28/2023 12:57:19 PM  [*] Sat Jan 28 12:57:19 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.165278 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.7463 & F1 0.8547
01/28/2023 12:57:25 PM  [*] Sat Jan 28 12:57:25 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.230582 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.5278 & F1 0.6909
01/28/2023 12:57:32 PM  [*] Sat Jan 28 12:57:32 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.200266 | Elapsed: 6.41s | FPR 0.0003 -> TPR 0.7727 & F1 0.8718
01/28/2023 12:57:38 PM  [*] Sat Jan 28 12:57:38 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.177762 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.7941 & F1 0.8852
01/28/2023 12:57:39 PM [!] Learning rate: 2.5e-06
01/28/2023 12:57:44 PM  [*] Sat Jan 28 12:57:44 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.190030 | Elapsed: 6.40s | FPR 0.0003 -> TPR 0.8000 & F1 0.8889
01/28/2023 12:57:51 PM  [*] Sat Jan 28 12:57:51 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.221784 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091
01/28/2023 12:57:57 PM  [*] Sat Jan 28 12:57:57 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.159844 | Elapsed: 6.33s | FPR 0.0003 -> TPR 0.8219 & F1 0.9023
01/28/2023 12:58:05 PM  [*] Sat Jan 28 12:58:05 2023:    2    | Tr.loss: 0.176361 | Elapsed:   77.71  s | FPR 0.0003 -> TPR: 0.40 & F1: 0.57 | AUC: 0.9783
01/28/2023 12:58:05 PM  [*] Started epoch: 3
01/28/2023 12:58:05 PM  [*] Sat Jan 28 12:58:05 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.238467 | Elapsed: 0.22s | FPR 0.0003 -> TPR 0.8158 & F1 0.8986
01/28/2023 12:58:12 PM  [*] Sat Jan 28 12:58:12 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.222563 | Elapsed: 6.54s | FPR 0.0003 -> TPR 0.6866 & F1 0.8142
01/28/2023 12:58:18 PM  [*] Sat Jan 28 12:58:18 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.127798 | Elapsed: 6.32s | FPR 0.0003 -> TPR 0.7846 & F1 0.8793
01/28/2023 12:58:25 PM  [*] Sat Jan 28 12:58:25 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.140911 | Elapsed: 6.49s | FPR 0.0003 -> TPR 0.6349 & F1 0.7767
01/28/2023 12:58:31 PM  [*] Sat Jan 28 12:58:31 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.167279 | Elapsed: 6.42s | FPR 0.0003 -> TPR 0.8060 & F1 0.8926
01/28/2023 12:58:38 PM  [*] Sat Jan 28 12:58:38 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.186317 | Elapsed: 6.50s | FPR 0.0003 -> TPR 0.5385 & F1 0.7000
01/28/2023 12:58:44 PM  [*] Sat Jan 28 12:58:44 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.157237 | Elapsed: 6.46s | FPR 0.0003 -> TPR 0.9474 & F1 0.9730
01/28/2023 12:58:45 PM [!] Learning rate: 2.5000000000000004e-07
01/28/2023 12:58:50 PM  [*] Sat Jan 28 12:58:50 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.098730 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.9219 & F1 0.9593
01/28/2023 12:58:57 PM  [*] Sat Jan 28 12:58:57 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.198689 | Elapsed: 6.32s | FPR 0.0003 -> TPR 0.6575 & F1 0.7934
01/28/2023 12:59:03 PM  [*] Sat Jan 28 12:59:03 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.157463 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.9437 & F1 0.9710
01/28/2023 12:59:09 PM  [*] Sat Jan 28 12:59:09 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.142196 | Elapsed: 6.35s | FPR 0.0003 -> TPR 0.7538 & F1 0.8596
01/28/2023 12:59:16 PM  [*] Sat Jan 28 12:59:16 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.237618 | Elapsed: 6.26s | FPR 0.0003 -> TPR 0.7463 & F1 0.8547
01/28/2023 12:59:23 PM  [*] Sat Jan 28 12:59:23 2023:    3    | Tr.loss: 0.169925 | Elapsed:   78.07  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9799
01/28/2023 12:59:24 PM [!] Sat Jan 28 12:59:24 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674907163-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674907163-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674907163-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674907163-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674907163-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674907163-trainTPRs.npy
01/28/2023 12:59:24 PM  [*] Evaluating pretrained model on test set...
01/28/2023 12:59:29 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.1664 | F1: 0.2854
01/28/2023 12:59:29 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2299 | F1: 0.3738
01/28/2023 12:59:29 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3123 | F1: 0.4757
01/28/2023 12:59:29 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.4477 | F1: 0.6175
01/28/2023 12:59:29 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.5655 | F1: 0.7185
01/28/2023 12:59:29 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.6397 | F1: 0.7683
01/28/2023 12:59:29 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8325 | F1: 0.8683
01/28/2023 12:59:29 PM  [*] Evaluating non_pretrained model on test set...
01/28/2023 12:59:34 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0241 | F1: 0.0471
01/28/2023 12:59:34 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1579 | F1: 0.2727
01/28/2023 12:59:34 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2801 | F1: 0.4374
01/28/2023 12:59:34 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3283 | F1: 0.4934
01/28/2023 12:59:34 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.3785 | F1: 0.5458
01/28/2023 12:59:34 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5197 | F1: 0.6727
01/28/2023 12:59:34 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7994 | F1: 0.8484
01/28/2023 12:59:34 PM  [*] Evaluating full_data model on test set...
01/28/2023 12:59:39 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0111 | F1: 0.0219
01/28/2023 12:59:39 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2419 | F1: 0.3896
01/28/2023 12:59:39 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3195 | F1: 0.4839
01/28/2023 12:59:39 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3468 | F1: 0.5140
01/28/2023 12:59:39 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4139 | F1: 0.5821
01/28/2023 12:59:39 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5583 | F1: 0.7050
01/28/2023 12:59:39 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.8075 | F1: 0.8533
01/28/2023 12:59:39 PM  [!] Running pre-training split 3/3
01/28/2023 12:59:43 PM  [!] Pre-training model...
01/28/2023 12:59:43 PM  [*] Masking sequences...
01/28/2023 01:00:06 PM  [*] Started epoch: 1
01/28/2023 01:00:06 PM  [*] Sat Jan 28 13:00:06 2023: Train Epoch: 1 [  0  /57094 (0 %)]	Loss: 413.179626 | Elapsed: 0.30s
01/28/2023 01:00:19 PM  [*] Sat Jan 28 13:00:19 2023: Train Epoch: 1 [6400 /57094 (11%)]	Loss: 244.411865 | Elapsed: 12.63s
01/28/2023 01:00:32 PM  [*] Sat Jan 28 13:00:32 2023: Train Epoch: 1 [12800/57094 (22%)]	Loss: 211.540680 | Elapsed: 12.62s
01/28/2023 01:00:45 PM  [*] Sat Jan 28 13:00:45 2023: Train Epoch: 1 [19200/57094 (34%)]	Loss: 229.995483 | Elapsed: 12.97s
01/28/2023 01:00:57 PM  [*] Sat Jan 28 13:00:57 2023: Train Epoch: 1 [25600/57094 (45%)]	Loss: 192.498077 | Elapsed: 12.70s
01/28/2023 01:01:10 PM  [*] Sat Jan 28 13:01:10 2023: Train Epoch: 1 [32000/57094 (56%)]	Loss: 217.465881 | Elapsed: 13.12s
01/28/2023 01:01:23 PM  [*] Sat Jan 28 13:01:23 2023: Train Epoch: 1 [38400/57094 (67%)]	Loss: 201.781723 | Elapsed: 13.12s
01/28/2023 01:01:36 PM  [*] Sat Jan 28 13:01:36 2023: Train Epoch: 1 [44800/57094 (78%)]	Loss: 188.838120 | Elapsed: 12.76s
01/28/2023 01:01:49 PM  [*] Sat Jan 28 13:01:49 2023: Train Epoch: 1 [51200/57094 (90%)]	Loss: 184.851440 | Elapsed: 12.59s
01/28/2023 01:02:02 PM  [*] Sat Jan 28 13:02:02 2023:    1    | Tr.loss: 207.751345 | Elapsed:  116.34  s
01/28/2023 01:02:02 PM  [*] Started epoch: 2
01/28/2023 01:02:02 PM  [*] Sat Jan 28 13:02:02 2023: Train Epoch: 2 [  0  /57094 (0 %)]	Loss: 193.120163 | Elapsed: 0.13s
01/28/2023 01:02:15 PM  [*] Sat Jan 28 13:02:15 2023: Train Epoch: 2 [6400 /57094 (11%)]	Loss: 176.641205 | Elapsed: 12.80s
01/28/2023 01:02:28 PM  [*] Sat Jan 28 13:02:28 2023: Train Epoch: 2 [12800/57094 (22%)]	Loss: 160.952530 | Elapsed: 12.55s
01/28/2023 01:02:41 PM  [*] Sat Jan 28 13:02:41 2023: Train Epoch: 2 [19200/57094 (34%)]	Loss: 186.980988 | Elapsed: 12.75s
01/28/2023 01:02:53 PM  [*] Sat Jan 28 13:02:53 2023: Train Epoch: 2 [25600/57094 (45%)]	Loss: 186.861389 | Elapsed: 12.51s
01/28/2023 01:03:06 PM  [*] Sat Jan 28 13:03:06 2023: Train Epoch: 2 [32000/57094 (56%)]	Loss: 212.006119 | Elapsed: 13.23s
01/28/2023 01:03:20 PM  [*] Sat Jan 28 13:03:20 2023: Train Epoch: 2 [38400/57094 (67%)]	Loss: 195.585541 | Elapsed: 13.25s
01/28/2023 01:03:33 PM  [*] Sat Jan 28 13:03:33 2023: Train Epoch: 2 [44800/57094 (78%)]	Loss: 192.601837 | Elapsed: 13.07s
01/28/2023 01:03:46 PM  [*] Sat Jan 28 13:03:46 2023: Train Epoch: 2 [51200/57094 (90%)]	Loss: 193.245316 | Elapsed: 12.91s
01/28/2023 01:03:59 PM  [*] Sat Jan 28 13:03:59 2023:    2    | Tr.loss: 185.557483 | Elapsed:  116.46  s
01/28/2023 01:03:59 PM  [*] Started epoch: 3
01/28/2023 01:03:59 PM  [*] Sat Jan 28 13:03:59 2023: Train Epoch: 3 [  0  /57094 (0 %)]	Loss: 189.897705 | Elapsed: 0.13s
01/28/2023 01:04:12 PM  [*] Sat Jan 28 13:04:12 2023: Train Epoch: 3 [6400 /57094 (11%)]	Loss: 200.308853 | Elapsed: 12.64s
01/28/2023 01:04:24 PM  [*] Sat Jan 28 13:04:24 2023: Train Epoch: 3 [12800/57094 (22%)]	Loss: 190.978638 | Elapsed: 12.81s
01/28/2023 01:04:37 PM  [*] Sat Jan 28 13:04:37 2023: Train Epoch: 3 [19200/57094 (34%)]	Loss: 189.560455 | Elapsed: 12.78s
01/28/2023 01:04:50 PM  [*] Sat Jan 28 13:04:50 2023: Train Epoch: 3 [25600/57094 (45%)]	Loss: 179.188705 | Elapsed: 12.66s
01/28/2023 01:05:03 PM  [*] Sat Jan 28 13:05:03 2023: Train Epoch: 3 [32000/57094 (56%)]	Loss: 180.602753 | Elapsed: 12.78s
01/28/2023 01:05:15 PM  [*] Sat Jan 28 13:05:15 2023: Train Epoch: 3 [38400/57094 (67%)]	Loss: 175.326508 | Elapsed: 12.60s
01/28/2023 01:05:28 PM  [*] Sat Jan 28 13:05:28 2023: Train Epoch: 3 [44800/57094 (78%)]	Loss: 153.729858 | Elapsed: 12.47s
01/28/2023 01:05:40 PM  [*] Sat Jan 28 13:05:40 2023: Train Epoch: 3 [51200/57094 (90%)]	Loss: 180.672485 | Elapsed: 12.22s
01/28/2023 01:05:53 PM  [*] Sat Jan 28 13:05:53 2023:    3    | Tr.loss: 180.450198 | Elapsed:  114.01  s
01/28/2023 01:05:53 PM  [*] Started epoch: 4
01/28/2023 01:05:53 PM  [*] Sat Jan 28 13:05:53 2023: Train Epoch: 4 [  0  /57094 (0 %)]	Loss: 184.351410 | Elapsed: 0.13s
01/28/2023 01:06:06 PM  [*] Sat Jan 28 13:06:06 2023: Train Epoch: 4 [6400 /57094 (11%)]	Loss: 185.233780 | Elapsed: 12.93s
01/28/2023 01:06:19 PM  [*] Sat Jan 28 13:06:19 2023: Train Epoch: 4 [12800/57094 (22%)]	Loss: 191.994583 | Elapsed: 12.76s
01/28/2023 01:06:31 PM  [*] Sat Jan 28 13:06:31 2023: Train Epoch: 4 [19200/57094 (34%)]	Loss: 175.469788 | Elapsed: 12.53s
01/28/2023 01:06:44 PM  [*] Sat Jan 28 13:06:44 2023: Train Epoch: 4 [25600/57094 (45%)]	Loss: 181.342072 | Elapsed: 12.47s
01/28/2023 01:06:56 PM  [*] Sat Jan 28 13:06:56 2023: Train Epoch: 4 [32000/57094 (56%)]	Loss: 161.819000 | Elapsed: 12.57s
01/28/2023 01:07:09 PM  [*] Sat Jan 28 13:07:09 2023: Train Epoch: 4 [38400/57094 (67%)]	Loss: 180.370102 | Elapsed: 12.62s
01/28/2023 01:07:21 PM  [*] Sat Jan 28 13:07:21 2023: Train Epoch: 4 [44800/57094 (78%)]	Loss: 178.172302 | Elapsed: 12.43s
01/28/2023 01:07:34 PM  [*] Sat Jan 28 13:07:34 2023: Train Epoch: 4 [51200/57094 (90%)]	Loss: 176.137405 | Elapsed: 12.43s
01/28/2023 01:07:47 PM  [*] Sat Jan 28 13:07:47 2023:    4    | Tr.loss: 177.894459 | Elapsed:  114.04  s
01/28/2023 01:07:47 PM  [*] Started epoch: 5
01/28/2023 01:07:47 PM  [*] Sat Jan 28 13:07:47 2023: Train Epoch: 5 [  0  /57094 (0 %)]	Loss: 178.174255 | Elapsed: 0.13s
01/28/2023 01:08:00 PM  [*] Sat Jan 28 13:08:00 2023: Train Epoch: 5 [6400 /57094 (11%)]	Loss: 145.996307 | Elapsed: 12.68s
01/28/2023 01:08:12 PM  [*] Sat Jan 28 13:08:12 2023: Train Epoch: 5 [12800/57094 (22%)]	Loss: 155.351944 | Elapsed: 12.71s
01/28/2023 01:08:25 PM  [*] Sat Jan 28 13:08:25 2023: Train Epoch: 5 [19200/57094 (34%)]	Loss: 171.691574 | Elapsed: 12.45s
01/28/2023 01:08:37 PM  [*] Sat Jan 28 13:08:37 2023: Train Epoch: 5 [25600/57094 (45%)]	Loss: 172.934296 | Elapsed: 12.32s
01/28/2023 01:08:50 PM  [*] Sat Jan 28 13:08:50 2023: Train Epoch: 5 [32000/57094 (56%)]	Loss: 177.779968 | Elapsed: 12.49s
01/28/2023 01:09:02 PM  [*] Sat Jan 28 13:09:02 2023: Train Epoch: 5 [38400/57094 (67%)]	Loss: 183.372620 | Elapsed: 12.87s
01/28/2023 01:09:15 PM  [*] Sat Jan 28 13:09:15 2023: Train Epoch: 5 [44800/57094 (78%)]	Loss: 177.692734 | Elapsed: 12.99s
01/28/2023 01:09:28 PM  [*] Sat Jan 28 13:09:28 2023: Train Epoch: 5 [51200/57094 (90%)]	Loss: 175.043976 | Elapsed: 12.84s
01/28/2023 01:09:41 PM  [*] Sat Jan 28 13:09:41 2023:    5    | Tr.loss: 176.190012 | Elapsed:  114.53  s
01/28/2023 01:09:41 PM  [*] Started epoch: 6
01/28/2023 01:09:42 PM  [*] Sat Jan 28 13:09:42 2023: Train Epoch: 6 [  0  /57094 (0 %)]	Loss: 183.654938 | Elapsed: 0.13s
01/28/2023 01:09:54 PM  [*] Sat Jan 28 13:09:54 2023: Train Epoch: 6 [6400 /57094 (11%)]	Loss: 192.278854 | Elapsed: 12.49s
01/28/2023 01:10:07 PM  [*] Sat Jan 28 13:10:07 2023: Train Epoch: 6 [12800/57094 (22%)]	Loss: 162.259613 | Elapsed: 12.60s
01/28/2023 01:10:19 PM  [*] Sat Jan 28 13:10:19 2023: Train Epoch: 6 [19200/57094 (34%)]	Loss: 163.700714 | Elapsed: 12.39s
01/28/2023 01:10:31 PM  [*] Sat Jan 28 13:10:31 2023: Train Epoch: 6 [25600/57094 (45%)]	Loss: 183.329147 | Elapsed: 12.48s
01/28/2023 01:10:44 PM  [*] Sat Jan 28 13:10:44 2023: Train Epoch: 6 [32000/57094 (56%)]	Loss: 181.672394 | Elapsed: 12.34s
01/28/2023 01:10:48 PM [!] Learning rate: 2.5e-05
01/28/2023 01:10:56 PM  [*] Sat Jan 28 13:10:56 2023: Train Epoch: 6 [38400/57094 (67%)]	Loss: 185.500092 | Elapsed: 12.23s
01/28/2023 01:11:08 PM  [*] Sat Jan 28 13:11:08 2023: Train Epoch: 6 [44800/57094 (78%)]	Loss: 160.591705 | Elapsed: 12.44s
01/28/2023 01:11:21 PM  [*] Sat Jan 28 13:11:21 2023: Train Epoch: 6 [51200/57094 (90%)]	Loss: 177.317566 | Elapsed: 12.34s
01/28/2023 01:11:34 PM  [*] Sat Jan 28 13:11:34 2023:    6    | Tr.loss: 174.951723 | Elapsed:  112.82  s
01/28/2023 01:11:34 PM  [*] Started epoch: 7
01/28/2023 01:11:34 PM  [*] Sat Jan 28 13:11:34 2023: Train Epoch: 7 [  0  /57094 (0 %)]	Loss: 180.765594 | Elapsed: 0.14s
01/28/2023 01:11:47 PM  [*] Sat Jan 28 13:11:47 2023: Train Epoch: 7 [6400 /57094 (11%)]	Loss: 177.930145 | Elapsed: 12.61s
01/28/2023 01:11:59 PM  [*] Sat Jan 28 13:11:59 2023: Train Epoch: 7 [12800/57094 (22%)]	Loss: 150.035522 | Elapsed: 12.25s
01/28/2023 01:12:12 PM  [*] Sat Jan 28 13:12:12 2023: Train Epoch: 7 [19200/57094 (34%)]	Loss: 169.511459 | Elapsed: 12.44s
01/28/2023 01:12:25 PM  [*] Sat Jan 28 13:12:25 2023: Train Epoch: 7 [25600/57094 (45%)]	Loss: 162.930725 | Elapsed: 13.04s
01/28/2023 01:12:37 PM  [*] Sat Jan 28 13:12:37 2023: Train Epoch: 7 [32000/57094 (56%)]	Loss: 171.686096 | Elapsed: 12.44s
01/28/2023 01:12:50 PM  [*] Sat Jan 28 13:12:50 2023: Train Epoch: 7 [38400/57094 (67%)]	Loss: 180.088608 | Elapsed: 12.52s
01/28/2023 01:13:02 PM  [*] Sat Jan 28 13:13:02 2023: Train Epoch: 7 [44800/57094 (78%)]	Loss: 171.171677 | Elapsed: 12.39s
01/28/2023 01:13:15 PM  [*] Sat Jan 28 13:13:15 2023: Train Epoch: 7 [51200/57094 (90%)]	Loss: 200.763489 | Elapsed: 12.60s
01/28/2023 01:13:28 PM  [*] Sat Jan 28 13:13:28 2023:    7    | Tr.loss: 173.893721 | Elapsed:  114.06  s
01/28/2023 01:13:28 PM  [*] Started epoch: 8
01/28/2023 01:13:28 PM  [*] Sat Jan 28 13:13:28 2023: Train Epoch: 8 [  0  /57094 (0 %)]	Loss: 185.127441 | Elapsed: 0.25s
01/28/2023 01:13:41 PM  [*] Sat Jan 28 13:13:41 2023: Train Epoch: 8 [6400 /57094 (11%)]	Loss: 179.357574 | Elapsed: 12.77s
01/28/2023 01:13:54 PM  [*] Sat Jan 28 13:13:54 2023: Train Epoch: 8 [12800/57094 (22%)]	Loss: 183.997528 | Elapsed: 12.63s
01/28/2023 01:14:07 PM  [*] Sat Jan 28 13:14:07 2023: Train Epoch: 8 [19200/57094 (34%)]	Loss: 170.855225 | Elapsed: 12.84s
01/28/2023 01:14:19 PM  [*] Sat Jan 28 13:14:19 2023: Train Epoch: 8 [25600/57094 (45%)]	Loss: 149.005142 | Elapsed: 12.57s
01/28/2023 01:14:32 PM  [*] Sat Jan 28 13:14:32 2023: Train Epoch: 8 [32000/57094 (56%)]	Loss: 173.321671 | Elapsed: 12.84s
01/28/2023 01:14:45 PM  [*] Sat Jan 28 13:14:45 2023: Train Epoch: 8 [38400/57094 (67%)]	Loss: 172.839874 | Elapsed: 12.77s
01/28/2023 01:14:58 PM  [*] Sat Jan 28 13:14:58 2023: Train Epoch: 8 [44800/57094 (78%)]	Loss: 168.534393 | Elapsed: 12.69s
01/28/2023 01:15:10 PM  [*] Sat Jan 28 13:15:10 2023: Train Epoch: 8 [51200/57094 (90%)]	Loss: 164.556152 | Elapsed: 12.88s
01/28/2023 01:15:25 PM  [*] Sat Jan 28 13:15:25 2023:    8    | Tr.loss: 173.595918 | Elapsed:  116.27  s
01/28/2023 01:15:25 PM  [*] Started epoch: 9
01/28/2023 01:15:25 PM  [*] Sat Jan 28 13:15:25 2023: Train Epoch: 9 [  0  /57094 (0 %)]	Loss: 163.740524 | Elapsed: 0.22s
01/28/2023 01:15:38 PM  [*] Sat Jan 28 13:15:38 2023: Train Epoch: 9 [6400 /57094 (11%)]	Loss: 165.011932 | Elapsed: 12.87s
01/28/2023 01:15:50 PM  [*] Sat Jan 28 13:15:50 2023: Train Epoch: 9 [12800/57094 (22%)]	Loss: 181.012268 | Elapsed: 12.48s
01/28/2023 01:16:03 PM  [*] Sat Jan 28 13:16:03 2023: Train Epoch: 9 [19200/57094 (34%)]	Loss: 164.138000 | Elapsed: 12.66s
01/28/2023 01:16:15 PM  [*] Sat Jan 28 13:16:15 2023: Train Epoch: 9 [25600/57094 (45%)]	Loss: 160.067184 | Elapsed: 12.66s
01/28/2023 01:16:28 PM  [*] Sat Jan 28 13:16:28 2023: Train Epoch: 9 [32000/57094 (56%)]	Loss: 194.450546 | Elapsed: 12.59s
01/28/2023 01:16:41 PM  [*] Sat Jan 28 13:16:41 2023: Train Epoch: 9 [38400/57094 (67%)]	Loss: 159.291794 | Elapsed: 12.51s
01/28/2023 01:16:54 PM  [*] Sat Jan 28 13:16:54 2023: Train Epoch: 9 [44800/57094 (78%)]	Loss: 173.419128 | Elapsed: 13.40s
01/28/2023 01:17:07 PM  [*] Sat Jan 28 13:17:07 2023: Train Epoch: 9 [51200/57094 (90%)]	Loss: 176.166824 | Elapsed: 12.94s
01/28/2023 01:17:20 PM  [*] Sat Jan 28 13:17:20 2023:    9    | Tr.loss: 173.437207 | Elapsed:  115.85  s
01/28/2023 01:17:20 PM  [*] Started epoch: 10
01/28/2023 01:17:21 PM  [*] Sat Jan 28 13:17:21 2023: Train Epoch: 10 [  0  /57094 (0 %)]	Loss: 152.341354 | Elapsed: 0.21s
01/28/2023 01:17:34 PM  [*] Sat Jan 28 13:17:34 2023: Train Epoch: 10 [6400 /57094 (11%)]	Loss: 162.032867 | Elapsed: 13.22s
01/28/2023 01:17:47 PM  [*] Sat Jan 28 13:17:47 2023: Train Epoch: 10 [12800/57094 (22%)]	Loss: 187.091873 | Elapsed: 12.76s
01/28/2023 01:17:59 PM  [*] Sat Jan 28 13:17:59 2023: Train Epoch: 10 [19200/57094 (34%)]	Loss: 182.033234 | Elapsed: 12.73s
01/28/2023 01:18:12 PM  [*] Sat Jan 28 13:18:12 2023: Train Epoch: 10 [25600/57094 (45%)]	Loss: 159.684219 | Elapsed: 12.56s
01/28/2023 01:18:25 PM  [*] Sat Jan 28 13:18:25 2023: Train Epoch: 10 [32000/57094 (56%)]	Loss: 179.263962 | Elapsed: 12.72s
01/28/2023 01:18:37 PM  [*] Sat Jan 28 13:18:37 2023: Train Epoch: 10 [38400/57094 (67%)]	Loss: 166.557724 | Elapsed: 12.65s
01/28/2023 01:18:50 PM  [*] Sat Jan 28 13:18:50 2023: Train Epoch: 10 [44800/57094 (78%)]	Loss: 171.324783 | Elapsed: 12.72s
01/28/2023 01:19:03 PM  [*] Sat Jan 28 13:19:03 2023: Train Epoch: 10 [51200/57094 (90%)]	Loss: 180.680298 | Elapsed: 12.60s
01/28/2023 01:19:17 PM  [*] Sat Jan 28 13:19:17 2023:   10    | Tr.loss: 173.279781 | Elapsed:  116.23  s
01/28/2023 01:19:17 PM [!] Sat Jan 28 13:19:17 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674908357-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674908357-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674908357-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\preTraining\training_files\1674908357-auc.npy
01/28/2023 01:19:18 PM  [!] Training pretrained model on downstream task...
01/28/2023 01:19:18 PM  [*] Started epoch: 1
01/28/2023 01:19:19 PM  [*] Sat Jan 28 13:19:19 2023: Train Epoch: 1 [  0  /19032 (0 %)]	Loss: 3.105134 | Elapsed: 0.35s | FPR 0.0003 -> TPR 0.0250 & F1 0.0488
01/28/2023 01:19:28 PM  [*] Sat Jan 28 13:19:28 2023: Train Epoch: 1 [6400 /19032 (34%)]	Loss: 0.357684 | Elapsed: 9.31s | FPR 0.0003 -> TPR 0.4429 & F1 0.6139
01/28/2023 01:19:37 PM  [*] Sat Jan 28 13:19:37 2023: Train Epoch: 1 [12800/19032 (67%)]	Loss: 0.198856 | Elapsed: 9.20s | FPR 0.0003 -> TPR 0.7750 & F1 0.8732
01/28/2023 01:19:47 PM  [*] Sat Jan 28 13:19:47 2023:    1    | Tr.loss: 0.412970 | Elapsed:   28.28  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.8759
01/28/2023 01:19:47 PM  [*] Started epoch: 2
01/28/2023 01:19:47 PM  [*] Sat Jan 28 13:19:47 2023: Train Epoch: 2 [  0  /19032 (0 %)]	Loss: 0.318404 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000
01/28/2023 01:19:56 PM  [*] Sat Jan 28 13:19:56 2023: Train Epoch: 2 [6400 /19032 (34%)]	Loss: 0.222650 | Elapsed: 9.27s | FPR 0.0003 -> TPR 0.6901 & F1 0.8167
01/28/2023 01:20:05 PM  [*] Sat Jan 28 13:20:05 2023: Train Epoch: 2 [12800/19032 (67%)]	Loss: 0.291238 | Elapsed: 9.23s | FPR 0.0003 -> TPR 0.8235 & F1 0.9032
01/28/2023 01:20:15 PM  [*] Sat Jan 28 13:20:15 2023:    2    | Tr.loss: 0.261594 | Elapsed:   28.04  s | FPR 0.0003 -> TPR: 0.35 & F1: 0.52 | AUC: 0.9524
01/28/2023 01:20:15 PM  [*] Started epoch: 3
01/28/2023 01:20:15 PM  [*] Sat Jan 28 13:20:15 2023: Train Epoch: 3 [  0  /19032 (0 %)]	Loss: 0.216725 | Elapsed: 0.09s | FPR 0.0003 -> TPR 0.8049 & F1 0.8919
01/28/2023 01:20:24 PM  [*] Sat Jan 28 13:20:24 2023: Train Epoch: 3 [6400 /19032 (34%)]	Loss: 0.219454 | Elapsed: 9.23s | FPR 0.0003 -> TPR 0.8056 & F1 0.8923
01/28/2023 01:20:33 PM  [*] Sat Jan 28 13:20:33 2023: Train Epoch: 3 [12800/19032 (67%)]	Loss: 0.181471 | Elapsed: 9.19s | FPR 0.0003 -> TPR 0.7143 & F1 0.8333
01/28/2023 01:20:43 PM  [*] Sat Jan 28 13:20:43 2023:    3    | Tr.loss: 0.191624 | Elapsed:   27.88  s | FPR 0.0003 -> TPR: 0.50 & F1: 0.67 | AUC: 0.9746
01/28/2023 01:20:43 PM [!] Sat Jan 28 13:20:43 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674908443-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674908443-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674908443-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674908443-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674908443-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_pretrained\training_files\1674908443-trainTPRs.npy
01/28/2023 01:20:43 PM  [!] Training non_pretrained model on downstream task...
01/28/2023 01:20:44 PM  [*] Started epoch: 1
01/28/2023 01:20:44 PM  [*] Sat Jan 28 13:20:44 2023: Train Epoch: 1 [  0  /19032 (0 %)]	Loss: 1.561262 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0571 & F1 0.1081
01/28/2023 01:20:50 PM  [*] Sat Jan 28 13:20:50 2023: Train Epoch: 1 [6400 /19032 (34%)]	Loss: 0.461266 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.2817 & F1 0.4396
01/28/2023 01:20:56 PM  [*] Sat Jan 28 13:20:56 2023: Train Epoch: 1 [12800/19032 (67%)]	Loss: 0.406053 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.4571 & F1 0.6275
01/28/2023 01:21:03 PM  [*] Sat Jan 28 13:21:03 2023:    1    | Tr.loss: 0.431671 | Elapsed:   19.24  s | FPR 0.0003 -> TPR: 0.02 & F1: 0.04 | AUC: 0.8543
01/28/2023 01:21:03 PM  [*] Started epoch: 2
01/28/2023 01:21:03 PM  [*] Sat Jan 28 13:21:03 2023: Train Epoch: 2 [  0  /19032 (0 %)]	Loss: 0.207832 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.8571 & F1 0.9231
01/28/2023 01:21:09 PM  [*] Sat Jan 28 13:21:09 2023: Train Epoch: 2 [6400 /19032 (34%)]	Loss: 0.290567 | Elapsed: 6.31s | FPR 0.0003 -> TPR 0.5738 & F1 0.7292
01/28/2023 01:21:16 PM  [*] Sat Jan 28 13:21:16 2023: Train Epoch: 2 [12800/19032 (67%)]	Loss: 0.184354 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.7969 & F1 0.8870
01/28/2023 01:21:22 PM  [*] Sat Jan 28 13:21:22 2023:    2    | Tr.loss: 0.266267 | Elapsed:   19.39  s | FPR 0.0003 -> TPR: 0.32 & F1: 0.49 | AUC: 0.9486
01/28/2023 01:21:22 PM  [*] Started epoch: 3
01/28/2023 01:21:22 PM  [*] Sat Jan 28 13:21:22 2023: Train Epoch: 3 [  0  /19032 (0 %)]	Loss: 0.220716 | Elapsed: 0.06s | FPR 0.0003 -> TPR 0.6957 & F1 0.8205
01/28/2023 01:21:29 PM  [*] Sat Jan 28 13:21:29 2023: Train Epoch: 3 [6400 /19032 (34%)]	Loss: 0.225408 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.7391 & F1 0.8500
01/28/2023 01:21:35 PM  [*] Sat Jan 28 13:21:35 2023: Train Epoch: 3 [12800/19032 (67%)]	Loss: 0.201059 | Elapsed: 6.35s | FPR 0.0003 -> TPR 0.6833 & F1 0.8119
01/28/2023 01:21:42 PM  [*] Sat Jan 28 13:21:42 2023:    3    | Tr.loss: 0.192973 | Elapsed:   19.37  s | FPR 0.0003 -> TPR: 0.38 & F1: 0.55 | AUC: 0.9740
01/28/2023 01:21:42 PM [!] Sat Jan 28 13:21:42 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674908502-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674908502-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674908502-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674908502-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674908502-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_non_pretrained\training_files\1674908502-trainTPRs.npy
01/28/2023 01:21:42 PM  [!] Training full_data model on downstream task...
01/28/2023 01:21:43 PM  [*] Started epoch: 1
01/28/2023 01:21:43 PM  [*] Sat Jan 28 13:21:43 2023: Train Epoch: 1 [  0  /76126 (0 %)]	Loss: 1.826692 | Elapsed: 0.07s | FPR 0.0003 -> TPR 0.0870 & F1 0.1600
01/28/2023 01:21:49 PM  [*] Sat Jan 28 13:21:49 2023: Train Epoch: 1 [6400 /76126 (8 %)]	Loss: 0.580017 | Elapsed: 6.35s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000
01/28/2023 01:21:55 PM  [*] Sat Jan 28 13:21:55 2023: Train Epoch: 1 [12800/76126 (17%)]	Loss: 0.384901 | Elapsed: 6.43s | FPR 0.0003 -> TPR 0.2571 & F1 0.4091
01/28/2023 01:22:02 PM  [*] Sat Jan 28 13:22:02 2023: Train Epoch: 1 [19200/76126 (25%)]	Loss: 0.400832 | Elapsed: 6.37s | FPR 0.0003 -> TPR 0.5156 & F1 0.6804
01/28/2023 01:22:08 PM  [*] Sat Jan 28 13:22:08 2023: Train Epoch: 1 [25600/76126 (34%)]	Loss: 0.342923 | Elapsed: 6.37s | FPR 0.0003 -> TPR 0.5763 & F1 0.7312
01/28/2023 01:22:15 PM  [*] Sat Jan 28 13:22:15 2023: Train Epoch: 1 [32000/76126 (42%)]	Loss: 0.211404 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.6234 & F1 0.7680
01/28/2023 01:22:21 PM  [*] Sat Jan 28 13:22:21 2023: Train Epoch: 1 [38400/76126 (50%)]	Loss: 0.286328 | Elapsed: 6.33s | FPR 0.0003 -> TPR 0.5333 & F1 0.6957
01/28/2023 01:22:27 PM  [*] Sat Jan 28 13:22:27 2023: Train Epoch: 1 [44800/76126 (59%)]	Loss: 0.309169 | Elapsed: 6.35s | FPR 0.0003 -> TPR 0.5538 & F1 0.7129
01/28/2023 01:22:34 PM  [*] Sat Jan 28 13:22:34 2023: Train Epoch: 1 [51200/76126 (67%)]	Loss: 0.131562 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.9062 & F1 0.9508
01/28/2023 01:22:40 PM  [*] Sat Jan 28 13:22:40 2023: Train Epoch: 1 [57600/76126 (76%)]	Loss: 0.212485 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.6471 & F1 0.7857
01/28/2023 01:22:46 PM [!] Learning rate: 2.5e-05
01/28/2023 01:22:46 PM  [*] Sat Jan 28 13:22:46 2023: Train Epoch: 1 [64000/76126 (84%)]	Loss: 0.238882 | Elapsed: 6.37s | FPR 0.0003 -> TPR 0.6923 & F1 0.8182
01/28/2023 01:22:53 PM  [*] Sat Jan 28 13:22:53 2023: Train Epoch: 1 [70400/76126 (92%)]	Loss: 0.215938 | Elapsed: 6.28s | FPR 0.0003 -> TPR 0.5873 & F1 0.7400
01/28/2023 01:23:00 PM  [*] Sat Jan 28 13:23:00 2023:    1    | Tr.loss: 0.297178 | Elapsed:   77.49  s | FPR 0.0003 -> TPR: 0.14 & F1: 0.24 | AUC: 0.9343
01/28/2023 01:23:00 PM  [*] Started epoch: 2
01/28/2023 01:23:00 PM  [*] Sat Jan 28 13:23:00 2023: Train Epoch: 2 [  0  /76126 (0 %)]	Loss: 0.190909 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.5682 & F1 0.7246
01/28/2023 01:23:07 PM  [*] Sat Jan 28 13:23:07 2023: Train Epoch: 2 [6400 /76126 (8 %)]	Loss: 0.141217 | Elapsed: 6.40s | FPR 0.0003 -> TPR 0.9275 & F1 0.9624
01/28/2023 01:23:13 PM  [*] Sat Jan 28 13:23:13 2023: Train Epoch: 2 [12800/76126 (17%)]	Loss: 0.202580 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.7941 & F1 0.8852
01/28/2023 01:23:19 PM  [*] Sat Jan 28 13:23:19 2023: Train Epoch: 2 [19200/76126 (25%)]	Loss: 0.136316 | Elapsed: 6.46s | FPR 0.0003 -> TPR 0.9403 & F1 0.9692
01/28/2023 01:23:26 PM  [*] Sat Jan 28 13:23:26 2023: Train Epoch: 2 [25600/76126 (34%)]	Loss: 0.237362 | Elapsed: 6.42s | FPR 0.0003 -> TPR 0.5758 & F1 0.7308
01/28/2023 01:23:32 PM  [*] Sat Jan 28 13:23:32 2023: Train Epoch: 2 [32000/76126 (42%)]	Loss: 0.087707 | Elapsed: 6.43s | FPR 0.0003 -> TPR 0.9385 & F1 0.9683
01/28/2023 01:23:39 PM  [*] Sat Jan 28 13:23:39 2023: Train Epoch: 2 [38400/76126 (50%)]	Loss: 0.202817 | Elapsed: 6.41s | FPR 0.0003 -> TPR 0.7465 & F1 0.8548
01/28/2023 01:23:45 PM  [*] Sat Jan 28 13:23:45 2023: Train Epoch: 2 [44800/76126 (59%)]	Loss: 0.214874 | Elapsed: 6.29s | FPR 0.0003 -> TPR 0.7761 & F1 0.8739
01/28/2023 01:23:51 PM  [*] Sat Jan 28 13:23:51 2023: Train Epoch: 2 [51200/76126 (67%)]	Loss: 0.122791 | Elapsed: 6.27s | FPR 0.0003 -> TPR 0.9189 & F1 0.9577
01/28/2023 01:23:52 PM [!] Learning rate: 2.5e-06
01/28/2023 01:23:58 PM  [*] Sat Jan 28 13:23:58 2023: Train Epoch: 2 [57600/76126 (76%)]	Loss: 0.174272 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.7568 & F1 0.8615
01/28/2023 01:24:04 PM  [*] Sat Jan 28 13:24:04 2023: Train Epoch: 2 [64000/76126 (84%)]	Loss: 0.109312 | Elapsed: 6.40s | FPR 0.0003 -> TPR 0.6769 & F1 0.8073
01/28/2023 01:24:10 PM  [*] Sat Jan 28 13:24:10 2023: Train Epoch: 2 [70400/76126 (92%)]	Loss: 0.186745 | Elapsed: 6.39s | FPR 0.0003 -> TPR 0.7632 & F1 0.8657
01/28/2023 01:24:18 PM  [*] Sat Jan 28 13:24:18 2023:    2    | Tr.loss: 0.168473 | Elapsed:   78.02  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9803
01/28/2023 01:24:18 PM  [*] Started epoch: 3
01/28/2023 01:24:18 PM  [*] Sat Jan 28 13:24:18 2023: Train Epoch: 3 [  0  /76126 (0 %)]	Loss: 0.136714 | Elapsed: 0.16s | FPR 0.0003 -> TPR 0.8750 & F1 0.9333
01/28/2023 01:24:25 PM  [*] Sat Jan 28 13:24:25 2023: Train Epoch: 3 [6400 /76126 (8 %)]	Loss: 0.103838 | Elapsed: 6.44s | FPR 0.0003 -> TPR 0.8169 & F1 0.8992
01/28/2023 01:24:31 PM  [*] Sat Jan 28 13:24:31 2023: Train Epoch: 3 [12800/76126 (17%)]	Loss: 0.254647 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.5352 & F1 0.6972
01/28/2023 01:24:37 PM  [*] Sat Jan 28 13:24:37 2023: Train Epoch: 3 [19200/76126 (25%)]	Loss: 0.087285 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.8657 & F1 0.9280
01/28/2023 01:24:44 PM  [*] Sat Jan 28 13:24:44 2023: Train Epoch: 3 [25600/76126 (34%)]	Loss: 0.110746 | Elapsed: 6.37s | FPR 0.0003 -> TPR 0.9697 & F1 0.9846
01/28/2023 01:24:50 PM  [*] Sat Jan 28 13:24:50 2023: Train Epoch: 3 [32000/76126 (42%)]	Loss: 0.170073 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.7391 & F1 0.8500
01/28/2023 01:24:57 PM  [*] Sat Jan 28 13:24:57 2023: Train Epoch: 3 [38400/76126 (50%)]	Loss: 0.142210 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.8154 & F1 0.8983
01/28/2023 01:24:58 PM [!] Learning rate: 2.5000000000000004e-07
01/28/2023 01:25:03 PM  [*] Sat Jan 28 13:25:03 2023: Train Epoch: 3 [44800/76126 (59%)]	Loss: 0.073733 | Elapsed: 6.36s | FPR 0.0003 -> TPR 0.8472 & F1 0.9173
01/28/2023 01:25:09 PM  [*] Sat Jan 28 13:25:09 2023: Train Epoch: 3 [51200/76126 (67%)]	Loss: 0.206113 | Elapsed: 6.42s | FPR 0.0003 -> TPR 0.5538 & F1 0.7129
01/28/2023 01:25:16 PM  [*] Sat Jan 28 13:25:16 2023: Train Epoch: 3 [57600/76126 (76%)]	Loss: 0.136743 | Elapsed: 6.34s | FPR 0.0003 -> TPR 0.7183 & F1 0.8361
01/28/2023 01:25:22 PM  [*] Sat Jan 28 13:25:22 2023: Train Epoch: 3 [64000/76126 (84%)]	Loss: 0.145058 | Elapsed: 6.30s | FPR 0.0003 -> TPR 0.8983 & F1 0.9464
01/28/2023 01:25:28 PM  [*] Sat Jan 28 13:25:28 2023: Train Epoch: 3 [70400/76126 (92%)]	Loss: 0.081825 | Elapsed: 6.31s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000
01/28/2023 01:25:36 PM  [*] Sat Jan 28 13:25:36 2023:    3    | Tr.loss: 0.160545 | Elapsed:   77.73  s | FPR 0.0003 -> TPR: 0.45 & F1: 0.62 | AUC: 0.9821
01/28/2023 01:25:36 PM [!] Sat Jan 28 13:25:36 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674908736-model.torch
		train time: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674908736-trainTime.npy
		train losses: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674908736-trainLosses.npy
		train AUC: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674908736-auc.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674908736-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107\downstreamTask_full_data\training_files\1674908736-trainTPRs.npy
01/28/2023 01:25:36 PM  [*] Evaluating pretrained model on test set...
01/28/2023 01:25:41 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0696 | F1: 0.1301
01/28/2023 01:25:41 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1046 | F1: 0.1894
01/28/2023 01:25:41 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2555 | F1: 0.4067
01/28/2023 01:25:41 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3523 | F1: 0.5201
01/28/2023 01:25:41 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4844 | F1: 0.6490
01/28/2023 01:25:41 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5753 | F1: 0.7188
01/28/2023 01:25:41 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7273 | F1: 0.8026
01/28/2023 01:25:41 PM  [*] Evaluating non_pretrained model on test set...
01/28/2023 01:25:47 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0772 | F1: 0.1434
01/28/2023 01:25:47 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.1603 | F1: 0.2762
01/28/2023 01:25:47 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.2114 | F1: 0.3488
01/28/2023 01:25:47 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.2544 | F1: 0.4048
01/28/2023 01:25:47 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.3077 | F1: 0.4675
01/28/2023 01:25:47 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.4465 | F1: 0.6066
01/28/2023 01:25:47 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7276 | F1: 0.8029
01/28/2023 01:25:47 PM  [*] Evaluating full_data model on test set...
01/28/2023 01:25:52 PM 	[!] Test set scores at FPR: 0.0001 --> TPR: 0.0915 | F1: 0.1676
01/28/2023 01:25:52 PM 	[!] Test set scores at FPR: 0.0003 --> TPR: 0.2508 | F1: 0.4009
01/28/2023 01:25:52 PM 	[!] Test set scores at FPR:  0.001 --> TPR: 0.3482 | F1: 0.5162
01/28/2023 01:25:52 PM 	[!] Test set scores at FPR:  0.003 --> TPR: 0.3871 | F1: 0.5571
01/28/2023 01:25:52 PM 	[!] Test set scores at FPR:   0.01 --> TPR: 0.4348 | F1: 0.6025
01/28/2023 01:25:52 PM 	[!] Test set scores at FPR:   0.03 --> TPR: 0.5506 | F1: 0.6987
01/28/2023 01:25:52 PM 	[!] Test set scores at FPR:    0.1 --> TPR: 0.7994 | F1: 0.8485
01/28/2023 01:25:52 PM  [!] Finished pre-training evaluation over 3 splits! Saved metrics to:
	C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\MaskedLanguageModeling\uSize_0.75_1674904107/metrics_MaskedLanguageModel_nSplits_3_limit_None.json
01/28/2023 01:25:52 PM  [!] Starting Masked Language Model evaluation over 3 splits!
01/28/2023 01:25:52 PM  [!] Loaded data and vocab. X train size: (76126, 512), X test size: (17407, 512), vocab size: 50002
01/28/2023 01:25:52 PM  [!] Initiating Self-Supervised Learning Pretraining
     Language Modeling MaskedLanguageModel
     Model TransformerEncoderLM with config:
	{'vocabSize': 50002, 'maxLen': 512, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'dropout': 0.3}

01/28/2023 01:25:52 PM  [!] Running pre-training split 1/3
01/28/2023 01:25:56 PM  [!] Pre-training model...
01/28/2023 01:25:57 PM  [*] Masking sequences...
