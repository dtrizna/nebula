01/13/2023 08:52:19 PM  [!] Using device: cuda:0 | Dataset size: 76126
01/13/2023 08:52:19 PM  [!] Epochs per fold: 3 | Model config: {'vocabSize': 50000, 'maxLen': 2048, 'dim': 64, 'heads': 4, 'depth': 4, 'meanOverSequence': True, 'classifierDropout': 0.3, 'hiddenNeurons': [256, 64]}
01/13/2023 08:52:20 PM  [!] Fold 1/2 | Train set size: 38063, Validation set size: 38063
01/13/2023 08:52:20 PM  [*] Started epoch: 1
01/13/2023 08:52:24 PM  [*] Fri Jan 13 20:52:24 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.694206 | FPR 0.001 -- TPR 0.0000 | F1 0.0000 | Elapsed: 4.03s
01/13/2023 08:55:00 PM  [*] Fri Jan 13 20:55:00 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.560657 | FPR 0.001 -- TPR 0.2341 | F1 0.3302 | Elapsed: 155.66s
01/13/2023 08:57:29 PM  [*] Fri Jan 13 20:57:29 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.287243 | FPR 0.001 -- TPR 0.4163 | F1 0.5123 | Elapsed: 149.20s
01/13/2023 08:59:58 PM  [*] Fri Jan 13 20:59:58 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.325177 | FPR 0.001 -- TPR 0.4932 | F1 0.5910 | Elapsed: 148.90s
01/13/2023 09:02:27 PM  [*] Fri Jan 13 21:02:27 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.303112 | FPR 0.001 -- TPR 0.5503 | F1 0.6465 | Elapsed: 149.04s
01/13/2023 09:04:56 PM  [*] Fri Jan 13 21:04:56 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.318980 | FPR 0.001 -- TPR 0.5821 | F1 0.6784 | Elapsed: 149.24s
01/13/2023 09:07:25 PM  [*] Fri Jan 13 21:07:25 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.161739 | FPR 0.001 -- TPR 0.6033 | F1 0.7005 | Elapsed: 149.29s
01/13/2023 09:09:54 PM  [*] Fri Jan 13 21:09:54 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.195548 | FPR 0.001 -- TPR 0.6220 | F1 0.7178 | Elapsed: 149.11s
01/13/2023 09:12:24 PM  [*] Fri Jan 13 21:12:24 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.395758 | FPR 0.001 -- TPR 0.6393 | F1 0.7336 | Elapsed: 149.20s
01/13/2023 09:14:53 PM  [*] Fri Jan 13 21:14:53 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.565411 | FPR 0.001 -- TPR 0.6555 | F1 0.7479 | Elapsed: 149.12s
01/13/2023 09:17:19 PM [!] Learning rate: 2.5e-05
01/13/2023 09:17:22 PM  [*] Fri Jan 13 21:17:22 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.333486 | FPR 0.001 -- TPR 0.6652 | F1 0.7569 | Elapsed: 149.07s
01/13/2023 09:19:51 PM  [*] Fri Jan 13 21:19:51 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.204491 | FPR 0.001 -- TPR 0.6739 | F1 0.7636 | Elapsed: 149.05s
01/13/2023 09:22:20 PM  [*] Fri Jan 13 21:22:20 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.173105 | FPR 0.001 -- TPR 0.6849 | F1 0.7732 | Elapsed: 148.98s
01/13/2023 09:24:49 PM  [*] Fri Jan 13 21:24:49 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.592496 | FPR 0.001 -- TPR 0.6924 | F1 0.7802 | Elapsed: 149.06s
01/13/2023 09:27:18 PM  [*] Fri Jan 13 21:27:18 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.214120 | FPR 0.001 -- TPR 0.6992 | F1 0.7856 | Elapsed: 149.24s
01/13/2023 09:29:47 PM  [*] Fri Jan 13 21:29:47 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.371718 | FPR 0.001 -- TPR 0.7086 | F1 0.7931 | Elapsed: 149.28s
01/13/2023 09:32:16 PM  [*] Fri Jan 13 21:32:16 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.305374 | FPR 0.001 -- TPR 0.7143 | F1 0.7981 | Elapsed: 148.83s
01/13/2023 09:34:45 PM  [*] Fri Jan 13 21:34:45 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.279039 | FPR 0.001 -- TPR 0.7179 | F1 0.8016 | Elapsed: 148.74s
01/13/2023 09:37:14 PM  [*] Fri Jan 13 21:37:14 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.207097 | FPR 0.001 -- TPR 0.7250 | F1 0.8074 | Elapsed: 148.96s
01/13/2023 09:39:43 PM  [*] Fri Jan 13 21:39:43 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.583645 | FPR 0.001 -- TPR 0.7273 | F1 0.8092 | Elapsed: 149.11s
01/13/2023 09:42:09 PM [!] Learning rate: 2.5e-06
01/13/2023 09:42:12 PM  [*] Fri Jan 13 21:42:12 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.133539 | FPR 0.001 -- TPR 0.7314 | F1 0.8126 | Elapsed: 149.26s
01/13/2023 09:44:42 PM  [*] Fri Jan 13 21:44:42 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.193254 | FPR 0.001 -- TPR 0.7343 | F1 0.8153 | Elapsed: 149.38s
01/13/2023 09:47:11 PM  [*] Fri Jan 13 21:47:11 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.151010 | FPR 0.001 -- TPR 0.7371 | F1 0.8178 | Elapsed: 149.24s
01/13/2023 09:49:40 PM  [*] Fri Jan 13 21:49:40 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.241759 | FPR 0.001 -- TPR 0.7398 | F1 0.8202 | Elapsed: 148.93s
01/13/2023 09:51:36 PM  [*] Fri Jan 13 21:51:36 2023:    1    | Tr.loss: 0.339571 | FPR 0.001 -- TPR: 0.74 |  F1: 0.82 | Elapsed:  3555.89 s
01/13/2023 09:51:36 PM  [*] Started epoch: 2
01/13/2023 09:51:37 PM  [*] Fri Jan 13 21:51:37 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.139083 | FPR 0.001 -- TPR 1.0000 | F1 1.0000 | Elapsed: 1.49s
01/13/2023 09:54:08 PM  [*] Fri Jan 13 21:54:08 2023: Train Epoch: 2 [1600 /38063 (4 %)]	Loss: 0.192992 | FPR 0.001 -- TPR 0.8033 | F1 0.8762 | Elapsed: 150.51s
01/13/2023 09:56:37 PM  [*] Fri Jan 13 21:56:37 2023: Train Epoch: 2 [3200 /38063 (8 %)]	Loss: 0.277674 | FPR 0.001 -- TPR 0.8223 | F1 0.8882 | Elapsed: 149.23s
01/13/2023 09:59:06 PM  [*] Fri Jan 13 21:59:06 2023: Train Epoch: 2 [4800 /38063 (13%)]	Loss: 0.228854 | FPR 0.001 -- TPR 0.8248 | F1 0.8914 | Elapsed: 149.35s
01/13/2023 10:01:36 PM  [*] Fri Jan 13 22:01:36 2023: Train Epoch: 2 [6400 /38063 (17%)]	Loss: 0.161681 | FPR 0.001 -- TPR 0.8235 | F1 0.8906 | Elapsed: 149.18s
01/13/2023 10:04:05 PM  [*] Fri Jan 13 22:04:05 2023: Train Epoch: 2 [8000 /38063 (21%)]	Loss: 0.407728 | FPR 0.001 -- TPR 0.8098 | F1 0.8803 | Elapsed: 149.30s
01/13/2023 10:06:34 PM  [*] Fri Jan 13 22:06:34 2023: Train Epoch: 2 [9600 /38063 (25%)]	Loss: 0.170687 | FPR 0.001 -- TPR 0.8043 | F1 0.8757 | Elapsed: 149.04s
01/13/2023 10:07:02 PM [!] Learning rate: 2.5000000000000004e-07
01/13/2023 10:09:03 PM  [*] Fri Jan 13 22:09:03 2023: Train Epoch: 2 [11200/38063 (29%)]	Loss: 0.095651 | FPR 0.001 -- TPR 0.8043 | F1 0.8758 | Elapsed: 148.79s
01/13/2023 10:11:32 PM  [*] Fri Jan 13 22:11:32 2023: Train Epoch: 2 [12800/38063 (34%)]	Loss: 0.295708 | FPR 0.001 -- TPR 0.8059 | F1 0.8758 | Elapsed: 148.95s
01/13/2023 10:14:01 PM  [*] Fri Jan 13 22:14:01 2023: Train Epoch: 2 [14400/38063 (38%)]	Loss: 0.277547 | FPR 0.001 -- TPR 0.8072 | F1 0.8769 | Elapsed: 149.23s
01/13/2023 10:16:30 PM  [*] Fri Jan 13 22:16:30 2023: Train Epoch: 2 [16000/38063 (42%)]	Loss: 0.140075 | FPR 0.001 -- TPR 0.8072 | F1 0.8772 | Elapsed: 149.21s
01/13/2023 10:18:59 PM  [*] Fri Jan 13 22:18:59 2023: Train Epoch: 2 [17600/38063 (46%)]	Loss: 0.292270 | FPR 0.001 -- TPR 0.8079 | F1 0.8780 | Elapsed: 148.95s
01/13/2023 10:21:28 PM  [*] Fri Jan 13 22:21:28 2023: Train Epoch: 2 [19200/38063 (50%)]	Loss: 0.216171 | FPR 0.001 -- TPR 0.8087 | F1 0.8787 | Elapsed: 148.89s
01/13/2023 10:23:57 PM  [*] Fri Jan 13 22:23:57 2023: Train Epoch: 2 [20800/38063 (55%)]	Loss: 0.148850 | FPR 0.001 -- TPR 0.8102 | F1 0.8799 | Elapsed: 148.96s
01/13/2023 10:26:26 PM  [*] Fri Jan 13 22:26:26 2023: Train Epoch: 2 [22400/38063 (59%)]	Loss: 0.176970 | FPR 0.001 -- TPR 0.8099 | F1 0.8797 | Elapsed: 149.12s
01/13/2023 10:28:55 PM  [*] Fri Jan 13 22:28:55 2023: Train Epoch: 2 [24000/38063 (63%)]	Loss: 0.278577 | FPR 0.001 -- TPR 0.8108 | F1 0.8805 | Elapsed: 149.21s
01/13/2023 10:31:24 PM  [*] Fri Jan 13 22:31:24 2023: Train Epoch: 2 [25600/38063 (67%)]	Loss: 0.192556 | FPR 0.001 -- TPR 0.8087 | F1 0.8788 | Elapsed: 149.20s
01/13/2023 10:31:53 PM [!] Learning rate: 2.5000000000000005e-08
01/13/2023 10:33:54 PM  [*] Fri Jan 13 22:33:54 2023: Train Epoch: 2 [27200/38063 (71%)]	Loss: 0.198498 | FPR 0.001 -- TPR 0.8095 | F1 0.8795 | Elapsed: 149.15s
01/13/2023 10:36:23 PM  [*] Fri Jan 13 22:36:23 2023: Train Epoch: 2 [28800/38063 (76%)]	Loss: 0.269429 | FPR 0.001 -- TPR 0.8089 | F1 0.8787 | Elapsed: 149.29s
01/13/2023 10:38:52 PM  [*] Fri Jan 13 22:38:52 2023: Train Epoch: 2 [30400/38063 (80%)]	Loss: 0.277888 | FPR 0.001 -- TPR 0.8104 | F1 0.8798 | Elapsed: 149.07s
01/13/2023 10:41:21 PM  [*] Fri Jan 13 22:41:21 2023: Train Epoch: 2 [32000/38063 (84%)]	Loss: 0.153299 | FPR 0.001 -- TPR 0.8102 | F1 0.8796 | Elapsed: 149.16s
01/13/2023 10:43:50 PM  [*] Fri Jan 13 22:43:50 2023: Train Epoch: 2 [33600/38063 (88%)]	Loss: 0.491064 | FPR 0.001 -- TPR 0.8096 | F1 0.8790 | Elapsed: 148.86s
01/13/2023 10:46:19 PM  [*] Fri Jan 13 22:46:19 2023: Train Epoch: 2 [35200/38063 (92%)]	Loss: 0.199576 | FPR 0.001 -- TPR 0.8101 | F1 0.8795 | Elapsed: 148.93s
01/13/2023 10:48:48 PM  [*] Fri Jan 13 22:48:48 2023: Train Epoch: 2 [36800/38063 (97%)]	Loss: 0.107247 | FPR 0.001 -- TPR 0.8103 | F1 0.8797 | Elapsed: 148.83s
01/13/2023 10:50:44 PM  [*] Fri Jan 13 22:50:44 2023:    2    | Tr.loss: 0.283674 | FPR 0.001 -- TPR: 0.81 |  F1: 0.88 | Elapsed:  3548.00 s
01/13/2023 10:50:44 PM  [*] Started epoch: 3
01/13/2023 10:50:45 PM  [*] Fri Jan 13 22:50:45 2023: Train Epoch: 3 [  0  /38063 (0 %)]	Loss: 0.323317 | FPR 0.001 -- TPR 0.9000 | F1 0.9474 | Elapsed: 1.49s
01/13/2023 10:53:14 PM  [*] Fri Jan 13 22:53:14 2023: Train Epoch: 3 [1600 /38063 (4 %)]	Loss: 0.290394 | FPR 0.001 -- TPR 0.8319 | F1 0.8955 | Elapsed: 149.05s
01/13/2023 10:55:44 PM  [*] Fri Jan 13 22:55:44 2023: Train Epoch: 3 [3200 /38063 (8 %)]	Loss: 0.135314 | FPR 0.001 -- TPR 0.8384 | F1 0.8948 | Elapsed: 149.23s
01/13/2023 10:56:43 PM [!] Learning rate: 2.500000000000001e-09
01/13/2023 10:58:13 PM  [*] Fri Jan 13 22:58:13 2023: Train Epoch: 3 [4800 /38063 (13%)]	Loss: 0.355301 | FPR 0.001 -- TPR 0.8349 | F1 0.8945 | Elapsed: 149.33s
01/13/2023 11:00:42 PM  [*] Fri Jan 13 23:00:42 2023: Train Epoch: 3 [6400 /38063 (17%)]	Loss: 0.211604 | FPR 0.001 -- TPR 0.8301 | F1 0.8924 | Elapsed: 149.12s
01/13/2023 11:03:11 PM  [*] Fri Jan 13 23:03:11 2023: Train Epoch: 3 [8000 /38063 (21%)]	Loss: 0.247740 | FPR 0.001 -- TPR 0.8286 | F1 0.8904 | Elapsed: 149.17s
01/13/2023 11:05:40 PM  [*] Fri Jan 13 23:05:40 2023: Train Epoch: 3 [9600 /38063 (25%)]	Loss: 0.131881 | FPR 0.001 -- TPR 0.8328 | F1 0.8939 | Elapsed: 149.07s
01/13/2023 11:08:09 PM  [*] Fri Jan 13 23:08:09 2023: Train Epoch: 3 [11200/38063 (29%)]	Loss: 0.148435 | FPR 0.001 -- TPR 0.8277 | F1 0.8910 | Elapsed: 148.89s
01/13/2023 11:10:38 PM  [*] Fri Jan 13 23:10:38 2023: Train Epoch: 3 [12800/38063 (34%)]	Loss: 0.281067 | FPR 0.001 -- TPR 0.8264 | F1 0.8904 | Elapsed: 148.98s
01/13/2023 11:13:07 PM  [*] Fri Jan 13 23:13:07 2023: Train Epoch: 3 [14400/38063 (38%)]	Loss: 0.144588 | FPR 0.001 -- TPR 0.8228 | F1 0.8881 | Elapsed: 149.12s
01/13/2023 11:15:36 PM  [*] Fri Jan 13 23:15:36 2023: Train Epoch: 3 [16000/38063 (42%)]	Loss: 0.088712 | FPR 0.001 -- TPR 0.8199 | F1 0.8862 | Elapsed: 148.96s
01/13/2023 11:18:05 PM  [*] Fri Jan 13 23:18:05 2023: Train Epoch: 3 [17600/38063 (46%)]	Loss: 0.281142 | FPR 0.001 -- TPR 0.8195 | F1 0.8862 | Elapsed: 148.81s
01/13/2023 11:20:34 PM  [*] Fri Jan 13 23:20:34 2023: Train Epoch: 3 [19200/38063 (50%)]	Loss: 0.214049 | FPR 0.001 -- TPR 0.8174 | F1 0.8835 | Elapsed: 148.86s
01/13/2023 11:21:33 PM [!] Learning rate: 2.500000000000001e-10
01/13/2023 11:23:03 PM  [*] Fri Jan 13 23:23:03 2023: Train Epoch: 3 [20800/38063 (55%)]	Loss: 0.203952 | FPR 0.001 -- TPR 0.8170 | F1 0.8836 | Elapsed: 148.87s
01/13/2023 11:25:33 PM  [*] Fri Jan 13 23:25:33 2023: Train Epoch: 3 [22400/38063 (59%)]	Loss: 0.552033 | FPR 0.001 -- TPR 0.8139 | F1 0.8814 | Elapsed: 150.04s
01/13/2023 11:28:02 PM  [*] Fri Jan 13 23:28:02 2023: Train Epoch: 3 [24000/38063 (63%)]	Loss: 0.223083 | FPR 0.001 -- TPR 0.8144 | F1 0.8818 | Elapsed: 149.10s
01/13/2023 11:30:31 PM  [*] Fri Jan 13 23:30:31 2023: Train Epoch: 3 [25600/38063 (67%)]	Loss: 0.153194 | FPR 0.001 -- TPR 0.8154 | F1 0.8823 | Elapsed: 148.77s
01/13/2023 11:33:00 PM  [*] Fri Jan 13 23:33:00 2023: Train Epoch: 3 [27200/38063 (71%)]	Loss: 0.170957 | FPR 0.001 -- TPR 0.8149 | F1 0.8823 | Elapsed: 149.00s
01/13/2023 11:35:29 PM  [*] Fri Jan 13 23:35:29 2023: Train Epoch: 3 [28800/38063 (76%)]	Loss: 0.282175 | FPR 0.001 -- TPR 0.8138 | F1 0.8816 | Elapsed: 149.10s
01/13/2023 11:37:58 PM  [*] Fri Jan 13 23:37:58 2023: Train Epoch: 3 [30400/38063 (80%)]	Loss: 0.248724 | FPR 0.001 -- TPR 0.8143 | F1 0.8820 | Elapsed: 149.23s
01/13/2023 11:40:27 PM  [*] Fri Jan 13 23:40:27 2023: Train Epoch: 3 [32000/38063 (84%)]	Loss: 0.162223 | FPR 0.001 -- TPR 0.8132 | F1 0.8813 | Elapsed: 149.29s
01/13/2023 11:42:57 PM  [*] Fri Jan 13 23:42:57 2023: Train Epoch: 3 [33600/38063 (88%)]	Loss: 0.137328 | FPR 0.001 -- TPR 0.8131 | F1 0.8812 | Elapsed: 149.26s
01/13/2023 11:45:26 PM  [*] Fri Jan 13 23:45:26 2023: Train Epoch: 3 [35200/38063 (92%)]	Loss: 0.154608 | FPR 0.001 -- TPR 0.8117 | F1 0.8800 | Elapsed: 149.17s
01/13/2023 11:46:25 PM [!] Learning rate: 2.5000000000000014e-11
01/13/2023 11:47:55 PM  [*] Fri Jan 13 23:47:55 2023: Train Epoch: 3 [36800/38063 (97%)]	Loss: 0.181518 | FPR 0.001 -- TPR 0.8111 | F1 0.8796 | Elapsed: 149.11s
01/13/2023 11:49:51 PM  [*] Fri Jan 13 23:49:51 2023:    3    | Tr.loss: 0.284310 | FPR 0.001 -- TPR: 0.81 |  F1: 0.88 | Elapsed:  3547.06 s
01/13/2023 11:49:51 PM [!] Fri Jan 13 23:49:51 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\crossValidation\_modelSelection_50k\Reformer\trainingFiles\trainingFiles_1673650191-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\crossValidation\_modelSelection_50k\Reformer\trainingFiles\trainingFiles_1673650191-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\crossValidation\_modelSelection_50k\Reformer\trainingFiles\trainingFiles_1673650191-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\crossValidation\_modelSelection_50k\Reformer\trainingFiles\trainingFiles_1673650191-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\crossValidation\_modelSelection_50k\Reformer\trainingFiles\trainingFiles_1673650191-trainTPRs.npy
01/13/2023 11:49:51 PM  [!] Evaluating model on validation set...
01/13/2023 11:49:51 PM  [*] Predicting batch: 0/2379
01/13/2023 11:50:29 PM  [*] Predicting batch: 100/2379
01/13/2023 11:51:07 PM  [*] Predicting batch: 200/2379
01/13/2023 11:51:44 PM  [*] Predicting batch: 300/2379
01/13/2023 11:52:22 PM  [*] Predicting batch: 400/2379
01/13/2023 11:53:00 PM  [*] Predicting batch: 500/2379
01/13/2023 11:53:37 PM  [*] Predicting batch: 600/2379
01/13/2023 11:54:15 PM  [*] Predicting batch: 700/2379
01/13/2023 11:54:53 PM  [*] Predicting batch: 800/2379
01/13/2023 11:55:30 PM  [*] Predicting batch: 900/2379
01/13/2023 11:56:08 PM  [*] Predicting batch: 1000/2379
01/13/2023 11:56:46 PM  [*] Predicting batch: 1100/2379
01/13/2023 11:57:23 PM  [*] Predicting batch: 1200/2379
01/13/2023 11:58:01 PM  [*] Predicting batch: 1300/2379
01/13/2023 11:58:39 PM  [*] Predicting batch: 1400/2379
01/13/2023 11:59:17 PM  [*] Predicting batch: 1500/2379
01/13/2023 11:59:54 PM  [*] Predicting batch: 1600/2379
01/14/2023 12:00:32 AM  [*] Predicting batch: 1700/2379
01/14/2023 12:01:09 AM  [*] Predicting batch: 1800/2379
01/14/2023 12:01:47 AM  [*] Predicting batch: 1900/2379
01/14/2023 12:02:25 AM  [*] Predicting batch: 2000/2379
01/14/2023 12:03:02 AM  [*] Predicting batch: 2100/2379
01/14/2023 12:03:40 AM  [*] Predicting batch: 2200/2379
01/14/2023 12:04:18 AM  [*] Predicting batch: 2300/2379
01/14/2023 12:04:47 AM  [!] This fold metrics on validation set:
01/14/2023 12:04:47 AM  [!] FPR: 0.0001 | TPR: 0.15945347979660754 | F1: 0.27504017139796466
01/14/2023 12:04:47 AM  [!] FPR: 0.0003 | TPR: 0.17276714668322787 | F1: 0.29460237614587814
01/14/2023 12:04:47 AM  [!] FPR: 0.001 | TPR: 0.2570352831580173 | F1: 0.40880328425471496
01/14/2023 12:04:47 AM  [!] FPR: 0.003 | TPR: 0.4031362807126499 | F1: 0.5740500207268205
01/14/2023 12:04:47 AM  [!] FPR: 0.01 | TPR: 0.5343709971664791 | F1: 0.6943736917761582
01/14/2023 12:04:47 AM  [!] FPR: 0.03 | TPR: 0.6646353297364438 | F1: 0.7917235001733903
01/14/2023 12:04:47 AM  [!] FPR: 0.1 | TPR: 0.8345301401234329 | F1: 0.8867259192048337
01/14/2023 12:04:48 AM  [!] Fold 2/2 | Train set size: 38063, Validation set size: 38063
01/14/2023 12:04:48 AM  [*] Started epoch: 1
01/14/2023 12:04:50 AM  [*] Sat Jan 14 00:04:50 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.718998 | FPR 0.001 -- TPR 0.0833 | F1 0.1538 | Elapsed: 1.71s
01/14/2023 12:04:51 AM  [!] Exception: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 2.75 GiB already allocated; 0 bytes free; 3.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
01/14/2023 12:04:51 AM  [!] Metrics saved to C:\Users\dtrizna\Code\nebula\scripts\..\evaluation\crossValidation\_modelSelection_50k\Reformer\metrics_trainSize_76126_ep_3_cv_2_vocabSize_50000_maxLen_2048_dim_64_heads_4_depth_4_meanOverSequence_True_classifierDropout_0.3_hiddenNeurons_256_64.json
01/14/2023 12:04:51 AM  [!] Average epoch time: 15493379179488009105354824943977049168672740530244131503492378167703379075466404148736890937959898569160852265642688563247070550010310765273123726172806389507291420867845664750291624153277629969989632.00s | Mean values over 2 folds:
	FPR: 0.0001 -- TPR: 0.1595 -- F1: 0.2750
	FPR: 0.0003 -- TPR: 0.1728 -- F1: 0.2946
	FPR:  0.001 -- TPR: 0.2570 -- F1: 0.4088
	FPR:  0.003 -- TPR: 0.4031 -- F1: 0.5741
	FPR:   0.01 -- TPR: 0.5344 -- F1: 0.6944
	FPR:   0.03 -- TPR: 0.6646 -- F1: 0.7917
	FPR:    0.1 -- TPR: 0.8345 -- F1: 0.8867

