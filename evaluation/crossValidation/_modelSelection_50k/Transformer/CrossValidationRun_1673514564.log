WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 50000, 'maxLen': 2048, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 8, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3}
WARNING:root: [!] Fold 1/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Jan 12 10:09:31 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 5.648564 | FPR 0.001 -- TPR 0.0000 | F1 0.0000 | Elapsed: 4.00s
WARNING:root: [*] Thu Jan 12 10:10:10 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.748572 | FPR 0.001 -- TPR 0.1375 | F1 0.2021 | Elapsed: 39.06s
WARNING:root: [*] Thu Jan 12 10:10:49 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.815980 | FPR 0.001 -- TPR 0.1497 | F1 0.2247 | Elapsed: 39.41s
WARNING:root: [*] Thu Jan 12 10:11:30 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.546411 | FPR 0.001 -- TPR 0.1754 | F1 0.2602 | Elapsed: 40.61s
WARNING:root: [*] Thu Jan 12 10:12:09 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.662476 | FPR 0.001 -- TPR 0.1958 | F1 0.2856 | Elapsed: 39.71s
WARNING:root: [*] Thu Jan 12 10:12:49 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.683702 | FPR 0.001 -- TPR 0.2118 | F1 0.3069 | Elapsed: 39.45s
WARNING:root: [*] Thu Jan 12 10:13:28 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.729881 | FPR 0.001 -- TPR 0.2329 | F1 0.3324 | Elapsed: 39.46s
WARNING:root: [*] Thu Jan 12 10:14:08 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.580959 | FPR 0.001 -- TPR 0.2536 | F1 0.3578 | Elapsed: 39.90s
WARNING:root: [*] Thu Jan 12 10:14:47 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.414223 | FPR 0.001 -- TPR 0.2734 | F1 0.3821 | Elapsed: 38.99s
WARNING:root: [*] Thu Jan 12 10:15:27 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.594939 | FPR 0.001 -- TPR 0.2897 | F1 0.4019 | Elapsed: 39.35s
WARNING:root: [*] Thu Jan 12 10:16:06 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.323992 | FPR 0.001 -- TPR 0.3056 | F1 0.4209 | Elapsed: 39.62s
WARNING:root: [*] Thu Jan 12 10:16:46 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.561471 | FPR 0.001 -- TPR 0.3196 | F1 0.4380 | Elapsed: 39.70s
WARNING:root: [*] Thu Jan 12 10:17:26 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.650914 | FPR 0.001 -- TPR 0.3348 | F1 0.4554 | Elapsed: 39.80s
WARNING:root: [*] Thu Jan 12 10:18:06 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.402010 | FPR 0.001 -- TPR 0.3466 | F1 0.4696 | Elapsed: 40.14s
WARNING:root: [*] Thu Jan 12 10:18:45 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.550694 | FPR 0.001 -- TPR 0.3549 | F1 0.4801 | Elapsed: 38.96s
WARNING:root: [*] Thu Jan 12 10:19:24 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.362452 | FPR 0.001 -- TPR 0.3630 | F1 0.4902 | Elapsed: 38.69s
WARNING:root: [*] Thu Jan 12 10:20:02 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.780167 | FPR 0.001 -- TPR 0.3725 | F1 0.5011 | Elapsed: 38.97s
WARNING:root: [*] Thu Jan 12 10:20:41 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.475645 | FPR 0.001 -- TPR 0.3802 | F1 0.5099 | Elapsed: 38.71s
WARNING:root: [*] Thu Jan 12 10:21:20 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.569763 | FPR 0.001 -- TPR 0.3885 | F1 0.5193 | Elapsed: 39.22s
WARNING:root: [*] Thu Jan 12 10:22:00 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.382483 | FPR 0.001 -- TPR 0.3987 | F1 0.5297 | Elapsed: 39.20s
WARNING:root:[!] Learning rate: 2.5e-05
WARNING:root: [*] Thu Jan 12 10:22:38 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.350795 | FPR 0.001 -- TPR 0.4108 | F1 0.5416 | Elapsed: 38.68s
WARNING:root: [*] Thu Jan 12 10:23:17 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.424952 | FPR 0.001 -- TPR 0.4204 | F1 0.5514 | Elapsed: 39.08s
WARNING:root: [*] Thu Jan 12 10:23:57 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.435041 | FPR 0.001 -- TPR 0.4289 | F1 0.5599 | Elapsed: 40.07s
WARNING:root: [*] Thu Jan 12 10:24:39 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.288747 | FPR 0.001 -- TPR 0.4370 | F1 0.5678 | Elapsed: 41.29s
WARNING:root: [*] Thu Jan 12 10:25:12 2023:    1    | Tr.loss: 0.572064 | FPR 0.001 -- TPR: 0.44 |  F1: 0.57 | Elapsed:  945.22  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Jan 12 10:25:12 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.407230 | FPR 0.001 -- TPR 0.5000 | F1 0.6667 | Elapsed: 0.41s
WARNING:root: [*] Thu Jan 12 10:25:51 2023: Train Epoch: 2 [1600 /38063 (4 %)]	Loss: 0.366339 | FPR 0.001 -- TPR 0.6408 | F1 0.7630 | Elapsed: 38.52s
WARNING:root: [*] Thu Jan 12 10:26:30 2023: Train Epoch: 2 [3200 /38063 (8 %)]	Loss: 0.179699 | FPR 0.001 -- TPR 0.6532 | F1 0.7748 | Elapsed: 39.06s
WARNING:root: [*] Thu Jan 12 10:27:09 2023: Train Epoch: 2 [4800 /38063 (13%)]	Loss: 0.335724 | FPR 0.001 -- TPR 0.6434 | F1 0.7676 | Elapsed: 39.12s
WARNING:root: [*] Thu Jan 12 10:27:47 2023: Train Epoch: 2 [6400 /38063 (17%)]	Loss: 0.204167 | FPR 0.001 -- TPR 0.6436 | F1 0.7676 | Elapsed: 38.09s
WARNING:root: [*] Thu Jan 12 10:28:27 2023: Train Epoch: 2 [8000 /38063 (21%)]	Loss: 0.322383 | FPR 0.001 -- TPR 0.6466 | F1 0.7703 | Elapsed: 39.54s
WARNING:root: [*] Thu Jan 12 10:29:07 2023: Train Epoch: 2 [9600 /38063 (25%)]	Loss: 0.283322 | FPR 0.001 -- TPR 0.6509 | F1 0.7736 | Elapsed: 40.38s
WARNING:root: [*] Thu Jan 12 10:29:44 2023: Train Epoch: 2 [11200/38063 (29%)]	Loss: 0.280200 | FPR 0.001 -- TPR 0.6560 | F1 0.7777 | Elapsed: 36.52s
WARNING:root: [*] Thu Jan 12 10:30:14 2023: Train Epoch: 2 [12800/38063 (34%)]	Loss: 0.306722 | FPR 0.001 -- TPR 0.6620 | F1 0.7817 | Elapsed: 29.99s
WARNING:root: [*] Thu Jan 12 10:30:44 2023: Train Epoch: 2 [14400/38063 (38%)]	Loss: 0.370093 | FPR 0.001 -- TPR 0.6684 | F1 0.7864 | Elapsed: 30.57s
WARNING:root: [*] Thu Jan 12 10:31:15 2023: Train Epoch: 2 [16000/38063 (42%)]	Loss: 0.354185 | FPR 0.001 -- TPR 0.6738 | F1 0.7904 | Elapsed: 31.15s
WARNING:root: [*] Thu Jan 12 10:31:46 2023: Train Epoch: 2 [17600/38063 (46%)]	Loss: 0.225070 | FPR 0.001 -- TPR 0.6769 | F1 0.7925 | Elapsed: 30.74s
WARNING:root: [*] Thu Jan 12 10:32:18 2023: Train Epoch: 2 [19200/38063 (50%)]	Loss: 0.450036 | FPR 0.001 -- TPR 0.6833 | F1 0.7971 | Elapsed: 31.84s
WARNING:root: [*] Thu Jan 12 10:32:49 2023: Train Epoch: 2 [20800/38063 (55%)]	Loss: 0.485288 | FPR 0.001 -- TPR 0.6883 | F1 0.8005 | Elapsed: 31.68s
WARNING:root: [*] Thu Jan 12 10:33:28 2023: Train Epoch: 2 [22400/38063 (59%)]	Loss: 0.210568 | FPR 0.001 -- TPR 0.6901 | F1 0.8010 | Elapsed: 38.97s
WARNING:root: [*] Thu Jan 12 10:34:08 2023: Train Epoch: 2 [24000/38063 (63%)]	Loss: 0.448568 | FPR 0.001 -- TPR 0.6917 | F1 0.8023 | Elapsed: 39.51s
WARNING:root: [*] Thu Jan 12 10:34:48 2023: Train Epoch: 2 [25600/38063 (67%)]	Loss: 0.426498 | FPR 0.001 -- TPR 0.6950 | F1 0.8048 | Elapsed: 39.91s
WARNING:root:[!] Learning rate: 2.5e-06
WARNING:root: [*] Thu Jan 12 10:35:28 2023: Train Epoch: 2 [27200/38063 (71%)]	Loss: 0.314745 | FPR 0.001 -- TPR 0.6972 | F1 0.8064 | Elapsed: 39.83s
WARNING:root: [*] Thu Jan 12 10:36:09 2023: Train Epoch: 2 [28800/38063 (76%)]	Loss: 0.343946 | FPR 0.001 -- TPR 0.6994 | F1 0.8077 | Elapsed: 41.11s
WARNING:root: [*] Thu Jan 12 10:36:49 2023: Train Epoch: 2 [30400/38063 (80%)]	Loss: 0.179425 | FPR 0.001 -- TPR 0.7021 | F1 0.8096 | Elapsed: 39.99s
WARNING:root: [*] Thu Jan 12 10:37:28 2023: Train Epoch: 2 [32000/38063 (84%)]	Loss: 0.395663 | FPR 0.001 -- TPR 0.7038 | F1 0.8108 | Elapsed: 39.66s
WARNING:root: [*] Thu Jan 12 10:38:09 2023: Train Epoch: 2 [33600/38063 (88%)]	Loss: 0.319212 | FPR 0.001 -- TPR 0.7071 | F1 0.8132 | Elapsed: 40.19s
WARNING:root: [*] Thu Jan 12 10:38:50 2023: Train Epoch: 2 [35200/38063 (92%)]	Loss: 0.313334 | FPR 0.001 -- TPR 0.7093 | F1 0.8145 | Elapsed: 41.36s
WARNING:root: [*] Thu Jan 12 10:39:30 2023: Train Epoch: 2 [36800/38063 (97%)]	Loss: 0.247665 | FPR 0.001 -- TPR 0.7127 | F1 0.8170 | Elapsed: 40.41s
WARNING:root: [*] Thu Jan 12 10:40:02 2023:    2    | Tr.loss: 0.360215 | FPR 0.001 -- TPR: 0.71 |  F1: 0.82 | Elapsed:  889.86  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Jan 12 10:40:02 2023: Train Epoch: 3 [  0  /38063 (0 %)]	Loss: 0.113658 | FPR 0.001 -- TPR 0.9167 | F1 0.9565 | Elapsed: 0.39s
WARNING:root: [*] Thu Jan 12 10:40:41 2023: Train Epoch: 3 [1600 /38063 (4 %)]	Loss: 0.241354 | FPR 0.001 -- TPR 0.8017 | F1 0.8826 | Elapsed: 38.75s
WARNING:root: [*] Thu Jan 12 10:41:20 2023: Train Epoch: 3 [3200 /38063 (8 %)]	Loss: 0.342325 | FPR 0.001 -- TPR 0.7860 | F1 0.8707 | Elapsed: 39.12s
WARNING:root: [*] Thu Jan 12 10:41:59 2023: Train Epoch: 3 [4800 /38063 (13%)]	Loss: 0.362334 | FPR 0.001 -- TPR 0.7696 | F1 0.8582 | Elapsed: 39.12s
WARNING:root: [*] Thu Jan 12 10:42:38 2023: Train Epoch: 3 [6400 /38063 (17%)]	Loss: 0.341454 | FPR 0.001 -- TPR 0.7668 | F1 0.8566 | Elapsed: 39.22s
WARNING:root: [*] Thu Jan 12 10:43:18 2023: Train Epoch: 3 [8000 /38063 (21%)]	Loss: 0.157161 | FPR 0.001 -- TPR 0.7649 | F1 0.8549 | Elapsed: 39.34s
WARNING:root: [*] Thu Jan 12 10:43:58 2023: Train Epoch: 3 [9600 /38063 (25%)]	Loss: 0.381679 | FPR 0.001 -- TPR 0.7681 | F1 0.8568 | Elapsed: 40.19s
WARNING:root: [*] Thu Jan 12 10:44:37 2023: Train Epoch: 3 [11200/38063 (29%)]	Loss: 0.210353 | FPR 0.001 -- TPR 0.7686 | F1 0.8558 | Elapsed: 39.25s
WARNING:root: [*] Thu Jan 12 10:45:16 2023: Train Epoch: 3 [12800/38063 (34%)]	Loss: 0.230188 | FPR 0.001 -- TPR 0.7649 | F1 0.8527 | Elapsed: 38.72s
WARNING:root: [*] Thu Jan 12 10:45:55 2023: Train Epoch: 3 [14400/38063 (38%)]	Loss: 0.312125 | FPR 0.001 -- TPR 0.7639 | F1 0.8523 | Elapsed: 39.43s
WARNING:root: [*] Thu Jan 12 10:46:35 2023: Train Epoch: 3 [16000/38063 (42%)]	Loss: 0.438940 | FPR 0.001 -- TPR 0.7606 | F1 0.8499 | Elapsed: 39.80s
WARNING:root: [*] Thu Jan 12 10:47:14 2023: Train Epoch: 3 [17600/38063 (46%)]	Loss: 0.190911 | FPR 0.001 -- TPR 0.7572 | F1 0.8473 | Elapsed: 38.94s
WARNING:root: [*] Thu Jan 12 10:47:53 2023: Train Epoch: 3 [19200/38063 (50%)]	Loss: 0.288801 | FPR 0.001 -- TPR 0.7585 | F1 0.8472 | Elapsed: 39.22s
WARNING:root:[!] Learning rate: 2.5000000000000004e-07
WARNING:root: [*] Thu Jan 12 10:48:32 2023: Train Epoch: 3 [20800/38063 (55%)]	Loss: 0.524556 | FPR 0.001 -- TPR 0.7626 | F1 0.8503 | Elapsed: 39.03s
WARNING:root: [*] Thu Jan 12 10:49:12 2023: Train Epoch: 3 [22400/38063 (59%)]	Loss: 0.332136 | FPR 0.001 -- TPR 0.7630 | F1 0.8510 | Elapsed: 39.44s
WARNING:root: [*] Thu Jan 12 10:49:51 2023: Train Epoch: 3 [24000/38063 (63%)]	Loss: 0.202811 | FPR 0.001 -- TPR 0.7636 | F1 0.8519 | Elapsed: 39.47s
WARNING:root: [*] Thu Jan 12 10:50:30 2023: Train Epoch: 3 [25600/38063 (67%)]	Loss: 0.453673 | FPR 0.001 -- TPR 0.7650 | F1 0.8528 | Elapsed: 39.29s
WARNING:root: [*] Thu Jan 12 10:51:09 2023: Train Epoch: 3 [27200/38063 (71%)]	Loss: 0.218495 | FPR 0.001 -- TPR 0.7666 | F1 0.8541 | Elapsed: 38.76s
WARNING:root: [*] Thu Jan 12 10:51:48 2023: Train Epoch: 3 [28800/38063 (76%)]	Loss: 0.206442 | FPR 0.001 -- TPR 0.7647 | F1 0.8528 | Elapsed: 38.83s
WARNING:root: [*] Thu Jan 12 10:52:27 2023: Train Epoch: 3 [30400/38063 (80%)]	Loss: 0.287776 | FPR 0.001 -- TPR 0.7663 | F1 0.8539 | Elapsed: 38.78s
WARNING:root: [*] Thu Jan 12 10:53:06 2023: Train Epoch: 3 [32000/38063 (84%)]	Loss: 0.231553 | FPR 0.001 -- TPR 0.7672 | F1 0.8543 | Elapsed: 38.89s
WARNING:root: [*] Thu Jan 12 10:53:45 2023: Train Epoch: 3 [33600/38063 (88%)]	Loss: 0.233560 | FPR 0.001 -- TPR 0.7682 | F1 0.8549 | Elapsed: 39.30s
WARNING:root: [*] Thu Jan 12 10:54:24 2023: Train Epoch: 3 [35200/38063 (92%)]	Loss: 0.425884 | FPR 0.001 -- TPR 0.7685 | F1 0.8552 | Elapsed: 39.12s
WARNING:root: [*] Thu Jan 12 10:55:03 2023: Train Epoch: 3 [36800/38063 (97%)]	Loss: 0.500150 | FPR 0.001 -- TPR 0.7676 | F1 0.8545 | Elapsed: 38.84s
WARNING:root: [*] Thu Jan 12 10:55:33 2023:    3    | Tr.loss: 0.323690 | FPR 0.001 -- TPR: 0.77 |  F1: 0.85 | Elapsed:  931.65  s
WARNING:root:[!] Thu Jan 12 10:55:34 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673517333-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673517333-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673517333-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673517333-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673517333-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/2379
WARNING:root: [*] Predicting batch: 100/2379
WARNING:root: [*] Predicting batch: 200/2379
WARNING:root: [*] Predicting batch: 300/2379
WARNING:root: [*] Predicting batch: 400/2379
WARNING:root: [*] Predicting batch: 500/2379
WARNING:root: [*] Predicting batch: 600/2379
WARNING:root: [*] Predicting batch: 700/2379
WARNING:root: [*] Predicting batch: 800/2379
WARNING:root: [*] Predicting batch: 900/2379
WARNING:root: [*] Predicting batch: 1000/2379
WARNING:root: [*] Predicting batch: 1100/2379
WARNING:root: [*] Predicting batch: 1200/2379
WARNING:root: [*] Predicting batch: 1300/2379
WARNING:root: [*] Predicting batch: 1400/2379
WARNING:root: [*] Predicting batch: 1500/2379
WARNING:root: [*] Predicting batch: 1600/2379
WARNING:root: [*] Predicting batch: 1700/2379
WARNING:root: [*] Predicting batch: 1800/2379
WARNING:root: [*] Predicting batch: 1900/2379
WARNING:root: [*] Predicting batch: 2000/2379
WARNING:root: [*] Predicting batch: 2100/2379
WARNING:root: [*] Predicting batch: 2200/2379
WARNING:root: [*] Predicting batch: 2300/2379
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.18394596902534643 | F1: 0.31072353538996167
WARNING:root: [!] FPR: 0.0003 | TPR: 0.25858789737220045 | F1: 0.41087948686320463
WARNING:root: [!] FPR: 0.001 | TPR: 0.3324923339673175 | F1: 0.4988788911214001
WARNING:root: [!] FPR: 0.003 | TPR: 0.5584753328416722 | F1: 0.716052454773932
WARNING:root: [!] FPR: 0.01 | TPR: 0.6157279819896752 | F1: 0.759922393350739
WARNING:root: [!] FPR: 0.03 | TPR: 0.6669254357023638 | F1: 0.793369349402041
WARNING:root: [!] FPR: 0.1 | TPR: 0.7317470791445095 | F1: 0.8224233831388372
WARNING:root: [!] Fold 2/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Jan 12 11:00:21 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 4.144037 | FPR 0.001 -- TPR 0.4167 | F1 0.5882 | Elapsed: 0.39s
WARNING:root: [*] Thu Jan 12 11:01:00 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.737201 | FPR 0.001 -- TPR 0.1819 | F1 0.2687 | Elapsed: 38.71s
WARNING:root: [*] Thu Jan 12 11:01:39 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.694726 | FPR 0.001 -- TPR 0.1906 | F1 0.2803 | Elapsed: 38.44s
WARNING:root: [*] Thu Jan 12 11:02:18 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.643906 | FPR 0.001 -- TPR 0.2054 | F1 0.2980 | Elapsed: 38.94s
WARNING:root: [*] Thu Jan 12 11:02:56 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.642124 | FPR 0.001 -- TPR 0.2059 | F1 0.3019 | Elapsed: 38.45s
WARNING:root: [*] Thu Jan 12 11:03:35 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.628132 | FPR 0.001 -- TPR 0.2210 | F1 0.3216 | Elapsed: 38.67s
WARNING:root: [*] Thu Jan 12 11:04:13 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.686732 | FPR 0.001 -- TPR 0.2440 | F1 0.3501 | Elapsed: 38.63s
WARNING:root: [*] Thu Jan 12 11:04:52 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.691334 | FPR 0.001 -- TPR 0.2621 | F1 0.3721 | Elapsed: 38.84s
WARNING:root: [*] Thu Jan 12 11:05:31 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.732396 | FPR 0.001 -- TPR 0.2802 | F1 0.3933 | Elapsed: 38.82s
WARNING:root: [*] Thu Jan 12 11:06:10 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.357230 | FPR 0.001 -- TPR 0.2975 | F1 0.4131 | Elapsed: 38.59s
WARNING:root: [*] Thu Jan 12 11:06:48 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.653350 | FPR 0.001 -- TPR 0.3158 | F1 0.4344 | Elapsed: 38.84s
WARNING:root: [*] Thu Jan 12 11:07:27 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.508791 | FPR 0.001 -- TPR 0.3322 | F1 0.4534 | Elapsed: 38.55s
WARNING:root: [*] Thu Jan 12 11:08:06 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.509848 | FPR 0.001 -- TPR 0.3461 | F1 0.4694 | Elapsed: 38.90s
WARNING:root: [*] Thu Jan 12 11:08:44 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.462870 | FPR 0.001 -- TPR 0.3596 | F1 0.4845 | Elapsed: 38.39s
WARNING:root: [*] Thu Jan 12 11:09:23 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.484316 | FPR 0.001 -- TPR 0.3713 | F1 0.4971 | Elapsed: 38.74s
WARNING:root: [*] Thu Jan 12 11:10:02 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.382185 | FPR 0.001 -- TPR 0.3848 | F1 0.5114 | Elapsed: 38.73s
WARNING:root: [*] Thu Jan 12 11:10:40 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.350759 | FPR 0.001 -- TPR 0.3956 | F1 0.5225 | Elapsed: 38.54s
WARNING:root: [*] Thu Jan 12 11:11:19 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.591278 | FPR 0.001 -- TPR 0.4056 | F1 0.5329 | Elapsed: 38.71s
WARNING:root: [*] Thu Jan 12 11:11:58 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.470395 | FPR 0.001 -- TPR 0.4131 | F1 0.5413 | Elapsed: 38.53s
WARNING:root: [*] Thu Jan 12 11:12:36 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.564355 | FPR 0.001 -- TPR 0.4215 | F1 0.5500 | Elapsed: 38.72s
WARNING:root:[!] Learning rate: 2.5e-05
WARNING:root: [*] Thu Jan 12 11:13:17 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.348719 | FPR 0.001 -- TPR 0.4327 | F1 0.5608 | Elapsed: 40.31s
WARNING:root: [*] Thu Jan 12 11:13:56 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.344363 | FPR 0.001 -- TPR 0.4445 | F1 0.5719 | Elapsed: 39.39s
WARNING:root: [*] Thu Jan 12 11:14:35 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.444593 | FPR 0.001 -- TPR 0.4570 | F1 0.5833 | Elapsed: 38.52s
WARNING:root: [*] Thu Jan 12 11:15:13 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.304977 | FPR 0.001 -- TPR 0.4693 | F1 0.5945 | Elapsed: 38.54s
WARNING:root: [*] Thu Jan 12 11:15:43 2023:    1    | Tr.loss: 0.582478 | FPR 0.001 -- TPR: 0.48 |  F1: 0.60 | Elapsed:  921.76  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Jan 12 11:15:43 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.309358 | FPR 0.001 -- TPR 0.7143 | F1 0.8333 | Elapsed: 0.39s
WARNING:root: [*] Thu Jan 12 11:16:22 2023: Train Epoch: 2 [1600 /38063 (4 %)]	Loss: 0.325788 | FPR 0.001 -- TPR 0.7162 | F1 0.8114 | Elapsed: 38.52s
WARNING:root: [*] Thu Jan 12 11:17:00 2023: Train Epoch: 2 [3200 /38063 (8 %)]	Loss: 0.394533 | FPR 0.001 -- TPR 0.7571 | F1 0.8455 | Elapsed: 38.57s
WARNING:root: [*] Thu Jan 12 11:17:39 2023: Train Epoch: 2 [4800 /38063 (13%)]	Loss: 0.412840 | FPR 0.001 -- TPR 0.7565 | F1 0.8470 | Elapsed: 38.29s
WARNING:root: [*] Thu Jan 12 11:18:17 2023: Train Epoch: 2 [6400 /38063 (17%)]	Loss: 0.445467 | FPR 0.001 -- TPR 0.7530 | F1 0.8451 | Elapsed: 38.51s
WARNING:root: [*] Thu Jan 12 11:18:55 2023: Train Epoch: 2 [8000 /38063 (21%)]	Loss: 0.417004 | FPR 0.001 -- TPR 0.7610 | F1 0.8514 | Elapsed: 38.32s
WARNING:root: [*] Thu Jan 12 11:19:34 2023: Train Epoch: 2 [9600 /38063 (25%)]	Loss: 0.272158 | FPR 0.001 -- TPR 0.7589 | F1 0.8495 | Elapsed: 38.50s
WARNING:root: [*] Thu Jan 12 11:20:12 2023: Train Epoch: 2 [11200/38063 (29%)]	Loss: 0.546302 | FPR 0.001 -- TPR 0.7623 | F1 0.8506 | Elapsed: 38.25s
WARNING:root: [*] Thu Jan 12 11:20:51 2023: Train Epoch: 2 [12800/38063 (34%)]	Loss: 0.302536 | FPR 0.001 -- TPR 0.7604 | F1 0.8488 | Elapsed: 38.42s
WARNING:root: [*] Thu Jan 12 11:21:29 2023: Train Epoch: 2 [14400/38063 (38%)]	Loss: 0.386669 | FPR 0.001 -- TPR 0.7629 | F1 0.8513 | Elapsed: 38.25s
WARNING:root: [*] Thu Jan 12 11:22:07 2023: Train Epoch: 2 [16000/38063 (42%)]	Loss: 0.432703 | FPR 0.001 -- TPR 0.7605 | F1 0.8498 | Elapsed: 38.61s
WARNING:root: [*] Thu Jan 12 11:22:46 2023: Train Epoch: 2 [17600/38063 (46%)]	Loss: 0.521584 | FPR 0.001 -- TPR 0.7583 | F1 0.8483 | Elapsed: 38.35s
WARNING:root: [*] Thu Jan 12 11:23:24 2023: Train Epoch: 2 [19200/38063 (50%)]	Loss: 0.340654 | FPR 0.001 -- TPR 0.7558 | F1 0.8464 | Elapsed: 38.30s
WARNING:root: [*] Thu Jan 12 11:24:03 2023: Train Epoch: 2 [20800/38063 (55%)]	Loss: 0.268374 | FPR 0.001 -- TPR 0.7552 | F1 0.8465 | Elapsed: 38.59s
WARNING:root: [*] Thu Jan 12 11:24:41 2023: Train Epoch: 2 [22400/38063 (59%)]	Loss: 0.421318 | FPR 0.001 -- TPR 0.7563 | F1 0.8474 | Elapsed: 38.37s
WARNING:root: [*] Thu Jan 12 11:25:20 2023: Train Epoch: 2 [24000/38063 (63%)]	Loss: 0.340346 | FPR 0.001 -- TPR 0.7582 | F1 0.8488 | Elapsed: 38.53s
WARNING:root: [*] Thu Jan 12 11:25:58 2023: Train Epoch: 2 [25600/38063 (67%)]	Loss: 0.478003 | FPR 0.001 -- TPR 0.7591 | F1 0.8494 | Elapsed: 38.26s
WARNING:root:[!] Learning rate: 2.5e-06
WARNING:root: [*] Thu Jan 12 11:26:36 2023: Train Epoch: 2 [27200/38063 (71%)]	Loss: 0.620714 | FPR 0.001 -- TPR 0.7591 | F1 0.8496 | Elapsed: 38.51s
WARNING:root: [*] Thu Jan 12 11:27:15 2023: Train Epoch: 2 [28800/38063 (76%)]	Loss: 0.391568 | FPR 0.001 -- TPR 0.7598 | F1 0.8502 | Elapsed: 38.23s
WARNING:root: [*] Thu Jan 12 11:27:53 2023: Train Epoch: 2 [30400/38063 (80%)]	Loss: 0.158546 | FPR 0.001 -- TPR 0.7610 | F1 0.8512 | Elapsed: 38.57s
WARNING:root: [*] Thu Jan 12 11:28:32 2023: Train Epoch: 2 [32000/38063 (84%)]	Loss: 0.391221 | FPR 0.001 -- TPR 0.7624 | F1 0.8522 | Elapsed: 38.72s
WARNING:root: [*] Thu Jan 12 11:29:10 2023: Train Epoch: 2 [33600/38063 (88%)]	Loss: 0.296488 | FPR 0.001 -- TPR 0.7617 | F1 0.8518 | Elapsed: 38.44s
WARNING:root: [*] Thu Jan 12 11:29:49 2023: Train Epoch: 2 [35200/38063 (92%)]	Loss: 0.391719 | FPR 0.001 -- TPR 0.7628 | F1 0.8527 | Elapsed: 38.31s
WARNING:root: [*] Thu Jan 12 11:30:27 2023: Train Epoch: 2 [36800/38063 (97%)]	Loss: 0.466564 | FPR 0.001 -- TPR 0.7633 | F1 0.8531 | Elapsed: 38.23s
WARNING:root: [*] Thu Jan 12 11:30:57 2023:    2    | Tr.loss: 0.394413 | FPR 0.001 -- TPR: 0.76 |  F1: 0.85 | Elapsed:  914.38  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Jan 12 11:30:58 2023: Train Epoch: 3 [  0  /38063 (0 %)]	Loss: 0.453962 | FPR 0.001 -- TPR 0.6364 | F1 0.7778 | Elapsed: 0.39s
WARNING:root: [*] Thu Jan 12 11:31:36 2023: Train Epoch: 3 [1600 /38063 (4 %)]	Loss: 0.335750 | FPR 0.001 -- TPR 0.7802 | F1 0.8708 | Elapsed: 38.28s
WARNING:root: [*] Thu Jan 12 11:32:14 2023: Train Epoch: 3 [3200 /38063 (8 %)]	Loss: 0.417442 | FPR 0.001 -- TPR 0.7787 | F1 0.8687 | Elapsed: 38.55s
WARNING:root: [*] Thu Jan 12 11:32:53 2023: Train Epoch: 3 [4800 /38063 (13%)]	Loss: 0.280413 | FPR 0.001 -- TPR 0.7761 | F1 0.8661 | Elapsed: 38.28s
WARNING:root: [*] Thu Jan 12 11:33:31 2023: Train Epoch: 3 [6400 /38063 (17%)]	Loss: 0.459718 | FPR 0.001 -- TPR 0.7824 | F1 0.8705 | Elapsed: 38.58s
WARNING:root: [*] Thu Jan 12 11:34:10 2023: Train Epoch: 3 [8000 /38063 (21%)]	Loss: 0.222363 | FPR 0.001 -- TPR 0.7787 | F1 0.8674 | Elapsed: 38.31s
WARNING:root: [*] Thu Jan 12 11:34:48 2023: Train Epoch: 3 [9600 /38063 (25%)]	Loss: 0.536168 | FPR 0.001 -- TPR 0.7825 | F1 0.8702 | Elapsed: 38.40s
WARNING:root: [*] Thu Jan 12 11:35:26 2023: Train Epoch: 3 [11200/38063 (29%)]	Loss: 0.402196 | FPR 0.001 -- TPR 0.7841 | F1 0.8713 | Elapsed: 38.34s
WARNING:root: [*] Thu Jan 12 11:36:05 2023: Train Epoch: 3 [12800/38063 (34%)]	Loss: 0.291975 | FPR 0.001 -- TPR 0.7813 | F1 0.8693 | Elapsed: 38.48s
WARNING:root: [*] Thu Jan 12 11:36:43 2023: Train Epoch: 3 [14400/38063 (38%)]	Loss: 0.285109 | FPR 0.001 -- TPR 0.7809 | F1 0.8688 | Elapsed: 38.47s
WARNING:root: [*] Thu Jan 12 11:37:22 2023: Train Epoch: 3 [16000/38063 (42%)]	Loss: 0.388263 | FPR 0.001 -- TPR 0.7806 | F1 0.8685 | Elapsed: 38.27s
WARNING:root: [*] Thu Jan 12 11:38:00 2023: Train Epoch: 3 [17600/38063 (46%)]	Loss: 0.294224 | FPR 0.001 -- TPR 0.7828 | F1 0.8699 | Elapsed: 38.54s
WARNING:root: [*] Thu Jan 12 11:38:38 2023: Train Epoch: 3 [19200/38063 (50%)]	Loss: 0.305450 | FPR 0.001 -- TPR 0.7817 | F1 0.8689 | Elapsed: 38.25s
WARNING:root:[!] Learning rate: 2.5000000000000004e-07
WARNING:root: [*] Thu Jan 12 11:39:17 2023: Train Epoch: 3 [20800/38063 (55%)]	Loss: 0.268147 | FPR 0.001 -- TPR 0.7816 | F1 0.8688 | Elapsed: 38.67s
WARNING:root: [*] Thu Jan 12 11:39:55 2023: Train Epoch: 3 [22400/38063 (59%)]	Loss: 0.442301 | FPR 0.001 -- TPR 0.7812 | F1 0.8684 | Elapsed: 38.19s
WARNING:root: [*] Thu Jan 12 11:40:34 2023: Train Epoch: 3 [24000/38063 (63%)]	Loss: 0.344683 | FPR 0.001 -- TPR 0.7824 | F1 0.8693 | Elapsed: 38.45s
WARNING:root: [*] Thu Jan 12 11:41:12 2023: Train Epoch: 3 [25600/38063 (67%)]	Loss: 0.373894 | FPR 0.001 -- TPR 0.7809 | F1 0.8682 | Elapsed: 38.22s
WARNING:root: [*] Thu Jan 12 11:41:50 2023: Train Epoch: 3 [27200/38063 (71%)]	Loss: 0.483495 | FPR 0.001 -- TPR 0.7810 | F1 0.8679 | Elapsed: 38.33s
WARNING:root: [*] Thu Jan 12 11:42:29 2023: Train Epoch: 3 [28800/38063 (76%)]	Loss: 0.350886 | FPR 0.001 -- TPR 0.7807 | F1 0.8674 | Elapsed: 38.36s
WARNING:root: [*] Thu Jan 12 11:43:07 2023: Train Epoch: 3 [30400/38063 (80%)]	Loss: 0.395878 | FPR 0.001 -- TPR 0.7801 | F1 0.8670 | Elapsed: 38.45s
WARNING:root: [*] Thu Jan 12 11:43:46 2023: Train Epoch: 3 [32000/38063 (84%)]	Loss: 0.236963 | FPR 0.001 -- TPR 0.7799 | F1 0.8670 | Elapsed: 38.55s
WARNING:root: [*] Thu Jan 12 11:44:24 2023: Train Epoch: 3 [33600/38063 (88%)]	Loss: 0.321991 | FPR 0.001 -- TPR 0.7801 | F1 0.8673 | Elapsed: 38.19s
WARNING:root: [*] Thu Jan 12 11:45:02 2023: Train Epoch: 3 [35200/38063 (92%)]	Loss: 0.399037 | FPR 0.001 -- TPR 0.7805 | F1 0.8677 | Elapsed: 38.53s
WARNING:root: [*] Thu Jan 12 11:45:41 2023: Train Epoch: 3 [36800/38063 (97%)]	Loss: 0.438512 | FPR 0.001 -- TPR 0.7802 | F1 0.8675 | Elapsed: 38.22s
WARNING:root: [*] Thu Jan 12 11:46:10 2023:    3    | Tr.loss: 0.382768 | FPR 0.001 -- TPR: 0.78 |  F1: 0.87 | Elapsed:  913.25  s
WARNING:root:[!] Thu Jan 12 11:46:11 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673520370-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673520370-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673520370-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673520370-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\trainingFiles\trainingFiles_1673520370-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/2379
WARNING:root: [*] Predicting batch: 100/2379
WARNING:root: [*] Predicting batch: 200/2379
WARNING:root: [*] Predicting batch: 300/2379
WARNING:root: [*] Predicting batch: 400/2379
WARNING:root: [*] Predicting batch: 500/2379
WARNING:root: [*] Predicting batch: 600/2379
WARNING:root: [*] Predicting batch: 700/2379
WARNING:root: [*] Predicting batch: 800/2379
WARNING:root: [*] Predicting batch: 900/2379
WARNING:root: [*] Predicting batch: 1000/2379
WARNING:root: [*] Predicting batch: 1100/2379
WARNING:root: [*] Predicting batch: 1200/2379
WARNING:root: [*] Predicting batch: 1300/2379
WARNING:root: [*] Predicting batch: 1400/2379
WARNING:root: [*] Predicting batch: 1500/2379
WARNING:root: [*] Predicting batch: 1600/2379
WARNING:root: [*] Predicting batch: 1700/2379
WARNING:root: [*] Predicting batch: 1800/2379
WARNING:root: [*] Predicting batch: 1900/2379
WARNING:root: [*] Predicting batch: 2000/2379
WARNING:root: [*] Predicting batch: 2100/2379
WARNING:root: [*] Predicting batch: 2200/2379
WARNING:root: [*] Predicting batch: 2300/2379
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.25528436295690765 | F1: 0.40672289754403373
WARNING:root: [!] FPR: 0.0003 | TPR: 0.286581805442018 | F1: 0.44545289526229803
WARNING:root: [!] FPR: 0.001 | TPR: 0.44910272879442564 | F1: 0.6196358558461786
WARNING:root: [!] FPR: 0.003 | TPR: 0.5820000778543345 | F1: 0.7351082921552718
WARNING:root: [!] FPR: 0.01 | TPR: 0.6299972750982911 | F1: 0.7707400704829032
WARNING:root: [!] FPR: 0.03 | TPR: 0.7648020553544319 | F1: 0.8596932636138885
WARNING:root: [!] FPR: 0.1 | TPR: 0.8967651523998599 | F1: 0.9221624001761304
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\Transformer_emb64\metrics_trainSize_76126_ep_3_cv_2_vocabSize_50000_maxLen_2048_dModel_64_nHeads_8_dHidden_256_nLayers_8_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.3.json
WARNING:root: [!] Average epoch time: 919.35s | Mean values over 2 folds:
	FPR: 0.0001 -- TPR: 0.2196 -- F1: 0.3587
	FPR: 0.0003 -- TPR: 0.2726 -- F1: 0.4282
	FPR:  0.001 -- TPR: 0.3908 -- F1: 0.5593
	FPR:  0.003 -- TPR: 0.5702 -- F1: 0.7256
	FPR:   0.01 -- TPR: 0.6229 -- F1: 0.7653
	FPR:   0.03 -- TPR: 0.7159 -- F1: 0.8265
	FPR:    0.1 -- TPR: 0.8143 -- F1: 0.8723

