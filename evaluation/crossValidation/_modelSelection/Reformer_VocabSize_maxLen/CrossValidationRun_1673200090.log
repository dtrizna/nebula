WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 2 | Model config: {'vocabSize': 10000, 'maxLen': 2048, 'dim': 64, 'heads': 4, 'depth': 4, 'meanOverSequence': True, 'hiddenNeurons': [64]}
WARNING:root: [!] Fold 1/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  8 18:48:15 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.731467 | FPR 0.001 -- TPR 0.3571 | F1 0.5263 | Elapsed: 3.04s
WARNING:root: [*] Sun Jan  8 18:50:44 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.460062 | FPR 0.001 -- TPR 0.3505 | F1 0.4504 | Elapsed: 149.44s
WARNING:root: [*] Sun Jan  8 18:53:13 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.570962 | FPR 0.001 -- TPR 0.4648 | F1 0.5709 | Elapsed: 148.87s
WARNING:root: [*] Sun Jan  8 18:55:42 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.419143 | FPR 0.001 -- TPR 0.5369 | F1 0.6423 | Elapsed: 148.88s
WARNING:root: [*] Sun Jan  8 18:58:11 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.376505 | FPR 0.001 -- TPR 0.5891 | F1 0.6897 | Elapsed: 148.56s
WARNING:root: [*] Sun Jan  8 19:00:39 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.208606 | FPR 0.001 -- TPR 0.6156 | F1 0.7144 | Elapsed: 148.44s
WARNING:root: [*] Sun Jan  8 19:03:08 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.175392 | FPR 0.001 -- TPR 0.6461 | F1 0.7404 | Elapsed: 148.64s
WARNING:root: [*] Sun Jan  8 19:05:36 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.464541 | FPR 0.001 -- TPR 0.6643 | F1 0.7570 | Elapsed: 148.51s
WARNING:root: [*] Sun Jan  8 19:08:05 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.158791 | FPR 0.001 -- TPR 0.6800 | F1 0.7709 | Elapsed: 148.33s
WARNING:root: [*] Sun Jan  8 19:10:33 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.169857 | FPR 0.001 -- TPR 0.6937 | F1 0.7826 | Elapsed: 148.41s
WARNING:root: [*] Sun Jan  8 19:13:01 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.297196 | FPR 0.001 -- TPR 0.7001 | F1 0.7883 | Elapsed: 148.37s
WARNING:root: [*] Sun Jan  8 19:15:30 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.667279 | FPR 0.001 -- TPR 0.6955 | F1 0.7857 | Elapsed: 148.58s
WARNING:root: [*] Sun Jan  8 19:17:59 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.450168 | FPR 0.001 -- TPR 0.6919 | F1 0.7843 | Elapsed: 148.95s
WARNING:root: [*] Sun Jan  8 19:20:28 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.409861 | FPR 0.001 -- TPR 0.6935 | F1 0.7853 | Elapsed: 149.14s
WARNING:root: [*] Sun Jan  8 19:22:57 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.388971 | FPR 0.001 -- TPR 0.7000 | F1 0.7913 | Elapsed: 148.70s
WARNING:root: [*] Sun Jan  8 19:25:25 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.329498 | FPR 0.001 -- TPR 0.7042 | F1 0.7953 | Elapsed: 148.34s
WARNING:root: [*] Sun Jan  8 19:27:54 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.234081 | FPR 0.001 -- TPR 0.7078 | F1 0.7989 | Elapsed: 148.42s
WARNING:root: [*] Sun Jan  8 19:30:22 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.332407 | FPR 0.001 -- TPR 0.7112 | F1 0.8024 | Elapsed: 148.53s
WARNING:root: [*] Sun Jan  8 19:32:50 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.473982 | FPR 0.001 -- TPR 0.7169 | F1 0.8071 | Elapsed: 148.35s
WARNING:root: [*] Sun Jan  8 19:35:19 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.306909 | FPR 0.001 -- TPR 0.7230 | F1 0.8121 | Elapsed: 149.00s
WARNING:root: [*] Sun Jan  8 19:37:48 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.371112 | FPR 0.001 -- TPR 0.7262 | F1 0.8149 | Elapsed: 148.86s
WARNING:root: [*] Sun Jan  8 19:40:17 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.471543 | FPR 0.001 -- TPR 0.7305 | F1 0.8185 | Elapsed: 148.70s
WARNING:root: [*] Sun Jan  8 19:42:46 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.327769 | FPR 0.001 -- TPR 0.7351 | F1 0.8219 | Elapsed: 148.88s
WARNING:root: [*] Sun Jan  8 19:45:15 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.506242 | FPR 0.001 -- TPR 0.7388 | F1 0.8249 | Elapsed: 148.95s
WARNING:root: [*] Sun Jan  8 19:47:11 2023:    1    | Tr.loss: 0.365305 | FPR 0.001 -- TPR: 0.74 |  F1: 0.83 | Elapsed:  3538.85 s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  8 19:47:12 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.250925 | FPR 0.001 -- TPR 0.9167 | F1 0.9565 | Elapsed: 1.52s
WARNING:root: [*] Sun Jan  8 19:49:41 2023: Train Epoch: 2 [1600 /38063 (4 %)]	Loss: 0.270500 | FPR 0.001 -- TPR 0.8507 | F1 0.9034 | Elapsed: 148.92s
WARNING:root:[!] Sun Jan  8 19:51:43 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673203903-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673203903-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673203903-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673203903-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673203903-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/2379
WARNING:root: [*] Predicting batch: 100/2379
WARNING:root: [*] Predicting batch: 200/2379
WARNING:root: [*] Predicting batch: 300/2379
WARNING:root: [*] Predicting batch: 400/2379
WARNING:root: [*] Predicting batch: 500/2379
WARNING:root: [*] Predicting batch: 600/2379
WARNING:root: [*] Predicting batch: 700/2379
WARNING:root: [*] Predicting batch: 800/2379
WARNING:root: [*] Predicting batch: 900/2379
WARNING:root: [*] Predicting batch: 1000/2379
WARNING:root: [*] Predicting batch: 1100/2379
WARNING:root: [*] Predicting batch: 1200/2379
WARNING:root: [*] Predicting batch: 1300/2379
WARNING:root: [*] Predicting batch: 1400/2379
WARNING:root: [*] Predicting batch: 1500/2379
WARNING:root: [*] Predicting batch: 1600/2379
WARNING:root: [*] Predicting batch: 1700/2379
WARNING:root: [*] Predicting batch: 1800/2379
WARNING:root: [*] Predicting batch: 1900/2379
WARNING:root: [*] Predicting batch: 2000/2379
WARNING:root: [*] Predicting batch: 2100/2379
WARNING:root: [*] Predicting batch: 2200/2379
WARNING:root: [*] Predicting batch: 2300/2379
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.12448084462213252 | F1: 0.22139380760070412
WARNING:root: [!] FPR: 0.0003 | TPR: 0.24216900205721384 | F1: 0.38987658178409623
WARNING:root: [!] FPR: 0.001 | TPR: 0.487986647517758 | F1: 0.6556966646673794
WARNING:root: [!] FPR: 0.003 | TPR: 0.6073438652330863 | F1: 0.7550547700622499
WARNING:root: [!] FPR: 0.01 | TPR: 0.6913402942203936 | F1: 0.8152047051284984
WARNING:root: [!] FPR: 0.03 | TPR: 0.7454877149400303 | F1: 0.8472363139088623
WARNING:root: [!] FPR: 0.1 | TPR: 0.8564996312541241 | F1: 0.899588242488483
WARNING:root: [!] Fold 2/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  8 20:06:41 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.662454 | FPR 0.001 -- TPR 0.7273 | F1 0.8421 | Elapsed: 1.58s
WARNING:root: [*] Sun Jan  8 20:09:09 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.310337 | FPR 0.001 -- TPR 0.4090 | F1 0.5139 | Elapsed: 148.83s
WARNING:root: [*] Sun Jan  8 20:11:38 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.520161 | FPR 0.001 -- TPR 0.4909 | F1 0.6003 | Elapsed: 148.50s
WARNING:root: [*] Sun Jan  8 20:14:06 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.278393 | FPR 0.001 -- TPR 0.5728 | F1 0.6764 | Elapsed: 148.43s
WARNING:root: [*] Sun Jan  8 20:16:35 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.438340 | FPR 0.001 -- TPR 0.6125 | F1 0.7116 | Elapsed: 148.57s
WARNING:root: [*] Sun Jan  8 20:19:03 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.655045 | FPR 0.001 -- TPR 0.6359 | F1 0.7346 | Elapsed: 148.45s
WARNING:root: [*] Sun Jan  8 20:21:32 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.369503 | FPR 0.001 -- TPR 0.6646 | F1 0.7594 | Elapsed: 148.62s
WARNING:root: [*] Sun Jan  8 20:24:00 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.682636 | FPR 0.001 -- TPR 0.6844 | F1 0.7747 | Elapsed: 148.47s
WARNING:root: [*] Sun Jan  8 20:26:29 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.334660 | FPR 0.001 -- TPR 0.7010 | F1 0.7891 | Elapsed: 148.43s
WARNING:root: [*] Sun Jan  8 20:28:57 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.297256 | FPR 0.001 -- TPR 0.7131 | F1 0.7994 | Elapsed: 148.49s
WARNING:root: [*] Sun Jan  8 20:31:26 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.208388 | FPR 0.001 -- TPR 0.7233 | F1 0.8082 | Elapsed: 148.61s
WARNING:root: [*] Sun Jan  8 20:33:56 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.252718 | FPR 0.001 -- TPR 0.7313 | F1 0.8152 | Elapsed: 150.02s
WARNING:root: [*] Sun Jan  8 20:36:30 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.431075 | FPR 0.001 -- TPR 0.7357 | F1 0.8196 | Elapsed: 153.80s
WARNING:root: [*] Sun Jan  8 20:39:06 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.302781 | FPR 0.001 -- TPR 0.7402 | F1 0.8231 | Elapsed: 156.31s
WARNING:root: [*] Sun Jan  8 20:41:42 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.199618 | FPR 0.001 -- TPR 0.7450 | F1 0.8275 | Elapsed: 156.09s
WARNING:root: [*] Sun Jan  8 20:44:17 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.213496 | FPR 0.001 -- TPR 0.7488 | F1 0.8307 | Elapsed: 154.94s
WARNING:root: [*] Sun Jan  8 20:46:51 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.248003 | FPR 0.001 -- TPR 0.7528 | F1 0.8342 | Elapsed: 153.64s
WARNING:root: [*] Sun Jan  8 20:49:25 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.106300 | FPR 0.001 -- TPR 0.7570 | F1 0.8375 | Elapsed: 153.86s
WARNING:root: [*] Sun Jan  8 20:51:58 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.205299 | FPR 0.001 -- TPR 0.7609 | F1 0.8402 | Elapsed: 153.54s
WARNING:root: [*] Sun Jan  8 20:54:32 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.256879 | FPR 0.001 -- TPR 0.7642 | F1 0.8429 | Elapsed: 153.52s
WARNING:root: [*] Sun Jan  8 20:57:07 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.291313 | FPR 0.001 -- TPR 0.7693 | F1 0.8468 | Elapsed: 155.45s
WARNING:root: [*] Sun Jan  8 20:59:42 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.190230 | FPR 0.001 -- TPR 0.7736 | F1 0.8500 | Elapsed: 154.89s
WARNING:root: [*] Sun Jan  8 21:02:13 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.257189 | FPR 0.001 -- TPR 0.7779 | F1 0.8531 | Elapsed: 150.65s
WARNING:root: [*] Sun Jan  8 21:04:43 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.186459 | FPR 0.001 -- TPR 0.7832 | F1 0.8570 | Elapsed: 150.36s
WARNING:root: [*] Sun Jan  8 21:06:40 2023:    1    | Tr.loss: 0.314217 | FPR 0.001 -- TPR: 0.79 |  F1: 0.86 | Elapsed:  3601.35 s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  8 21:06:42 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.120746 | FPR 0.001 -- TPR 1.0000 | F1 1.0000 | Elapsed: 1.52s
WARNING:root: [*] Sun Jan  8 21:09:15 2023: Train Epoch: 2 [1600 /38063 (4 %)]	Loss: 0.284041 | FPR 0.001 -- TPR 0.8806 | F1 0.9282 | Elapsed: 152.93s
WARNING:root:[!] Sun Jan  8 21:10:36 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673208636-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673208636-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673208636-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673208636-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\trainingFiles\trainingFiles_1673208636-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/2379
WARNING:root: [*] Predicting batch: 100/2379
WARNING:root: [*] Predicting batch: 200/2379
WARNING:root: [*] Predicting batch: 300/2379
WARNING:root: [*] Predicting batch: 400/2379
WARNING:root: [*] Predicting batch: 500/2379
WARNING:root: [*] Predicting batch: 600/2379
WARNING:root: [*] Predicting batch: 700/2379
WARNING:root: [*] Predicting batch: 800/2379
WARNING:root: [*] Predicting batch: 900/2379
WARNING:root: [*] Predicting batch: 1000/2379
WARNING:root: [*] Predicting batch: 1100/2379
WARNING:root: [*] Predicting batch: 1200/2379
WARNING:root: [*] Predicting batch: 1300/2379
WARNING:root: [*] Predicting batch: 1400/2379
WARNING:root: [*] Predicting batch: 1500/2379
WARNING:root: [*] Predicting batch: 1600/2379
WARNING:root: [*] Predicting batch: 1700/2379
WARNING:root: [*] Predicting batch: 1800/2379
WARNING:root: [*] Predicting batch: 1900/2379
WARNING:root: [*] Predicting batch: 2000/2379
WARNING:root: [*] Predicting batch: 2100/2379
WARNING:root: [*] Predicting batch: 2200/2379
WARNING:root: [*] Predicting batch: 2300/2379
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.26509400910895714 | F1: 0.41907692307692307
WARNING:root: [!] FPR: 0.0003 | TPR: 0.3598816614114991 | F1: 0.5292383433036608
WARNING:root: [!] FPR: 0.001 | TPR: 0.5532329012417766 | F1: 0.7121489239095032
WARNING:root: [!] FPR: 0.003 | TPR: 0.6052785238818171 | F1: 0.7534342822531799
WARNING:root: [!] FPR: 0.01 | TPR: 0.6967962941336759 | F1: 0.8189970717423134
WARNING:root: [!] FPR: 0.03 | TPR: 0.8171590953326326 | F1: 0.8922893819603843
WARNING:root: [!] FPR: 0.1 | TPR: 0.9368990618552688 | F1: 0.9439541906891007
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_MeanOverSeq\metrics_trainSize_76126_ep_2_cv_2_vocabSize_10000_maxLen_2048_dim_64_heads_4_depth_4_meanOverSequence_True_hiddenNeurons_64.json
WARNING:root: [!] Average epoch time: 1785.05s | Mean values over 2 folds:
	FPR: 0.0001 -- TPR: 0.1948 -- F1: 0.3202
	FPR: 0.0003 -- TPR: 0.3010 -- F1: 0.4596
	FPR:  0.001 -- TPR: 0.5206 -- F1: 0.6839
	FPR:  0.003 -- TPR: 0.6063 -- F1: 0.7542
	FPR:   0.01 -- TPR: 0.6941 -- F1: 0.8171
	FPR:   0.03 -- TPR: 0.7813 -- F1: 0.8698
	FPR:    0.1 -- TPR: 0.8967 -- F1: 0.9218

