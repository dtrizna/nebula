WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 2 | Model config: {'vocabSize': 10000, 'maxLen': 2048, 'dim': 64, 'heads': 4, 'depth': 4, 'hiddenNeurons': [64]}
WARNING:root: [!] Fold 1/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  8 15:45:58 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.717358 | FPR 0.001 -- TPR 0.6364 | F1 0.7778 | Elapsed: 3.17s
WARNING:root: [*] Sun Jan  8 15:48:28 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.439058 | FPR 0.001 -- TPR 0.5812 | F1 0.6975 | Elapsed: 149.93s
WARNING:root: [*] Sun Jan  8 15:50:57 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.565840 | FPR 0.001 -- TPR 0.6359 | F1 0.7425 | Elapsed: 149.15s
WARNING:root: [*] Sun Jan  8 15:53:26 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.486659 | FPR 0.001 -- TPR 0.6409 | F1 0.7498 | Elapsed: 148.79s
WARNING:root: [*] Sun Jan  8 15:55:55 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.353463 | FPR 0.001 -- TPR 0.6587 | F1 0.7655 | Elapsed: 148.99s
WARNING:root: [*] Sun Jan  8 15:58:24 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.277386 | FPR 0.001 -- TPR 0.6784 | F1 0.7824 | Elapsed: 148.90s
WARNING:root: [*] Sun Jan  8 16:00:53 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.289920 | FPR 0.001 -- TPR 0.6921 | F1 0.7938 | Elapsed: 148.88s
WARNING:root: [*] Sun Jan  8 16:03:22 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.265103 | FPR 0.001 -- TPR 0.6997 | F1 0.7994 | Elapsed: 148.95s
WARNING:root: [*] Sun Jan  8 16:05:51 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.177960 | FPR 0.001 -- TPR 0.7120 | F1 0.8093 | Elapsed: 149.00s
WARNING:root: [*] Sun Jan  8 16:08:20 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.274319 | FPR 0.001 -- TPR 0.7190 | F1 0.8140 | Elapsed: 148.89s
WARNING:root: [*] Sun Jan  8 16:10:49 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.269196 | FPR 0.001 -- TPR 0.7280 | F1 0.8210 | Elapsed: 148.93s
WARNING:root: [*] Sun Jan  8 16:13:18 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.380450 | FPR 0.001 -- TPR 0.7324 | F1 0.8240 | Elapsed: 149.03s
WARNING:root: [*] Sun Jan  8 16:15:33 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.167830 | FPR 0.001 -- TPR 0.7390 | F1 0.8294 | Elapsed: 135.86s
WARNING:root: [*] Sun Jan  8 16:18:03 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.228396 | FPR 0.001 -- TPR 0.7426 | F1 0.8315 | Elapsed: 149.97s
WARNING:root: [*] Sun Jan  8 16:20:33 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.141577 | FPR 0.001 -- TPR 0.7488 | F1 0.8365 | Elapsed: 149.79s
WARNING:root: [*] Sun Jan  8 16:23:03 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.515654 | FPR 0.001 -- TPR 0.7539 | F1 0.8404 | Elapsed: 149.80s
WARNING:root: [*] Sun Jan  8 16:25:37 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.318961 | FPR 0.001 -- TPR 0.7549 | F1 0.8415 | Elapsed: 153.60s
WARNING:root: [*] Sun Jan  8 16:28:08 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.183213 | FPR 0.001 -- TPR 0.7584 | F1 0.8443 | Elapsed: 150.94s
WARNING:root: [*] Sun Jan  8 16:30:39 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.378502 | FPR 0.001 -- TPR 0.7622 | F1 0.8471 | Elapsed: 151.35s
WARNING:root: [*] Sun Jan  8 16:33:08 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.230489 | FPR 0.001 -- TPR 0.7656 | F1 0.8498 | Elapsed: 149.40s
WARNING:root: [*] Sun Jan  8 16:35:43 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.267613 | FPR 0.001 -- TPR 0.7710 | F1 0.8531 | Elapsed: 154.74s
WARNING:root: [*] Sun Jan  8 16:38:13 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.378228 | FPR 0.001 -- TPR 0.7739 | F1 0.8551 | Elapsed: 150.09s
WARNING:root: [*] Sun Jan  8 16:40:44 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.413734 | FPR 0.001 -- TPR 0.7786 | F1 0.8585 | Elapsed: 151.16s
WARNING:root: [*] Sun Jan  8 16:43:15 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.205034 | FPR 0.001 -- TPR 0.7835 | F1 0.8619 | Elapsed: 150.49s
WARNING:root: [*] Sun Jan  8 16:45:12 2023:    1    | Tr.loss: 0.317624 | FPR 0.001 -- TPR: 0.79 |  F1: 0.86 | Elapsed:  3556.69 s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  8 16:45:13 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.320105 | FPR 0.001 -- TPR 0.6364 | F1 0.7778 | Elapsed: 1.51s
WARNING:root:[!] Sun Jan  8 16:45:33 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR2.5e4_VocabSize_maxLen\trainingFiles\trainingFiles_1673192733-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR2.5e4_VocabSize_maxLen\trainingFiles\trainingFiles_1673192733-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR2.5e4_VocabSize_maxLen\trainingFiles\trainingFiles_1673192733-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR2.5e4_VocabSize_maxLen\trainingFiles\trainingFiles_1673192733-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR2.5e4_VocabSize_maxLen\trainingFiles\trainingFiles_1673192733-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/2379
WARNING:root: [*] Predicting batch: 100/2379
WARNING:root: [*] Predicting batch: 200/2379
WARNING:root: [*] Predicting batch: 300/2379
WARNING:root: [*] Predicting batch: 400/2379
WARNING:root: [*] Predicting batch: 500/2379
WARNING:root: [*] Predicting batch: 600/2379
WARNING:root: [*] Predicting batch: 700/2379
WARNING:root: [*] Predicting batch: 800/2379
WARNING:root: [*] Predicting batch: 900/2379
WARNING:root: [*] Predicting batch: 1000/2379
WARNING:root: [*] Predicting batch: 1100/2379
WARNING:root: [*] Predicting batch: 1200/2379
WARNING:root: [*] Predicting batch: 1300/2379
WARNING:root: [*] Predicting batch: 1400/2379
WARNING:root: [*] Predicting batch: 1500/2379
WARNING:root: [*] Predicting batch: 1600/2379
WARNING:root: [*] Predicting batch: 1700/2379
WARNING:root: [*] Predicting batch: 1800/2379
WARNING:root: [*] Predicting batch: 1900/2379
WARNING:root: [*] Predicting batch: 2000/2379
WARNING:root: [*] Predicting batch: 2100/2379
WARNING:root: [*] Predicting batch: 2200/2379
WARNING:root: [*] Predicting batch: 2300/2379
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.307223537631487 | F1: 0.4700397885860206
WARNING:root: [!] FPR: 0.0003 | TPR: 0.40383495710903233 | F1: 0.575283384019906
WARNING:root: [!] FPR: 0.001 | TPR: 0.6036952218297559 | F1: 0.7526616337591948
WARNING:root: [!] FPR: 0.003 | TPR: 0.7068664363622249 | F1: 0.8275846398545785
WARNING:root: [!] FPR: 0.01 | TPR: 0.8653107169196134 | F1: 0.9254239398908237
WARNING:root: [!] FPR: 0.03 | TPR: 0.901331366688662 | F1: 0.9410167568334245
WARNING:root: [!] FPR: 0.1 | TPR: 0.9526840818227691 | F1: 0.9524807419911908
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR2.5e4_VocabSize_maxLen\metrics_trainSize_76126_ep_2_cv_2_vocabSize_10000_maxLen_2048_dim_64_heads_4_depth_4_hiddenNeurons_64.json
WARNING:root: [!] Average epoch time: 1778.35s | Mean values over 2 folds:
	FPR: 0.0001 -- TPR: 0.3072 -- F1: 0.4700
	FPR: 0.0003 -- TPR: 0.4038 -- F1: 0.5753
	FPR:  0.001 -- TPR: 0.6037 -- F1: 0.7527
	FPR:  0.003 -- TPR: 0.7069 -- F1: 0.8276
	FPR:   0.01 -- TPR: 0.8653 -- F1: 0.9254
	FPR:   0.03 -- TPR: 0.9013 -- F1: 0.9410
	FPR:    0.1 -- TPR: 0.9527 -- F1: 0.9525

