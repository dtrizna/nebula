WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 2 | Model config: {'vocabSize': 10000, 'maxLen': 2048, 'dim': 64, 'heads': 4, 'depth': 4, 'meanOverSequence': True, 'hiddenNeurons': [64]}
WARNING:root: [!] Fold 1/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Tue Jan 10 16:37:49 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.750942 | FPR 0.001 -- TPR 0.0000 | F1 0.0000 | Elapsed: 3.82s
WARNING:root: [*] Tue Jan 10 16:40:21 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.676663 | FPR 0.001 -- TPR 0.2187 | F1 0.3137 | Elapsed: 152.74s
WARNING:root: [*] Tue Jan 10 16:42:50 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.482912 | FPR 0.001 -- TPR 0.3444 | F1 0.4506 | Elapsed: 148.39s
WARNING:root: [*] Tue Jan 10 16:45:18 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.520491 | FPR 0.001 -- TPR 0.4311 | F1 0.5380 | Elapsed: 148.69s
WARNING:root: [*] Tue Jan 10 16:47:47 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.385977 | FPR 0.001 -- TPR 0.4796 | F1 0.5865 | Elapsed: 148.70s
WARNING:root:[!] Learning rate: 2.5e-05
WARNING:root: [*] Tue Jan 10 16:50:16 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.402954 | FPR 0.001 -- TPR 0.5123 | F1 0.6202 | Elapsed: 148.47s
WARNING:root: [*] Tue Jan 10 16:52:44 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.188476 | FPR 0.001 -- TPR 0.5407 | F1 0.6464 | Elapsed: 148.50s
WARNING:root: [*] Tue Jan 10 16:55:13 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.408914 | FPR 0.001 -- TPR 0.5649 | F1 0.6679 | Elapsed: 148.44s
WARNING:root: [*] Tue Jan 10 16:57:41 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.394944 | FPR 0.001 -- TPR 0.5824 | F1 0.6847 | Elapsed: 148.41s
WARNING:root: [*] Tue Jan 10 17:00:09 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.356840 | FPR 0.001 -- TPR 0.5927 | F1 0.6939 | Elapsed: 148.30s
WARNING:root:[!] Learning rate: 2.5e-06
WARNING:root: [*] Tue Jan 10 17:02:37 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.445689 | FPR 0.001 -- TPR 0.5998 | F1 0.7011 | Elapsed: 148.26s
WARNING:root: [*] Tue Jan 10 17:05:06 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.296369 | FPR 0.001 -- TPR 0.6118 | F1 0.7121 | Elapsed: 148.71s
WARNING:root: [*] Tue Jan 10 17:07:35 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.238245 | FPR 0.001 -- TPR 0.6192 | F1 0.7193 | Elapsed: 148.77s
WARNING:root: [*] Tue Jan 10 17:10:04 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.345915 | FPR 0.001 -- TPR 0.6245 | F1 0.7245 | Elapsed: 148.93s
WARNING:root: [*] Tue Jan 10 17:12:33 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.327636 | FPR 0.001 -- TPR 0.6320 | F1 0.7303 | Elapsed: 148.76s
WARNING:root:[!] Learning rate: 2.5000000000000004e-07
WARNING:root: [*] Tue Jan 10 17:15:01 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.387467 | FPR 0.001 -- TPR 0.6377 | F1 0.7352 | Elapsed: 148.71s
WARNING:root: [*] Tue Jan 10 17:17:30 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.433413 | FPR 0.001 -- TPR 0.6407 | F1 0.7378 | Elapsed: 148.78s
WARNING:root: [*] Tue Jan 10 17:19:59 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.145751 | FPR 0.001 -- TPR 0.6443 | F1 0.7414 | Elapsed: 148.81s
WARNING:root: [*] Tue Jan 10 17:22:28 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.318703 | FPR 0.001 -- TPR 0.6468 | F1 0.7434 | Elapsed: 148.75s
WARNING:root: [*] Tue Jan 10 17:24:56 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.560957 | FPR 0.001 -- TPR 0.6483 | F1 0.7453 | Elapsed: 148.74s
WARNING:root:[!] Learning rate: 2.5000000000000005e-08
WARNING:root: [*] Tue Jan 10 17:27:25 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.396143 | FPR 0.001 -- TPR 0.6529 | F1 0.7495 | Elapsed: 148.47s
WARNING:root: [*] Tue Jan 10 17:29:54 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.435419 | FPR 0.001 -- TPR 0.6545 | F1 0.7511 | Elapsed: 148.68s
WARNING:root: [*] Tue Jan 10 17:32:22 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.592536 | FPR 0.001 -- TPR 0.6567 | F1 0.7529 | Elapsed: 148.61s
WARNING:root: [*] Tue Jan 10 17:34:51 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.448730 | FPR 0.001 -- TPR 0.6572 | F1 0.7532 | Elapsed: 148.64s
WARNING:root: [*] Tue Jan 10 17:36:47 2023:    1    | Tr.loss: 0.394117 | FPR 0.001 -- TPR: 0.66 |  F1: 0.75 | Elapsed:  3541.91 s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Tue Jan 10 17:36:48 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.337621 | FPR 0.001 -- TPR 0.6154 | F1 0.7619 | Elapsed: 1.48s
WARNING:root: [*] Tue Jan 10 17:39:17 2023: Train Epoch: 2 [1600 /38063 (4 %)]	Loss: 0.485132 | FPR 0.001 -- TPR 0.7164 | F1 0.7932 | Elapsed: 148.69s
WARNING:root:[!] Learning rate: 2.500000000000001e-09
WARNING:root: [*] Tue Jan 10 17:41:46 2023: Train Epoch: 2 [3200 /38063 (8 %)]	Loss: 0.328704 | FPR 0.001 -- TPR 0.7020 | F1 0.7861 | Elapsed: 148.66s
WARNING:root: [*] Tue Jan 10 17:44:14 2023: Train Epoch: 2 [4800 /38063 (13%)]	Loss: 0.518730 | FPR 0.001 -- TPR 0.7022 | F1 0.7878 | Elapsed: 148.72s
WARNING:root: [*] Tue Jan 10 17:46:43 2023: Train Epoch: 2 [6400 /38063 (17%)]	Loss: 0.315471 | FPR 0.001 -- TPR 0.6950 | F1 0.7844 | Elapsed: 148.60s
WARNING:root: [*] Tue Jan 10 17:49:11 2023: Train Epoch: 2 [8000 /38063 (21%)]	Loss: 0.302246 | FPR 0.001 -- TPR 0.6948 | F1 0.7838 | Elapsed: 148.53s
WARNING:root: [*] Tue Jan 10 17:51:40 2023: Train Epoch: 2 [9600 /38063 (25%)]	Loss: 0.380688 | FPR 0.001 -- TPR 0.6964 | F1 0.7861 | Elapsed: 148.61s
WARNING:root:[!] Learning rate: 2.500000000000001e-10
WARNING:root: [*] Tue Jan 10 17:54:09 2023: Train Epoch: 2 [11200/38063 (29%)]	Loss: 0.376833 | FPR 0.001 -- TPR 0.6959 | F1 0.7872 | Elapsed: 148.79s
WARNING:root: [*] Tue Jan 10 17:56:37 2023: Train Epoch: 2 [12800/38063 (34%)]	Loss: 0.215554 | FPR 0.001 -- TPR 0.6951 | F1 0.7868 | Elapsed: 148.60s
WARNING:root: [*] Tue Jan 10 17:59:06 2023: Train Epoch: 2 [14400/38063 (38%)]	Loss: 0.307716 | FPR 0.001 -- TPR 0.6982 | F1 0.7901 | Elapsed: 148.59s
WARNING:root: [*] Tue Jan 10 18:01:35 2023: Train Epoch: 2 [16000/38063 (42%)]	Loss: 0.543430 | FPR 0.001 -- TPR 0.6992 | F1 0.7910 | Elapsed: 148.63s
WARNING:root: [*] Tue Jan 10 18:04:03 2023: Train Epoch: 2 [17600/38063 (46%)]	Loss: 0.455916 | FPR 0.001 -- TPR 0.7031 | F1 0.7925 | Elapsed: 148.72s
WARNING:root:[!] Learning rate: 2.5000000000000014e-11
WARNING:root: [*] Tue Jan 10 18:06:32 2023: Train Epoch: 2 [19200/38063 (50%)]	Loss: 0.552690 | FPR 0.001 -- TPR 0.7029 | F1 0.7921 | Elapsed: 148.41s
WARNING:root: [*] Tue Jan 10 18:09:00 2023: Train Epoch: 2 [20800/38063 (55%)]	Loss: 0.255974 | FPR 0.001 -- TPR 0.7028 | F1 0.7923 | Elapsed: 148.38s
WARNING:root: [*] Tue Jan 10 18:11:28 2023: Train Epoch: 2 [22400/38063 (59%)]	Loss: 0.413814 | FPR 0.001 -- TPR 0.7027 | F1 0.7925 | Elapsed: 148.28s
WARNING:root: [*] Tue Jan 10 18:13:57 2023: Train Epoch: 2 [24000/38063 (63%)]	Loss: 0.499207 | FPR 0.001 -- TPR 0.7013 | F1 0.7902 | Elapsed: 148.38s
WARNING:root: [*] Tue Jan 10 18:16:25 2023: Train Epoch: 2 [25600/38063 (67%)]	Loss: 0.315295 | FPR 0.001 -- TPR 0.7048 | F1 0.7938 | Elapsed: 148.24s
WARNING:root:[!] Learning rate: 2.5000000000000015e-12
WARNING:root: [*] Tue Jan 10 18:18:53 2023: Train Epoch: 2 [27200/38063 (71%)]	Loss: 0.326223 | FPR 0.001 -- TPR 0.7034 | F1 0.7932 | Elapsed: 148.29s
WARNING:root: [*] Tue Jan 10 18:21:22 2023: Train Epoch: 2 [28800/38063 (76%)]	Loss: 0.467431 | FPR 0.001 -- TPR 0.7031 | F1 0.7930 | Elapsed: 148.33s
WARNING:root: [*] Tue Jan 10 18:23:50 2023: Train Epoch: 2 [30400/38063 (80%)]	Loss: 0.310389 | FPR 0.001 -- TPR 0.7050 | F1 0.7948 | Elapsed: 148.31s
WARNING:root: [*] Tue Jan 10 18:26:19 2023: Train Epoch: 2 [32000/38063 (84%)]	Loss: 0.326513 | FPR 0.001 -- TPR 0.7042 | F1 0.7942 | Elapsed: 148.79s
WARNING:root: [*] Tue Jan 10 18:28:47 2023: Train Epoch: 2 [33600/38063 (88%)]	Loss: 0.283871 | FPR 0.001 -- TPR 0.7054 | F1 0.7954 | Elapsed: 148.59s
WARNING:root:[!] Learning rate: 2.5000000000000015e-13
WARNING:root: [*] Tue Jan 10 18:31:16 2023: Train Epoch: 2 [35200/38063 (92%)]	Loss: 0.458456 | FPR 0.001 -- TPR 0.7066 | F1 0.7967 | Elapsed: 148.76s
WARNING:root: [*] Tue Jan 10 18:33:45 2023: Train Epoch: 2 [36800/38063 (97%)]	Loss: 0.445152 | FPR 0.001 -- TPR 0.7075 | F1 0.7976 | Elapsed: 148.89s
WARNING:root: [*] Tue Jan 10 18:35:41 2023:    2    | Tr.loss: 0.365676 | FPR 0.001 -- TPR: 0.71 |  F1: 0.80 | Elapsed:  3534.18 s
WARNING:root:[!] Tue Jan 10 18:35:41 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673372141-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673372141-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673372141-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673372141-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673372141-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/2379
WARNING:root: [*] Predicting batch: 100/2379
WARNING:root: [*] Predicting batch: 200/2379
WARNING:root: [*] Predicting batch: 300/2379
WARNING:root: [*] Predicting batch: 400/2379
WARNING:root: [*] Predicting batch: 500/2379
WARNING:root: [*] Predicting batch: 600/2379
WARNING:root: [*] Predicting batch: 700/2379
WARNING:root: [*] Predicting batch: 800/2379
WARNING:root: [*] Predicting batch: 900/2379
WARNING:root: [*] Predicting batch: 1000/2379
WARNING:root: [*] Predicting batch: 1100/2379
WARNING:root: [*] Predicting batch: 1200/2379
WARNING:root: [*] Predicting batch: 1300/2379
WARNING:root: [*] Predicting batch: 1400/2379
WARNING:root: [*] Predicting batch: 1500/2379
WARNING:root: [*] Predicting batch: 1600/2379
WARNING:root: [*] Predicting batch: 1700/2379
WARNING:root: [*] Predicting batch: 1800/2379
WARNING:root: [*] Predicting batch: 1900/2379
WARNING:root: [*] Predicting batch: 2000/2379
WARNING:root: [*] Predicting batch: 2100/2379
WARNING:root: [*] Predicting batch: 2200/2379
WARNING:root: [*] Predicting batch: 2300/2379
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.08372472149982534 | F1: 0.15451289398280804
WARNING:root: [!] FPR: 0.0003 | TPR: 0.08671350386212785 | F1: 0.15957142857142856
WARNING:root: [!] FPR: 0.001 | TPR: 0.09599037379187206 | F1: 0.1750920419144718
WARNING:root: [!] FPR: 0.003 | TPR: 0.19089391763381594 | F1: 0.3202135625223817
WARNING:root: [!] FPR: 0.01 | TPR: 0.28086791134572836 | F1: 0.4369565217391304
WARNING:root: [!] FPR: 0.03 | TPR: 0.5735356907192485 | F1: 0.7224014862618557
WARNING:root: [!] FPR: 0.1 | TPR: 0.7039941000659861 | F1: 0.8037668956348326
WARNING:root: [!] Fold 2/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Tue Jan 10 18:50:36 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.760582 | FPR 0.001 -- TPR 0.2500 | F1 0.4000 | Elapsed: 1.53s
WARNING:root: [*] Tue Jan 10 18:53:05 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.621425 | FPR 0.001 -- TPR 0.2629 | F1 0.3661 | Elapsed: 148.70s
WARNING:root: [*] Tue Jan 10 18:55:33 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.521762 | FPR 0.001 -- TPR 0.3933 | F1 0.4984 | Elapsed: 148.75s
WARNING:root: [*] Tue Jan 10 18:58:02 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.476216 | FPR 0.001 -- TPR 0.4710 | F1 0.5771 | Elapsed: 148.64s
WARNING:root: [*] Tue Jan 10 19:00:31 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.292891 | FPR 0.001 -- TPR 0.5197 | F1 0.6208 | Elapsed: 148.54s
WARNING:root:[!] Learning rate: 2.5e-05
WARNING:root: [*] Tue Jan 10 19:02:59 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.521786 | FPR 0.001 -- TPR 0.5524 | F1 0.6543 | Elapsed: 148.36s
WARNING:root: [*] Tue Jan 10 19:05:27 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.283501 | FPR 0.001 -- TPR 0.5748 | F1 0.6767 | Elapsed: 148.41s
WARNING:root: [*] Tue Jan 10 19:07:56 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.230189 | FPR 0.001 -- TPR 0.5991 | F1 0.6995 | Elapsed: 148.29s
WARNING:root: [*] Tue Jan 10 19:10:24 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.379688 | FPR 0.001 -- TPR 0.6132 | F1 0.7124 | Elapsed: 148.54s
WARNING:root: [*] Tue Jan 10 19:12:53 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.138981 | FPR 0.001 -- TPR 0.6299 | F1 0.7276 | Elapsed: 148.78s
WARNING:root:[!] Learning rate: 2.5e-06
WARNING:root: [*] Tue Jan 10 19:15:22 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.405784 | FPR 0.001 -- TPR 0.6439 | F1 0.7402 | Elapsed: 148.66s
WARNING:root: [*] Tue Jan 10 19:17:50 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.294833 | FPR 0.001 -- TPR 0.6543 | F1 0.7496 | Elapsed: 148.68s
WARNING:root: [*] Tue Jan 10 19:20:19 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.563777 | FPR 0.001 -- TPR 0.6634 | F1 0.7583 | Elapsed: 148.78s
WARNING:root: [*] Tue Jan 10 19:22:48 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.218127 | FPR 0.001 -- TPR 0.6698 | F1 0.7641 | Elapsed: 148.79s
WARNING:root: [*] Tue Jan 10 19:25:16 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.379678 | FPR 0.001 -- TPR 0.6760 | F1 0.7697 | Elapsed: 148.65s
WARNING:root:[!] Learning rate: 2.5000000000000004e-07
WARNING:root: [*] Tue Jan 10 19:27:45 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.286975 | FPR 0.001 -- TPR 0.6823 | F1 0.7753 | Elapsed: 148.76s
WARNING:root: [*] Tue Jan 10 19:30:14 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.430617 | FPR 0.001 -- TPR 0.6868 | F1 0.7798 | Elapsed: 148.78s
WARNING:root: [*] Tue Jan 10 19:32:43 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.255222 | FPR 0.001 -- TPR 0.6918 | F1 0.7840 | Elapsed: 148.73s
WARNING:root: [*] Tue Jan 10 19:35:12 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.338649 | FPR 0.001 -- TPR 0.6949 | F1 0.7870 | Elapsed: 148.86s
WARNING:root: [*] Tue Jan 10 19:37:41 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.257357 | FPR 0.001 -- TPR 0.6958 | F1 0.7885 | Elapsed: 149.02s
WARNING:root:[!] Learning rate: 2.5000000000000005e-08
WARNING:root: [*] Tue Jan 10 19:40:10 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.398800 | FPR 0.001 -- TPR 0.7003 | F1 0.7919 | Elapsed: 149.33s
WARNING:root: [*] Tue Jan 10 19:42:39 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.192422 | FPR 0.001 -- TPR 0.7012 | F1 0.7928 | Elapsed: 148.65s
WARNING:root: [*] Tue Jan 10 19:45:07 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.448323 | FPR 0.001 -- TPR 0.7039 | F1 0.7952 | Elapsed: 148.75s
WARNING:root: [*] Tue Jan 10 19:47:36 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.352731 | FPR 0.001 -- TPR 0.7063 | F1 0.7974 | Elapsed: 148.77s
WARNING:root: [*] Tue Jan 10 19:49:32 2023:    1    | Tr.loss: 0.372052 | FPR 0.001 -- TPR: 0.71 |  F1: 0.80 | Elapsed:  3537.74 s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Tue Jan 10 19:49:34 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.520215 | FPR 0.001 -- TPR 0.7273 | F1 0.8421 | Elapsed: 1.48s
WARNING:root: [*] Tue Jan 10 19:52:02 2023: Train Epoch: 2 [1600 /38063 (4 %)]	Loss: 0.730285 | FPR 0.001 -- TPR 0.7205 | F1 0.8167 | Elapsed: 148.50s
WARNING:root:[!] Learning rate: 2.500000000000001e-09
WARNING:root: [*] Tue Jan 10 19:54:31 2023: Train Epoch: 2 [3200 /38063 (8 %)]	Loss: 0.286285 | FPR 0.001 -- TPR 0.7369 | F1 0.8280 | Elapsed: 148.51s
WARNING:root: [*] Tue Jan 10 19:56:59 2023: Train Epoch: 2 [4800 /38063 (13%)]	Loss: 0.235763 | FPR 0.001 -- TPR 0.7327 | F1 0.8261 | Elapsed: 148.53s
WARNING:root: [*] Tue Jan 10 19:59:28 2023: Train Epoch: 2 [6400 /38063 (17%)]	Loss: 0.215964 | FPR 0.001 -- TPR 0.7437 | F1 0.8344 | Elapsed: 148.65s
WARNING:root: [*] Tue Jan 10 20:01:56 2023: Train Epoch: 2 [8000 /38063 (21%)]	Loss: 0.236479 | FPR 0.001 -- TPR 0.7417 | F1 0.8326 | Elapsed: 148.45s
WARNING:root: [*] Tue Jan 10 20:04:25 2023: Train Epoch: 2 [9600 /38063 (25%)]	Loss: 0.327112 | FPR 0.001 -- TPR 0.7439 | F1 0.8352 | Elapsed: 148.52s
WARNING:root:[!] Learning rate: 2.500000000000001e-10
WARNING:root: [*] Tue Jan 10 20:06:53 2023: Train Epoch: 2 [11200/38063 (29%)]	Loss: 0.402859 | FPR 0.001 -- TPR 0.7442 | F1 0.8351 | Elapsed: 148.51s
WARNING:root: [*] Tue Jan 10 20:09:22 2023: Train Epoch: 2 [12800/38063 (34%)]	Loss: 0.252665 | FPR 0.001 -- TPR 0.7460 | F1 0.8366 | Elapsed: 148.85s
WARNING:root: [*] Tue Jan 10 20:11:51 2023: Train Epoch: 2 [14400/38063 (38%)]	Loss: 0.231988 | FPR 0.001 -- TPR 0.7429 | F1 0.8338 | Elapsed: 148.50s
WARNING:root: [*] Tue Jan 10 20:14:19 2023: Train Epoch: 2 [16000/38063 (42%)]	Loss: 0.245350 | FPR 0.001 -- TPR 0.7457 | F1 0.8351 | Elapsed: 148.42s
WARNING:root: [*] Tue Jan 10 20:16:48 2023: Train Epoch: 2 [17600/38063 (46%)]	Loss: 0.267815 | FPR 0.001 -- TPR 0.7477 | F1 0.8365 | Elapsed: 148.55s
WARNING:root:[!] Learning rate: 2.5000000000000014e-11
WARNING:root: [*] Tue Jan 10 20:19:16 2023: Train Epoch: 2 [19200/38063 (50%)]	Loss: 0.278857 | FPR 0.001 -- TPR 0.7490 | F1 0.8370 | Elapsed: 148.66s
WARNING:root: [*] Tue Jan 10 20:21:45 2023: Train Epoch: 2 [20800/38063 (55%)]	Loss: 0.194712 | FPR 0.001 -- TPR 0.7478 | F1 0.8363 | Elapsed: 148.72s
WARNING:root: [*] Tue Jan 10 20:24:14 2023: Train Epoch: 2 [22400/38063 (59%)]	Loss: 0.683614 | FPR 0.001 -- TPR 0.7476 | F1 0.8358 | Elapsed: 148.62s
WARNING:root: [*] Tue Jan 10 20:26:42 2023: Train Epoch: 2 [24000/38063 (63%)]	Loss: 0.254611 | FPR 0.001 -- TPR 0.7491 | F1 0.8373 | Elapsed: 148.93s
WARNING:root: [*] Tue Jan 10 20:29:11 2023: Train Epoch: 2 [25600/38063 (67%)]	Loss: 0.460563 | FPR 0.001 -- TPR 0.7500 | F1 0.8380 | Elapsed: 148.87s
WARNING:root:[!] Learning rate: 2.5000000000000015e-12
WARNING:root: [*] Tue Jan 10 20:31:40 2023: Train Epoch: 2 [27200/38063 (71%)]	Loss: 0.613027 | FPR 0.001 -- TPR 0.7512 | F1 0.8386 | Elapsed: 148.92s
WARNING:root: [*] Tue Jan 10 20:34:09 2023: Train Epoch: 2 [28800/38063 (76%)]	Loss: 0.289899 | FPR 0.001 -- TPR 0.7509 | F1 0.8382 | Elapsed: 148.81s
WARNING:root: [*] Tue Jan 10 20:36:38 2023: Train Epoch: 2 [30400/38063 (80%)]	Loss: 0.547434 | FPR 0.001 -- TPR 0.7515 | F1 0.8387 | Elapsed: 148.99s
WARNING:root: [*] Tue Jan 10 20:39:07 2023: Train Epoch: 2 [32000/38063 (84%)]	Loss: 0.201430 | FPR 0.001 -- TPR 0.7501 | F1 0.8378 | Elapsed: 148.95s
WARNING:root: [*] Tue Jan 10 20:41:36 2023: Train Epoch: 2 [33600/38063 (88%)]	Loss: 0.528977 | FPR 0.001 -- TPR 0.7496 | F1 0.8375 | Elapsed: 149.17s
WARNING:root:[!] Learning rate: 2.5000000000000015e-13
WARNING:root: [*] Tue Jan 10 20:44:05 2023: Train Epoch: 2 [35200/38063 (92%)]	Loss: 0.228228 | FPR 0.001 -- TPR 0.7506 | F1 0.8382 | Elapsed: 148.76s
WARNING:root: [*] Tue Jan 10 20:46:34 2023: Train Epoch: 2 [36800/38063 (97%)]	Loss: 0.436775 | FPR 0.001 -- TPR 0.7513 | F1 0.8383 | Elapsed: 148.87s
WARNING:root: [*] Tue Jan 10 20:48:30 2023:    2    | Tr.loss: 0.342356 | FPR 0.001 -- TPR: 0.75 |  F1: 0.84 | Elapsed:  3537.84 s
WARNING:root:[!] Tue Jan 10 20:48:30 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673380110-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673380110-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673380110-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673380110-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\trainingFiles\trainingFiles_1673380110-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/2379
WARNING:root: [*] Predicting batch: 100/2379
WARNING:root: [*] Predicting batch: 200/2379
WARNING:root: [*] Predicting batch: 300/2379
WARNING:root: [*] Predicting batch: 400/2379
WARNING:root: [*] Predicting batch: 500/2379
WARNING:root: [*] Predicting batch: 600/2379
WARNING:root: [*] Predicting batch: 700/2379
WARNING:root: [*] Predicting batch: 800/2379
WARNING:root: [*] Predicting batch: 900/2379
WARNING:root: [*] Predicting batch: 1000/2379
WARNING:root: [*] Predicting batch: 1100/2379
WARNING:root: [*] Predicting batch: 1200/2379
WARNING:root: [*] Predicting batch: 1300/2379
WARNING:root: [*] Predicting batch: 1400/2379
WARNING:root: [*] Predicting batch: 1500/2379
WARNING:root: [*] Predicting batch: 1600/2379
WARNING:root: [*] Predicting batch: 1700/2379
WARNING:root: [*] Predicting batch: 1800/2379
WARNING:root: [*] Predicting batch: 1900/2379
WARNING:root: [*] Predicting batch: 2000/2379
WARNING:root: [*] Predicting batch: 2100/2379
WARNING:root: [*] Predicting batch: 2200/2379
WARNING:root: [*] Predicting batch: 2300/2379
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.2444236832885671 | F1: 0.3928180424786512
WARNING:root: [!] FPR: 0.0003 | TPR: 0.32317334267585346 | F1: 0.48843913631817376
WARNING:root: [!] FPR: 0.001 | TPR: 0.3662657168437853 | F1: 0.5359726573625747
WARNING:root: [!] FPR: 0.003 | TPR: 0.5019658219471369 | F1: 0.6677714196939488
WARNING:root: [!] FPR: 0.01 | TPR: 0.5755770952547783 | F1: 0.7284102665155918
WARNING:root: [!] FPR: 0.03 | TPR: 0.6487990968897194 | F1: 0.7801624265686802
WARNING:root: [!] FPR: 0.1 | TPR: 0.789754369574526 | F1: 0.8594243111007561
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_LR_tests\metrics_trainSize_76126_ep_2_cv_2_stepLR500_vocabSize_10000_maxLen_2048_dim_64_heads_4_depth_4_meanOverSequence_True_hiddenNeurons_64.json
WARNING:root: [!] Average epoch time: 3537.92s | Mean values over 2 folds:
	FPR: 0.0001 -- TPR: 0.1641 -- F1: 0.2737
	FPR: 0.0003 -- TPR: 0.2049 -- F1: 0.3240
	FPR:  0.001 -- TPR: 0.2311 -- F1: 0.3555
	FPR:  0.003 -- TPR: 0.3464 -- F1: 0.4940
	FPR:   0.01 -- TPR: 0.4282 -- F1: 0.5827
	FPR:   0.03 -- TPR: 0.6112 -- F1: 0.7513
	FPR:    0.1 -- TPR: 0.7469 -- F1: 0.8316

