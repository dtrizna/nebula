WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 2 | Model config: {'vocabSize': 10000, 'maxLen': 2048, 'dim': 64, 'heads': 4, 'depth': 4, 'hiddenNeurons': [64]}
WARNING:root: [!] Fold 1/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  8 07:14:08 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.811949 | FPR 0.001 -- TPR 0.0000 | F1 0.0000 | Elapsed: 4.90s
WARNING:root: [*] Sun Jan  8 07:16:39 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.393507 | FPR 0.001 -- TPR 0.6364 | F1 0.7778 | Elapsed: 150.27s
WARNING:root: [*] Sun Jan  8 07:19:09 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.395894 | FPR 0.001 -- TPR 0.5833 | F1 0.7368 | Elapsed: 149.94s
WARNING:root: [*] Sun Jan  8 07:21:38 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.349115 | FPR 0.001 -- TPR 0.9091 | F1 0.9524 | Elapsed: 149.64s
WARNING:root: [*] Sun Jan  8 07:24:08 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.337673 | FPR 0.001 -- TPR 0.5833 | F1 0.7368 | Elapsed: 149.73s
WARNING:root: [*] Sun Jan  8 07:26:38 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.313779 | FPR 0.001 -- TPR 0.7000 | F1 0.8235 | Elapsed: 149.81s
WARNING:root: [*] Sun Jan  8 07:29:07 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.352093 | FPR 0.001 -- TPR 0.6000 | F1 0.7500 | Elapsed: 149.68s
WARNING:root: [*] Sun Jan  8 07:31:37 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.331039 | FPR 0.001 -- TPR 0.5714 | F1 0.7273 | Elapsed: 149.87s
WARNING:root: [*] Sun Jan  8 07:34:07 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.318057 | FPR 0.001 -- TPR 0.6667 | F1 0.8000 | Elapsed: 149.85s
WARNING:root: [*] Sun Jan  8 07:36:37 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.308100 | FPR 0.001 -- TPR 0.8182 | F1 0.9000 | Elapsed: 149.72s
WARNING:root: [*] Sun Jan  8 07:39:07 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.391666 | FPR 0.001 -- TPR 0.4000 | F1 0.5714 | Elapsed: 149.84s
WARNING:root: [*] Sun Jan  8 07:41:36 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.530203 | FPR 0.001 -- TPR 0.3000 | F1 0.4615 | Elapsed: 149.70s
WARNING:root: [*] Sun Jan  8 07:44:07 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.187055 | FPR 0.001 -- TPR 0.8333 | F1 0.9091 | Elapsed: 150.18s
WARNING:root: [*] Sun Jan  8 07:46:38 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.225366 | FPR 0.001 -- TPR 0.8182 | F1 0.9000 | Elapsed: 151.33s
WARNING:root: [*] Sun Jan  8 07:49:08 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.325393 | FPR 0.001 -- TPR 0.8182 | F1 0.9000 | Elapsed: 150.54s
WARNING:root: [*] Sun Jan  8 07:51:38 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.349226 | FPR 0.001 -- TPR 0.5556 | F1 0.7143 | Elapsed: 150.01s
WARNING:root: [*] Sun Jan  8 07:54:08 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.281715 | FPR 0.001 -- TPR 0.8000 | F1 0.8889 | Elapsed: 149.86s
WARNING:root: [*] Sun Jan  8 07:56:38 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.478949 | FPR 0.001 -- TPR 0.7500 | F1 0.8571 | Elapsed: 149.83s
WARNING:root: [*] Sun Jan  8 07:59:08 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.487601 | FPR 0.001 -- TPR 0.4444 | F1 0.6154 | Elapsed: 149.86s
WARNING:root: [*] Sun Jan  8 08:01:38 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.193406 | FPR 0.001 -- TPR 0.8333 | F1 0.9091 | Elapsed: 149.75s
WARNING:root: [*] Sun Jan  8 08:04:08 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.217883 | FPR 0.001 -- TPR 0.6667 | F1 0.8000 | Elapsed: 150.25s
WARNING:root: [*] Sun Jan  8 08:06:38 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.280287 | FPR 0.001 -- TPR 0.5833 | F1 0.7368 | Elapsed: 150.30s
WARNING:root: [*] Sun Jan  8 08:09:08 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.350176 | FPR 0.001 -- TPR 0.5714 | F1 0.7273 | Elapsed: 149.86s
WARNING:root: [*] Sun Jan  8 08:11:38 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.368109 | FPR 0.001 -- TPR 0.5385 | F1 0.7000 | Elapsed: 149.78s
WARNING:root: [*] Sun Jan  8 08:13:35 2023:    1    | Tr.loss: 0.436357 | FPR 0.001 -- TPR: nan |  F1: 0.73 | Elapsed:  3571.37 s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  8 08:13:36 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.282962 | FPR 0.001 -- TPR 0.6667 | F1 0.8000 | Elapsed: 1.50s
WARNING:root: [*] Sun Jan  8 08:16:06 2023: Train Epoch: 2 [1600 /38063 (4 %)]	Loss: 0.332710 | FPR 0.001 -- TPR 0.8182 | F1 0.9000 | Elapsed: 149.74s
WARNING:root: [*] Sun Jan  8 08:18:36 2023: Train Epoch: 2 [3200 /38063 (8 %)]	Loss: 0.280229 | FPR 0.001 -- TPR 0.7500 | F1 0.8571 | Elapsed: 149.74s
WARNING:root: [*] Sun Jan  8 08:21:06 2023: Train Epoch: 2 [4800 /38063 (13%)]	Loss: 0.368131 | FPR 0.001 -- TPR 0.6667 | F1 0.8000 | Elapsed: 149.80s
WARNING:root: [*] Sun Jan  8 08:23:35 2023: Train Epoch: 2 [6400 /38063 (17%)]	Loss: 0.353935 | FPR 0.001 -- TPR 0.6000 | F1 0.7500 | Elapsed: 149.77s
WARNING:root: [*] Sun Jan  8 08:26:06 2023: Train Epoch: 2 [8000 /38063 (21%)]	Loss: 0.254367 | FPR 0.001 -- TPR 0.6923 | F1 0.8182 | Elapsed: 150.74s
WARNING:root: [*] Sun Jan  8 08:28:36 2023: Train Epoch: 2 [9600 /38063 (25%)]	Loss: 0.353711 | FPR 0.001 -- TPR 0.6364 | F1 0.7778 | Elapsed: 150.41s
WARNING:root: [*] Sun Jan  8 08:31:07 2023: Train Epoch: 2 [11200/38063 (29%)]	Loss: 0.387755 | FPR 0.001 -- TPR 0.7000 | F1 0.8235 | Elapsed: 150.40s
WARNING:root: [*] Sun Jan  8 08:33:37 2023: Train Epoch: 2 [12800/38063 (34%)]	Loss: 0.240489 | FPR 0.001 -- TPR 0.6154 | F1 0.7619 | Elapsed: 149.88s
WARNING:root: [*] Sun Jan  8 08:36:07 2023: Train Epoch: 2 [14400/38063 (38%)]	Loss: 0.493742 | FPR 0.001 -- TPR 0.7500 | F1 0.8571 | Elapsed: 149.92s
WARNING:root: [*] Sun Jan  8 08:38:37 2023: Train Epoch: 2 [16000/38063 (42%)]	Loss: 0.197629 | FPR 0.001 -- TPR 0.8889 | F1 0.9412 | Elapsed: 149.92s
WARNING:root: [*] Sun Jan  8 08:41:07 2023: Train Epoch: 2 [17600/38063 (46%)]	Loss: 0.509454 | FPR 0.001 -- TPR 0.3750 | F1 0.5455 | Elapsed: 149.92s
WARNING:root: [*] Sun Jan  8 08:43:37 2023: Train Epoch: 2 [19200/38063 (50%)]	Loss: 0.312303 | FPR 0.001 -- TPR 0.6364 | F1 0.7778 | Elapsed: 150.07s
WARNING:root: [*] Sun Jan  8 08:46:07 2023: Train Epoch: 2 [20800/38063 (55%)]	Loss: 0.197593 | FPR 0.001 -- TPR 0.7692 | F1 0.8696 | Elapsed: 150.04s
WARNING:root: [*] Sun Jan  8 08:48:37 2023: Train Epoch: 2 [22400/38063 (59%)]	Loss: 0.367722 | FPR 0.001 -- TPR 0.5833 | F1 0.7368 | Elapsed: 149.88s
WARNING:root: [*] Sun Jan  8 08:51:06 2023: Train Epoch: 2 [24000/38063 (63%)]	Loss: 0.475002 | FPR 0.001 -- TPR 0.3333 | F1 0.5000 | Elapsed: 149.95s
WARNING:root: [*] Sun Jan  8 08:53:36 2023: Train Epoch: 2 [25600/38063 (67%)]	Loss: 0.357342 | FPR 0.001 -- TPR 0.4286 | F1 0.6000 | Elapsed: 149.90s
WARNING:root: [*] Sun Jan  8 08:56:06 2023: Train Epoch: 2 [27200/38063 (71%)]	Loss: 0.246543 | FPR 0.001 -- TPR 0.7143 | F1 0.8333 | Elapsed: 149.94s
WARNING:root: [*] Sun Jan  8 08:58:36 2023: Train Epoch: 2 [28800/38063 (76%)]	Loss: 0.302835 | FPR 0.001 -- TPR 0.8000 | F1 0.8889 | Elapsed: 150.09s
WARNING:root: [*] Sun Jan  8 09:01:06 2023: Train Epoch: 2 [30400/38063 (80%)]	Loss: 0.213502 | FPR 0.001 -- TPR 1.0000 | F1 1.0000 | Elapsed: 149.82s
WARNING:root: [*] Sun Jan  8 09:03:36 2023: Train Epoch: 2 [32000/38063 (84%)]	Loss: 0.370605 | FPR 0.001 -- TPR 0.7000 | F1 0.8235 | Elapsed: 150.15s
WARNING:root: [*] Sun Jan  8 09:06:06 2023: Train Epoch: 2 [33600/38063 (88%)]	Loss: 0.176388 | FPR 0.001 -- TPR 1.0000 | F1 1.0000 | Elapsed: 150.00s
WARNING:root: [*] Sun Jan  8 09:08:36 2023: Train Epoch: 2 [35200/38063 (92%)]	Loss: 0.317988 | FPR 0.001 -- TPR 0.6250 | F1 0.7692 | Elapsed: 149.68s
WARNING:root: [*] Sun Jan  8 09:11:06 2023: Train Epoch: 2 [36800/38063 (97%)]	Loss: 0.121789 | FPR 0.001 -- TPR 1.0000 | F1 1.0000 | Elapsed: 150.19s
WARNING:root: [*] Sun Jan  8 09:13:04 2023:    2    | Tr.loss: 0.332322 | FPR 0.001 -- TPR: nan |  F1: 0.77 | Elapsed:  3569.03 s
WARNING:root:[!] Sun Jan  8 09:13:04 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_VocabSize_maxLen\trainingFiles\trainingFiles_1673165584-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_VocabSize_maxLen\trainingFiles\trainingFiles_1673165584-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_VocabSize_maxLen\trainingFiles\trainingFiles_1673165584-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_VocabSize_maxLen\trainingFiles\trainingFiles_1673165584-trainF1s.npy
WARNING:root: [!] Fold 2/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  8 09:27:59 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.815325 | FPR 0.001 -- TPR 0.0000 | F1 0.0000 | Elapsed: 1.54s
