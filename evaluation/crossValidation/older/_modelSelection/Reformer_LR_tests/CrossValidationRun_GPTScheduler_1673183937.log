WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 2 | Model config: {'vocabSize': 10000, 'maxLen': 2048, 'dim': 64, 'heads': 4, 'depth': 4, 'hiddenNeurons': [64]}
WARNING:root: [!] Fold 1/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sun Jan  8 14:19:02 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.648964 | FPR 0.001 -- TPR 0.0000 | F1 0.0000 | Elapsed: 3.14s
WARNING:root: [*] Sun Jan  8 14:21:32 2023: Train Epoch: 1 [1600 /38063 (4 %)]	Loss: 0.678766 | FPR 0.001 -- TPR 0.2172 | F1 0.3195 | Elapsed: 150.19s
WARNING:root: [*] Sun Jan  8 14:24:02 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.692498 | FPR 0.001 -- TPR 0.2238 | F1 0.3238 | Elapsed: 149.84s
WARNING:root: [*] Sun Jan  8 14:26:31 2023: Train Epoch: 1 [4800 /38063 (13%)]	Loss: 0.680341 | FPR 0.001 -- TPR 0.2239 | F1 0.3207 | Elapsed: 149.23s
WARNING:root: [*] Sun Jan  8 14:29:01 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.639037 | FPR 0.001 -- TPR 0.2303 | F1 0.3301 | Elapsed: 149.54s
WARNING:root: [*] Sun Jan  8 14:31:29 2023: Train Epoch: 1 [8000 /38063 (21%)]	Loss: 0.656894 | FPR 0.001 -- TPR 0.2380 | F1 0.3402 | Elapsed: 148.86s
WARNING:root: [*] Sun Jan  8 14:33:58 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.643041 | FPR 0.001 -- TPR 0.2431 | F1 0.3462 | Elapsed: 148.84s
WARNING:root: [*] Sun Jan  8 14:36:27 2023: Train Epoch: 1 [11200/38063 (29%)]	Loss: 0.699479 | FPR 0.001 -- TPR 0.2383 | F1 0.3402 | Elapsed: 148.93s
WARNING:root: [*] Sun Jan  8 14:38:56 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.667207 | FPR 0.001 -- TPR 0.2354 | F1 0.3360 | Elapsed: 148.84s
WARNING:root: [*] Sun Jan  8 14:41:26 2023: Train Epoch: 1 [14400/38063 (38%)]	Loss: 0.682172 | FPR 0.001 -- TPR 0.2390 | F1 0.3401 | Elapsed: 149.57s
WARNING:root: [*] Sun Jan  8 14:43:55 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.660294 | FPR 0.001 -- TPR 0.2392 | F1 0.3411 | Elapsed: 149.93s
WARNING:root: [*] Sun Jan  8 14:46:24 2023: Train Epoch: 1 [17600/38063 (46%)]	Loss: 0.665582 | FPR 0.001 -- TPR 0.2370 | F1 0.3386 | Elapsed: 148.87s
WARNING:root: [*] Sun Jan  8 14:48:53 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.658050 | FPR 0.001 -- TPR 0.2363 | F1 0.3377 | Elapsed: 148.68s
WARNING:root: [*] Sun Jan  8 14:51:22 2023: Train Epoch: 1 [20800/38063 (55%)]	Loss: 0.735823 | FPR 0.001 -- TPR 0.2362 | F1 0.3379 | Elapsed: 148.90s
WARNING:root: [*] Sun Jan  8 14:53:51 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.696279 | FPR 0.001 -- TPR 0.2386 | F1 0.3409 | Elapsed: 148.81s
WARNING:root: [*] Sun Jan  8 14:56:20 2023: Train Epoch: 1 [24000/38063 (63%)]	Loss: 0.672624 | FPR 0.001 -- TPR 0.2369 | F1 0.3384 | Elapsed: 148.76s
WARNING:root: [*] Sun Jan  8 14:58:49 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.679250 | FPR 0.001 -- TPR 0.2367 | F1 0.3381 | Elapsed: 149.30s
WARNING:root: [*] Sun Jan  8 15:01:18 2023: Train Epoch: 1 [27200/38063 (71%)]	Loss: 0.674921 | FPR 0.001 -- TPR 0.2375 | F1 0.3393 | Elapsed: 149.30s
WARNING:root: [*] Sun Jan  8 15:03:48 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.741260 | FPR 0.001 -- TPR 0.2345 | F1 0.3355 | Elapsed: 149.42s
WARNING:root: [*] Sun Jan  8 15:06:17 2023: Train Epoch: 1 [30400/38063 (80%)]	Loss: 0.671899 | FPR 0.001 -- TPR 0.2341 | F1 0.3348 | Elapsed: 149.37s
WARNING:root: [*] Sun Jan  8 15:08:46 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.670134 | FPR 0.001 -- TPR 0.2348 | F1 0.3348 | Elapsed: 149.31s
WARNING:root: [*] Sun Jan  8 15:11:16 2023: Train Epoch: 1 [33600/38063 (88%)]	Loss: 0.728573 | FPR 0.001 -- TPR 0.2366 | F1 0.3374 | Elapsed: 149.38s
WARNING:root: [*] Sun Jan  8 15:13:45 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.695291 | FPR 0.001 -- TPR 0.2374 | F1 0.3380 | Elapsed: 149.25s
WARNING:root: [*] Sun Jan  8 15:16:14 2023: Train Epoch: 1 [36800/38063 (97%)]	Loss: 0.655201 | FPR 0.001 -- TPR 0.2366 | F1 0.3373 | Elapsed: 149.12s
WARNING:root: [*] Sun Jan  8 15:18:10 2023:    1    | Tr.loss: 0.685645 | FPR 0.001 -- TPR: 0.24 |  F1: 0.34 | Elapsed:  3551.73 s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sun Jan  8 15:18:12 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.756018 | FPR 0.001 -- TPR 0.0714 | F1 0.1333 | Elapsed: 1.49s
WARNING:root: [*] Sun Jan  8 15:20:41 2023: Train Epoch: 2 [1600 /38063 (4 %)]	Loss: 0.673881 | FPR 0.001 -- TPR 0.2101 | F1 0.2996 | Elapsed: 149.32s
WARNING:root: [*] Sun Jan  8 15:23:10 2023: Train Epoch: 2 [3200 /38063 (8 %)]	Loss: 0.604773 | FPR 0.001 -- TPR 0.2350 | F1 0.3280 | Elapsed: 149.24s
WARNING:root: [*] Sun Jan  8 15:25:40 2023: Train Epoch: 2 [4800 /38063 (13%)]	Loss: 0.668419 | FPR 0.001 -- TPR 0.2313 | F1 0.3292 | Elapsed: 149.33s
WARNING:root: [*] Sun Jan  8 15:28:10 2023: Train Epoch: 2 [6400 /38063 (17%)]	Loss: 0.693769 | FPR 0.001 -- TPR 0.2413 | F1 0.3410 | Elapsed: 149.89s
WARNING:root:[!] Sun Jan  8 15:29:45 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_GPT_Scheduler_VocabSize_maxLen\trainingFiles\trainingFiles_1673188185-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_GPT_Scheduler_VocabSize_maxLen\trainingFiles\trainingFiles_1673188185-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_GPT_Scheduler_VocabSize_maxLen\trainingFiles\trainingFiles_1673188185-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_GPT_Scheduler_VocabSize_maxLen\trainingFiles\trainingFiles_1673188185-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_GPT_Scheduler_VocabSize_maxLen\trainingFiles\trainingFiles_1673188185-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/2379
WARNING:root: [*] Predicting batch: 100/2379
WARNING:root: [*] Predicting batch: 200/2379
WARNING:root: [*] Predicting batch: 300/2379
WARNING:root: [*] Predicting batch: 400/2379
WARNING:root: [*] Predicting batch: 500/2379
WARNING:root: [*] Predicting batch: 600/2379
WARNING:root: [*] Predicting batch: 700/2379
WARNING:root: [*] Predicting batch: 800/2379
WARNING:root: [*] Predicting batch: 900/2379
WARNING:root: [*] Predicting batch: 1000/2379
WARNING:root: [*] Predicting batch: 1100/2379
WARNING:root: [*] Predicting batch: 1200/2379
WARNING:root: [*] Predicting batch: 1300/2379
WARNING:root: [*] Predicting batch: 1400/2379
WARNING:root: [*] Predicting batch: 1500/2379
WARNING:root: [*] Predicting batch: 1600/2379
WARNING:root: [*] Predicting batch: 1700/2379
WARNING:root: [*] Predicting batch: 1800/2379
WARNING:root: [*] Predicting batch: 1900/2379
WARNING:root: [*] Predicting batch: 2000/2379
WARNING:root: [*] Predicting batch: 2100/2379
WARNING:root: [*] Predicting batch: 2200/2379
WARNING:root: [*] Predicting batch: 2300/2379
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.0018321443885705375 | F1: 0.0036574452355939454
WARNING:root: [!] FPR: 0.0003 | TPR: 0.002689743889603555 | F1: 0.005364431486880467
WARNING:root: [!] FPR: 0.001 | TPR: 0.02545511246248002 | F1: 0.049623831598145755
WARNING:root: [!] FPR: 0.003 | TPR: 0.026156784781507035 | F1: 0.05091047040971168
WARNING:root: [!] FPR: 0.01 | TPR: 0.04588157330526644 | F1: 0.08733397640424427
WARNING:root: [!] FPR: 0.03 | TPR: 0.08529216855728375 | F1: 0.15510580228972462
WARNING:root: [!] FPR: 0.1 | TPR: 0.2244571784976416 | F1: 0.3526997641726134
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection\Reformer_GPT_Scheduler_VocabSize_maxLen\metrics_trainSize_76126_ep_2_cv_2_vocabSize_10000_maxLen_2048_dim_64_heads_4_depth_4_hiddenNeurons_64.json
WARNING:root: [!] Average epoch time: 1775.87s | Mean values over 2 folds:
	FPR: 0.0001 -- TPR: 0.0018 -- F1: 0.0037
	FPR: 0.0003 -- TPR: 0.0027 -- F1: 0.0054
	FPR:  0.001 -- TPR: 0.0255 -- F1: 0.0496
	FPR:  0.003 -- TPR: 0.0262 -- F1: 0.0509
	FPR:   0.01 -- TPR: 0.0459 -- F1: 0.0873
	FPR:   0.03 -- TPR: 0.0853 -- F1: 0.1551
	FPR:    0.1 -- TPR: 0.2245 -- F1: 0.3527

