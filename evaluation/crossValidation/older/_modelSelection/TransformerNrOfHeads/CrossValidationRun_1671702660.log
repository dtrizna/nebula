WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'nTokens': 2000, 'dModel': 32, 'nHeads': 2, 'dHidden': 200, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:51:02 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.680468 | F1-score: 0.76 | Elapsed: 1.49s
WARNING:root: [*] Thu Dec 22 10:51:05 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.575070 | F1-score: 0.84 | Elapsed: 3.41s
WARNING:root: [*] Thu Dec 22 10:51:08 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.326646 | F1-score: 0.86 | Elapsed: 3.46s
WARNING:root: [*] Thu Dec 22 10:51:12 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.392660 | F1-score: 0.87 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:51:15 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.441617 | F1-score: 0.88 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:51:18 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.429291 | F1-score: 0.88 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:51:22 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.529840 | F1-score: 0.88 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:51:25 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.355370 | F1-score: 0.88 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:51:28 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.393618 | F1-score: 0.89 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:51:31 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.319293 | F1-score: 0.89 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:51:33 2022:    1    | Tr.loss: 0.419870 | Tr.F1.:   0.89    |   33.04  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:51:33 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.370925 | F1-score: 0.93 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:51:36 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.375416 | F1-score: 0.91 | Elapsed: 3.34s
WARNING:root: [*] Thu Dec 22 10:51:40 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.396825 | F1-score: 0.91 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:51:43 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.250216 | F1-score: 0.91 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:51:46 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.504621 | F1-score: 0.92 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:51:50 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.436832 | F1-score: 0.92 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:51:53 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.305978 | F1-score: 0.92 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:51:56 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.242967 | F1-score: 0.92 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:52:00 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.297313 | F1-score: 0.92 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:52:03 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.257907 | F1-score: 0.92 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:52:05 2022:    2    | Tr.loss: 0.307696 | Tr.F1.:   0.92    |   31.54  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:52:05 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.241381 | F1-score: 0.94 | Elapsed: 0.03s
WARNING:root: [*] Thu Dec 22 10:52:08 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.192204 | F1-score: 0.93 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:52:11 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.213634 | F1-score: 0.93 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:52:15 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.156301 | F1-score: 0.93 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:52:18 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.300006 | F1-score: 0.93 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:52:21 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.224848 | F1-score: 0.93 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:52:25 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.290931 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:52:28 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.135051 | F1-score: 0.93 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:52:31 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.273467 | F1-score: 0.93 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:52:35 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.369002 | F1-score: 0.93 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:52:36 2022:    3    | Tr.loss: 0.270283 | Tr.F1.:   0.93    |   31.55  s
WARNING:root:
        [!] Thu Dec 22 10:52:36 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702756-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702756-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702756-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702756-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:52:41 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.743903 | F1-score: 0.15 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:52:44 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.558713 | F1-score: 0.83 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:52:47 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.519019 | F1-score: 0.85 | Elapsed: 3.35s
WARNING:root: [*] Thu Dec 22 10:52:51 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.316990 | F1-score: 0.86 | Elapsed: 3.34s
WARNING:root: [*] Thu Dec 22 10:52:54 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.322068 | F1-score: 0.87 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:52:57 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.351938 | F1-score: 0.87 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:53:01 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.423029 | F1-score: 0.88 | Elapsed: 3.36s
WARNING:root: [*] Thu Dec 22 10:53:04 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.319269 | F1-score: 0.88 | Elapsed: 3.35s
WARNING:root: [*] Thu Dec 22 10:53:07 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.303530 | F1-score: 0.88 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:53:11 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.399742 | F1-score: 0.89 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:53:12 2022:    1    | Tr.loss: 0.424980 | Tr.F1.:   0.89    |   31.66  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:53:12 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.293302 | F1-score: 0.92 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:53:16 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.254210 | F1-score: 0.91 | Elapsed: 3.34s
WARNING:root: [*] Thu Dec 22 10:53:19 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.240232 | F1-score: 0.91 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:53:22 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.284783 | F1-score: 0.92 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:53:26 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.460431 | F1-score: 0.92 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:53:29 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.285795 | F1-score: 0.92 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:53:32 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.455343 | F1-score: 0.92 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:53:35 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.248326 | F1-score: 0.92 | Elapsed: 3.27s
WARNING:root: [*] Thu Dec 22 10:53:39 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.514260 | F1-score: 0.92 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:53:42 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.258088 | F1-score: 0.92 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:53:44 2022:    2    | Tr.loss: 0.306403 | Tr.F1.:   0.92    |   31.28  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:53:44 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.226051 | F1-score: 0.94 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:53:47 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.405782 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:53:50 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.604380 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:53:54 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.212669 | F1-score: 0.93 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:53:57 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.318376 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:54:00 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.266710 | F1-score: 0.93 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:54:03 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.140717 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:54:07 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.226093 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:54:10 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.252987 | F1-score: 0.93 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:54:13 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.314591 | F1-score: 0.93 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:54:15 2022:    3    | Tr.loss: 0.267600 | Tr.F1.:   0.93    |   31.32  s
WARNING:root:
        [!] Thu Dec 22 10:54:15 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702855-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702855-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702855-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702855-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:54:20 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.676229 | F1-score: 0.73 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:54:23 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.539759 | F1-score: 0.83 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:54:26 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.483605 | F1-score: 0.86 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:54:29 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.354964 | F1-score: 0.87 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:54:33 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.494532 | F1-score: 0.87 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:54:36 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.427838 | F1-score: 0.88 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:54:39 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.437487 | F1-score: 0.88 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:54:43 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.320235 | F1-score: 0.88 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:54:46 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.451844 | F1-score: 0.88 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:54:49 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.311533 | F1-score: 0.88 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:54:51 2022:    1    | Tr.loss: 0.427948 | Tr.F1.:   0.88    |   31.24  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:54:51 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.525250 | F1-score: 0.83 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:54:54 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.302563 | F1-score: 0.90 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:54:57 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.454650 | F1-score: 0.90 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:55:01 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.419885 | F1-score: 0.90 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:55:04 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.423238 | F1-score: 0.91 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:55:07 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.410757 | F1-score: 0.91 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:55:11 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.286843 | F1-score: 0.91 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:55:14 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.234528 | F1-score: 0.91 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:55:17 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.405107 | F1-score: 0.91 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:55:20 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.224138 | F1-score: 0.91 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:55:22 2022:    2    | Tr.loss: 0.340266 | Tr.F1.:   0.91    |   31.33  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:55:22 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.416483 | F1-score: 0.88 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:55:25 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.364180 | F1-score: 0.92 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:55:29 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.427408 | F1-score: 0.92 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:55:32 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.305349 | F1-score: 0.92 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:55:35 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.400318 | F1-score: 0.92 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:55:39 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.226707 | F1-score: 0.92 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:55:42 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.260372 | F1-score: 0.92 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:55:45 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.254260 | F1-score: 0.92 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:55:49 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.157022 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:55:52 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.310252 | F1-score: 0.93 | Elapsed: 3.39s
WARNING:root: [*] Thu Dec 22 10:55:54 2022:    3    | Tr.loss: 0.284201 | Tr.F1.:   0.93    |   31.62  s
WARNING:root:
        [!] Thu Dec 22 10:55:54 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702954-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702954-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702954-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671702954-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\metrics_trainSize_91096_ep_3_cv_3_nTokens_2000_dModel_32_nHeads_2_dHidden_200_nLayers_2_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 31.62s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0016 -- F1: 0.0033
	FPR:  0.001 -- TPR: 0.0850 -- F1: 0.1560
	FPR:   0.01 -- TPR: 0.1666 -- F1: 0.2788
	FPR:    0.1 -- TPR: 0.4119 -- F1: 0.5621

WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'nTokens': 2000, 'dModel': 32, 'nHeads': 4, 'dHidden': 200, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:56:29 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.697356 | F1-score: 0.41 | Elapsed: 0.34s
WARNING:root: [*] Thu Dec 22 10:56:33 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.616616 | F1-score: 0.83 | Elapsed: 3.97s
WARNING:root: [*] Thu Dec 22 10:56:37 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.504820 | F1-score: 0.85 | Elapsed: 3.97s
WARNING:root: [*] Thu Dec 22 10:56:41 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.471701 | F1-score: 0.86 | Elapsed: 3.98s
WARNING:root: [*] Thu Dec 22 10:56:44 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.452238 | F1-score: 0.87 | Elapsed: 3.98s
WARNING:root: [*] Thu Dec 22 10:56:48 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.410144 | F1-score: 0.87 | Elapsed: 3.96s
WARNING:root: [*] Thu Dec 22 10:56:52 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.362355 | F1-score: 0.88 | Elapsed: 3.98s
WARNING:root: [*] Thu Dec 22 10:56:56 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.294939 | F1-score: 0.88 | Elapsed: 3.95s
WARNING:root: [*] Thu Dec 22 10:57:00 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.355705 | F1-score: 0.88 | Elapsed: 3.88s
WARNING:root: [*] Thu Dec 22 10:57:04 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.590420 | F1-score: 0.89 | Elapsed: 3.89s
WARNING:root: [*] Thu Dec 22 10:57:06 2022:    1    | Tr.loss: 0.429789 | Tr.F1.:   0.89    |   37.80  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:57:06 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.288759 | F1-score: 0.95 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:57:10 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.397677 | F1-score: 0.92 | Elapsed: 3.89s
WARNING:root: [*] Thu Dec 22 10:57:14 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.417021 | F1-score: 0.92 | Elapsed: 3.84s
WARNING:root: [*] Thu Dec 22 10:57:18 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.258032 | F1-score: 0.92 | Elapsed: 3.88s
WARNING:root: [*] Thu Dec 22 10:57:22 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.201706 | F1-score: 0.92 | Elapsed: 3.97s
WARNING:root: [*] Thu Dec 22 10:57:26 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.303577 | F1-score: 0.92 | Elapsed: 3.93s
WARNING:root: [*] Thu Dec 22 10:57:30 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.253960 | F1-score: 0.92 | Elapsed: 3.92s
WARNING:root: [*] Thu Dec 22 10:57:33 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.339165 | F1-score: 0.92 | Elapsed: 3.91s
WARNING:root: [*] Thu Dec 22 10:57:37 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.246738 | F1-score: 0.92 | Elapsed: 3.90s
WARNING:root: [*] Thu Dec 22 10:57:41 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.266161 | F1-score: 0.92 | Elapsed: 3.90s
WARNING:root: [*] Thu Dec 22 10:57:43 2022:    2    | Tr.loss: 0.303353 | Tr.F1.:   0.92    |   37.06  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:57:43 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.229880 | F1-score: 0.94 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:57:47 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.234563 | F1-score: 0.93 | Elapsed: 3.93s
WARNING:root: [*] Thu Dec 22 10:57:51 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.349576 | F1-score: 0.93 | Elapsed: 3.95s
WARNING:root: [*] Thu Dec 22 10:57:55 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.304522 | F1-score: 0.93 | Elapsed: 3.88s
WARNING:root: [*] Thu Dec 22 10:57:59 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.330134 | F1-score: 0.93 | Elapsed: 3.94s
WARNING:root: [*] Thu Dec 22 10:58:03 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.184754 | F1-score: 0.93 | Elapsed: 3.93s
WARNING:root: [*] Thu Dec 22 10:58:07 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.367845 | F1-score: 0.93 | Elapsed: 3.94s
WARNING:root: [*] Thu Dec 22 10:58:11 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.168345 | F1-score: 0.93 | Elapsed: 3.90s
WARNING:root: [*] Thu Dec 22 10:58:15 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.358763 | F1-score: 0.93 | Elapsed: 3.89s
WARNING:root: [*] Thu Dec 22 10:58:18 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.268988 | F1-score: 0.93 | Elapsed: 3.96s
WARNING:root: [*] Thu Dec 22 10:58:20 2022:    3    | Tr.loss: 0.265626 | Tr.F1.:   0.93    |   37.30  s
WARNING:root:
        [!] Thu Dec 22 10:58:20 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703100-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703100-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703100-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703100-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:58:26 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.648298 | F1-score: 0.91 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:58:30 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.450352 | F1-score: 0.84 | Elapsed: 3.93s
WARNING:root: [*] Thu Dec 22 10:58:34 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.387482 | F1-score: 0.86 | Elapsed: 3.90s
WARNING:root: [*] Thu Dec 22 10:58:38 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.440191 | F1-score: 0.87 | Elapsed: 3.96s
WARNING:root: [*] Thu Dec 22 10:58:41 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.517778 | F1-score: 0.88 | Elapsed: 3.95s
WARNING:root: [*] Thu Dec 22 10:58:45 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.418744 | F1-score: 0.88 | Elapsed: 3.89s
WARNING:root: [*] Thu Dec 22 10:58:49 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.312228 | F1-score: 0.88 | Elapsed: 3.87s
WARNING:root: [*] Thu Dec 22 10:58:53 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.286205 | F1-score: 0.88 | Elapsed: 3.89s
WARNING:root: [*] Thu Dec 22 10:58:57 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.362433 | F1-score: 0.89 | Elapsed: 3.89s
WARNING:root: [*] Thu Dec 22 10:59:01 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.294079 | F1-score: 0.89 | Elapsed: 3.87s
WARNING:root: [*] Thu Dec 22 10:59:03 2022:    1    | Tr.loss: 0.415961 | Tr.F1.:   0.89    |   37.11  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:59:03 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.361987 | F1-score: 0.89 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:59:07 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.193068 | F1-score: 0.92 | Elapsed: 3.95s
WARNING:root: [*] Thu Dec 22 10:59:11 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.262475 | F1-score: 0.92 | Elapsed: 3.97s
WARNING:root: [*] Thu Dec 22 10:59:15 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.284617 | F1-score: 0.92 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 10:59:18 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.315377 | F1-score: 0.92 | Elapsed: 3.84s
WARNING:root: [*] Thu Dec 22 10:59:22 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.275290 | F1-score: 0.92 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 10:59:26 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.304835 | F1-score: 0.92 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 10:59:30 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.295422 | F1-score: 0.92 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 10:59:34 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.232598 | F1-score: 0.92 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 10:59:38 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.207773 | F1-score: 0.92 | Elapsed: 3.84s
WARNING:root: [*] Thu Dec 22 10:59:40 2022:    2    | Tr.loss: 0.293263 | Tr.F1.:   0.93    |   36.82  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:59:40 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.253010 | F1-score: 0.94 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:59:44 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.261885 | F1-score: 0.93 | Elapsed: 3.90s
WARNING:root: [*] Thu Dec 22 10:59:47 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.205119 | F1-score: 0.93 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 10:59:51 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.184572 | F1-score: 0.93 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 10:59:55 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.183287 | F1-score: 0.94 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 10:59:59 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.272690 | F1-score: 0.94 | Elapsed: 3.88s
WARNING:root: [*] Thu Dec 22 11:00:03 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.277305 | F1-score: 0.94 | Elapsed: 3.90s
WARNING:root: [*] Thu Dec 22 11:00:07 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.213333 | F1-score: 0.94 | Elapsed: 3.89s
WARNING:root: [*] Thu Dec 22 11:00:11 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.307868 | F1-score: 0.94 | Elapsed: 3.93s
WARNING:root: [*] Thu Dec 22 11:00:15 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.221744 | F1-score: 0.94 | Elapsed: 3.93s
WARNING:root: [*] Thu Dec 22 11:00:17 2022:    3    | Tr.loss: 0.254360 | Tr.F1.:   0.94    |   36.92  s
WARNING:root:
        [!] Thu Dec 22 11:00:17 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703217-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703217-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703217-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703217-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 11:00:22 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.672536 | F1-score: 0.80 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 11:00:26 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.538943 | F1-score: 0.84 | Elapsed: 3.87s
WARNING:root: [*] Thu Dec 22 11:00:30 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.316888 | F1-score: 0.86 | Elapsed: 3.84s
WARNING:root: [*] Thu Dec 22 11:00:33 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.348722 | F1-score: 0.87 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 11:00:37 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.504167 | F1-score: 0.88 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 11:00:41 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.506389 | F1-score: 0.88 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 11:00:45 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.436589 | F1-score: 0.88 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 11:00:49 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.373846 | F1-score: 0.89 | Elapsed: 3.87s
WARNING:root: [*] Thu Dec 22 11:00:53 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.326513 | F1-score: 0.89 | Elapsed: 3.84s
WARNING:root: [*] Thu Dec 22 11:00:57 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.320186 | F1-score: 0.89 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 11:00:58 2022:    1    | Tr.loss: 0.406669 | Tr.F1.:   0.89    |   36.60  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 11:00:58 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.445968 | F1-score: 0.88 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 11:01:02 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.357266 | F1-score: 0.92 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 11:01:06 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.430153 | F1-score: 0.92 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 11:01:10 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.431943 | F1-score: 0.92 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 11:01:14 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.324141 | F1-score: 0.92 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 11:01:18 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.216964 | F1-score: 0.92 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 11:01:22 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.233322 | F1-score: 0.93 | Elapsed: 3.88s
WARNING:root: [*] Thu Dec 22 11:01:25 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.560925 | F1-score: 0.93 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 11:01:29 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.153374 | F1-score: 0.93 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 11:01:33 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.176730 | F1-score: 0.93 | Elapsed: 3.84s
WARNING:root: [*] Thu Dec 22 11:01:35 2022:    2    | Tr.loss: 0.287687 | Tr.F1.:   0.93    |   36.61  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 11:01:35 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.301401 | F1-score: 0.93 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 11:01:39 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.221001 | F1-score: 0.93 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 11:01:43 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.224061 | F1-score: 0.93 | Elapsed: 3.84s
WARNING:root: [*] Thu Dec 22 11:01:47 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.227810 | F1-score: 0.94 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 11:01:50 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.211417 | F1-score: 0.94 | Elapsed: 3.86s
WARNING:root: [*] Thu Dec 22 11:01:54 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.157404 | F1-score: 0.93 | Elapsed: 3.85s
WARNING:root: [*] Thu Dec 22 11:01:58 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.166763 | F1-score: 0.94 | Elapsed: 3.87s
WARNING:root: [*] Thu Dec 22 11:02:02 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.186396 | F1-score: 0.94 | Elapsed: 3.87s
WARNING:root: [*] Thu Dec 22 11:02:06 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.290243 | F1-score: 0.94 | Elapsed: 3.89s
WARNING:root: [*] Thu Dec 22 11:02:10 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.234695 | F1-score: 0.94 | Elapsed: 3.89s
WARNING:root: [*] Thu Dec 22 11:02:12 2022:    3    | Tr.loss: 0.250526 | Tr.F1.:   0.94    |   36.74  s
WARNING:root:
        [!] Thu Dec 22 11:02:12 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703332-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703332-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703332-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703332-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\metrics_trainSize_91096_ep_3_cv_3_nTokens_2000_dModel_32_nHeads_4_dHidden_200_nLayers_2_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 37.00s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0011 -- F1: 0.0023
	FPR:  0.001 -- TPR: 0.0358 -- F1: 0.0681
	FPR:   0.01 -- TPR: 0.2723 -- F1: 0.4264
	FPR:    0.1 -- TPR: 0.5080 -- F1: 0.6561

WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'nTokens': 2000, 'dModel': 32, 'nHeads': 8, 'dHidden': 200, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 11:02:47 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.709393 | F1-score: 0.49 | Elapsed: 0.31s
WARNING:root: [*] Thu Dec 22 11:02:52 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.529811 | F1-score: 0.83 | Elapsed: 4.97s
WARNING:root: [*] Thu Dec 22 11:02:57 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.363918 | F1-score: 0.86 | Elapsed: 4.97s
WARNING:root: [*] Thu Dec 22 11:03:02 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.506092 | F1-score: 0.87 | Elapsed: 4.99s
WARNING:root: [*] Thu Dec 22 11:03:07 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.430535 | F1-score: 0.87 | Elapsed: 4.99s
WARNING:root: [*] Thu Dec 22 11:03:12 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.433087 | F1-score: 0.88 | Elapsed: 4.99s
WARNING:root: [*] Thu Dec 22 11:03:17 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.380878 | F1-score: 0.88 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:03:22 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.355884 | F1-score: 0.88 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:03:27 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.320521 | F1-score: 0.88 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:03:32 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.288607 | F1-score: 0.89 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:03:35 2022:    1    | Tr.loss: 0.428577 | Tr.F1.:   0.89    |   47.72  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 11:03:35 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.421468 | F1-score: 0.89 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 11:03:40 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.468313 | F1-score: 0.90 | Elapsed: 5.04s
WARNING:root: [*] Thu Dec 22 11:03:45 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.469612 | F1-score: 0.91 | Elapsed: 5.03s
WARNING:root: [*] Thu Dec 22 11:03:50 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.322883 | F1-score: 0.91 | Elapsed: 5.03s
WARNING:root: [*] Thu Dec 22 11:03:55 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.329662 | F1-score: 0.91 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:04:00 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.238102 | F1-score: 0.91 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:04:05 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.187511 | F1-score: 0.91 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:04:10 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.267445 | F1-score: 0.91 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:04:15 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.234571 | F1-score: 0.91 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:04:20 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.228260 | F1-score: 0.91 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:04:22 2022:    2    | Tr.loss: 0.332863 | Tr.F1.:   0.91    |   47.63  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 11:04:22 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.435209 | F1-score: 0.86 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 11:04:27 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.309631 | F1-score: 0.92 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:04:32 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.361906 | F1-score: 0.92 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:04:37 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.337260 | F1-score: 0.93 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:04:42 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.448785 | F1-score: 0.93 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:04:47 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.206731 | F1-score: 0.93 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:04:52 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.463007 | F1-score: 0.93 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:04:57 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.223223 | F1-score: 0.93 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:05:02 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.169402 | F1-score: 0.93 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:05:07 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.219197 | F1-score: 0.93 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:05:10 2022:    3    | Tr.loss: 0.278610 | Tr.F1.:   0.93    |   47.54  s
WARNING:root:
        [!] Thu Dec 22 11:05:10 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703510-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703510-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703510-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703510-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 11:05:16 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.709373 | F1-score: 0.39 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 11:05:21 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.534536 | F1-score: 0.83 | Elapsed: 4.97s
WARNING:root: [*] Thu Dec 22 11:05:26 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.533907 | F1-score: 0.85 | Elapsed: 4.98s
WARNING:root: [*] Thu Dec 22 11:05:31 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.396274 | F1-score: 0.86 | Elapsed: 4.95s
WARNING:root: [*] Thu Dec 22 11:05:36 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.474984 | F1-score: 0.87 | Elapsed: 4.96s
WARNING:root: [*] Thu Dec 22 11:05:41 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.441177 | F1-score: 0.87 | Elapsed: 4.97s
WARNING:root: [*] Thu Dec 22 11:05:46 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.420128 | F1-score: 0.88 | Elapsed: 4.99s
WARNING:root: [*] Thu Dec 22 11:05:51 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.477145 | F1-score: 0.88 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:05:56 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.328723 | F1-score: 0.88 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:06:01 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.307395 | F1-score: 0.89 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:06:04 2022:    1    | Tr.loss: 0.425937 | Tr.F1.:   0.89    |   47.33  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 11:06:04 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.376883 | F1-score: 0.90 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 11:06:09 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.276929 | F1-score: 0.91 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:06:14 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.330807 | F1-score: 0.91 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:06:19 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.224523 | F1-score: 0.91 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:06:24 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.367726 | F1-score: 0.92 | Elapsed: 5.04s
WARNING:root: [*] Thu Dec 22 11:06:29 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.229332 | F1-score: 0.92 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:06:34 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.300282 | F1-score: 0.92 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:06:39 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.305324 | F1-score: 0.92 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:06:44 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.298803 | F1-score: 0.92 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:06:49 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.260270 | F1-score: 0.92 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:06:51 2022:    2    | Tr.loss: 0.306991 | Tr.F1.:   0.92    |   47.59  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 11:06:51 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.217627 | F1-score: 0.95 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 11:06:56 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.299972 | F1-score: 0.93 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:07:01 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.189526 | F1-score: 0.93 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:07:06 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.230925 | F1-score: 0.93 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:07:11 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.250759 | F1-score: 0.93 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:07:16 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.352386 | F1-score: 0.93 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:07:21 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.214367 | F1-score: 0.93 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:07:27 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.197511 | F1-score: 0.93 | Elapsed: 5.03s
WARNING:root: [*] Thu Dec 22 11:07:32 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.281572 | F1-score: 0.93 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:07:37 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.243200 | F1-score: 0.94 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:07:39 2022:    3    | Tr.loss: 0.257074 | Tr.F1.:   0.94    |   47.60  s
WARNING:root:
        [!] Thu Dec 22 11:07:39 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703659-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703659-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703659-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703659-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 11:07:46 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.790591 | F1-score: 0.04 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 11:07:51 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.533989 | F1-score: 0.81 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:07:56 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.424795 | F1-score: 0.85 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:08:01 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.373522 | F1-score: 0.86 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:08:06 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.362687 | F1-score: 0.87 | Elapsed: 5.05s
WARNING:root: [*] Thu Dec 22 11:08:11 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.507495 | F1-score: 0.87 | Elapsed: 5.04s
WARNING:root: [*] Thu Dec 22 11:08:16 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.483129 | F1-score: 0.87 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:08:21 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.447876 | F1-score: 0.88 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:08:26 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.357038 | F1-score: 0.88 | Elapsed: 5.03s
WARNING:root: [*] Thu Dec 22 11:08:31 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.227326 | F1-score: 0.88 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:08:33 2022:    1    | Tr.loss: 0.428822 | Tr.F1.:   0.88    |   47.69  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 11:08:33 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.365632 | F1-score: 0.90 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 11:08:38 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.337500 | F1-score: 0.91 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:08:43 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.374750 | F1-score: 0.91 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:08:48 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.376460 | F1-score: 0.91 | Elapsed: 5.03s
WARNING:root: [*] Thu Dec 22 11:08:53 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.221643 | F1-score: 0.91 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:08:58 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.484660 | F1-score: 0.91 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 11:09:03 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.355373 | F1-score: 0.91 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:09:08 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.215595 | F1-score: 0.92 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:09:13 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.341723 | F1-score: 0.92 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:09:18 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.242114 | F1-score: 0.92 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:09:21 2022:    2    | Tr.loss: 0.319153 | Tr.F1.:   0.92    |   47.62  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 11:09:21 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.225257 | F1-score: 0.97 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 11:09:26 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.277957 | F1-score: 0.93 | Elapsed: 5.04s
WARNING:root: [*] Thu Dec 22 11:09:31 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.211177 | F1-score: 0.93 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:09:36 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.201747 | F1-score: 0.93 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:09:41 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.233924 | F1-score: 0.93 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:09:46 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.168692 | F1-score: 0.93 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:09:51 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.295496 | F1-score: 0.93 | Elapsed: 5.03s
WARNING:root: [*] Thu Dec 22 11:09:56 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.300333 | F1-score: 0.93 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 11:10:01 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.385037 | F1-score: 0.93 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:10:06 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.241963 | F1-score: 0.93 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 11:10:09 2022:    3    | Tr.loss: 0.272254 | Tr.F1.:   0.93    |   47.64  s
WARNING:root:
        [!] Thu Dec 22 11:10:09 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703809-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703809-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703809-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671703809-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\metrics_trainSize_91096_ep_3_cv_3_nTokens_2000_dModel_32_nHeads_8_dHidden_200_nLayers_2_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 47.60s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0071 -- F1: 0.0140
	FPR:  0.001 -- TPR: 0.1019 -- F1: 0.1846
	FPR:   0.01 -- TPR: 0.2888 -- F1: 0.4464
	FPR:    0.1 -- TPR: 0.4634 -- F1: 0.6162

WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'nTokens': 2000, 'dModel': 32, 'nHeads': 16, 'dHidden': 200, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 11:10:46 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.725468 | F1-score: 0.04 | Elapsed: 0.38s
WARNING:root: [*] Thu Dec 22 11:10:53 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.532309 | F1-score: 0.82 | Elapsed: 7.26s
WARNING:root: [*] Thu Dec 22 11:11:00 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.324320 | F1-score: 0.85 | Elapsed: 7.27s
WARNING:root: [*] Thu Dec 22 11:11:07 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.485659 | F1-score: 0.86 | Elapsed: 7.28s
WARNING:root: [*] Thu Dec 22 11:11:15 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.324505 | F1-score: 0.87 | Elapsed: 7.31s
WARNING:root: [*] Thu Dec 22 11:11:22 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.443300 | F1-score: 0.88 | Elapsed: 7.31s
WARNING:root: [*] Thu Dec 22 11:11:29 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.327255 | F1-score: 0.88 | Elapsed: 7.35s
WARNING:root: [*] Thu Dec 22 11:11:37 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.281167 | F1-score: 0.88 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:11:44 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.373267 | F1-score: 0.89 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:11:51 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.251812 | F1-score: 0.89 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:11:55 2022:    1    | Tr.loss: 0.405972 | Tr.F1.:   0.89    |   69.70  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 11:11:55 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.233723 | F1-score: 0.95 | Elapsed: 0.08s
WARNING:root: [*] Thu Dec 22 11:12:02 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.299450 | F1-score: 0.92 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:12:10 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.304396 | F1-score: 0.92 | Elapsed: 7.37s
WARNING:root: [*] Thu Dec 22 11:12:17 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.224216 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:12:24 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.213655 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:12:32 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.242694 | F1-score: 0.92 | Elapsed: 7.30s
WARNING:root: [*] Thu Dec 22 11:12:39 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.156812 | F1-score: 0.93 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:12:46 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.218438 | F1-score: 0.93 | Elapsed: 7.30s
WARNING:root: [*] Thu Dec 22 11:12:54 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.319220 | F1-score: 0.93 | Elapsed: 7.30s
WARNING:root: [*] Thu Dec 22 11:13:01 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.167742 | F1-score: 0.93 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:13:04 2022:    2    | Tr.loss: 0.282497 | Tr.F1.:   0.93    |   69.51  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 11:13:04 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.163753 | F1-score: 0.94 | Elapsed: 0.08s
WARNING:root: [*] Thu Dec 22 11:13:12 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.260347 | F1-score: 0.93 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:13:19 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.284577 | F1-score: 0.94 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:13:26 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.267948 | F1-score: 0.94 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:13:34 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.247359 | F1-score: 0.94 | Elapsed: 7.31s
WARNING:root: [*] Thu Dec 22 11:13:41 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.294914 | F1-score: 0.94 | Elapsed: 7.30s
WARNING:root: [*] Thu Dec 22 11:13:48 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.409679 | F1-score: 0.94 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:13:56 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.308028 | F1-score: 0.94 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:14:03 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.212685 | F1-score: 0.94 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:14:10 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.239691 | F1-score: 0.94 | Elapsed: 7.31s
WARNING:root: [*] Thu Dec 22 11:14:14 2022:    3    | Tr.loss: 0.253414 | Tr.F1.:   0.94    |   69.50  s
WARNING:root:
        [!] Thu Dec 22 11:14:14 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704054-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704054-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704054-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704054-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 11:14:23 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.738249 | F1-score: 0.18 | Elapsed: 0.08s
WARNING:root: [*] Thu Dec 22 11:14:31 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.546939 | F1-score: 0.83 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:14:38 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.621172 | F1-score: 0.85 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:14:45 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.474538 | F1-score: 0.86 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:14:53 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.346669 | F1-score: 0.87 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:15:00 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.354897 | F1-score: 0.88 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:15:07 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.380150 | F1-score: 0.88 | Elapsed: 7.35s
WARNING:root: [*] Thu Dec 22 11:15:15 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.414567 | F1-score: 0.88 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:15:22 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.343893 | F1-score: 0.88 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:15:29 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.335914 | F1-score: 0.89 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:15:33 2022:    1    | Tr.loss: 0.419756 | Tr.F1.:   0.89    |   69.55  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 11:15:33 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.307739 | F1-score: 0.90 | Elapsed: 0.08s
WARNING:root: [*] Thu Dec 22 11:15:40 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.301288 | F1-score: 0.91 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:15:48 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.418475 | F1-score: 0.92 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:15:55 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.277887 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:16:02 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.455775 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:16:09 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.228349 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:16:17 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.275579 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:16:24 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.281544 | F1-score: 0.92 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:16:32 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.242820 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:16:39 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.302000 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:16:42 2022:    2    | Tr.loss: 0.302422 | Tr.F1.:   0.92    |   69.58  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 11:16:42 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.224413 | F1-score: 0.96 | Elapsed: 0.09s
WARNING:root: [*] Thu Dec 22 11:16:50 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.290382 | F1-score: 0.93 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:16:57 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.162129 | F1-score: 0.93 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:17:04 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.184516 | F1-score: 0.93 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:17:12 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.244429 | F1-score: 0.93 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:17:19 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.225854 | F1-score: 0.93 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:17:26 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.279534 | F1-score: 0.93 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:17:34 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.223927 | F1-score: 0.93 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:17:41 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.201271 | F1-score: 0.93 | Elapsed: 7.35s
WARNING:root: [*] Thu Dec 22 11:17:48 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.237943 | F1-score: 0.93 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:17:52 2022:    3    | Tr.loss: 0.262415 | Tr.F1.:   0.94    |   69.62  s
WARNING:root:
        [!] Thu Dec 22 11:17:52 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704272-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704272-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704272-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704272-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 11:18:01 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.689044 | F1-score: 0.67 | Elapsed: 0.09s
WARNING:root: [*] Thu Dec 22 11:18:09 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.555355 | F1-score: 0.83 | Elapsed: 7.35s
WARNING:root: [*] Thu Dec 22 11:18:16 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.319103 | F1-score: 0.86 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:18:23 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.434579 | F1-score: 0.87 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:18:31 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.454931 | F1-score: 0.87 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:18:38 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.501808 | F1-score: 0.87 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:18:45 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.280429 | F1-score: 0.88 | Elapsed: 7.35s
WARNING:root: [*] Thu Dec 22 11:18:53 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.417007 | F1-score: 0.88 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:19:00 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.489861 | F1-score: 0.88 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:19:07 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.443367 | F1-score: 0.88 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:19:11 2022:    1    | Tr.loss: 0.430329 | Tr.F1.:   0.88    |   69.66  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 11:19:11 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.423451 | F1-score: 0.92 | Elapsed: 0.09s
WARNING:root: [*] Thu Dec 22 11:19:18 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.435350 | F1-score: 0.91 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:19:26 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.250485 | F1-score: 0.91 | Elapsed: 7.35s
WARNING:root: [*] Thu Dec 22 11:19:33 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.388146 | F1-score: 0.91 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:19:40 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.301228 | F1-score: 0.91 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:19:48 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.310547 | F1-score: 0.91 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:19:55 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.352893 | F1-score: 0.91 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:20:02 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.414569 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:20:10 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.434292 | F1-score: 0.92 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:20:17 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.308554 | F1-score: 0.92 | Elapsed: 7.36s
WARNING:root: [*] Thu Dec 22 11:20:21 2022:    2    | Tr.loss: 0.320780 | Tr.F1.:   0.92    |   69.69  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 11:20:21 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.300959 | F1-score: 0.92 | Elapsed: 0.08s
WARNING:root: [*] Thu Dec 22 11:20:28 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.298148 | F1-score: 0.93 | Elapsed: 7.36s
WARNING:root: [*] Thu Dec 22 11:20:35 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.256851 | F1-score: 0.93 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:20:43 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.196876 | F1-score: 0.93 | Elapsed: 7.32s
WARNING:root: [*] Thu Dec 22 11:20:50 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.340367 | F1-score: 0.93 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:20:57 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.229544 | F1-score: 0.93 | Elapsed: 7.35s
WARNING:root: [*] Thu Dec 22 11:21:05 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.268982 | F1-score: 0.93 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:21:12 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.301761 | F1-score: 0.93 | Elapsed: 7.36s
WARNING:root: [*] Thu Dec 22 11:21:20 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.297012 | F1-score: 0.93 | Elapsed: 7.34s
WARNING:root: [*] Thu Dec 22 11:21:27 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.075873 | F1-score: 0.93 | Elapsed: 7.33s
WARNING:root: [*] Thu Dec 22 11:21:30 2022:    3    | Tr.loss: 0.263958 | Tr.F1.:   0.93    |   69.68  s
WARNING:root:
        [!] Thu Dec 22 11:21:30 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704490-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704490-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704490-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\trainingFiles\trainingFiles_1671704490-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfHeads\metrics_trainSize_91096_ep_3_cv_3_nTokens_2000_dModel_32_nHeads_16_dHidden_200_nLayers_2_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 69.61s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0149 -- F1: 0.0285
	FPR:  0.001 -- TPR: 0.0838 -- F1: 0.1545
	FPR:   0.01 -- TPR: 0.3314 -- F1: 0.4946
	FPR:    0.1 -- TPR: 0.5383 -- F1: 0.6801

