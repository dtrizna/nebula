WARNING:root: [!] Using vocabSize: 1000 | maxLen: 512
WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 1000, 'embeddingDim': 64, 'hiddenDim': 128, 'lstmLayers': 1, 'bidirectionalLSTM': True, 'outputDim': 1}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 20:36:47 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.700744 | F1-score: 0.33 | Elapsed: 0.35s
WARNING:root: [*] Thu Dec 22 20:36:52 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.414031 | F1-score: 0.85 | Elapsed: 4.82s
WARNING:root: [*] Thu Dec 22 20:36:57 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.623188 | F1-score: 0.87 | Elapsed: 4.85s
WARNING:root: [*] Thu Dec 22 20:37:02 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.582648 | F1-score: 0.87 | Elapsed: 4.95s
WARNING:root: [*] Thu Dec 22 20:37:07 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.404739 | F1-score: 0.88 | Elapsed: 4.95s
WARNING:root: [*] Thu Dec 22 20:37:12 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.422817 | F1-score: 0.88 | Elapsed: 4.99s
WARNING:root: [*] Thu Dec 22 20:37:17 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.329592 | F1-score: 0.89 | Elapsed: 4.96s
WARNING:root: [*] Thu Dec 22 20:37:22 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.431165 | F1-score: 0.89 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 20:37:27 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.309094 | F1-score: 0.89 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 20:37:32 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.370636 | F1-score: 0.89 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 20:37:34 2022:    1    | Tr.loss: 0.395696 | Tr.F1.:   0.89    |   47.30  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 20:37:34 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.319014 | F1-score: 0.95 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 20:37:39 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.301134 | F1-score: 0.91 | Elapsed: 5.05s
WARNING:root: [*] Thu Dec 22 20:37:44 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.351366 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 20:37:49 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.468498 | F1-score: 0.91 | Elapsed: 5.05s
WARNING:root: [*] Thu Dec 22 20:37:54 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.418373 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 20:37:59 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.258517 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:38:04 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.289038 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:38:10 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.279012 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:38:15 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.270271 | F1-score: 0.92 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 20:38:20 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.313125 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:38:22 2022:    2    | Tr.loss: 0.302127 | Tr.F1.:   0.92    |   48.13  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 20:38:22 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.329567 | F1-score: 0.92 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 20:38:27 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.200040 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:38:32 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.342734 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 20:38:37 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.484704 | F1-score: 0.93 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 20:38:42 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.328686 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:38:48 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.227358 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 20:38:53 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.387277 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 20:38:58 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.348703 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:39:03 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.236666 | F1-score: 0.93 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 20:39:08 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.195529 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 20:39:10 2022:    3    | Tr.loss: 0.266934 | Tr.F1.:   0.93    |   48.19  s
WARNING:root:
        [!] Thu Dec 22 20:39:10 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671737950-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671737950-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671737950-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671737950-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 20:39:20 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.727425 | F1-score: 0.00 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 20:39:25 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.481894 | F1-score: 0.84 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:39:30 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.363577 | F1-score: 0.86 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 20:39:35 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.507992 | F1-score: 0.87 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:39:40 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.353559 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:39:45 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.249821 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:39:50 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.435064 | F1-score: 0.88 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 20:39:55 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.269110 | F1-score: 0.89 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 20:40:01 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.378831 | F1-score: 0.89 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 20:40:06 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.348604 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:40:08 2022:    1    | Tr.loss: 0.398889 | Tr.F1.:   0.89    |   48.15  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 20:40:08 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.275802 | F1-score: 0.93 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 20:40:13 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.308867 | F1-score: 0.91 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 20:40:18 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.321905 | F1-score: 0.91 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 20:40:23 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.435177 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 20:40:28 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.268689 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:40:34 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.312745 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:40:39 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.243127 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:40:44 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.428481 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 20:40:49 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.242075 | F1-score: 0.92 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 20:40:54 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.329293 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:40:56 2022:    2    | Tr.loss: 0.309193 | Tr.F1.:   0.92    |   48.22  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 20:40:56 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.325904 | F1-score: 0.92 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 20:41:01 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.244299 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 20:41:06 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.293358 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:41:12 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.173905 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:41:17 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.208082 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:41:22 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.242102 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:41:27 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.334103 | F1-score: 0.93 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 20:41:32 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.233009 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 20:41:37 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.320943 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 20:41:42 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.316035 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:41:44 2022:    3    | Tr.loss: 0.271155 | Tr.F1.:   0.93    |   48.18  s
WARNING:root:
        [!] Thu Dec 22 20:41:44 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738104-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738104-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738104-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738104-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 20:41:54 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.691022 | F1-score: 0.69 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 20:41:59 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.405740 | F1-score: 0.85 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 20:42:04 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.450576 | F1-score: 0.87 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:42:09 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.362275 | F1-score: 0.87 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 20:42:14 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.415983 | F1-score: 0.88 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 20:42:19 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.408332 | F1-score: 0.88 | Elapsed: 4.97s
WARNING:root: [*] Thu Dec 22 20:42:24 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.384993 | F1-score: 0.88 | Elapsed: 4.97s
WARNING:root: [*] Thu Dec 22 20:42:29 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.284254 | F1-score: 0.89 | Elapsed: 4.97s
WARNING:root: [*] Thu Dec 22 20:42:34 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.233016 | F1-score: 0.89 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 20:42:39 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.365492 | F1-score: 0.89 | Elapsed: 5.05s
WARNING:root: [*] Thu Dec 22 20:42:42 2022:    1    | Tr.loss: 0.394511 | Tr.F1.:   0.89    |   47.73  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 20:42:42 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.405013 | F1-score: 0.90 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 20:42:47 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.291333 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 20:42:52 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.390123 | F1-score: 0.91 | Elapsed: 5.01s
WARNING:root: [*] Thu Dec 22 20:42:57 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.290072 | F1-score: 0.91 | Elapsed: 5.18s
WARNING:root: [*] Thu Dec 22 20:43:02 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.231747 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 20:43:07 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.181099 | F1-score: 0.92 | Elapsed: 5.17s
WARNING:root: [*] Thu Dec 22 20:43:13 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.276280 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 20:43:18 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.334198 | F1-score: 0.92 | Elapsed: 5.25s
WARNING:root: [*] Thu Dec 22 20:43:23 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.197908 | F1-score: 0.92 | Elapsed: 5.16s
WARNING:root: [*] Thu Dec 22 20:43:28 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.264553 | F1-score: 0.92 | Elapsed: 5.21s
WARNING:root: [*] Thu Dec 22 20:43:31 2022:    2    | Tr.loss: 0.296972 | Tr.F1.:   0.92    |   48.79  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 20:43:31 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.286892 | F1-score: 0.91 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 20:43:36 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.346019 | F1-score: 0.93 | Elapsed: 5.19s
WARNING:root: [*] Thu Dec 22 20:43:41 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.278688 | F1-score: 0.93 | Elapsed: 5.21s
WARNING:root: [*] Thu Dec 22 20:43:46 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.240254 | F1-score: 0.93 | Elapsed: 5.14s
WARNING:root: [*] Thu Dec 22 20:43:51 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.313462 | F1-score: 0.93 | Elapsed: 5.25s
WARNING:root: [*] Thu Dec 22 20:43:57 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.251291 | F1-score: 0.93 | Elapsed: 5.21s
WARNING:root: [*] Thu Dec 22 20:44:02 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.136852 | F1-score: 0.93 | Elapsed: 5.20s
WARNING:root: [*] Thu Dec 22 20:44:07 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.291352 | F1-score: 0.93 | Elapsed: 5.15s
WARNING:root: [*] Thu Dec 22 20:44:12 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.199389 | F1-score: 0.93 | Elapsed: 5.15s
WARNING:root: [*] Thu Dec 22 20:44:17 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.308918 | F1-score: 0.93 | Elapsed: 5.24s
WARNING:root: [*] Thu Dec 22 20:44:20 2022:    3    | Tr.loss: 0.261454 | Tr.F1.:   0.93    |   49.27  s
WARNING:root:
        [!] Thu Dec 22 20:44:20 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738260-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738260-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738260-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738260-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\metrics_trainSize_91096_ep_3_cv_3_maxLen_512_vocabSize_1000_embeddingDim_64_hiddenDim_128_lstmLayers_1_bidirectionalLSTM_True_outputDim_1.json
WARNING:root: [!] Average epoch time: 48.22s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0248 -- F1: 0.0462
	FPR:  0.001 -- TPR: 0.1284 -- F1: 0.2275
	FPR:   0.01 -- TPR: 0.3623 -- F1: 0.5300
	FPR:    0.1 -- TPR: 0.7313 -- F1: 0.8221

WARNING:root: [!] Using vocabSize: 1500 | maxLen: 1024
WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 1500, 'embeddingDim': 64, 'hiddenDim': 128, 'lstmLayers': 1, 'bidirectionalLSTM': True, 'outputDim': 1}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 20:45:00 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.682810 | F1-score: 0.79 | Elapsed: 0.39s
WARNING:root: [*] Thu Dec 22 20:45:10 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.547684 | F1-score: 0.84 | Elapsed: 9.78s
WARNING:root: [*] Thu Dec 22 20:45:20 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.447734 | F1-score: 0.86 | Elapsed: 10.10s
WARNING:root: [*] Thu Dec 22 20:45:30 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.432742 | F1-score: 0.87 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:45:40 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.446239 | F1-score: 0.87 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:45:51 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.202833 | F1-score: 0.88 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:46:01 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.454155 | F1-score: 0.88 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:46:11 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.370256 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:46:21 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.606527 | F1-score: 0.88 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:46:31 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.371631 | F1-score: 0.89 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:46:36 2022:    1    | Tr.loss: 0.413317 | Tr.F1.:   0.89    |   96.33  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 20:46:36 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.327183 | F1-score: 0.92 | Elapsed: 0.09s
WARNING:root: [*] Thu Dec 22 20:46:46 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.438674 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:46:57 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.331309 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:47:07 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.347791 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:47:17 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.320676 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:47:27 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.296372 | F1-score: 0.91 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:47:37 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.384082 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:47:47 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.278707 | F1-score: 0.91 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 20:47:58 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.298521 | F1-score: 0.91 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:48:08 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.405027 | F1-score: 0.91 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:48:13 2022:    2    | Tr.loss: 0.318717 | Tr.F1.:   0.91    |   96.40  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 20:48:13 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.250732 | F1-score: 0.93 | Elapsed: 0.11s
WARNING:root: [*] Thu Dec 22 20:48:23 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.408857 | F1-score: 0.92 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:48:33 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.371853 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:48:43 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.220989 | F1-score: 0.92 | Elapsed: 10.18s
WARNING:root: [*] Thu Dec 22 20:48:54 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.291795 | F1-score: 0.92 | Elapsed: 10.36s
WARNING:root: [*] Thu Dec 22 20:49:04 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.305162 | F1-score: 0.92 | Elapsed: 10.30s
WARNING:root: [*] Thu Dec 22 20:49:14 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.259298 | F1-score: 0.92 | Elapsed: 10.32s
WARNING:root: [*] Thu Dec 22 20:49:24 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.228031 | F1-score: 0.92 | Elapsed: 10.23s
WARNING:root: [*] Thu Dec 22 20:49:35 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.283971 | F1-score: 0.92 | Elapsed: 10.23s
WARNING:root: [*] Thu Dec 22 20:49:45 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.295717 | F1-score: 0.92 | Elapsed: 10.29s
WARNING:root: [*] Thu Dec 22 20:49:50 2022:    3    | Tr.loss: 0.284101 | Tr.F1.:   0.92    |   97.22  s
WARNING:root:
        [!] Thu Dec 22 20:49:50 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738590-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738590-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738590-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738590-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 20:50:09 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.731387 | F1-score: 0.00 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 20:50:19 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.557383 | F1-score: 0.83 | Elapsed: 10.18s
WARNING:root: [*] Thu Dec 22 20:50:30 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.346201 | F1-score: 0.85 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:50:40 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.520576 | F1-score: 0.86 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:50:50 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.373523 | F1-score: 0.87 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:51:00 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.405481 | F1-score: 0.87 | Elapsed: 10.18s
WARNING:root: [*] Thu Dec 22 20:51:10 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.446268 | F1-score: 0.88 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:51:20 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.422855 | F1-score: 0.88 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:51:31 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.401535 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:51:41 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.414251 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:51:46 2022:    1    | Tr.loss: 0.425476 | Tr.F1.:   0.88    |   96.50  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 20:51:46 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.278952 | F1-score: 0.95 | Elapsed: 0.12s
WARNING:root: [*] Thu Dec 22 20:51:56 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.414434 | F1-score: 0.90 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:52:06 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.271431 | F1-score: 0.90 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:52:16 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.295251 | F1-score: 0.90 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:52:26 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.254306 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:52:37 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.259531 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:52:47 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.312915 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:52:57 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.240976 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:53:07 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.327594 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:53:17 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.362999 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:53:22 2022:    2    | Tr.loss: 0.323414 | Tr.F1.:   0.91    |   96.45  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 20:53:22 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.269424 | F1-score: 0.93 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 20:53:32 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.364323 | F1-score: 0.92 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:53:42 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.365862 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:53:53 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.353548 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:54:03 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.231889 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:54:13 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.197666 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:54:23 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.254158 | F1-score: 0.92 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:54:33 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.234641 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:54:43 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.271546 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:54:54 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.390502 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:54:58 2022:    3    | Tr.loss: 0.286441 | Tr.F1.:   0.92    |   96.40  s
WARNING:root:
        [!] Thu Dec 22 20:54:58 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738898-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738898-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738898-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671738898-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 20:55:18 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.704364 | F1-score: 0.00 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 20:55:28 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.559986 | F1-score: 0.83 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:55:38 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.524135 | F1-score: 0.85 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:55:48 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.328183 | F1-score: 0.86 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:55:58 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.404704 | F1-score: 0.87 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:56:08 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.303204 | F1-score: 0.87 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:56:19 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.436502 | F1-score: 0.87 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:56:29 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.401976 | F1-score: 0.88 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:56:39 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.390376 | F1-score: 0.88 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:56:49 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.475775 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:56:54 2022:    1    | Tr.loss: 0.425433 | Tr.F1.:   0.88    |   96.39  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 20:56:54 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.419228 | F1-score: 0.94 | Elapsed: 0.09s
WARNING:root: [*] Thu Dec 22 20:57:04 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.308560 | F1-score: 0.90 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:57:14 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.364339 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:57:25 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.292103 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:57:35 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.263869 | F1-score: 0.91 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:57:45 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.273677 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:57:55 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.328771 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:58:05 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.347609 | F1-score: 0.91 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:58:15 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.396781 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:58:25 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.204026 | F1-score: 0.91 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 20:58:30 2022:    2    | Tr.loss: 0.321491 | Tr.F1.:   0.91    |   96.38  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 20:58:30 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.225263 | F1-score: 0.94 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 20:58:41 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.205104 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:58:51 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.272601 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:59:01 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.341665 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:59:11 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.230116 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:59:21 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.320757 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 20:59:31 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.210458 | F1-score: 0.92 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 20:59:42 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.233324 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 20:59:52 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.269530 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:00:02 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.205429 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:00:07 2022:    3    | Tr.loss: 0.283344 | Tr.F1.:   0.92    |   96.38  s
WARNING:root:
        [!] Thu Dec 22 21:00:07 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739207-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739207-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739207-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739207-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\metrics_trainSize_91096_ep_3_cv_3_maxLen_1024_vocabSize_1500_embeddingDim_64_hiddenDim_128_lstmLayers_1_bidirectionalLSTM_True_outputDim_1.json
WARNING:root: [!] Average epoch time: 96.49s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0296 -- F1: 0.0564
	FPR:  0.001 -- TPR: 0.1826 -- F1: 0.3083
	FPR:   0.01 -- TPR: 0.3608 -- F1: 0.5277
	FPR:    0.1 -- TPR: 0.6195 -- F1: 0.7468

WARNING:root: [!] Using vocabSize: 1500 | maxLen: 512
WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 1500, 'embeddingDim': 64, 'hiddenDim': 128, 'lstmLayers': 1, 'bidirectionalLSTM': True, 'outputDim': 1}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:00:56 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.694808 | F1-score: 0.46 | Elapsed: 0.22s
WARNING:root: [*] Thu Dec 22 21:01:01 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.353134 | F1-score: 0.85 | Elapsed: 4.97s
WARNING:root: [*] Thu Dec 22 21:01:06 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.383307 | F1-score: 0.87 | Elapsed: 5.00s
WARNING:root: [*] Thu Dec 22 21:01:11 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.399404 | F1-score: 0.88 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 21:01:16 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.414203 | F1-score: 0.88 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:01:22 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.524237 | F1-score: 0.88 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:01:27 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.355166 | F1-score: 0.88 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:01:32 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.394293 | F1-score: 0.89 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:01:37 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.403314 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:01:42 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.329249 | F1-score: 0.89 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:01:44 2022:    1    | Tr.loss: 0.401541 | Tr.F1.:   0.89    |   48.12  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:01:44 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.361174 | F1-score: 0.91 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 21:01:49 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.259184 | F1-score: 0.90 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:01:54 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.320800 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:02:00 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.243289 | F1-score: 0.91 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:02:05 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.237136 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:02:10 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.213076 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:02:15 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.309354 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:02:20 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.277931 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:02:25 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.334274 | F1-score: 0.91 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:02:30 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.368610 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:02:32 2022:    2    | Tr.loss: 0.308883 | Tr.F1.:   0.92    |   48.23  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:02:33 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.264057 | F1-score: 0.94 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 21:02:38 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.352459 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:02:43 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.434910 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:02:48 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.272122 | F1-score: 0.93 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:02:53 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.281378 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:02:58 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.265986 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:03:03 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.194360 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:03:08 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.277116 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:03:13 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.229803 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:03:18 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.257395 | F1-score: 0.93 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:03:21 2022:    3    | Tr.loss: 0.269484 | Tr.F1.:   0.93    |   48.21  s
WARNING:root:
        [!] Thu Dec 22 21:03:21 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739401-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739401-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739401-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739401-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:03:30 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.691293 | F1-score: 0.75 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 21:03:36 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.457418 | F1-score: 0.85 | Elapsed: 5.17s
WARNING:root: [*] Thu Dec 22 21:03:41 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.436119 | F1-score: 0.87 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:03:46 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.541601 | F1-score: 0.88 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:03:51 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.487528 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:03:56 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.336977 | F1-score: 0.88 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:04:01 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.402312 | F1-score: 0.88 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:04:06 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.304236 | F1-score: 0.88 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:04:11 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.269327 | F1-score: 0.89 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:04:16 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.368183 | F1-score: 0.89 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:04:19 2022:    1    | Tr.loss: 0.402643 | Tr.F1.:   0.89    |   48.40  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:04:19 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.286255 | F1-score: 0.95 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 21:04:24 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.386563 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:04:29 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.327631 | F1-score: 0.91 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:04:34 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.265781 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:04:39 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.334505 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:04:44 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.356673 | F1-score: 0.92 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:04:49 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.238248 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:04:54 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.409299 | F1-score: 0.92 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:05:00 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.230010 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:05:05 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.224713 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:05:07 2022:    2    | Tr.loss: 0.307324 | Tr.F1.:   0.92    |   48.40  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:05:07 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.270915 | F1-score: 0.93 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 21:05:12 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.230075 | F1-score: 0.93 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:05:17 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.226948 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:05:23 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.341988 | F1-score: 0.93 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:05:28 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.287380 | F1-score: 0.93 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:05:33 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.291585 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:05:38 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.263626 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:05:43 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.409215 | F1-score: 0.93 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:05:48 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.256685 | F1-score: 0.93 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:05:53 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.170023 | F1-score: 0.93 | Elapsed: 5.13s
WARNING:root: [*] Thu Dec 22 21:05:56 2022:    3    | Tr.loss: 0.272412 | Tr.F1.:   0.93    |   48.49  s
WARNING:root:
        [!] Thu Dec 22 21:05:56 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739556-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739556-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739556-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739556-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:06:05 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.681149 | F1-score: 0.84 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 21:06:10 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.377594 | F1-score: 0.84 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:06:16 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.412570 | F1-score: 0.87 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:06:21 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.440621 | F1-score: 0.87 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:06:26 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.477467 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:06:31 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.301237 | F1-score: 0.88 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:06:36 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.386269 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:06:41 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.337207 | F1-score: 0.89 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:06:46 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.328346 | F1-score: 0.89 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:06:51 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.359066 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:06:54 2022:    1    | Tr.loss: 0.398203 | Tr.F1.:   0.89    |   48.39  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:06:54 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.238643 | F1-score: 0.96 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 21:06:59 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.338353 | F1-score: 0.91 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:07:04 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.346744 | F1-score: 0.91 | Elapsed: 5.14s
WARNING:root: [*] Thu Dec 22 21:07:09 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.239562 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:07:14 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.359974 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:07:19 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.443921 | F1-score: 0.92 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:07:24 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.251665 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:07:30 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.321242 | F1-score: 0.92 | Elapsed: 5.13s
WARNING:root: [*] Thu Dec 22 21:07:35 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.218843 | F1-score: 0.92 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:07:40 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.324803 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:07:42 2022:    2    | Tr.loss: 0.296514 | Tr.F1.:   0.92    |   48.50  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:07:42 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.333413 | F1-score: 0.91 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 21:07:47 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.424075 | F1-score: 0.93 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:07:52 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.356039 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:07:58 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.173220 | F1-score: 0.93 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:08:03 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.133383 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:08:08 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.204475 | F1-score: 0.93 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:08:13 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.209662 | F1-score: 0.93 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:08:18 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.191156 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:08:23 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.199742 | F1-score: 0.93 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:08:28 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.149181 | F1-score: 0.93 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:08:31 2022:    3    | Tr.loss: 0.260294 | Tr.F1.:   0.93    |   48.41  s
WARNING:root:
        [!] Thu Dec 22 21:08:31 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739711-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739711-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739711-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671739711-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\metrics_trainSize_91096_ep_3_cv_3_maxLen_512_vocabSize_1500_embeddingDim_64_hiddenDim_128_lstmLayers_1_bidirectionalLSTM_True_outputDim_1.json
WARNING:root: [!] Average epoch time: 48.35s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0237 -- F1: 0.0443
	FPR:  0.001 -- TPR: 0.1223 -- F1: 0.2174
	FPR:   0.01 -- TPR: 0.3575 -- F1: 0.5232
	FPR:    0.1 -- TPR: 0.7921 -- F1: 0.8629

WARNING:root: [!] Using vocabSize: 2000 | maxLen: 1024
WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 2000, 'embeddingDim': 64, 'hiddenDim': 128, 'lstmLayers': 1, 'bidirectionalLSTM': True, 'outputDim': 1}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:09:11 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.691935 | F1-score: 0.66 | Elapsed: 0.25s
WARNING:root: [*] Thu Dec 22 21:09:20 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.523681 | F1-score: 0.85 | Elapsed: 9.67s
WARNING:root: [*] Thu Dec 22 21:09:30 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.426069 | F1-score: 0.87 | Elapsed: 9.82s
WARNING:root: [*] Thu Dec 22 21:09:40 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.539873 | F1-score: 0.87 | Elapsed: 9.96s
WARNING:root: [*] Thu Dec 22 21:09:50 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.587282 | F1-score: 0.88 | Elapsed: 10.04s
WARNING:root: [*] Thu Dec 22 21:10:00 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.390070 | F1-score: 0.88 | Elapsed: 10.09s
WARNING:root: [*] Thu Dec 22 21:10:10 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.436819 | F1-score: 0.88 | Elapsed: 10.06s
WARNING:root: [*] Thu Dec 22 21:10:20 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.362119 | F1-score: 0.88 | Elapsed: 10.01s
WARNING:root: [*] Thu Dec 22 21:10:31 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.336915 | F1-score: 0.88 | Elapsed: 10.07s
WARNING:root: [*] Thu Dec 22 21:10:41 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.332980 | F1-score: 0.89 | Elapsed: 10.04s
WARNING:root: [*] Thu Dec 22 21:10:45 2022:    1    | Tr.loss: 0.418262 | Tr.F1.:   0.89    |   94.81  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:10:45 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.315788 | F1-score: 0.93 | Elapsed: 0.11s
WARNING:root: [*] Thu Dec 22 21:10:55 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.412108 | F1-score: 0.90 | Elapsed: 10.05s
WARNING:root: [*] Thu Dec 22 21:11:06 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.421512 | F1-score: 0.90 | Elapsed: 10.10s
WARNING:root: [*] Thu Dec 22 21:11:15 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.216542 | F1-score: 0.90 | Elapsed: 9.86s
WARNING:root: [*] Thu Dec 22 21:11:25 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.292864 | F1-score: 0.91 | Elapsed: 9.88s
WARNING:root: [*] Thu Dec 22 21:11:35 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.287819 | F1-score: 0.91 | Elapsed: 9.96s
WARNING:root: [*] Thu Dec 22 21:11:45 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.268857 | F1-score: 0.91 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:11:56 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.328178 | F1-score: 0.91 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:12:06 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.430021 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:12:16 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.260961 | F1-score: 0.91 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:12:21 2022:    2    | Tr.loss: 0.321953 | Tr.F1.:   0.91    |   95.36  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:12:21 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.319265 | F1-score: 0.90 | Elapsed: 0.11s
WARNING:root: [*] Thu Dec 22 21:12:31 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.334451 | F1-score: 0.92 | Elapsed: 10.10s
WARNING:root: [*] Thu Dec 22 21:12:41 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.305592 | F1-score: 0.92 | Elapsed: 9.94s
WARNING:root: [*] Thu Dec 22 21:12:51 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.280890 | F1-score: 0.92 | Elapsed: 10.00s
WARNING:root: [*] Thu Dec 22 21:13:01 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.291632 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:13:11 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.281624 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:13:21 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.178424 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:13:31 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.330276 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:13:42 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.270914 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:13:52 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.491984 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:13:57 2022:    3    | Tr.loss: 0.285384 | Tr.F1.:   0.92    |   95.89  s
WARNING:root:
        [!] Thu Dec 22 21:13:57 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740037-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740037-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740037-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740037-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:14:16 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.690324 | F1-score: 0.75 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 21:14:26 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.570345 | F1-score: 0.84 | Elapsed: 10.11s
WARNING:root: [*] Thu Dec 22 21:14:36 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.364501 | F1-score: 0.85 | Elapsed: 10.10s
WARNING:root: [*] Thu Dec 22 21:14:46 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.394867 | F1-score: 0.86 | Elapsed: 10.02s
WARNING:root: [*] Thu Dec 22 21:14:56 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.358308 | F1-score: 0.87 | Elapsed: 10.02s
WARNING:root: [*] Thu Dec 22 21:15:06 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.349012 | F1-score: 0.87 | Elapsed: 10.10s
WARNING:root: [*] Thu Dec 22 21:15:16 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.347875 | F1-score: 0.88 | Elapsed: 10.12s
WARNING:root: [*] Thu Dec 22 21:15:26 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.326364 | F1-score: 0.88 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:15:36 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.407012 | F1-score: 0.88 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:15:46 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.388488 | F1-score: 0.88 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:15:51 2022:    1    | Tr.loss: 0.423774 | Tr.F1.:   0.88    |   95.82  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:15:51 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.424586 | F1-score: 0.88 | Elapsed: 0.11s
WARNING:root: [*] Thu Dec 22 21:16:02 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.419375 | F1-score: 0.90 | Elapsed: 10.10s
WARNING:root: [*] Thu Dec 22 21:16:12 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.404869 | F1-score: 0.90 | Elapsed: 10.10s
WARNING:root: [*] Thu Dec 22 21:16:22 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.267320 | F1-score: 0.91 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:16:32 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.281579 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:16:42 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.244803 | F1-score: 0.91 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:16:52 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.329583 | F1-score: 0.91 | Elapsed: 10.12s
WARNING:root: [*] Thu Dec 22 21:17:02 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.310869 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:17:13 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.303679 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:17:23 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.277742 | F1-score: 0.91 | Elapsed: 10.06s
WARNING:root: [*] Thu Dec 22 21:17:27 2022:    2    | Tr.loss: 0.318930 | Tr.F1.:   0.91    |   96.09  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:17:28 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.275214 | F1-score: 0.91 | Elapsed: 0.11s
WARNING:root: [*] Thu Dec 22 21:17:38 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.302111 | F1-score: 0.92 | Elapsed: 10.07s
WARNING:root: [*] Thu Dec 22 21:17:48 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.313281 | F1-score: 0.92 | Elapsed: 10.12s
WARNING:root: [*] Thu Dec 22 21:17:58 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.196760 | F1-score: 0.92 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:18:08 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.320581 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:18:18 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.272900 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:18:28 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.327740 | F1-score: 0.92 | Elapsed: 9.97s
WARNING:root: [*] Thu Dec 22 21:18:38 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.230006 | F1-score: 0.92 | Elapsed: 9.81s
WARNING:root: [*] Thu Dec 22 21:18:48 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.273570 | F1-score: 0.92 | Elapsed: 9.83s
WARNING:root: [*] Thu Dec 22 21:18:58 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.396997 | F1-score: 0.92 | Elapsed: 10.03s
WARNING:root: [*] Thu Dec 22 21:19:03 2022:    3    | Tr.loss: 0.283638 | Tr.F1.:   0.92    |   95.21  s
WARNING:root:
        [!] Thu Dec 22 21:19:03 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740343-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740343-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740343-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740343-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:19:22 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.690292 | F1-score: 0.67 | Elapsed: 0.12s
WARNING:root: [*] Thu Dec 22 21:19:32 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.601119 | F1-score: 0.83 | Elapsed: 10.05s
WARNING:root: [*] Thu Dec 22 21:19:42 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.511287 | F1-score: 0.86 | Elapsed: 10.04s
WARNING:root: [*] Thu Dec 22 21:19:52 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.453189 | F1-score: 0.87 | Elapsed: 10.11s
WARNING:root: [*] Thu Dec 22 21:20:02 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.501109 | F1-score: 0.87 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:20:12 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.385654 | F1-score: 0.88 | Elapsed: 10.12s
WARNING:root: [*] Thu Dec 22 21:20:22 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.391663 | F1-score: 0.88 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:20:32 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.348099 | F1-score: 0.88 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:20:43 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.275056 | F1-score: 0.88 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:20:53 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.286879 | F1-score: 0.88 | Elapsed: 10.11s
WARNING:root: [*] Thu Dec 22 21:20:57 2022:    1    | Tr.loss: 0.416818 | Tr.F1.:   0.89    |   95.97  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:20:58 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.338142 | F1-score: 0.94 | Elapsed: 0.11s
WARNING:root: [*] Thu Dec 22 21:21:08 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.328768 | F1-score: 0.91 | Elapsed: 10.11s
WARNING:root: [*] Thu Dec 22 21:21:18 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.467945 | F1-score: 0.91 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:21:28 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.428682 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:21:38 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.278899 | F1-score: 0.91 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:21:48 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.328589 | F1-score: 0.91 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:21:58 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.341587 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:22:09 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.328348 | F1-score: 0.91 | Elapsed: 10.12s
WARNING:root: [*] Thu Dec 22 21:22:19 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.350661 | F1-score: 0.91 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:22:29 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.479084 | F1-score: 0.91 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:22:34 2022:    2    | Tr.loss: 0.320596 | Tr.F1.:   0.91    |   96.19  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:22:34 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.289155 | F1-score: 0.95 | Elapsed: 0.11s
WARNING:root: [*] Thu Dec 22 21:22:44 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.390744 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:22:54 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.286011 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:23:04 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.351046 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:23:14 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.394116 | F1-score: 0.92 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:23:25 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.317608 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:23:35 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.325954 | F1-score: 0.92 | Elapsed: 10.10s
WARNING:root: [*] Thu Dec 22 21:23:45 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.266202 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:23:55 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.438407 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:24:05 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.207597 | F1-score: 0.92 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:24:10 2022:    3    | Tr.loss: 0.285938 | Tr.F1.:   0.92    |   96.20  s
WARNING:root:
        [!] Thu Dec 22 21:24:10 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740650-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740650-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740650-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740650-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\metrics_trainSize_91096_ep_3_cv_3_maxLen_1024_vocabSize_2000_embeddingDim_64_hiddenDim_128_lstmLayers_1_bidirectionalLSTM_True_outputDim_1.json
WARNING:root: [!] Average epoch time: 95.73s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0489 -- F1: 0.0911
	FPR:  0.001 -- TPR: 0.1685 -- F1: 0.2883
	FPR:   0.01 -- TPR: 0.3414 -- F1: 0.5070
	FPR:    0.1 -- TPR: 0.5599 -- F1: 0.7001

WARNING:root: [!] Using vocabSize: 2000 | maxLen: 512
WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 2000, 'embeddingDim': 64, 'hiddenDim': 128, 'lstmLayers': 1, 'bidirectionalLSTM': True, 'outputDim': 1}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:25:00 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.700114 | F1-score: 0.00 | Elapsed: 0.38s
WARNING:root: [*] Thu Dec 22 21:25:05 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.332161 | F1-score: 0.84 | Elapsed: 4.92s
WARNING:root: [*] Thu Dec 22 21:25:10 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.402019 | F1-score: 0.87 | Elapsed: 4.92s
WARNING:root: [*] Thu Dec 22 21:25:15 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.380367 | F1-score: 0.88 | Elapsed: 4.95s
WARNING:root: [*] Thu Dec 22 21:25:20 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.307451 | F1-score: 0.88 | Elapsed: 5.02s
WARNING:root: [*] Thu Dec 22 21:25:25 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.311758 | F1-score: 0.88 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:25:30 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.370215 | F1-score: 0.88 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:25:35 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.410043 | F1-score: 0.89 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:25:40 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.406309 | F1-score: 0.89 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:25:45 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.276791 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:25:47 2022:    1    | Tr.loss: 0.399240 | Tr.F1.:   0.89    |   47.93  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:25:47 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.428198 | F1-score: 0.88 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 21:25:53 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.352391 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:25:58 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.336496 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:26:03 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.376781 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:26:08 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.342843 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:26:13 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.336071 | F1-score: 0.91 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:26:18 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.292458 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:26:23 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.364934 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:26:28 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.277777 | F1-score: 0.91 | Elapsed: 5.05s
WARNING:root: [*] Thu Dec 22 21:26:33 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.230145 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:26:36 2022:    2    | Tr.loss: 0.309945 | Tr.F1.:   0.92    |   48.16  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:26:36 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.211558 | F1-score: 0.96 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 21:26:41 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.284442 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:26:46 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.190272 | F1-score: 0.92 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:26:51 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.253945 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:26:56 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.318977 | F1-score: 0.92 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:27:01 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.192678 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:27:06 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.163490 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:27:11 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.287253 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:27:16 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.228723 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:27:21 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.201381 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:27:24 2022:    3    | Tr.loss: 0.271092 | Tr.F1.:   0.93    |   48.19  s
WARNING:root:
        [!] Thu Dec 22 21:27:24 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740844-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740844-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740844-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740844-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:27:33 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.686685 | F1-score: 0.81 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 21:27:39 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.447393 | F1-score: 0.86 | Elapsed: 5.13s
WARNING:root: [*] Thu Dec 22 21:27:44 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.500411 | F1-score: 0.88 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:27:49 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.504152 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:27:54 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.315395 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:27:59 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.339130 | F1-score: 0.88 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:28:04 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.355996 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:28:09 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.479932 | F1-score: 0.89 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:28:14 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.383198 | F1-score: 0.89 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:28:19 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.243902 | F1-score: 0.89 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:28:22 2022:    1    | Tr.loss: 0.397763 | Tr.F1.:   0.89    |   48.26  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:28:22 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.329482 | F1-score: 0.90 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 21:28:27 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.253864 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:28:32 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.322294 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:28:37 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.173928 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:28:42 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.222428 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:28:47 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.179038 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:28:52 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.235352 | F1-score: 0.92 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:28:57 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.226790 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:29:02 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.259981 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:29:07 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.254587 | F1-score: 0.92 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:29:10 2022:    2    | Tr.loss: 0.302856 | Tr.F1.:   0.92    |   48.18  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:29:10 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.233887 | F1-score: 0.95 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 21:29:15 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.413627 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:29:20 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.350302 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:29:25 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.155765 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:29:30 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.296618 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:29:35 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.300367 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:29:40 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.319637 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:29:45 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.226977 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:29:50 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.206232 | F1-score: 0.93 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:29:56 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.395071 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:29:58 2022:    3    | Tr.loss: 0.268561 | Tr.F1.:   0.93    |   48.18  s
WARNING:root:
        [!] Thu Dec 22 21:29:58 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740998-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740998-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740998-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671740998-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:30:08 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.695793 | F1-score: 0.26 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 21:30:13 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.436293 | F1-score: 0.84 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:30:18 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.281689 | F1-score: 0.87 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:30:23 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.417919 | F1-score: 0.87 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:30:28 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.366168 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:30:33 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.551579 | F1-score: 0.88 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:30:38 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.304617 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:30:43 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.247351 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:30:48 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.379894 | F1-score: 0.89 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:30:53 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.277268 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:30:56 2022:    1    | Tr.loss: 0.398238 | Tr.F1.:   0.89    |   48.20  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:30:56 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.296264 | F1-score: 0.91 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 21:31:01 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.254192 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:31:06 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.259816 | F1-score: 0.91 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:31:11 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.392592 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:31:16 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.319572 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:31:21 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.236407 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:31:26 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.284609 | F1-score: 0.92 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:31:31 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.299774 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:31:36 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.410496 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:31:41 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.272096 | F1-score: 0.92 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:31:44 2022:    2    | Tr.loss: 0.299602 | Tr.F1.:   0.92    |   48.24  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:31:44 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.283671 | F1-score: 0.94 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 21:31:49 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.223928 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:31:54 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.263417 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:31:59 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.255724 | F1-score: 0.93 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:32:04 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.262914 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:32:09 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.366867 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:32:14 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.146942 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:32:20 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.262345 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:32:25 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.339530 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:32:30 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.247273 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:32:32 2022:    3    | Tr.loss: 0.263448 | Tr.F1.:   0.93    |   48.22  s
WARNING:root:
        [!] Thu Dec 22 21:32:32 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741152-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741152-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741152-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741152-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\metrics_trainSize_91096_ep_3_cv_3_maxLen_512_vocabSize_2000_embeddingDim_64_hiddenDim_128_lstmLayers_1_bidirectionalLSTM_True_outputDim_1.json
WARNING:root: [!] Average epoch time: 48.17s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0471 -- F1: 0.0880
	FPR:  0.001 -- TPR: 0.1312 -- F1: 0.2319
	FPR:   0.01 -- TPR: 0.3750 -- F1: 0.5439
	FPR:    0.1 -- TPR: 0.7079 -- F1: 0.8104

WARNING:root: [!] Using vocabSize: 500 | maxLen: 1024
WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 500, 'embeddingDim': 64, 'hiddenDim': 128, 'lstmLayers': 1, 'bidirectionalLSTM': True, 'outputDim': 1}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:33:13 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.709244 | F1-score: 0.04 | Elapsed: 0.42s
WARNING:root: [*] Thu Dec 22 21:33:22 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.572808 | F1-score: 0.83 | Elapsed: 9.59s
WARNING:root: [*] Thu Dec 22 21:33:32 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.437975 | F1-score: 0.85 | Elapsed: 9.74s
WARNING:root: [*] Thu Dec 22 21:33:42 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.491779 | F1-score: 0.86 | Elapsed: 10.01s
WARNING:root: [*] Thu Dec 22 21:33:52 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.441942 | F1-score: 0.87 | Elapsed: 10.10s
WARNING:root: [*] Thu Dec 22 21:34:02 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.262395 | F1-score: 0.88 | Elapsed: 10.01s
WARNING:root: [*] Thu Dec 22 21:34:12 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.301335 | F1-score: 0.88 | Elapsed: 10.04s
WARNING:root: [*] Thu Dec 22 21:34:23 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.354090 | F1-score: 0.88 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:34:33 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.370670 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:34:43 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.327422 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:34:48 2022:    1    | Tr.loss: 0.421635 | Tr.F1.:   0.88    |   95.24  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:34:48 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.380149 | F1-score: 0.89 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 21:34:58 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.465383 | F1-score: 0.90 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:35:08 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.351273 | F1-score: 0.90 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:35:18 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.307215 | F1-score: 0.90 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:35:28 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.336881 | F1-score: 0.90 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:35:39 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.262814 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:35:49 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.191664 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:35:59 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.263472 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:36:09 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.288648 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:36:19 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.192442 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:36:24 2022:    2    | Tr.loss: 0.324869 | Tr.F1.:   0.91    |   96.35  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:36:24 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.288322 | F1-score: 0.93 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 21:36:34 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.342183 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:36:44 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.328157 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:36:55 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.326330 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:37:05 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.266030 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:37:15 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.248562 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:37:25 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.234824 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:37:35 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.202575 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:37:45 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.240553 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:37:56 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.251226 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:38:00 2022:    3    | Tr.loss: 0.288359 | Tr.F1.:   0.92    |   96.37  s
WARNING:root:
        [!] Thu Dec 22 21:38:00 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741480-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741480-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741480-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741480-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:38:20 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.703898 | F1-score: 0.00 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 21:38:30 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.553338 | F1-score: 0.83 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:38:40 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.614400 | F1-score: 0.85 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:38:50 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.424589 | F1-score: 0.86 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:39:00 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.416101 | F1-score: 0.87 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:39:10 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.332737 | F1-score: 0.87 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:39:20 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.436529 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:39:31 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.370927 | F1-score: 0.88 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:39:41 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.381079 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:39:51 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.345328 | F1-score: 0.88 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:39:56 2022:    1    | Tr.loss: 0.420693 | Tr.F1.:   0.88    |   96.34  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:39:56 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.386594 | F1-score: 0.92 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 21:40:06 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.329840 | F1-score: 0.91 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:40:16 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.317514 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:40:26 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.318698 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:40:37 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.341271 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:40:47 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.329023 | F1-score: 0.91 | Elapsed: 10.18s
WARNING:root: [*] Thu Dec 22 21:40:57 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.272466 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:41:07 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.342334 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:41:17 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.294066 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:41:27 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.381851 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:41:32 2022:    2    | Tr.loss: 0.329766 | Tr.F1.:   0.91    |   96.38  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:41:32 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.260664 | F1-score: 0.96 | Elapsed: 0.11s
WARNING:root: [*] Thu Dec 22 21:41:42 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.318530 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:41:53 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.211570 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:42:03 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.357346 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:42:13 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.349744 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:42:23 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.363559 | F1-score: 0.92 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 21:42:33 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.285609 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:42:43 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.240748 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:42:54 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.291778 | F1-score: 0.92 | Elapsed: 10.18s
WARNING:root: [*] Thu Dec 22 21:43:04 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.252268 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:43:09 2022:    3    | Tr.loss: 0.294433 | Tr.F1.:   0.92    |   96.39  s
WARNING:root:
        [!] Thu Dec 22 21:43:09 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741789-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741789-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741789-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671741789-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:43:28 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.727487 | F1-score: 0.00 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 21:43:38 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.564837 | F1-score: 0.83 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:43:48 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.487990 | F1-score: 0.85 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:43:58 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.448751 | F1-score: 0.86 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:44:08 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.363822 | F1-score: 0.87 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:44:18 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.344370 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:44:29 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.337761 | F1-score: 0.88 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:44:39 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.359362 | F1-score: 0.88 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:44:49 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.226695 | F1-score: 0.88 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:44:59 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.333088 | F1-score: 0.88 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:45:04 2022:    1    | Tr.loss: 0.421831 | Tr.F1.:   0.88    |   96.19  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:45:04 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.258350 | F1-score: 0.95 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 21:45:14 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.355360 | F1-score: 0.90 | Elapsed: 9.83s
WARNING:root: [*] Thu Dec 22 21:45:24 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.290001 | F1-score: 0.90 | Elapsed: 9.89s
WARNING:root: [*] Thu Dec 22 21:45:34 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.391408 | F1-score: 0.90 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:45:44 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.388156 | F1-score: 0.90 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:45:54 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.337566 | F1-score: 0.90 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:46:04 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.246673 | F1-score: 0.91 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:46:14 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.283126 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:46:25 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.242187 | F1-score: 0.91 | Elapsed: 10.17s
WARNING:root: [*] Thu Dec 22 21:46:35 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.345300 | F1-score: 0.91 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:46:40 2022:    2    | Tr.loss: 0.328573 | Tr.F1.:   0.91    |   95.76  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:46:40 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.323141 | F1-score: 0.93 | Elapsed: 0.09s
WARNING:root: [*] Thu Dec 22 21:46:50 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.305191 | F1-score: 0.92 | Elapsed: 10.13s
WARNING:root: [*] Thu Dec 22 21:47:00 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.277121 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:47:10 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.425904 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:47:20 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.286158 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:47:30 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.385994 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:47:41 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.535875 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:47:51 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.251350 | F1-score: 0.92 | Elapsed: 10.16s
WARNING:root: [*] Thu Dec 22 21:48:01 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.220932 | F1-score: 0.92 | Elapsed: 10.14s
WARNING:root: [*] Thu Dec 22 21:48:11 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.234596 | F1-score: 0.92 | Elapsed: 10.15s
WARNING:root: [*] Thu Dec 22 21:48:16 2022:    3    | Tr.loss: 0.294380 | Tr.F1.:   0.92    |   96.27  s
WARNING:root:
        [!] Thu Dec 22 21:48:16 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742096-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742096-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742096-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742096-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\metrics_trainSize_91096_ep_3_cv_3_maxLen_1024_vocabSize_500_embeddingDim_64_hiddenDim_128_lstmLayers_1_bidirectionalLSTM_True_outputDim_1.json
WARNING:root: [!] Average epoch time: 96.15s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0220 -- F1: 0.0413
	FPR:  0.001 -- TPR: 0.1373 -- F1: 0.2406
	FPR:   0.01 -- TPR: 0.3288 -- F1: 0.4930
	FPR:    0.1 -- TPR: 0.5823 -- F1: 0.7178

WARNING:root: [!] Using vocabSize: 500 | maxLen: 512
WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 500, 'embeddingDim': 64, 'hiddenDim': 128, 'lstmLayers': 1, 'bidirectionalLSTM': True, 'outputDim': 1}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:49:06 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.706225 | F1-score: 0.00 | Elapsed: 0.38s
WARNING:root: [*] Thu Dec 22 21:49:11 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.517922 | F1-score: 0.84 | Elapsed: 4.96s
WARNING:root: [*] Thu Dec 22 21:49:16 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.440058 | F1-score: 0.86 | Elapsed: 4.96s
WARNING:root: [*] Thu Dec 22 21:49:21 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.335622 | F1-score: 0.87 | Elapsed: 4.99s
WARNING:root: [*] Thu Dec 22 21:49:26 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.427510 | F1-score: 0.88 | Elapsed: 5.03s
WARNING:root: [*] Thu Dec 22 21:49:31 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.404254 | F1-score: 0.88 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:49:36 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.396779 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:49:41 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.402461 | F1-score: 0.88 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:49:46 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.393266 | F1-score: 0.89 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:49:51 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.225276 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:49:54 2022:    1    | Tr.loss: 0.405184 | Tr.F1.:   0.89    |   48.12  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:49:54 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.413841 | F1-score: 0.89 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 21:49:59 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.239580 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:50:04 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.269547 | F1-score: 0.91 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:50:09 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.363703 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:50:14 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.270097 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:50:19 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.254099 | F1-score: 0.91 | Elapsed: 5.06s
WARNING:root: [*] Thu Dec 22 21:50:24 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.367729 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:50:29 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.271895 | F1-score: 0.91 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:50:34 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.216883 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:50:39 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.359455 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:50:42 2022:    2    | Tr.loss: 0.324367 | Tr.F1.:   0.91    |   48.14  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:50:42 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.264833 | F1-score: 0.92 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 21:50:47 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.243998 | F1-score: 0.92 | Elapsed: 5.16s
WARNING:root: [*] Thu Dec 22 21:50:52 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.307929 | F1-score: 0.92 | Elapsed: 5.15s
WARNING:root: [*] Thu Dec 22 21:50:57 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.269459 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:51:03 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.327586 | F1-score: 0.92 | Elapsed: 5.19s
WARNING:root: [*] Thu Dec 22 21:51:08 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.229881 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:51:13 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.378042 | F1-score: 0.92 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:51:18 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.294843 | F1-score: 0.92 | Elapsed: 5.16s
WARNING:root: [*] Thu Dec 22 21:51:23 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.313624 | F1-score: 0.92 | Elapsed: 5.23s
WARNING:root: [*] Thu Dec 22 21:51:28 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.289110 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:51:31 2022:    3    | Tr.loss: 0.288939 | Tr.F1.:   0.92    |   48.79  s
WARNING:root:
        [!] Thu Dec 22 21:51:31 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742291-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742291-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742291-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742291-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:51:40 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.685508 | F1-score: 0.68 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 21:51:46 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.404319 | F1-score: 0.86 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:51:51 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.345085 | F1-score: 0.87 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:51:56 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.343084 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:52:01 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.354790 | F1-score: 0.88 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:52:06 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.490784 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:52:11 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.339668 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:52:16 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.333339 | F1-score: 0.89 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:52:21 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.250640 | F1-score: 0.89 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:52:26 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.289605 | F1-score: 0.89 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:52:29 2022:    1    | Tr.loss: 0.392614 | Tr.F1.:   0.89    |   48.38  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:52:29 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.358108 | F1-score: 0.89 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 21:52:34 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.372456 | F1-score: 0.91 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:52:39 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.409585 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:52:44 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.297779 | F1-score: 0.91 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:52:49 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.340924 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:52:54 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.391685 | F1-score: 0.91 | Elapsed: 5.13s
WARNING:root: [*] Thu Dec 22 21:52:59 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.300221 | F1-score: 0.91 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:53:05 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.267238 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:53:10 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.268065 | F1-score: 0.92 | Elapsed: 5.13s
WARNING:root: [*] Thu Dec 22 21:53:15 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.245600 | F1-score: 0.92 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:53:17 2022:    2    | Tr.loss: 0.305457 | Tr.F1.:   0.92    |   48.46  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:53:17 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.342958 | F1-score: 0.93 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 21:53:22 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.330110 | F1-score: 0.93 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:53:28 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.252954 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:53:33 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.175869 | F1-score: 0.93 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:53:38 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.278739 | F1-score: 0.92 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:53:43 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.286562 | F1-score: 0.93 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:53:48 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.192020 | F1-score: 0.93 | Elapsed: 5.13s
WARNING:root: [*] Thu Dec 22 21:53:53 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.229111 | F1-score: 0.93 | Elapsed: 5.12s
WARNING:root: [*] Thu Dec 22 21:53:58 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.359108 | F1-score: 0.93 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:54:03 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.209126 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:54:06 2022:    3    | Tr.loss: 0.275221 | Tr.F1.:   0.93    |   48.52  s
WARNING:root:
        [!] Thu Dec 22 21:54:06 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742446-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742446-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742446-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742446-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 21:54:16 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.684857 | F1-score: 0.83 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 21:54:21 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.571606 | F1-score: 0.86 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:54:26 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.470572 | F1-score: 0.88 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:54:31 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.242370 | F1-score: 0.88 | Elapsed: 5.13s
WARNING:root: [*] Thu Dec 22 21:54:36 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.470482 | F1-score: 0.88 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:54:41 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.426376 | F1-score: 0.88 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:54:46 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.328214 | F1-score: 0.89 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:54:51 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.316890 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:54:56 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.333395 | F1-score: 0.89 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:55:01 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.402884 | F1-score: 0.89 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:55:04 2022:    1    | Tr.loss: 0.396856 | Tr.F1.:   0.89    |   48.35  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 21:55:04 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.300038 | F1-score: 0.94 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 21:55:09 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.431640 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:55:14 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.271256 | F1-score: 0.91 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:55:19 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.229086 | F1-score: 0.91 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:55:24 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.332675 | F1-score: 0.91 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:55:29 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.349123 | F1-score: 0.91 | Elapsed: 5.11s
WARNING:root: [*] Thu Dec 22 21:55:34 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.329372 | F1-score: 0.92 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:55:40 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.274881 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:55:45 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.211158 | F1-score: 0.92 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:55:50 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.379081 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:55:52 2022:    2    | Tr.loss: 0.305513 | Tr.F1.:   0.92    |   48.34  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 21:55:52 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.413956 | F1-score: 0.88 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 21:55:57 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.356741 | F1-score: 0.92 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:56:02 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.324857 | F1-score: 0.92 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:56:07 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.233430 | F1-score: 0.92 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:56:13 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.206445 | F1-score: 0.92 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:56:18 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.391299 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:56:23 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.159440 | F1-score: 0.93 | Elapsed: 5.08s
WARNING:root: [*] Thu Dec 22 21:56:28 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.282043 | F1-score: 0.93 | Elapsed: 5.09s
WARNING:root: [*] Thu Dec 22 21:56:33 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.260489 | F1-score: 0.93 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 21:56:38 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.202167 | F1-score: 0.93 | Elapsed: 5.10s
WARNING:root: [*] Thu Dec 22 21:56:40 2022:    3    | Tr.loss: 0.272716 | Tr.F1.:   0.93    |   48.32  s
WARNING:root:
        [!] Thu Dec 22 21:56:40 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742600-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742600-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742600-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\trainingFiles\trainingFiles_1671742600-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\LSTM_VocabSize_maxLen\metrics_trainSize_91096_ep_3_cv_3_maxLen_512_vocabSize_500_embeddingDim_64_hiddenDim_128_lstmLayers_1_bidirectionalLSTM_True_outputDim_1.json
WARNING:root: [!] Average epoch time: 48.38s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0251 -- F1: 0.0467
	FPR:  0.001 -- TPR: 0.1305 -- F1: 0.2308
	FPR:   0.01 -- TPR: 0.3306 -- F1: 0.4948
	FPR:    0.1 -- TPR: 0.6803 -- F1: 0.7913

