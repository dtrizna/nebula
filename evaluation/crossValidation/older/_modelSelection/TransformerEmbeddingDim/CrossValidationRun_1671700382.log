WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'nTokens': 1500, 'dModel': 16, 'nHeads': 2, 'dHidden': 200, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:13:04 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.705913 | F1-score: 0.17 | Elapsed: 1.66s
WARNING:root: [*] Thu Dec 22 10:13:07 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.566294 | F1-score: 0.82 | Elapsed: 3.13s
WARNING:root: [*] Thu Dec 22 10:13:10 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.416910 | F1-score: 0.84 | Elapsed: 3.09s
WARNING:root: [*] Thu Dec 22 10:13:13 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.380159 | F1-score: 0.86 | Elapsed: 3.11s
WARNING:root: [*] Thu Dec 22 10:13:16 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.379827 | F1-score: 0.87 | Elapsed: 3.03s
WARNING:root: [*] Thu Dec 22 10:13:19 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.469695 | F1-score: 0.87 | Elapsed: 3.01s
WARNING:root: [*] Thu Dec 22 10:13:22 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.262858 | F1-score: 0.87 | Elapsed: 3.00s
WARNING:root: [*] Thu Dec 22 10:13:25 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.352748 | F1-score: 0.88 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:13:28 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.489050 | F1-score: 0.88 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:13:31 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.283586 | F1-score: 0.88 | Elapsed: 3.00s
WARNING:root: [*] Thu Dec 22 10:13:32 2022:    1    | Tr.loss: 0.447144 | Tr.F1.:   0.88    |   30.44  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:13:32 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.357505 | F1-score: 0.89 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:13:35 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.459031 | F1-score: 0.90 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:13:38 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.289255 | F1-score: 0.90 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:13:41 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.393894 | F1-score: 0.90 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:13:44 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.400302 | F1-score: 0.90 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:13:47 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.338659 | F1-score: 0.90 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:13:50 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.462947 | F1-score: 0.90 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:13:53 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.326263 | F1-score: 0.90 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:13:56 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.421593 | F1-score: 0.90 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:13:59 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.299071 | F1-score: 0.90 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:14:01 2022:    2    | Tr.loss: 0.377924 | Tr.F1.:   0.90    |   28.26  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:14:01 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.389858 | F1-score: 0.91 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:14:04 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.435857 | F1-score: 0.90 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:14:07 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.401467 | F1-score: 0.90 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:14:10 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.512657 | F1-score: 0.90 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:14:13 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.383442 | F1-score: 0.90 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:14:16 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.319261 | F1-score: 0.91 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:14:19 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.261927 | F1-score: 0.91 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:14:21 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.315240 | F1-score: 0.91 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:14:24 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.305368 | F1-score: 0.91 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:14:27 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.250226 | F1-score: 0.91 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:14:29 2022:    3    | Tr.loss: 0.344691 | Tr.F1.:   0.91    |   28.25  s
WARNING:root:
        [!] Thu Dec 22 10:14:29 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700469-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700469-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700469-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700469-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:14:33 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.706743 | F1-score: 0.12 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:14:36 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.664655 | F1-score: 0.82 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:14:39 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.624348 | F1-score: 0.83 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:14:42 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.487187 | F1-score: 0.85 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:14:45 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.317913 | F1-score: 0.86 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:14:48 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.389004 | F1-score: 0.87 | Elapsed: 3.00s
WARNING:root: [*] Thu Dec 22 10:14:51 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.310761 | F1-score: 0.87 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:14:54 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.495616 | F1-score: 0.87 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:14:57 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.426183 | F1-score: 0.88 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:15:00 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.404052 | F1-score: 0.88 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:15:01 2022:    1    | Tr.loss: 0.447610 | Tr.F1.:   0.88    |   28.32  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:15:01 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.471655 | F1-score: 0.82 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:15:04 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.527745 | F1-score: 0.90 | Elapsed: 3.04s
WARNING:root: [*] Thu Dec 22 10:15:07 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.452496 | F1-score: 0.90 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:15:10 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.325660 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:15:13 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.292116 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:15:16 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.435617 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:15:19 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.434544 | F1-score: 0.90 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:15:22 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.361141 | F1-score: 0.90 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:15:25 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.485153 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:15:28 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.192630 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:15:29 2022:    2    | Tr.loss: 0.366113 | Tr.F1.:   0.90    |   28.20  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:15:29 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.317272 | F1-score: 0.88 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:15:32 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.377249 | F1-score: 0.91 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:15:35 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.381067 | F1-score: 0.91 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:15:38 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.327684 | F1-score: 0.92 | Elapsed: 2.99s
WARNING:root: [*] Thu Dec 22 10:15:41 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.263876 | F1-score: 0.92 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:15:44 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.241318 | F1-score: 0.92 | Elapsed: 2.94s
WARNING:root: [*] Thu Dec 22 10:15:47 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.252073 | F1-score: 0.92 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:15:50 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.360129 | F1-score: 0.92 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:15:53 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.258321 | F1-score: 0.92 | Elapsed: 2.94s
WARNING:root: [*] Thu Dec 22 10:15:56 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.390384 | F1-score: 0.92 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:15:58 2022:    3    | Tr.loss: 0.308538 | Tr.F1.:   0.92    |   28.10  s
WARNING:root:
        [!] Thu Dec 22 10:15:58 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700558-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700558-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700558-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700558-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:16:02 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.694963 | F1-score: 0.53 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:16:05 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.640904 | F1-score: 0.83 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:16:08 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.472957 | F1-score: 0.85 | Elapsed: 2.94s
WARNING:root: [*] Thu Dec 22 10:16:11 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.429536 | F1-score: 0.86 | Elapsed: 2.98s
WARNING:root: [*] Thu Dec 22 10:16:13 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.413081 | F1-score: 0.87 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:16:16 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.404795 | F1-score: 0.87 | Elapsed: 2.94s
WARNING:root: [*] Thu Dec 22 10:16:19 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.435050 | F1-score: 0.87 | Elapsed: 2.94s
WARNING:root: [*] Thu Dec 22 10:16:22 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.408828 | F1-score: 0.88 | Elapsed: 3.01s
WARNING:root: [*] Thu Dec 22 10:16:25 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.516843 | F1-score: 0.88 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:16:28 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.394885 | F1-score: 0.88 | Elapsed: 2.97s
WARNING:root: [*] Thu Dec 22 10:16:30 2022:    1    | Tr.loss: 0.445268 | Tr.F1.:   0.88    |   28.08  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:16:30 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.351348 | F1-score: 0.91 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:16:33 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.454680 | F1-score: 0.89 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:16:36 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.347224 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:16:39 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.192132 | F1-score: 0.90 | Elapsed: 2.93s
WARNING:root: [*] Thu Dec 22 10:16:42 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.444461 | F1-score: 0.90 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:16:44 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.265211 | F1-score: 0.90 | Elapsed: 2.94s
WARNING:root: [*] Thu Dec 22 10:16:47 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.358752 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:16:50 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.336672 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:16:53 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.253251 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:16:56 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.316278 | F1-score: 0.90 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:16:58 2022:    2    | Tr.loss: 0.372446 | Tr.F1.:   0.90    |   28.01  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:16:58 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.293541 | F1-score: 0.94 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:17:01 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.350364 | F1-score: 0.91 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:17:04 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.325669 | F1-score: 0.91 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:17:07 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.232279 | F1-score: 0.91 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:17:10 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.286634 | F1-score: 0.91 | Elapsed: 2.95s
WARNING:root: [*] Thu Dec 22 10:17:13 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.463434 | F1-score: 0.91 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:17:15 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.356546 | F1-score: 0.91 | Elapsed: 2.93s
WARNING:root: [*] Thu Dec 22 10:17:18 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.227528 | F1-score: 0.91 | Elapsed: 2.96s
WARNING:root: [*] Thu Dec 22 10:17:21 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.200127 | F1-score: 0.91 | Elapsed: 2.94s
WARNING:root: [*] Thu Dec 22 10:17:24 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.271466 | F1-score: 0.91 | Elapsed: 2.94s
WARNING:root: [*] Thu Dec 22 10:17:26 2022:    3    | Tr.loss: 0.328367 | Tr.F1.:   0.91    |   28.02  s
WARNING:root:
        [!] Thu Dec 22 10:17:26 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700646-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700646-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700646-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700646-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\metrics_trainSize_91096_ep_3_cv_3_nTokens_1500_dModel_16_nHeads_2_dHidden_200_nLayers_2_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 28.41s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0005 -- F1: 0.0009
	FPR:  0.001 -- TPR: 0.0014 -- F1: 0.0029
	FPR:   0.01 -- TPR: 0.0954 -- F1: 0.1728
	FPR:    0.1 -- TPR: 0.3576 -- F1: 0.5006

WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'nTokens': 1500, 'dModel': 32, 'nHeads': 2, 'dHidden': 200, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:18:00 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.698463 | F1-score: 0.60 | Elapsed: 0.31s
WARNING:root: [*] Thu Dec 22 10:18:03 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.631206 | F1-score: 0.83 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:18:07 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.576542 | F1-score: 0.86 | Elapsed: 3.35s
WARNING:root: [*] Thu Dec 22 10:18:10 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.379511 | F1-score: 0.87 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:18:13 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.254301 | F1-score: 0.87 | Elapsed: 3.34s
WARNING:root: [*] Thu Dec 22 10:18:17 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.414470 | F1-score: 0.88 | Elapsed: 3.34s
WARNING:root: [*] Thu Dec 22 10:18:20 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.424157 | F1-score: 0.88 | Elapsed: 3.34s
WARNING:root: [*] Thu Dec 22 10:18:23 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.330116 | F1-score: 0.88 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:18:27 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.283780 | F1-score: 0.89 | Elapsed: 3.37s
WARNING:root: [*] Thu Dec 22 10:18:30 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.340387 | F1-score: 0.89 | Elapsed: 3.39s
WARNING:root: [*] Thu Dec 22 10:18:32 2022:    1    | Tr.loss: 0.418908 | Tr.F1.:   0.89    |   32.02  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:18:32 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.274539 | F1-score: 0.94 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:18:35 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.268839 | F1-score: 0.91 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:18:39 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.343655 | F1-score: 0.91 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:18:42 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.236217 | F1-score: 0.91 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:18:45 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.349359 | F1-score: 0.91 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:18:48 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.392418 | F1-score: 0.92 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:18:52 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.265723 | F1-score: 0.92 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:18:55 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.357199 | F1-score: 0.92 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:18:58 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.220399 | F1-score: 0.92 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:19:02 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.368976 | F1-score: 0.92 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:19:03 2022:    2    | Tr.loss: 0.300234 | Tr.F1.:   0.92    |   31.35  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:19:03 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.220989 | F1-score: 0.96 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:19:07 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.257522 | F1-score: 0.93 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:19:10 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.321197 | F1-score: 0.93 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:19:13 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.267023 | F1-score: 0.93 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:19:17 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.247367 | F1-score: 0.93 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:19:20 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.288114 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:19:23 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.653898 | F1-score: 0.93 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:19:26 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.220583 | F1-score: 0.93 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:19:30 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.302784 | F1-score: 0.93 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:19:33 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.138773 | F1-score: 0.93 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:19:35 2022:    3    | Tr.loss: 0.264446 | Tr.F1.:   0.93    |   31.41  s
WARNING:root:
        [!] Thu Dec 22 10:19:35 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700775-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700775-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700775-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700775-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:19:39 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.681560 | F1-score: 0.72 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:19:43 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.591588 | F1-score: 0.83 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:19:46 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.440942 | F1-score: 0.86 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:19:49 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.494882 | F1-score: 0.87 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:19:52 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.397511 | F1-score: 0.87 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:19:56 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.458908 | F1-score: 0.88 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:19:59 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.528229 | F1-score: 0.88 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:20:02 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.359357 | F1-score: 0.88 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:20:06 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.386075 | F1-score: 0.88 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:20:09 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.260784 | F1-score: 0.89 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:20:11 2022:    1    | Tr.loss: 0.418747 | Tr.F1.:   0.89    |   31.46  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:20:11 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.429189 | F1-score: 0.84 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:20:14 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.434772 | F1-score: 0.91 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:20:17 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.301833 | F1-score: 0.91 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:20:21 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.271388 | F1-score: 0.92 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:20:24 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.286979 | F1-score: 0.91 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:20:27 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.245524 | F1-score: 0.91 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:20:31 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.234886 | F1-score: 0.92 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:20:34 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.420838 | F1-score: 0.92 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:20:37 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.267609 | F1-score: 0.92 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:20:41 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.368526 | F1-score: 0.92 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:20:42 2022:    2    | Tr.loss: 0.311920 | Tr.F1.:   0.92    |   31.45  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:20:42 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.268861 | F1-score: 0.93 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:20:45 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.222484 | F1-score: 0.94 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:20:49 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.297546 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:20:52 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.266506 | F1-score: 0.93 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:20:55 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.186680 | F1-score: 0.93 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:20:59 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.235681 | F1-score: 0.93 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:21:02 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.189399 | F1-score: 0.93 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:21:05 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.312413 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:21:09 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.243563 | F1-score: 0.93 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:21:12 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.185326 | F1-score: 0.93 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:21:13 2022:    3    | Tr.loss: 0.266741 | Tr.F1.:   0.93    |   31.40  s
WARNING:root:
        [!] Thu Dec 22 10:21:14 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700873-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700873-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700873-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700873-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 10:21:18 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.650251 | F1-score: 0.84 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 10:21:21 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.521545 | F1-score: 0.83 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:21:25 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.502523 | F1-score: 0.86 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:21:28 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.410965 | F1-score: 0.87 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:21:31 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.202516 | F1-score: 0.87 | Elapsed: 3.29s
WARNING:root: [*] Thu Dec 22 10:21:35 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.548597 | F1-score: 0.88 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:21:38 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.371133 | F1-score: 0.88 | Elapsed: 3.41s
WARNING:root: [*] Thu Dec 22 10:21:42 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.443273 | F1-score: 0.88 | Elapsed: 3.56s
WARNING:root: [*] Thu Dec 22 10:21:45 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.418261 | F1-score: 0.88 | Elapsed: 3.40s
WARNING:root: [*] Thu Dec 22 10:21:48 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.265800 | F1-score: 0.89 | Elapsed: 3.28s
WARNING:root: [*] Thu Dec 22 10:21:50 2022:    1    | Tr.loss: 0.427412 | Tr.F1.:   0.89    |   31.82  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 10:21:50 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.380842 | F1-score: 0.91 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:21:53 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.228515 | F1-score: 0.91 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:21:57 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.350153 | F1-score: 0.91 | Elapsed: 3.43s
WARNING:root: [*] Thu Dec 22 10:22:00 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.302536 | F1-score: 0.91 | Elapsed: 3.39s
WARNING:root: [*] Thu Dec 22 10:22:03 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.294302 | F1-score: 0.91 | Elapsed: 3.36s
WARNING:root: [*] Thu Dec 22 10:22:07 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.167720 | F1-score: 0.91 | Elapsed: 3.49s
WARNING:root: [*] Thu Dec 22 10:22:10 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.191682 | F1-score: 0.92 | Elapsed: 3.57s
WARNING:root: [*] Thu Dec 22 10:22:14 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.290895 | F1-score: 0.92 | Elapsed: 3.46s
WARNING:root: [*] Thu Dec 22 10:22:17 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.289015 | F1-score: 0.92 | Elapsed: 3.47s
WARNING:root: [*] Thu Dec 22 10:22:21 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.273827 | F1-score: 0.92 | Elapsed: 3.41s
WARNING:root: [*] Thu Dec 22 10:22:22 2022:    2    | Tr.loss: 0.321065 | Tr.F1.:   0.92    |   32.58  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 10:22:23 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.294810 | F1-score: 0.96 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 10:22:26 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.295060 | F1-score: 0.93 | Elapsed: 3.38s
WARNING:root: [*] Thu Dec 22 10:22:29 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.227504 | F1-score: 0.93 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:22:33 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.246575 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:22:36 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.450098 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:22:39 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.316933 | F1-score: 0.93 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:22:42 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.243590 | F1-score: 0.93 | Elapsed: 3.33s
WARNING:root: [*] Thu Dec 22 10:22:46 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.228233 | F1-score: 0.93 | Elapsed: 3.30s
WARNING:root: [*] Thu Dec 22 10:22:49 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.220786 | F1-score: 0.93 | Elapsed: 3.32s
WARNING:root: [*] Thu Dec 22 10:22:52 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.186749 | F1-score: 0.93 | Elapsed: 3.31s
WARNING:root: [*] Thu Dec 22 10:22:54 2022:    3    | Tr.loss: 0.272063 | Tr.F1.:   0.93    |   31.57  s
WARNING:root:
        [!] Thu Dec 22 10:22:54 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700974-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700974-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700974-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\trainingFiles\trainingFiles_1671700974-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerEmbeddingDim\metrics_trainSize_91096_ep_3_cv_3_nTokens_1500_dModel_32_nHeads_2_dHidden_200_nLayers_2_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 31.67s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0042 -- F1: 0.0083
	FPR:  0.001 -- TPR: 0.0740 -- F1: 0.1365
	FPR:   0.01 -- TPR: 0.3082 -- F1: 0.4615
	FPR:    0.1 -- TPR: 0.6790 -- F1: 0.7772

