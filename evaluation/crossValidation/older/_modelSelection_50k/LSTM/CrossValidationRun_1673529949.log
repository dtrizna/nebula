WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 50000, 'embeddingDim': 64, 'lstmHidden': 256, 'lstmLayers': 1, 'lstmDropout': 0.1, 'lstmBidirectional': True, 'hiddenNeurons': [256, 64], 'batchNormFFNN': False}
WARNING:root: [!] Fold 1/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Jan 12 14:25:52 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.717822 | FPR 0.001 -- TPR 0.6296 | F1 0.7727 | Elapsed: 0.82s
WARNING:root: [*] Thu Jan 12 14:26:12 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.579401 | FPR 0.001 -- TPR 0.1099 | F1 0.1759 | Elapsed: 19.98s
WARNING:root: [*] Thu Jan 12 14:26:33 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.572460 | FPR 0.001 -- TPR 0.1483 | F1 0.2327 | Elapsed: 20.67s
WARNING:root: [*] Thu Jan 12 14:26:54 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.469320 | FPR 0.001 -- TPR 0.2347 | F1 0.3351 | Elapsed: 21.36s
WARNING:root: [*] Thu Jan 12 14:27:16 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.681494 | FPR 0.001 -- TPR 0.2884 | F1 0.3954 | Elapsed: 21.93s
WARNING:root: [*] Thu Jan 12 14:27:38 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.376007 | FPR 0.001 -- TPR 0.3278 | F1 0.4402 | Elapsed: 21.95s
WARNING:root: [*] Thu Jan 12 14:28:00 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.412818 | FPR 0.001 -- TPR 0.3543 | F1 0.4707 | Elapsed: 22.03s
WARNING:root: [*] Thu Jan 12 14:28:22 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.369398 | FPR 0.001 -- TPR 0.3755 | F1 0.4937 | Elapsed: 22.00s
WARNING:root: [*] Thu Jan 12 14:28:44 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.342259 | FPR 0.001 -- TPR 0.3939 | F1 0.5128 | Elapsed: 22.01s
WARNING:root: [*] Thu Jan 12 14:29:06 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.344127 | FPR 0.001 -- TPR 0.4114 | F1 0.5310 | Elapsed: 21.93s
WARNING:root:[!] Learning rate: 2.5e-05
WARNING:root: [*] Thu Jan 12 14:29:28 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.477803 | FPR 0.001 -- TPR 0.4319 | F1 0.5511 | Elapsed: 21.96s
WARNING:root: [*] Thu Jan 12 14:29:50 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.372901 | FPR 0.001 -- TPR 0.4435 | F1 0.5631 | Elapsed: 22.01s
WARNING:root: [*] Thu Jan 12 14:30:09 2023:    1    | Tr.loss: 0.454368 | FPR 0.001 -- TPR: 0.46 |  F1: 0.58 | Elapsed:  258.05  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Jan 12 14:30:10 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.387247 | FPR 0.001 -- TPR 0.4737 | F1 0.6429 | Elapsed: 0.23s
WARNING:root: [*] Thu Jan 12 14:30:32 2023: Train Epoch: 2 [3200 /38063 (8 %)]	Loss: 0.274234 | FPR 0.001 -- TPR 0.5828 | F1 0.7035 | Elapsed: 21.92s
WARNING:root: [*] Thu Jan 12 14:30:54 2023: Train Epoch: 2 [6400 /38063 (17%)]	Loss: 0.270631 | FPR 0.001 -- TPR 0.5945 | F1 0.7137 | Elapsed: 21.90s
WARNING:root: [*] Thu Jan 12 14:31:15 2023: Train Epoch: 2 [9600 /38063 (25%)]	Loss: 0.437862 | FPR 0.001 -- TPR 0.6038 | F1 0.7229 | Elapsed: 21.95s
WARNING:root: [*] Thu Jan 12 14:31:37 2023: Train Epoch: 2 [12800/38063 (34%)]	Loss: 0.172455 | FPR 0.001 -- TPR 0.6093 | F1 0.7254 | Elapsed: 21.90s
WARNING:root: [*] Thu Jan 12 14:31:59 2023: Train Epoch: 2 [16000/38063 (42%)]	Loss: 0.193097 | FPR 0.001 -- TPR 0.6132 | F1 0.7290 | Elapsed: 21.92s
WARNING:root: [*] Thu Jan 12 14:32:21 2023: Train Epoch: 2 [19200/38063 (50%)]	Loss: 0.360419 | FPR 0.001 -- TPR 0.6177 | F1 0.7331 | Elapsed: 21.88s
WARNING:root: [*] Thu Jan 12 14:32:43 2023: Train Epoch: 2 [22400/38063 (59%)]	Loss: 0.325910 | FPR 0.001 -- TPR 0.6169 | F1 0.7333 | Elapsed: 21.91s
WARNING:root: [*] Thu Jan 12 14:33:05 2023: Train Epoch: 2 [25600/38063 (67%)]	Loss: 0.159155 | FPR 0.001 -- TPR 0.6142 | F1 0.7310 | Elapsed: 22.05s
WARNING:root:[!] Learning rate: 2.5e-06
WARNING:root: [*] Thu Jan 12 14:33:27 2023: Train Epoch: 2 [28800/38063 (76%)]	Loss: 0.308226 | FPR 0.001 -- TPR 0.6132 | F1 0.7300 | Elapsed: 21.93s
WARNING:root: [*] Thu Jan 12 14:33:49 2023: Train Epoch: 2 [32000/38063 (84%)]	Loss: 0.389320 | FPR 0.001 -- TPR 0.6131 | F1 0.7296 | Elapsed: 21.94s
WARNING:root: [*] Thu Jan 12 14:34:11 2023: Train Epoch: 2 [35200/38063 (92%)]	Loss: 0.215543 | FPR 0.001 -- TPR 0.6169 | F1 0.7332 | Elapsed: 21.97s
WARNING:root: [*] Thu Jan 12 14:34:30 2023:    2    | Tr.loss: 0.349161 | FPR 0.001 -- TPR: 0.62 |  F1: 0.73 | Elapsed:  260.95  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Jan 12 14:34:31 2023: Train Epoch: 3 [  0  /38063 (0 %)]	Loss: 0.206185 | FPR 0.001 -- TPR 0.8333 | F1 0.9091 | Elapsed: 0.22s
WARNING:root: [*] Thu Jan 12 14:34:53 2023: Train Epoch: 3 [3200 /38063 (8 %)]	Loss: 0.194395 | FPR 0.001 -- TPR 0.6092 | F1 0.7334 | Elapsed: 21.96s
WARNING:root: [*] Thu Jan 12 14:35:15 2023: Train Epoch: 3 [6400 /38063 (17%)]	Loss: 0.308247 | FPR 0.001 -- TPR 0.5965 | F1 0.7196 | Elapsed: 21.95s
WARNING:root: [*] Thu Jan 12 14:35:36 2023: Train Epoch: 3 [9600 /38063 (25%)]	Loss: 0.355875 | FPR 0.001 -- TPR 0.6142 | F1 0.7311 | Elapsed: 21.93s
WARNING:root: [*] Thu Jan 12 14:35:59 2023: Train Epoch: 3 [12800/38063 (34%)]	Loss: 0.481214 | FPR 0.001 -- TPR 0.6193 | F1 0.7348 | Elapsed: 22.07s
WARNING:root: [*] Thu Jan 12 14:36:20 2023: Train Epoch: 3 [16000/38063 (42%)]	Loss: 0.408686 | FPR 0.001 -- TPR 0.6279 | F1 0.7431 | Elapsed: 21.93s
WARNING:root: [*] Thu Jan 12 14:36:42 2023: Train Epoch: 3 [19200/38063 (50%)]	Loss: 0.309597 | FPR 0.001 -- TPR 0.6215 | F1 0.7381 | Elapsed: 21.91s
WARNING:root:[!] Learning rate: 2.5000000000000004e-07
WARNING:root: [*] Thu Jan 12 14:37:04 2023: Train Epoch: 3 [22400/38063 (59%)]	Loss: 0.202961 | FPR 0.001 -- TPR 0.6250 | F1 0.7418 | Elapsed: 21.92s
WARNING:root: [*] Thu Jan 12 14:37:26 2023: Train Epoch: 3 [25600/38063 (67%)]	Loss: 0.325240 | FPR 0.001 -- TPR 0.6244 | F1 0.7407 | Elapsed: 21.90s
WARNING:root: [*] Thu Jan 12 14:37:48 2023: Train Epoch: 3 [28800/38063 (76%)]	Loss: 0.273996 | FPR 0.001 -- TPR 0.6263 | F1 0.7416 | Elapsed: 21.91s
WARNING:root: [*] Thu Jan 12 14:38:10 2023: Train Epoch: 3 [32000/38063 (84%)]	Loss: 0.371659 | FPR 0.001 -- TPR 0.6284 | F1 0.7425 | Elapsed: 21.89s
WARNING:root: [*] Thu Jan 12 14:38:32 2023: Train Epoch: 3 [35200/38063 (92%)]	Loss: 0.320703 | FPR 0.001 -- TPR 0.6261 | F1 0.7406 | Elapsed: 21.92s
WARNING:root: [*] Thu Jan 12 14:38:51 2023:    3    | Tr.loss: 0.343581 | FPR 0.001 -- TPR: 0.63 |  F1: 0.74 | Elapsed:  260.93  s
WARNING:root:[!] Thu Jan 12 14:38:51 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673530731-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673530731-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673530731-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673530731-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673530731-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/1190
WARNING:root: [*] Predicting batch: 100/1190
WARNING:root: [*] Predicting batch: 200/1190
WARNING:root: [*] Predicting batch: 300/1190
WARNING:root: [*] Predicting batch: 400/1190
WARNING:root: [*] Predicting batch: 500/1190
WARNING:root: [*] Predicting batch: 600/1190
WARNING:root: [*] Predicting batch: 700/1190
WARNING:root: [*] Predicting batch: 800/1190
WARNING:root: [*] Predicting batch: 900/1190
WARNING:root: [*] Predicting batch: 1000/1190
WARNING:root: [*] Predicting batch: 1100/1190
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.08314249116950666 | F1: 0.15351537303805632
WARNING:root: [!] FPR: 0.0003 | TPR: 0.08523852035865388 | F1: 0.15707030970602961
WARNING:root: [!] FPR: 0.001 | TPR: 0.0955634048829717 | F1: 0.17438113113999362
WARNING:root: [!] FPR: 0.003 | TPR: 0.1504095019989908 | F1: 0.2611713958347375
WARNING:root: [!] FPR: 0.01 | TPR: 0.3208089120055894 | F1: 0.48402682205499115
WARNING:root: [!] FPR: 0.03 | TPR: 0.5278112021115553 | F1: 0.6845205134658946
WARNING:root: [!] FPR: 0.1 | TPR: 0.7830221635679074 | F1: 0.8554043166687869
WARNING:root: [!] Fold 2/2 | Train set size: 38063, Validation set size: 38063
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Jan 12 14:40:33 2023: Train Epoch: 1 [  0  /38063 (0 %)]	Loss: 0.675076 | FPR 0.001 -- TPR 0.3043 | F1 0.4667 | Elapsed: 0.27s
WARNING:root: [*] Thu Jan 12 14:40:55 2023: Train Epoch: 1 [3200 /38063 (8 %)]	Loss: 0.586124 | FPR 0.001 -- TPR 0.1016 | F1 0.1690 | Elapsed: 22.36s
WARNING:root: [*] Thu Jan 12 14:41:18 2023: Train Epoch: 1 [6400 /38063 (17%)]	Loss: 0.485463 | FPR 0.001 -- TPR 0.1491 | F1 0.2368 | Elapsed: 22.30s
WARNING:root: [*] Thu Jan 12 14:41:40 2023: Train Epoch: 1 [9600 /38063 (25%)]	Loss: 0.530489 | FPR 0.001 -- TPR 0.2005 | F1 0.3018 | Elapsed: 21.99s
WARNING:root: [*] Thu Jan 12 14:42:02 2023: Train Epoch: 1 [12800/38063 (34%)]	Loss: 0.302232 | FPR 0.001 -- TPR 0.2610 | F1 0.3693 | Elapsed: 22.04s
WARNING:root: [*] Thu Jan 12 14:42:24 2023: Train Epoch: 1 [16000/38063 (42%)]	Loss: 0.369361 | FPR 0.001 -- TPR 0.3020 | F1 0.4149 | Elapsed: 22.05s
WARNING:root: [*] Thu Jan 12 14:42:46 2023: Train Epoch: 1 [19200/38063 (50%)]	Loss: 0.272165 | FPR 0.001 -- TPR 0.3274 | F1 0.4422 | Elapsed: 22.01s
WARNING:root: [*] Thu Jan 12 14:43:08 2023: Train Epoch: 1 [22400/38063 (59%)]	Loss: 0.340940 | FPR 0.001 -- TPR 0.3440 | F1 0.4608 | Elapsed: 21.95s
WARNING:root: [*] Thu Jan 12 14:43:30 2023: Train Epoch: 1 [25600/38063 (67%)]	Loss: 0.405005 | FPR 0.001 -- TPR 0.3632 | F1 0.4819 | Elapsed: 22.00s
WARNING:root: [*] Thu Jan 12 14:43:51 2023: Train Epoch: 1 [28800/38063 (76%)]	Loss: 0.329581 | FPR 0.001 -- TPR 0.3801 | F1 0.5005 | Elapsed: 21.81s
WARNING:root:[!] Learning rate: 2.5e-05
WARNING:root: [*] Thu Jan 12 14:44:12 2023: Train Epoch: 1 [32000/38063 (84%)]	Loss: 0.410780 | FPR 0.001 -- TPR 0.3983 | F1 0.5183 | Elapsed: 20.96s
WARNING:root: [*] Thu Jan 12 14:44:33 2023: Train Epoch: 1 [35200/38063 (92%)]	Loss: 0.437684 | FPR 0.001 -- TPR 0.4125 | F1 0.5324 | Elapsed: 20.62s
WARNING:root: [*] Thu Jan 12 14:44:51 2023:    1    | Tr.loss: 0.459750 | FPR 0.001 -- TPR: 0.42 |  F1: 0.54 | Elapsed:  258.46  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Jan 12 14:44:51 2023: Train Epoch: 2 [  0  /38063 (0 %)]	Loss: 0.378015 | FPR 0.001 -- TPR 0.3500 | F1 0.5185 | Elapsed: 0.20s
WARNING:root: [*] Thu Jan 12 14:45:12 2023: Train Epoch: 2 [3200 /38063 (8 %)]	Loss: 0.451828 | FPR 0.001 -- TPR 0.5215 | F1 0.6448 | Elapsed: 20.55s
WARNING:root: [*] Thu Jan 12 14:45:33 2023: Train Epoch: 2 [6400 /38063 (17%)]	Loss: 0.119253 | FPR 0.001 -- TPR 0.5368 | F1 0.6578 | Elapsed: 20.62s
WARNING:root: [*] Thu Jan 12 14:45:53 2023: Train Epoch: 2 [9600 /38063 (25%)]	Loss: 0.164428 | FPR 0.001 -- TPR 0.5366 | F1 0.6543 | Elapsed: 20.59s
WARNING:root: [*] Thu Jan 12 14:46:14 2023: Train Epoch: 2 [12800/38063 (34%)]	Loss: 0.263561 | FPR 0.001 -- TPR 0.5502 | F1 0.6686 | Elapsed: 20.50s
WARNING:root: [*] Thu Jan 12 14:46:34 2023: Train Epoch: 2 [16000/38063 (42%)]	Loss: 0.418891 | FPR 0.001 -- TPR 0.5570 | F1 0.6745 | Elapsed: 20.60s
WARNING:root: [*] Thu Jan 12 14:46:55 2023: Train Epoch: 2 [19200/38063 (50%)]	Loss: 0.354318 | FPR 0.001 -- TPR 0.5586 | F1 0.6757 | Elapsed: 20.55s
WARNING:root: [*] Thu Jan 12 14:47:15 2023: Train Epoch: 2 [22400/38063 (59%)]	Loss: 0.354080 | FPR 0.001 -- TPR 0.5589 | F1 0.6766 | Elapsed: 20.24s
WARNING:root: [*] Thu Jan 12 14:47:37 2023: Train Epoch: 2 [25600/38063 (67%)]	Loss: 0.420264 | FPR 0.001 -- TPR 0.5604 | F1 0.6783 | Elapsed: 21.78s
WARNING:root:[!] Learning rate: 2.5e-06
WARNING:root: [*] Thu Jan 12 14:47:57 2023: Train Epoch: 2 [28800/38063 (76%)]	Loss: 0.549521 | FPR 0.001 -- TPR 0.5662 | F1 0.6843 | Elapsed: 20.51s
WARNING:root: [*] Thu Jan 12 14:48:18 2023: Train Epoch: 2 [32000/38063 (84%)]	Loss: 0.159616 | FPR 0.001 -- TPR 0.5680 | F1 0.6856 | Elapsed: 20.59s
WARNING:root: [*] Thu Jan 12 14:48:38 2023: Train Epoch: 2 [35200/38063 (92%)]	Loss: 0.299140 | FPR 0.001 -- TPR 0.5685 | F1 0.6860 | Elapsed: 20.47s
WARNING:root: [*] Thu Jan 12 14:48:57 2023:    2    | Tr.loss: 0.356087 | FPR 0.001 -- TPR: 0.57 |  F1: 0.69 | Elapsed:  245.38  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Jan 12 14:48:57 2023: Train Epoch: 3 [  0  /38063 (0 %)]	Loss: 0.226201 | FPR 0.001 -- TPR 0.7895 | F1 0.8824 | Elapsed: 0.22s
WARNING:root: [*] Thu Jan 12 14:49:17 2023: Train Epoch: 3 [3200 /38063 (8 %)]	Loss: 0.295128 | FPR 0.001 -- TPR 0.6300 | F1 0.7425 | Elapsed: 20.56s
WARNING:root: [*] Thu Jan 12 14:49:38 2023: Train Epoch: 3 [6400 /38063 (17%)]	Loss: 0.432282 | FPR 0.001 -- TPR 0.6074 | F1 0.7232 | Elapsed: 20.53s
WARNING:root: [*] Thu Jan 12 14:49:58 2023: Train Epoch: 3 [9600 /38063 (25%)]	Loss: 0.220374 | FPR 0.001 -- TPR 0.5955 | F1 0.7126 | Elapsed: 20.48s
WARNING:root: [*] Thu Jan 12 14:50:19 2023: Train Epoch: 3 [12800/38063 (34%)]	Loss: 0.446674 | FPR 0.001 -- TPR 0.5829 | F1 0.7017 | Elapsed: 20.42s
WARNING:root: [*] Thu Jan 12 14:50:39 2023: Train Epoch: 3 [16000/38063 (42%)]	Loss: 0.422206 | FPR 0.001 -- TPR 0.5768 | F1 0.6942 | Elapsed: 20.50s
WARNING:root: [*] Thu Jan 12 14:51:00 2023: Train Epoch: 3 [19200/38063 (50%)]	Loss: 0.216247 | FPR 0.001 -- TPR 0.5797 | F1 0.6970 | Elapsed: 20.46s
WARNING:root:[!] Learning rate: 2.5000000000000004e-07
WARNING:root: [*] Thu Jan 12 14:51:20 2023: Train Epoch: 3 [22400/38063 (59%)]	Loss: 0.305491 | FPR 0.001 -- TPR 0.5843 | F1 0.7006 | Elapsed: 20.48s
WARNING:root: [*] Thu Jan 12 14:51:41 2023: Train Epoch: 3 [25600/38063 (67%)]	Loss: 0.188212 | FPR 0.001 -- TPR 0.5883 | F1 0.7044 | Elapsed: 20.48s
WARNING:root: [*] Thu Jan 12 14:52:01 2023: Train Epoch: 3 [28800/38063 (76%)]	Loss: 0.322662 | FPR 0.001 -- TPR 0.5869 | F1 0.7031 | Elapsed: 20.51s
WARNING:root: [*] Thu Jan 12 14:52:22 2023: Train Epoch: 3 [32000/38063 (84%)]	Loss: 0.291452 | FPR 0.001 -- TPR 0.5853 | F1 0.7006 | Elapsed: 20.47s
WARNING:root: [*] Thu Jan 12 14:52:42 2023: Train Epoch: 3 [35200/38063 (92%)]	Loss: 0.288589 | FPR 0.001 -- TPR 0.5853 | F1 0.7003 | Elapsed: 20.74s
WARNING:root: [*] Thu Jan 12 14:53:01 2023:    3    | Tr.loss: 0.350458 | FPR 0.001 -- TPR: 0.58 |  F1: 0.70 | Elapsed:  244.41  s
WARNING:root:[!] Thu Jan 12 14:53:01 2023: Dumped results:
                model     : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673531581-model.torch
                losses    : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673531581-train_losses.npy
                duration  : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673531581-trainTime.npy
		train F1s : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673531581-trainF1s.npy
		train TPRs: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\trainingFiles\trainingFiles_1673531581-trainTPRs.npy
WARNING:root: [!] Evaluating model on validation set...
WARNING:root: [*] Predicting batch: 0/1190
WARNING:root: [*] Predicting batch: 100/1190
WARNING:root: [*] Predicting batch: 200/1190
WARNING:root: [*] Predicting batch: 300/1190
WARNING:root: [*] Predicting batch: 400/1190
WARNING:root: [*] Predicting batch: 500/1190
WARNING:root: [*] Predicting batch: 600/1190
WARNING:root: [*] Predicting batch: 700/1190
WARNING:root: [*] Predicting batch: 800/1190
WARNING:root: [*] Predicting batch: 900/1190
WARNING:root: [*] Predicting batch: 1000/1190
WARNING:root: [*] Predicting batch: 1100/1190
WARNING:root: [!] This fold metrics on validation set:
WARNING:root: [!] FPR: 0.0001 | TPR: 0.083498773794231 | F1: 0.1541225076342734
WARNING:root: [!] FPR: 0.0003 | TPR: 0.08470551597960216 | F1: 0.15616477680493757
WARNING:root: [!] FPR: 0.001 | TPR: 0.09330841994628052 | F1: 0.17061712577407645
WARNING:root: [!] FPR: 0.003 | TPR: 0.12269843123515901 | F1: 0.2182976660433548
WARNING:root: [!] FPR: 0.01 | TPR: 0.24492973646307759 | F1: 0.391988287698969
WARNING:root: [!] FPR: 0.03 | TPR: 0.5160185293316205 | F1: 0.6743310611455896
WARNING:root: [!] FPR: 0.1 | TPR: 0.713340340223442 | F1: 0.8099268524452499
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\_modelSelection_50k\LSTM\metrics_trainSize_76126_ep_3_cv_2_vocabSize_50000_embeddingDim_64_lstmHidden_256_lstmLayers_1_lstmDropout_0.1_lstmBidirectional_True_hiddenNeurons_256_64_batchNormFFNN_False.json
WARNING:root: [!] Average epoch time: 254.70s | Mean values over 2 folds:
	FPR: 0.0001 -- TPR: 0.0833 -- F1: 0.1538
	FPR: 0.0003 -- TPR: 0.0850 -- F1: 0.1566
	FPR:  0.001 -- TPR: 0.0944 -- F1: 0.1725
	FPR:  0.003 -- TPR: 0.1366 -- F1: 0.2397
	FPR:   0.01 -- TPR: 0.2829 -- F1: 0.4380
	FPR:   0.03 -- TPR: 0.5219 -- F1: 0.6794
	FPR:    0.1 -- TPR: 0.7482 -- F1: 0.8327

