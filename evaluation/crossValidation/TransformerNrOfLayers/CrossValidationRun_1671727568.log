WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 2000, 'dModel': 32, 'nHeads': 8, 'dHidden': 128, 'nLayers': 1, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 17:46:09 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.688469 | F1-score: 0.58 | Elapsed: 1.50s
WARNING:root: [*] Thu Dec 22 17:46:12 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.412671 | F1-score: 0.83 | Elapsed: 2.64s
WARNING:root: [*] Thu Dec 22 17:46:15 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.438176 | F1-score: 0.86 | Elapsed: 2.71s
WARNING:root: [*] Thu Dec 22 17:46:17 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.708829 | F1-score: 0.87 | Elapsed: 2.61s
WARNING:root: [*] Thu Dec 22 17:46:20 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.408568 | F1-score: 0.87 | Elapsed: 2.60s
WARNING:root: [*] Thu Dec 22 17:46:23 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.430256 | F1-score: 0.88 | Elapsed: 2.76s
WARNING:root: [*] Thu Dec 22 17:46:25 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.265607 | F1-score: 0.88 | Elapsed: 2.62s
WARNING:root: [*] Thu Dec 22 17:46:28 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.452412 | F1-score: 0.88 | Elapsed: 2.70s
WARNING:root: [*] Thu Dec 22 17:46:31 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.279323 | F1-score: 0.89 | Elapsed: 2.73s
WARNING:root: [*] Thu Dec 22 17:46:33 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.414796 | F1-score: 0.89 | Elapsed: 2.64s
WARNING:root: [*] Thu Dec 22 17:46:35 2022:    1    | Tr.loss: 0.419202 | Tr.F1.:   0.89    |   26.81  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 17:46:35 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.298601 | F1-score: 0.92 | Elapsed: 0.03s
WARNING:root: [*] Thu Dec 22 17:46:37 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.421775 | F1-score: 0.91 | Elapsed: 2.62s
WARNING:root: [*] Thu Dec 22 17:46:40 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.279870 | F1-score: 0.91 | Elapsed: 2.64s
WARNING:root: [*] Thu Dec 22 17:46:43 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.329841 | F1-score: 0.91 | Elapsed: 2.63s
WARNING:root: [*] Thu Dec 22 17:46:45 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.287423 | F1-score: 0.91 | Elapsed: 2.68s
WARNING:root: [*] Thu Dec 22 17:46:48 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.366462 | F1-score: 0.92 | Elapsed: 2.65s
WARNING:root: [*] Thu Dec 22 17:46:51 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.284565 | F1-score: 0.92 | Elapsed: 2.64s
WARNING:root: [*] Thu Dec 22 17:46:53 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.333835 | F1-score: 0.92 | Elapsed: 2.68s
WARNING:root: [*] Thu Dec 22 17:46:56 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.187992 | F1-score: 0.92 | Elapsed: 2.67s
WARNING:root: [*] Thu Dec 22 17:46:59 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.244645 | F1-score: 0.92 | Elapsed: 2.63s
WARNING:root: [*] Thu Dec 22 17:47:00 2022:    2    | Tr.loss: 0.306894 | Tr.F1.:   0.92    |   25.18  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 17:47:00 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.291862 | F1-score: 0.92 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 17:47:03 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.297856 | F1-score: 0.94 | Elapsed: 2.68s
WARNING:root: [*] Thu Dec 22 17:47:05 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.310910 | F1-score: 0.93 | Elapsed: 2.71s
WARNING:root: [*] Thu Dec 22 17:47:08 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.337032 | F1-score: 0.93 | Elapsed: 2.66s
WARNING:root: [*] Thu Dec 22 17:47:11 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.207535 | F1-score: 0.93 | Elapsed: 2.73s
WARNING:root: [*] Thu Dec 22 17:47:14 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.273252 | F1-score: 0.93 | Elapsed: 2.82s
WARNING:root: [*] Thu Dec 22 17:47:16 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.206748 | F1-score: 0.93 | Elapsed: 2.80s
WARNING:root: [*] Thu Dec 22 17:47:19 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.303591 | F1-score: 0.93 | Elapsed: 2.81s
WARNING:root: [*] Thu Dec 22 17:47:22 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.335948 | F1-score: 0.93 | Elapsed: 2.75s
WARNING:root: [*] Thu Dec 22 17:47:25 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.285921 | F1-score: 0.94 | Elapsed: 2.77s
WARNING:root: [*] Thu Dec 22 17:47:26 2022:    3    | Tr.loss: 0.255425 | Tr.F1.:   0.94    |   26.10  s
WARNING:root:
        [!] Thu Dec 22 17:47:26 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727646-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727646-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727646-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727646-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 17:47:29 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.689581 | F1-score: 0.64 | Elapsed: 0.03s
WARNING:root: [*] Thu Dec 22 17:47:32 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.480390 | F1-score: 0.83 | Elapsed: 2.72s
WARNING:root: [*] Thu Dec 22 17:47:35 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.531339 | F1-score: 0.86 | Elapsed: 2.73s
WARNING:root: [*] Thu Dec 22 17:47:38 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.394660 | F1-score: 0.87 | Elapsed: 2.75s
WARNING:root: [*] Thu Dec 22 17:47:40 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.388199 | F1-score: 0.87 | Elapsed: 2.71s
WARNING:root: [*] Thu Dec 22 17:47:43 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.343905 | F1-score: 0.88 | Elapsed: 2.74s
WARNING:root: [*] Thu Dec 22 17:47:46 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.340548 | F1-score: 0.88 | Elapsed: 2.69s
WARNING:root: [*] Thu Dec 22 17:47:49 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.329221 | F1-score: 0.88 | Elapsed: 2.74s
WARNING:root: [*] Thu Dec 22 17:47:51 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.300439 | F1-score: 0.89 | Elapsed: 2.78s
WARNING:root: [*] Thu Dec 22 17:47:54 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.208936 | F1-score: 0.89 | Elapsed: 3.00s
WARNING:root: [*] Thu Dec 22 17:47:56 2022:    1    | Tr.loss: 0.402174 | Tr.F1.:   0.89    |   26.36  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 17:47:56 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.279956 | F1-score: 0.91 | Elapsed: 0.03s
WARNING:root: [*] Thu Dec 22 17:47:59 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.456879 | F1-score: 0.92 | Elapsed: 2.93s
WARNING:root: [*] Thu Dec 22 17:48:02 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.442028 | F1-score: 0.92 | Elapsed: 2.79s
WARNING:root: [*] Thu Dec 22 17:48:04 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.282633 | F1-score: 0.92 | Elapsed: 2.78s
WARNING:root: [*] Thu Dec 22 17:48:07 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.261180 | F1-score: 0.92 | Elapsed: 2.92s
WARNING:root: [*] Thu Dec 22 17:48:10 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.217913 | F1-score: 0.92 | Elapsed: 2.87s
WARNING:root: [*] Thu Dec 22 17:48:13 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.237671 | F1-score: 0.92 | Elapsed: 2.84s
WARNING:root: [*] Thu Dec 22 17:48:16 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.189219 | F1-score: 0.92 | Elapsed: 2.71s
WARNING:root: [*] Thu Dec 22 17:48:18 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.283517 | F1-score: 0.93 | Elapsed: 2.73s
WARNING:root: [*] Thu Dec 22 17:48:21 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.292173 | F1-score: 0.93 | Elapsed: 2.69s
WARNING:root: [*] Thu Dec 22 17:48:22 2022:    2    | Tr.loss: 0.284628 | Tr.F1.:   0.93    |   26.61  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 17:48:22 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.267936 | F1-score: 0.95 | Elapsed: 0.03s
WARNING:root: [*] Thu Dec 22 17:48:25 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.205199 | F1-score: 0.94 | Elapsed: 2.68s
WARNING:root: [*] Thu Dec 22 17:48:28 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.221099 | F1-score: 0.94 | Elapsed: 2.71s
WARNING:root: [*] Thu Dec 22 17:48:30 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.426844 | F1-score: 0.94 | Elapsed: 2.66s
WARNING:root: [*] Thu Dec 22 17:48:33 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.207624 | F1-score: 0.94 | Elapsed: 2.66s
WARNING:root: [*] Thu Dec 22 17:48:36 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.404664 | F1-score: 0.94 | Elapsed: 2.67s
WARNING:root: [*] Thu Dec 22 17:48:39 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.200698 | F1-score: 0.94 | Elapsed: 2.81s
WARNING:root: [*] Thu Dec 22 17:48:41 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.184219 | F1-score: 0.94 | Elapsed: 2.68s
WARNING:root: [*] Thu Dec 22 17:48:44 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.141356 | F1-score: 0.94 | Elapsed: 2.61s
WARNING:root: [*] Thu Dec 22 17:48:47 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.190338 | F1-score: 0.94 | Elapsed: 2.65s
WARNING:root: [*] Thu Dec 22 17:48:48 2022:    3    | Tr.loss: 0.248946 | Tr.F1.:   0.94    |   25.37  s
WARNING:root:
        [!] Thu Dec 22 17:48:48 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727728-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727728-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727728-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727728-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 17:48:51 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.676637 | F1-score: 0.75 | Elapsed: 0.03s
WARNING:root: [*] Thu Dec 22 17:48:54 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.557790 | F1-score: 0.84 | Elapsed: 2.61s
WARNING:root: [*] Thu Dec 22 17:48:56 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.518650 | F1-score: 0.86 | Elapsed: 2.59s
WARNING:root: [*] Thu Dec 22 17:48:59 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.542755 | F1-score: 0.87 | Elapsed: 2.62s
WARNING:root: [*] Thu Dec 22 17:49:01 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.450472 | F1-score: 0.88 | Elapsed: 2.54s
WARNING:root: [*] Thu Dec 22 17:49:04 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.385273 | F1-score: 0.88 | Elapsed: 2.67s
WARNING:root: [*] Thu Dec 22 17:49:07 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.524887 | F1-score: 0.88 | Elapsed: 2.54s
WARNING:root: [*] Thu Dec 22 17:49:09 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.434150 | F1-score: 0.89 | Elapsed: 2.55s
WARNING:root: [*] Thu Dec 22 17:49:12 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.361026 | F1-score: 0.89 | Elapsed: 2.54s
WARNING:root: [*] Thu Dec 22 17:49:14 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.343548 | F1-score: 0.89 | Elapsed: 2.57s
WARNING:root: [*] Thu Dec 22 17:49:15 2022:    1    | Tr.loss: 0.401916 | Tr.F1.:   0.89    |   24.49  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 17:49:15 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.539078 | F1-score: 0.86 | Elapsed: 0.02s
WARNING:root: [*] Thu Dec 22 17:49:18 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.435646 | F1-score: 0.92 | Elapsed: 2.59s
WARNING:root: [*] Thu Dec 22 17:49:21 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.280922 | F1-score: 0.92 | Elapsed: 2.55s
WARNING:root: [*] Thu Dec 22 17:49:23 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.545922 | F1-score: 0.92 | Elapsed: 2.54s
WARNING:root: [*] Thu Dec 22 17:49:26 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.264296 | F1-score: 0.92 | Elapsed: 2.57s
WARNING:root: [*] Thu Dec 22 17:49:28 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.177097 | F1-score: 0.92 | Elapsed: 2.55s
WARNING:root: [*] Thu Dec 22 17:49:31 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.369281 | F1-score: 0.92 | Elapsed: 2.54s
WARNING:root: [*] Thu Dec 22 17:49:33 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.244301 | F1-score: 0.92 | Elapsed: 2.55s
WARNING:root: [*] Thu Dec 22 17:49:36 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.259775 | F1-score: 0.92 | Elapsed: 2.56s
WARNING:root: [*] Thu Dec 22 17:49:38 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.277249 | F1-score: 0.93 | Elapsed: 2.55s
WARNING:root: [*] Thu Dec 22 17:49:40 2022:    2    | Tr.loss: 0.290748 | Tr.F1.:   0.93    |   24.26  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 17:49:40 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.274152 | F1-score: 0.93 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 17:49:42 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.211292 | F1-score: 0.93 | Elapsed: 2.57s
WARNING:root: [*] Thu Dec 22 17:49:45 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.407943 | F1-score: 0.93 | Elapsed: 2.55s
WARNING:root: [*] Thu Dec 22 17:49:47 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.216087 | F1-score: 0.93 | Elapsed: 2.55s
WARNING:root: [*] Thu Dec 22 17:49:50 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.169258 | F1-score: 0.93 | Elapsed: 2.64s
WARNING:root: [*] Thu Dec 22 17:49:53 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.212887 | F1-score: 0.93 | Elapsed: 2.69s
WARNING:root: [*] Thu Dec 22 17:49:55 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.243397 | F1-score: 0.94 | Elapsed: 2.57s
WARNING:root: [*] Thu Dec 22 17:49:58 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.340911 | F1-score: 0.94 | Elapsed: 2.55s
WARNING:root: [*] Thu Dec 22 17:50:00 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.290613 | F1-score: 0.94 | Elapsed: 2.56s
WARNING:root: [*] Thu Dec 22 17:50:03 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.225166 | F1-score: 0.94 | Elapsed: 2.57s
WARNING:root: [*] Thu Dec 22 17:50:04 2022:    3    | Tr.loss: 0.252226 | Tr.F1.:   0.94    |   24.53  s
WARNING:root:
        [!] Thu Dec 22 17:50:04 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727804-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727804-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727804-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727804-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\metrics_trainSize_91096_ep_3_cv_3_vocabSize_2000_dModel_32_nHeads_8_dHidden_128_nLayers_1_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 25.52s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0322 -- F1: 0.0588
	FPR:  0.001 -- TPR: 0.1327 -- F1: 0.2330
	FPR:   0.01 -- TPR: 0.3139 -- F1: 0.4761
	FPR:    0.1 -- TPR: 0.8247 -- F1: 0.8834

WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 2000, 'dModel': 32, 'nHeads': 8, 'dHidden': 128, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 17:50:38 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.689054 | F1-score: 0.65 | Elapsed: 0.38s
WARNING:root: [*] Thu Dec 22 17:50:43 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.555431 | F1-score: 0.83 | Elapsed: 4.73s
WARNING:root: [*] Thu Dec 22 17:50:47 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.427229 | F1-score: 0.86 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:50:52 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.289654 | F1-score: 0.87 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:50:57 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.535339 | F1-score: 0.87 | Elapsed: 4.75s
WARNING:root: [*] Thu Dec 22 17:51:01 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.363003 | F1-score: 0.88 | Elapsed: 4.75s
WARNING:root: [*] Thu Dec 22 17:51:06 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.500205 | F1-score: 0.88 | Elapsed: 4.75s
WARNING:root: [*] Thu Dec 22 17:51:11 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.303802 | F1-score: 0.88 | Elapsed: 4.75s
WARNING:root: [*] Thu Dec 22 17:51:16 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.392314 | F1-score: 0.89 | Elapsed: 4.76s
WARNING:root: [*] Thu Dec 22 17:51:20 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.184527 | F1-score: 0.89 | Elapsed: 4.75s
WARNING:root: [*] Thu Dec 22 17:51:23 2022:    1    | Tr.loss: 0.408675 | Tr.F1.:   0.89    |   45.35  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 17:51:23 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.468397 | F1-score: 0.88 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 17:51:28 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.261674 | F1-score: 0.92 | Elapsed: 4.77s
WARNING:root: [*] Thu Dec 22 17:51:32 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.628056 | F1-score: 0.92 | Elapsed: 4.75s
WARNING:root: [*] Thu Dec 22 17:51:37 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.362425 | F1-score: 0.92 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:51:42 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.424019 | F1-score: 0.92 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:51:46 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.241482 | F1-score: 0.92 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:51:51 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.169816 | F1-score: 0.92 | Elapsed: 4.75s
WARNING:root: [*] Thu Dec 22 17:51:56 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.337161 | F1-score: 0.92 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:52:01 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.323243 | F1-score: 0.93 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:52:05 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.293735 | F1-score: 0.93 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:52:08 2022:    2    | Tr.loss: 0.289150 | Tr.F1.:   0.93    |   44.87  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 17:52:08 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.232520 | F1-score: 0.95 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 17:52:12 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.263599 | F1-score: 0.93 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:52:17 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.268083 | F1-score: 0.93 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:52:22 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.178599 | F1-score: 0.94 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:52:26 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.257184 | F1-score: 0.94 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:52:31 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.181685 | F1-score: 0.94 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:52:36 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.128779 | F1-score: 0.94 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:52:41 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.335369 | F1-score: 0.94 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:52:45 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.271342 | F1-score: 0.94 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:52:50 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.202546 | F1-score: 0.94 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:52:52 2022:    3    | Tr.loss: 0.253615 | Tr.F1.:   0.94    |   44.62  s
WARNING:root:
        [!] Thu Dec 22 17:52:52 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727972-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727972-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727972-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671727972-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 17:52:58 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.741230 | F1-score: 0.04 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 17:53:03 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.569935 | F1-score: 0.82 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:53:08 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.438317 | F1-score: 0.85 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:53:12 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.463564 | F1-score: 0.87 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:53:17 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.525261 | F1-score: 0.87 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:53:22 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.497426 | F1-score: 0.88 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:53:27 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.425864 | F1-score: 0.88 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:53:31 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.376079 | F1-score: 0.88 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:53:36 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.463059 | F1-score: 0.88 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:53:41 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.319667 | F1-score: 0.89 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:53:43 2022:    1    | Tr.loss: 0.422042 | Tr.F1.:   0.89    |   44.62  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 17:53:43 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.403625 | F1-score: 0.92 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 17:53:48 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.443215 | F1-score: 0.91 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:53:52 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.389064 | F1-score: 0.91 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:53:57 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.298299 | F1-score: 0.91 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:54:02 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.187325 | F1-score: 0.91 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:54:06 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.399043 | F1-score: 0.91 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:54:11 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.325186 | F1-score: 0.91 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:54:16 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.396529 | F1-score: 0.92 | Elapsed: 5.07s
WARNING:root: [*] Thu Dec 22 17:54:21 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.119333 | F1-score: 0.92 | Elapsed: 4.75s
WARNING:root: [*] Thu Dec 22 17:54:26 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.211527 | F1-score: 0.92 | Elapsed: 4.85s
WARNING:root: [*] Thu Dec 22 17:54:28 2022:    2    | Tr.loss: 0.308448 | Tr.F1.:   0.92    |   45.19  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 17:54:28 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.444087 | F1-score: 0.90 | Elapsed: 0.06s
WARNING:root: [*] Thu Dec 22 17:54:33 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.199995 | F1-score: 0.93 | Elapsed: 4.74s
WARNING:root: [*] Thu Dec 22 17:54:38 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.215944 | F1-score: 0.93 | Elapsed: 4.75s
WARNING:root: [*] Thu Dec 22 17:54:42 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.371865 | F1-score: 0.93 | Elapsed: 4.81s
WARNING:root: [*] Thu Dec 22 17:54:47 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.294493 | F1-score: 0.93 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:54:52 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.253812 | F1-score: 0.93 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:54:57 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.170835 | F1-score: 0.93 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:55:01 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.249421 | F1-score: 0.93 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:55:06 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.134017 | F1-score: 0.93 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:55:11 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.211323 | F1-score: 0.93 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:55:13 2022:    3    | Tr.loss: 0.259480 | Tr.F1.:   0.93    |   44.84  s
WARNING:root:
        [!] Thu Dec 22 17:55:13 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728113-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728113-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728113-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728113-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 17:55:19 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.656648 | F1-score: 0.84 | Elapsed: 0.07s
WARNING:root: [*] Thu Dec 22 17:55:24 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.537675 | F1-score: 0.84 | Elapsed: 4.73s
WARNING:root: [*] Thu Dec 22 17:55:29 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.454118 | F1-score: 0.86 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:55:33 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.510722 | F1-score: 0.87 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:55:38 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.330084 | F1-score: 0.87 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:55:43 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.522100 | F1-score: 0.88 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:55:47 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.290268 | F1-score: 0.88 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:55:52 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.457420 | F1-score: 0.89 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:55:57 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.399211 | F1-score: 0.89 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:56:02 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.367396 | F1-score: 0.89 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:56:04 2022:    1    | Tr.loss: 0.407840 | Tr.F1.:   0.89    |   44.74  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 17:56:04 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.338310 | F1-score: 0.93 | Elapsed: 0.05s
WARNING:root: [*] Thu Dec 22 17:56:09 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.348641 | F1-score: 0.92 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:56:13 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.472912 | F1-score: 0.92 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:56:18 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.231969 | F1-score: 0.92 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:56:23 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.182231 | F1-score: 0.92 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:56:27 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.201785 | F1-score: 0.92 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:56:32 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.278945 | F1-score: 0.92 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:56:37 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.212514 | F1-score: 0.92 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:56:42 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.151870 | F1-score: 0.92 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:56:46 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.266560 | F1-score: 0.92 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:56:49 2022:    2    | Tr.loss: 0.298456 | Tr.F1.:   0.92    |   44.71  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 17:56:49 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.306868 | F1-score: 0.91 | Elapsed: 0.04s
WARNING:root: [*] Thu Dec 22 17:56:53 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.391304 | F1-score: 0.93 | Elapsed: 4.74s
WARNING:root: [*] Thu Dec 22 17:56:58 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.148179 | F1-score: 0.93 | Elapsed: 4.69s
WARNING:root: [*] Thu Dec 22 17:57:03 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.272674 | F1-score: 0.93 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:57:07 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.203091 | F1-score: 0.93 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:57:12 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.195832 | F1-score: 0.93 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:57:17 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.251244 | F1-score: 0.93 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:57:22 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.312160 | F1-score: 0.93 | Elapsed: 4.70s
WARNING:root: [*] Thu Dec 22 17:57:26 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.196728 | F1-score: 0.94 | Elapsed: 4.72s
WARNING:root: [*] Thu Dec 22 17:57:31 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.190194 | F1-score: 0.94 | Elapsed: 4.71s
WARNING:root: [*] Thu Dec 22 17:57:33 2022:    3    | Tr.loss: 0.261593 | Tr.F1.:   0.94    |   44.69  s
WARNING:root:
        [!] Thu Dec 22 17:57:33 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728253-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728253-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728253-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728253-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\metrics_trainSize_91096_ep_3_cv_3_vocabSize_2000_dModel_32_nHeads_8_dHidden_128_nLayers_2_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 44.85s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0048 -- F1: 0.0095
	FPR:  0.001 -- TPR: 0.1089 -- F1: 0.1962
	FPR:   0.01 -- TPR: 0.3197 -- F1: 0.4820
	FPR:    0.1 -- TPR: 0.5886 -- F1: 0.7163

WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 2000, 'dModel': 32, 'nHeads': 8, 'dHidden': 128, 'nLayers': 4, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 17:58:10 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.658517 | F1-score: 0.81 | Elapsed: 0.29s
WARNING:root: [*] Thu Dec 22 17:58:19 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.563384 | F1-score: 0.84 | Elapsed: 8.96s
WARNING:root: [*] Thu Dec 22 17:58:28 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.415940 | F1-score: 0.85 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 17:58:37 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.515027 | F1-score: 0.86 | Elapsed: 9.02s
WARNING:root: [*] Thu Dec 22 17:58:46 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.387915 | F1-score: 0.87 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 17:58:55 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.438760 | F1-score: 0.87 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 17:59:04 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.324905 | F1-score: 0.88 | Elapsed: 8.98s
WARNING:root: [*] Thu Dec 22 17:59:13 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.357069 | F1-score: 0.88 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 17:59:22 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.517122 | F1-score: 0.88 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 17:59:31 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.531845 | F1-score: 0.88 | Elapsed: 9.02s
WARNING:root: [*] Thu Dec 22 17:59:35 2022:    1    | Tr.loss: 0.451044 | Tr.F1.:   0.88    |   85.59  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 17:59:35 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.324145 | F1-score: 0.93 | Elapsed: 0.11s
WARNING:root: [*] Thu Dec 22 17:59:44 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.380245 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 17:59:53 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.422921 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:00:02 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.422857 | F1-score: 0.89 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:00:11 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.369079 | F1-score: 0.89 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:00:20 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.327808 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:00:29 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.371993 | F1-score: 0.89 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:00:38 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.366541 | F1-score: 0.90 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:00:47 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.483951 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:00:56 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.470199 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:01:00 2022:    2    | Tr.loss: 0.386498 | Tr.F1.:   0.90    |   85.39  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 18:01:00 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.224960 | F1-score: 0.94 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 18:01:09 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.405423 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:01:18 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.413625 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:01:27 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.308730 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:01:36 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.260853 | F1-score: 0.90 | Elapsed: 8.98s
WARNING:root: [*] Thu Dec 22 18:01:45 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.429162 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:01:54 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.339352 | F1-score: 0.90 | Elapsed: 8.98s
WARNING:root: [*] Thu Dec 22 18:02:03 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.396520 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:02:12 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.306032 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:02:21 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.398398 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:02:26 2022:    3    | Tr.loss: 0.370754 | Tr.F1.:   0.90    |   85.33  s
WARNING:root:
        [!] Thu Dec 22 18:02:26 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728546-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728546-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728546-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728546-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 18:02:37 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.668929 | F1-score: 0.78 | Elapsed: 0.08s
WARNING:root: [*] Thu Dec 22 18:02:46 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.516644 | F1-score: 0.83 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:02:55 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.393435 | F1-score: 0.85 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:03:04 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.475048 | F1-score: 0.87 | Elapsed: 9.03s
WARNING:root: [*] Thu Dec 22 18:03:13 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.335280 | F1-score: 0.87 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:03:22 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.342578 | F1-score: 0.88 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:03:31 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.374933 | F1-score: 0.88 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:03:40 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.338127 | F1-score: 0.88 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:03:49 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.479306 | F1-score: 0.88 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:03:58 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.410014 | F1-score: 0.88 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:04:03 2022:    1    | Tr.loss: 0.437451 | Tr.F1.:   0.88    |   85.44  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 18:04:03 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.350726 | F1-score: 0.90 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 18:04:12 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.480110 | F1-score: 0.88 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:04:21 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.426405 | F1-score: 0.89 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:04:30 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.415888 | F1-score: 0.89 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:04:39 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.350160 | F1-score: 0.89 | Elapsed: 8.98s
WARNING:root: [*] Thu Dec 22 18:04:48 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.487430 | F1-score: 0.89 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:04:57 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.264773 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:05:06 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.274470 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:05:15 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.417774 | F1-score: 0.89 | Elapsed: 8.97s
WARNING:root: [*] Thu Dec 22 18:05:24 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.399150 | F1-score: 0.90 | Elapsed: 9.02s
WARNING:root: [*] Thu Dec 22 18:05:28 2022:    2    | Tr.loss: 0.387420 | Tr.F1.:   0.90    |   85.36  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 18:05:28 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.387221 | F1-score: 0.88 | Elapsed: 0.09s
WARNING:root: [*] Thu Dec 22 18:05:37 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.413032 | F1-score: 0.90 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:05:46 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.363445 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:05:55 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.300495 | F1-score: 0.90 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:06:04 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.285789 | F1-score: 0.90 | Elapsed: 8.99s
WARNING:root: [*] Thu Dec 22 18:06:13 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.284759 | F1-score: 0.90 | Elapsed: 8.98s
WARNING:root: [*] Thu Dec 22 18:06:22 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.381393 | F1-score: 0.90 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:06:31 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.325399 | F1-score: 0.90 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:06:40 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.430158 | F1-score: 0.90 | Elapsed: 8.98s
WARNING:root: [*] Thu Dec 22 18:06:49 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.374806 | F1-score: 0.90 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:06:54 2022:    3    | Tr.loss: 0.382891 | Tr.F1.:   0.90    |   85.38  s
WARNING:root:
        [!] Thu Dec 22 18:06:54 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728814-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728814-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728814-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671728814-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 18:07:05 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.722144 | F1-score: 0.27 | Elapsed: 0.10s
WARNING:root: [*] Thu Dec 22 18:07:14 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.602930 | F1-score: 0.83 | Elapsed: 9.03s
WARNING:root: [*] Thu Dec 22 18:07:23 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.451805 | F1-score: 0.84 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:07:32 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.495956 | F1-score: 0.85 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:07:41 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.298529 | F1-score: 0.86 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:07:50 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.385225 | F1-score: 0.87 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:07:59 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.421396 | F1-score: 0.87 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:08:08 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.347073 | F1-score: 0.87 | Elapsed: 9.04s
WARNING:root: [*] Thu Dec 22 18:08:17 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.332983 | F1-score: 0.87 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:08:26 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.434435 | F1-score: 0.88 | Elapsed: 9.02s
WARNING:root: [*] Thu Dec 22 18:08:31 2022:    1    | Tr.loss: 0.459315 | Tr.F1.:   0.88    |   85.55  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 18:08:31 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.399312 | F1-score: 0.87 | Elapsed: 0.09s
WARNING:root: [*] Thu Dec 22 18:08:40 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.426531 | F1-score: 0.90 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:08:49 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.375558 | F1-score: 0.90 | Elapsed: 9.02s
WARNING:root: [*] Thu Dec 22 18:08:58 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.479917 | F1-score: 0.90 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:09:07 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.393094 | F1-score: 0.90 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:09:16 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.266742 | F1-score: 0.90 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:09:25 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.416824 | F1-score: 0.90 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:09:34 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.343069 | F1-score: 0.90 | Elapsed: 9.02s
WARNING:root: [*] Thu Dec 22 18:09:43 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.307370 | F1-score: 0.90 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:09:52 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.423705 | F1-score: 0.90 | Elapsed: 9.02s
WARNING:root: [*] Thu Dec 22 18:09:56 2022:    2    | Tr.loss: 0.371168 | Tr.F1.:   0.90    |   85.54  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 18:09:56 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.299833 | F1-score: 0.90 | Elapsed: 0.09s
WARNING:root: [*] Thu Dec 22 18:10:05 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.368100 | F1-score: 0.92 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:10:14 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.305150 | F1-score: 0.92 | Elapsed: 9.02s
WARNING:root: [*] Thu Dec 22 18:10:24 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.232088 | F1-score: 0.92 | Elapsed: 9.02s
WARNING:root: [*] Thu Dec 22 18:10:33 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.344144 | F1-score: 0.92 | Elapsed: 9.03s
WARNING:root: [*] Thu Dec 22 18:10:42 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.338176 | F1-score: 0.92 | Elapsed: 9.05s
WARNING:root: [*] Thu Dec 22 18:10:51 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.322778 | F1-score: 0.92 | Elapsed: 9.00s
WARNING:root: [*] Thu Dec 22 18:11:00 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.233349 | F1-score: 0.92 | Elapsed: 9.03s
WARNING:root: [*] Thu Dec 22 18:11:09 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.283918 | F1-score: 0.92 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:11:18 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.251243 | F1-score: 0.92 | Elapsed: 9.01s
WARNING:root: [*] Thu Dec 22 18:11:22 2022:    3    | Tr.loss: 0.303810 | Tr.F1.:   0.92    |   85.59  s
WARNING:root:
        [!] Thu Dec 22 18:11:22 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671729082-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671729082-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671729082-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671729082-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\metrics_trainSize_91096_ep_3_cv_3_vocabSize_2000_dModel_32_nHeads_8_dHidden_128_nLayers_4_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 85.46s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0002 -- F1: 0.0005
	FPR:  0.001 -- TPR: 0.0073 -- F1: 0.0143
	FPR:   0.01 -- TPR: 0.0462 -- F1: 0.0809
	FPR:    0.1 -- TPR: 0.2589 -- F1: 0.3719

WARNING:root: [!] Using device: cuda:0 | Dataset size: 91096
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 2000, 'dModel': 32, 'nHeads': 8, 'dHidden': 128, 'nLayers': 8, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.5}
WARNING:root: [!] Fold 1/3 | Train set size: 60730, Validation set size: 30366
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 18:12:05 2022: Train Epoch: 1 [  0  /60730 (0 %)]	Loss: 0.699875 | F1-score: 0.39 | Elapsed: 0.55s
WARNING:root: [*] Thu Dec 22 18:12:22 2022: Train Epoch: 1 [6400 /60730 (11%)]	Loss: 0.570213 | F1-score: 0.83 | Elapsed: 17.53s
WARNING:root: [*] Thu Dec 22 18:12:40 2022: Train Epoch: 1 [12800/60730 (21%)]	Loss: 0.601383 | F1-score: 0.83 | Elapsed: 17.66s
WARNING:root: [*] Thu Dec 22 18:12:58 2022: Train Epoch: 1 [19200/60730 (32%)]	Loss: 0.598588 | F1-score: 0.83 | Elapsed: 17.74s
WARNING:root: [*] Thu Dec 22 18:13:15 2022: Train Epoch: 1 [25600/60730 (42%)]	Loss: 0.614880 | F1-score: 0.83 | Elapsed: 17.73s
WARNING:root: [*] Thu Dec 22 18:13:33 2022: Train Epoch: 1 [32000/60730 (53%)]	Loss: 0.664914 | F1-score: 0.83 | Elapsed: 17.72s
WARNING:root: [*] Thu Dec 22 18:13:51 2022: Train Epoch: 1 [38400/60730 (63%)]	Loss: 0.692709 | F1-score: 0.84 | Elapsed: 17.77s
WARNING:root: [*] Thu Dec 22 18:14:09 2022: Train Epoch: 1 [44800/60730 (74%)]	Loss: 0.353970 | F1-score: 0.85 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:14:26 2022: Train Epoch: 1 [51200/60730 (84%)]	Loss: 0.494870 | F1-score: 0.85 | Elapsed: 17.79s
WARNING:root: [*] Thu Dec 22 18:14:44 2022: Train Epoch: 1 [57600/60730 (95%)]	Loss: 0.603899 | F1-score: 0.86 | Elapsed: 17.74s
WARNING:root: [*] Thu Dec 22 18:14:53 2022:    1    | Tr.loss: 0.544761 | Tr.F1.:   0.86    |  168.47  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 18:14:53 2022: Train Epoch: 2 [  0  /60730 (0 %)]	Loss: 0.612124 | F1-score: 0.83 | Elapsed: 0.19s
WARNING:root: [*] Thu Dec 22 18:15:11 2022: Train Epoch: 2 [6400 /60730 (11%)]	Loss: 0.625604 | F1-score: 0.87 | Elapsed: 17.72s
WARNING:root: [*] Thu Dec 22 18:15:28 2022: Train Epoch: 2 [12800/60730 (21%)]	Loss: 0.522662 | F1-score: 0.86 | Elapsed: 17.72s
WARNING:root: [*] Thu Dec 22 18:15:46 2022: Train Epoch: 2 [19200/60730 (32%)]	Loss: 0.606525 | F1-score: 0.85 | Elapsed: 17.73s
WARNING:root: [*] Thu Dec 22 18:16:04 2022: Train Epoch: 2 [25600/60730 (42%)]	Loss: 0.610283 | F1-score: 0.85 | Elapsed: 17.73s
WARNING:root: [*] Thu Dec 22 18:16:22 2022: Train Epoch: 2 [32000/60730 (53%)]	Loss: 0.813116 | F1-score: 0.85 | Elapsed: 17.78s
WARNING:root: [*] Thu Dec 22 18:16:39 2022: Train Epoch: 2 [38400/60730 (63%)]	Loss: 0.544130 | F1-score: 0.85 | Elapsed: 17.80s
WARNING:root: [*] Thu Dec 22 18:16:57 2022: Train Epoch: 2 [44800/60730 (74%)]	Loss: 0.523306 | F1-score: 0.85 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:17:15 2022: Train Epoch: 2 [51200/60730 (84%)]	Loss: 0.535644 | F1-score: 0.86 | Elapsed: 17.74s
WARNING:root: [*] Thu Dec 22 18:17:33 2022: Train Epoch: 2 [57600/60730 (95%)]	Loss: 0.452856 | F1-score: 0.86 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:17:41 2022:    2    | Tr.loss: 0.524966 | Tr.F1.:   0.86    |  168.40  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 18:17:41 2022: Train Epoch: 3 [  0  /60730 (0 %)]	Loss: 0.568289 | F1-score: 0.87 | Elapsed: 0.18s
WARNING:root: [*] Thu Dec 22 18:17:59 2022: Train Epoch: 3 [6400 /60730 (11%)]	Loss: 0.457365 | F1-score: 0.88 | Elapsed: 17.72s
WARNING:root: [*] Thu Dec 22 18:18:17 2022: Train Epoch: 3 [12800/60730 (21%)]	Loss: 0.464445 | F1-score: 0.88 | Elapsed: 17.76s
WARNING:root: [*] Thu Dec 22 18:18:34 2022: Train Epoch: 3 [19200/60730 (32%)]	Loss: 0.399942 | F1-score: 0.88 | Elapsed: 17.72s
WARNING:root: [*] Thu Dec 22 18:18:52 2022: Train Epoch: 3 [25600/60730 (42%)]	Loss: 0.637027 | F1-score: 0.88 | Elapsed: 17.73s
WARNING:root: [*] Thu Dec 22 18:19:10 2022: Train Epoch: 3 [32000/60730 (53%)]	Loss: 0.668154 | F1-score: 0.88 | Elapsed: 17.79s
WARNING:root: [*] Thu Dec 22 18:19:28 2022: Train Epoch: 3 [38400/60730 (63%)]	Loss: 0.576178 | F1-score: 0.87 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:19:46 2022: Train Epoch: 3 [44800/60730 (74%)]	Loss: 0.518978 | F1-score: 0.87 | Elapsed: 17.77s
WARNING:root: [*] Thu Dec 22 18:20:03 2022: Train Epoch: 3 [51200/60730 (84%)]	Loss: 0.625228 | F1-score: 0.86 | Elapsed: 17.72s
WARNING:root: [*] Thu Dec 22 18:20:21 2022: Train Epoch: 3 [57600/60730 (95%)]	Loss: 0.610224 | F1-score: 0.86 | Elapsed: 17.73s
WARNING:root: [*] Thu Dec 22 18:20:29 2022:    3    | Tr.loss: 0.521486 | Tr.F1.:   0.86    |  168.38  s
WARNING:root:
        [!] Thu Dec 22 18:20:29 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671729629-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671729629-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671729629-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671729629-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 18:20:53 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.752260 | F1-score: 0.20 | Elapsed: 0.19s
WARNING:root: [*] Thu Dec 22 18:21:11 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.591785 | F1-score: 0.83 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:21:28 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.647584 | F1-score: 0.83 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:21:46 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.748427 | F1-score: 0.83 | Elapsed: 17.74s
WARNING:root: [*] Thu Dec 22 18:22:04 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.456828 | F1-score: 0.84 | Elapsed: 17.72s
WARNING:root: [*] Thu Dec 22 18:22:22 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.543889 | F1-score: 0.84 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:22:39 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.562117 | F1-score: 0.85 | Elapsed: 17.79s
WARNING:root: [*] Thu Dec 22 18:22:57 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.421082 | F1-score: 0.85 | Elapsed: 17.76s
WARNING:root: [*] Thu Dec 22 18:23:15 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.670769 | F1-score: 0.85 | Elapsed: 17.73s
WARNING:root: [*] Thu Dec 22 18:23:33 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.432648 | F1-score: 0.86 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:23:41 2022:    1    | Tr.loss: 0.544335 | Tr.F1.:   0.86    |  168.44  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 18:23:41 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.362989 | F1-score: 0.92 | Elapsed: 0.19s
WARNING:root: [*] Thu Dec 22 18:23:59 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.438709 | F1-score: 0.83 | Elapsed: 17.78s
WARNING:root: [*] Thu Dec 22 18:24:17 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.486334 | F1-score: 0.85 | Elapsed: 17.83s
WARNING:root: [*] Thu Dec 22 18:24:35 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.489178 | F1-score: 0.85 | Elapsed: 17.77s
WARNING:root: [*] Thu Dec 22 18:24:52 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.861043 | F1-score: 0.86 | Elapsed: 17.76s
WARNING:root: [*] Thu Dec 22 18:25:10 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.460942 | F1-score: 0.85 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:25:28 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.575841 | F1-score: 0.85 | Elapsed: 17.78s
WARNING:root: [*] Thu Dec 22 18:25:46 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.502076 | F1-score: 0.85 | Elapsed: 17.78s
WARNING:root: [*] Thu Dec 22 18:26:04 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.474671 | F1-score: 0.85 | Elapsed: 17.82s
WARNING:root: [*] Thu Dec 22 18:26:21 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.485083 | F1-score: 0.85 | Elapsed: 17.79s
WARNING:root: [*] Thu Dec 22 18:26:30 2022:    2    | Tr.loss: 0.536710 | Tr.F1.:   0.85    |  168.79  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 18:26:30 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.511131 | F1-score: 0.80 | Elapsed: 0.19s
WARNING:root: [*] Thu Dec 22 18:26:48 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.525795 | F1-score: 0.82 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:27:06 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.647282 | F1-score: 0.83 | Elapsed: 17.73s
WARNING:root: [*] Thu Dec 22 18:27:23 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.502188 | F1-score: 0.83 | Elapsed: 17.79s
WARNING:root: [*] Thu Dec 22 18:27:41 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.544800 | F1-score: 0.83 | Elapsed: 17.83s
WARNING:root: [*] Thu Dec 22 18:27:59 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.540329 | F1-score: 0.83 | Elapsed: 17.80s
WARNING:root: [*] Thu Dec 22 18:28:17 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.527349 | F1-score: 0.83 | Elapsed: 17.79s
WARNING:root: [*] Thu Dec 22 18:28:35 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.605518 | F1-score: 0.83 | Elapsed: 17.77s
WARNING:root: [*] Thu Dec 22 18:28:52 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.561055 | F1-score: 0.83 | Elapsed: 17.82s
WARNING:root: [*] Thu Dec 22 18:29:10 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.546088 | F1-score: 0.83 | Elapsed: 17.82s
WARNING:root: [*] Thu Dec 22 18:29:19 2022:    3    | Tr.loss: 0.561055 | Tr.F1.:   0.83    |  168.84  s
WARNING:root:
        [!] Thu Dec 22 18:29:19 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671730159-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671730159-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671730159-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671730159-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 60731, Validation set size: 30365
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Thu Dec 22 18:29:42 2022: Train Epoch: 1 [  0  /60731 (0 %)]	Loss: 0.666210 | F1-score: 0.81 | Elapsed: 0.21s
WARNING:root: [*] Thu Dec 22 18:30:00 2022: Train Epoch: 1 [6400 /60731 (11%)]	Loss: 0.561183 | F1-score: 0.84 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:30:18 2022: Train Epoch: 1 [12800/60731 (21%)]	Loss: 0.634440 | F1-score: 0.83 | Elapsed: 17.78s
WARNING:root: [*] Thu Dec 22 18:30:36 2022: Train Epoch: 1 [19200/60731 (32%)]	Loss: 0.609151 | F1-score: 0.84 | Elapsed: 17.81s
WARNING:root: [*] Thu Dec 22 18:30:53 2022: Train Epoch: 1 [25600/60731 (42%)]	Loss: 0.705947 | F1-score: 0.84 | Elapsed: 17.78s
WARNING:root: [*] Thu Dec 22 18:31:11 2022: Train Epoch: 1 [32000/60731 (53%)]	Loss: 0.579221 | F1-score: 0.84 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:31:29 2022: Train Epoch: 1 [38400/60731 (63%)]	Loss: 0.416324 | F1-score: 0.85 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:31:47 2022: Train Epoch: 1 [44800/60731 (74%)]	Loss: 0.483395 | F1-score: 0.85 | Elapsed: 17.77s
WARNING:root: [*] Thu Dec 22 18:32:04 2022: Train Epoch: 1 [51200/60731 (84%)]	Loss: 0.477459 | F1-score: 0.85 | Elapsed: 17.81s
WARNING:root: [*] Thu Dec 22 18:32:22 2022: Train Epoch: 1 [57600/60731 (95%)]	Loss: 0.430711 | F1-score: 0.86 | Elapsed: 17.81s
WARNING:root: [*] Thu Dec 22 18:32:31 2022:    1    | Tr.loss: 0.541161 | Tr.F1.:   0.86    |  168.79  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Thu Dec 22 18:32:31 2022: Train Epoch: 2 [  0  /60731 (0 %)]	Loss: 0.534206 | F1-score: 0.87 | Elapsed: 0.18s
WARNING:root: [*] Thu Dec 22 18:32:49 2022: Train Epoch: 2 [6400 /60731 (11%)]	Loss: 0.455545 | F1-score: 0.87 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:33:06 2022: Train Epoch: 2 [12800/60731 (21%)]	Loss: 0.435631 | F1-score: 0.88 | Elapsed: 17.80s
WARNING:root: [*] Thu Dec 22 18:33:24 2022: Train Epoch: 2 [19200/60731 (32%)]	Loss: 0.368316 | F1-score: 0.87 | Elapsed: 17.81s
WARNING:root: [*] Thu Dec 22 18:33:42 2022: Train Epoch: 2 [25600/60731 (42%)]	Loss: 0.485580 | F1-score: 0.88 | Elapsed: 17.77s
WARNING:root: [*] Thu Dec 22 18:34:00 2022: Train Epoch: 2 [32000/60731 (53%)]	Loss: 0.541431 | F1-score: 0.87 | Elapsed: 17.77s
WARNING:root: [*] Thu Dec 22 18:34:18 2022: Train Epoch: 2 [38400/60731 (63%)]	Loss: 0.599051 | F1-score: 0.87 | Elapsed: 17.74s
WARNING:root: [*] Thu Dec 22 18:34:35 2022: Train Epoch: 2 [44800/60731 (74%)]	Loss: 0.518689 | F1-score: 0.86 | Elapsed: 17.79s
WARNING:root: [*] Thu Dec 22 18:34:53 2022: Train Epoch: 2 [51200/60731 (84%)]	Loss: 0.551168 | F1-score: 0.86 | Elapsed: 17.82s
WARNING:root: [*] Thu Dec 22 18:35:11 2022: Train Epoch: 2 [57600/60731 (95%)]	Loss: 0.600752 | F1-score: 0.86 | Elapsed: 17.78s
WARNING:root: [*] Thu Dec 22 18:35:20 2022:    2    | Tr.loss: 0.512022 | Tr.F1.:   0.86    |  168.77  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Thu Dec 22 18:35:20 2022: Train Epoch: 3 [  0  /60731 (0 %)]	Loss: 0.511775 | F1-score: 0.81 | Elapsed: 0.18s
WARNING:root: [*] Thu Dec 22 18:35:37 2022: Train Epoch: 3 [6400 /60731 (11%)]	Loss: 0.619238 | F1-score: 0.84 | Elapsed: 17.76s
WARNING:root: [*] Thu Dec 22 18:35:55 2022: Train Epoch: 3 [12800/60731 (21%)]	Loss: 0.599205 | F1-score: 0.84 | Elapsed: 17.75s
WARNING:root: [*] Thu Dec 22 18:36:13 2022: Train Epoch: 3 [19200/60731 (32%)]	Loss: 0.492384 | F1-score: 0.84 | Elapsed: 17.82s
WARNING:root: [*] Thu Dec 22 18:36:31 2022: Train Epoch: 3 [25600/60731 (42%)]	Loss: 0.559192 | F1-score: 0.84 | Elapsed: 17.83s
WARNING:root: [*] Thu Dec 22 18:36:49 2022: Train Epoch: 3 [32000/60731 (53%)]	Loss: 0.497654 | F1-score: 0.84 | Elapsed: 17.78s
WARNING:root: [*] Thu Dec 22 18:37:06 2022: Train Epoch: 3 [38400/60731 (63%)]	Loss: 0.516086 | F1-score: 0.85 | Elapsed: 17.77s
WARNING:root: [*] Thu Dec 22 18:37:24 2022: Train Epoch: 3 [44800/60731 (74%)]	Loss: 0.654202 | F1-score: 0.85 | Elapsed: 17.74s
WARNING:root: [*] Thu Dec 22 18:37:42 2022: Train Epoch: 3 [51200/60731 (84%)]	Loss: 0.567576 | F1-score: 0.86 | Elapsed: 17.77s
WARNING:root: [*] Thu Dec 22 18:38:00 2022: Train Epoch: 3 [57600/60731 (95%)]	Loss: 0.521671 | F1-score: 0.86 | Elapsed: 17.83s
WARNING:root: [*] Thu Dec 22 18:38:08 2022:    3    | Tr.loss: 0.516393 | Tr.F1.:   0.86    |  168.88  s
WARNING:root:
        [!] Thu Dec 22 18:38:08 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671730688-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671730688-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671730688-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\trainingFiles\trainingFiles_1671730688-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation\TransformerNrOfLayers\metrics_trainSize_91096_ep_3_cv_3_vocabSize_2000_dModel_32_nHeads_8_dHidden_128_nLayers_8_numClasses_1_hiddenNeurons_64_layerNorm_False_dropout_0.5.json
WARNING:root: [!] Average epoch time: 168.64s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.0699 -- F1: 0.1243
	FPR:  0.001 -- TPR: 0.0715 -- F1: 0.1269
	FPR:   0.01 -- TPR: 0.2423 -- F1: 0.3317
	FPR:    0.1 -- TPR: 0.3515 -- F1: 0.4663

