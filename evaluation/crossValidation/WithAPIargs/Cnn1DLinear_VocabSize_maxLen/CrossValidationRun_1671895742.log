WARNING:root: [!] Skipping maxLen_1024_vocabSize_10000_ as it already exists
WARNING:root: [!] Skipping maxLen_2048_vocabSize_10000_ as it already exists
WARNING:root: [!] Skipping maxLen_4096_vocabSize_10000_ as it already exists
WARNING:root: [!] Skipping maxLen_512_vocabSize_10000_ as it already exists
WARNING:root: [!] Skipping maxLen_6144_vocabSize_10000_ as it already exists
WARNING:root: [!] Skipping maxLen_1024_vocabSize_1000_ as it already exists
WARNING:root: [!] Skipping maxLen_2048_vocabSize_1000_ as it already exists
WARNING:root: [!] Skipping maxLen_4096_vocabSize_1000_ as it already exists
WARNING:root: [!] Skipping maxLen_512_vocabSize_1000_ as it already exists
WARNING:root: [!] Skipping maxLen_6144_vocabSize_1000_ as it already exists
WARNING:root: [!] Skipping maxLen_1024_vocabSize_15000_ as it already exists
WARNING:root: [!] Skipping maxLen_2048_vocabSize_15000_ as it already exists
WARNING:root: [!] Skipping maxLen_4096_vocabSize_15000_ as it already exists
WARNING:root: [!] Skipping maxLen_512_vocabSize_15000_ as it already exists
WARNING:root: [!] Skipping maxLen_6144_vocabSize_15000_ as it already exists
WARNING:root: [!] Skipping maxLen_1024_vocabSize_1500_ as it already exists
WARNING:root: [!] Skipping maxLen_2048_vocabSize_1500_ as it already exists
WARNING:root: [!] Skipping maxLen_4096_vocabSize_1500_ as it already exists
WARNING:root: [!] Skipping maxLen_512_vocabSize_1500_ as it already exists
WARNING:root: [!] Skipping maxLen_6144_vocabSize_1500_ as it already exists
WARNING:root: [!] Skipping maxLen_1024_vocabSize_20000_ as it already exists
WARNING:root: [!] Skipping maxLen_2048_vocabSize_20000_ as it already exists
WARNING:root: [!] Skipping maxLen_4096_vocabSize_20000_ as it already exists
WARNING:root: [!] Skipping maxLen_512_vocabSize_20000_ as it already exists
WARNING:root: [!] Skipping maxLen_6144_vocabSize_20000_ as it already exists
WARNING:root: [!] Skipping maxLen_1024_vocabSize_2000_ as it already exists
WARNING:root: [!] Skipping maxLen_2048_vocabSize_2000_ as it already exists
WARNING:root: [!] Skipping maxLen_4096_vocabSize_2000_ as it already exists
WARNING:root: [!] Skipping maxLen_512_vocabSize_2000_ as it already exists
WARNING:root: [!] Skipping maxLen_6144_vocabSize_2000_ as it already exists
WARNING:root: [!] Skipping maxLen_1024_vocabSize_25000_ as it already exists
WARNING:root: [!] Skipping maxLen_2048_vocabSize_25000_ as it already exists
WARNING:root: [!] Skipping maxLen_4096_vocabSize_25000_ as it already exists
WARNING:root: [!] Skipping maxLen_512_vocabSize_25000_ as it already exists
WARNING:root: [!] Skipping maxLen_6144_vocabSize_25000_ as it already exists
WARNING:root: [!] Skipping maxLen_1024_vocabSize_2500_ as it already exists
WARNING:root: [!] Skipping maxLen_2048_vocabSize_2500_ as it already exists
WARNING:root: [!] Skipping maxLen_4096_vocabSize_2500_ as it already exists
WARNING:root: [!] Skipping maxLen_512_vocabSize_2500_ as it already exists
WARNING:root: [!] Skipping maxLen_6144_vocabSize_2500_ as it already exists
WARNING:root: [!] Skipping maxLen_1024_vocabSize_5000_ as it already exists
WARNING:root: [!] Skipping maxLen_2048_vocabSize_5000_ as it already exists
WARNING:root: [!] Skipping maxLen_4096_vocabSize_5000_ as it already exists
WARNING:root: [!] Skipping maxLen_512_vocabSize_5000_ as it already exists
WARNING:root: [!] Skipping maxLen_6144_vocabSize_5000_ as it already exists
WARNING:root: [!] Running Cross Validation with vocabSize: 500 | maxLen: 1024
WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 500, 'embeddingDim': 64, 'hiddenNeurons': [512, 256, 128], 'batchNormConv': False, 'batchNormFFNN': False, 'filterSizes': [2, 3, 4, 5]}
WARNING:root: [!] Fold 1/3 | Train set size: 50750, Validation set size: 25376
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 16:29:25 2022: Train Epoch: 1 [  0  /50750 (0 %)]	Loss: 0.730390 | F1-score: 0.21 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 16:29:29 2022: Train Epoch: 1 [6400 /50750 (13%)]	Loss: 0.222607 | F1-score: 0.87 | Elapsed: 3.72s
WARNING:root: [*] Sat Dec 24 16:29:33 2022: Train Epoch: 1 [12800/50750 (25%)]	Loss: 0.124683 | F1-score: 0.90 | Elapsed: 3.74s
WARNING:root: [*] Sat Dec 24 16:29:36 2022: Train Epoch: 1 [19200/50750 (38%)]	Loss: 0.195743 | F1-score: 0.91 | Elapsed: 3.91s
WARNING:root: [*] Sat Dec 24 16:29:40 2022: Train Epoch: 1 [25600/50750 (50%)]	Loss: 0.209036 | F1-score: 0.92 | Elapsed: 3.88s
WARNING:root: [*] Sat Dec 24 16:29:44 2022: Train Epoch: 1 [32000/50750 (63%)]	Loss: 0.067824 | F1-score: 0.93 | Elapsed: 3.99s
WARNING:root: [*] Sat Dec 24 16:29:48 2022: Train Epoch: 1 [38400/50750 (76%)]	Loss: 0.137304 | F1-score: 0.93 | Elapsed: 3.89s
WARNING:root: [*] Sat Dec 24 16:29:52 2022: Train Epoch: 1 [44800/50750 (88%)]	Loss: 0.137654 | F1-score: 0.93 | Elapsed: 3.84s
WARNING:root: [*] Sat Dec 24 16:29:56 2022:    1    | Tr.loss: 0.209137 | Tr.F1.:   0.94    |   32.79  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 16:29:56 2022: Train Epoch: 2 [  0  /50750 (0 %)]	Loss: 0.107625 | F1-score: 0.96 | Elapsed: 0.05s
WARNING:root: [*] Sat Dec 24 16:30:00 2022: Train Epoch: 2 [6400 /50750 (13%)]	Loss: 0.066683 | F1-score: 0.96 | Elapsed: 3.87s
WARNING:root: [*] Sat Dec 24 16:30:03 2022: Train Epoch: 2 [12800/50750 (25%)]	Loss: 0.094391 | F1-score: 0.96 | Elapsed: 3.88s
WARNING:root: [*] Sat Dec 24 16:30:07 2022: Train Epoch: 2 [19200/50750 (38%)]	Loss: 0.093290 | F1-score: 0.96 | Elapsed: 3.87s
WARNING:root: [*] Sat Dec 24 16:30:11 2022: Train Epoch: 2 [25600/50750 (50%)]	Loss: 0.204804 | F1-score: 0.96 | Elapsed: 3.87s
WARNING:root: [*] Sat Dec 24 16:30:15 2022: Train Epoch: 2 [32000/50750 (63%)]	Loss: 0.214446 | F1-score: 0.96 | Elapsed: 3.84s
WARNING:root: [*] Sat Dec 24 16:30:19 2022: Train Epoch: 2 [38400/50750 (76%)]	Loss: 0.091103 | F1-score: 0.96 | Elapsed: 3.84s
WARNING:root: [*] Sat Dec 24 16:30:23 2022: Train Epoch: 2 [44800/50750 (88%)]	Loss: 0.182556 | F1-score: 0.96 | Elapsed: 3.86s
WARNING:root: [*] Sat Dec 24 16:30:26 2022:    2    | Tr.loss: 0.125232 | Tr.F1.:   0.96    |   30.61  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 16:30:26 2022: Train Epoch: 3 [  0  /50750 (0 %)]	Loss: 0.121612 | F1-score: 0.93 | Elapsed: 0.05s
WARNING:root: [*] Sat Dec 24 16:30:30 2022: Train Epoch: 3 [6400 /50750 (13%)]	Loss: 0.085819 | F1-score: 0.96 | Elapsed: 3.84s
WARNING:root: [*] Sat Dec 24 16:30:34 2022: Train Epoch: 3 [12800/50750 (25%)]	Loss: 0.158757 | F1-score: 0.96 | Elapsed: 3.93s
WARNING:root: [*] Sat Dec 24 16:30:38 2022: Train Epoch: 3 [19200/50750 (38%)]	Loss: 0.047290 | F1-score: 0.96 | Elapsed: 3.97s
WARNING:root: [*] Sat Dec 24 16:30:42 2022: Train Epoch: 3 [25600/50750 (50%)]	Loss: 0.062253 | F1-score: 0.96 | Elapsed: 3.92s
WARNING:root: [*] Sat Dec 24 16:30:46 2022: Train Epoch: 3 [32000/50750 (63%)]	Loss: 0.218390 | F1-score: 0.96 | Elapsed: 3.95s
WARNING:root: [*] Sat Dec 24 16:30:50 2022: Train Epoch: 3 [38400/50750 (76%)]	Loss: 0.097506 | F1-score: 0.96 | Elapsed: 3.92s
WARNING:root: [*] Sat Dec 24 16:30:54 2022: Train Epoch: 3 [44800/50750 (88%)]	Loss: 0.125089 | F1-score: 0.96 | Elapsed: 3.99s
WARNING:root: [*] Sat Dec 24 16:30:57 2022:    3    | Tr.loss: 0.112464 | Tr.F1.:   0.96    |   31.18  s
WARNING:root:
        [!] Sat Dec 24 16:30:57 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671895857-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671895857-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671895857-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671895857-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 50751, Validation set size: 25375
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 16:31:02 2022: Train Epoch: 1 [  0  /50751 (0 %)]	Loss: 0.744316 | F1-score: 0.12 | Elapsed: 0.04s
WARNING:root: [*] Sat Dec 24 16:31:06 2022: Train Epoch: 1 [6400 /50751 (13%)]	Loss: 0.137514 | F1-score: 0.86 | Elapsed: 4.03s
WARNING:root: [*] Sat Dec 24 16:31:10 2022: Train Epoch: 1 [12800/50751 (25%)]	Loss: 0.171686 | F1-score: 0.90 | Elapsed: 4.00s
WARNING:root: [*] Sat Dec 24 16:31:14 2022: Train Epoch: 1 [19200/50751 (38%)]	Loss: 0.302103 | F1-score: 0.91 | Elapsed: 3.97s
WARNING:root: [*] Sat Dec 24 16:31:18 2022: Train Epoch: 1 [25600/50751 (50%)]	Loss: 0.268877 | F1-score: 0.92 | Elapsed: 3.93s
WARNING:root: [*] Sat Dec 24 16:31:22 2022: Train Epoch: 1 [32000/50751 (63%)]	Loss: 0.108167 | F1-score: 0.92 | Elapsed: 3.95s
WARNING:root: [*] Sat Dec 24 16:31:26 2022: Train Epoch: 1 [38400/50751 (76%)]	Loss: 0.109646 | F1-score: 0.93 | Elapsed: 3.95s
WARNING:root: [*] Sat Dec 24 16:31:30 2022: Train Epoch: 1 [44800/50751 (88%)]	Loss: 0.065686 | F1-score: 0.93 | Elapsed: 3.97s
WARNING:root: [*] Sat Dec 24 16:31:33 2022:    1    | Tr.loss: 0.211605 | Tr.F1.:   0.94    |   31.46  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 16:31:33 2022: Train Epoch: 2 [  0  /50751 (0 %)]	Loss: 0.080437 | F1-score: 0.98 | Elapsed: 0.04s
WARNING:root: [*] Sat Dec 24 16:31:37 2022: Train Epoch: 2 [6400 /50751 (13%)]	Loss: 0.185689 | F1-score: 0.96 | Elapsed: 3.91s
WARNING:root: [*] Sat Dec 24 16:31:41 2022: Train Epoch: 2 [12800/50751 (25%)]	Loss: 0.075095 | F1-score: 0.96 | Elapsed: 3.94s
WARNING:root: [*] Sat Dec 24 16:31:45 2022: Train Epoch: 2 [19200/50751 (38%)]	Loss: 0.131685 | F1-score: 0.96 | Elapsed: 3.96s
WARNING:root: [*] Sat Dec 24 16:31:49 2022: Train Epoch: 2 [25600/50751 (50%)]	Loss: 0.342981 | F1-score: 0.96 | Elapsed: 3.97s
WARNING:root: [*] Sat Dec 24 16:31:53 2022: Train Epoch: 2 [32000/50751 (63%)]	Loss: 0.162559 | F1-score: 0.96 | Elapsed: 3.98s
WARNING:root: [*] Sat Dec 24 16:31:57 2022: Train Epoch: 2 [38400/50751 (76%)]	Loss: 0.187026 | F1-score: 0.96 | Elapsed: 4.01s
WARNING:root: [*] Sat Dec 24 16:32:01 2022: Train Epoch: 2 [44800/50751 (88%)]	Loss: 0.140626 | F1-score: 0.96 | Elapsed: 3.99s
WARNING:root: [*] Sat Dec 24 16:32:05 2022:    2    | Tr.loss: 0.127712 | Tr.F1.:   0.96    |   31.48  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 16:32:05 2022: Train Epoch: 3 [  0  /50751 (0 %)]	Loss: 0.132630 | F1-score: 0.95 | Elapsed: 0.06s
WARNING:root: [*] Sat Dec 24 16:32:09 2022: Train Epoch: 3 [6400 /50751 (13%)]	Loss: 0.135406 | F1-score: 0.97 | Elapsed: 3.94s
WARNING:root: [*] Sat Dec 24 16:32:13 2022: Train Epoch: 3 [12800/50751 (25%)]	Loss: 0.050595 | F1-score: 0.97 | Elapsed: 3.94s
WARNING:root: [*] Sat Dec 24 16:32:17 2022: Train Epoch: 3 [19200/50751 (38%)]	Loss: 0.081836 | F1-score: 0.97 | Elapsed: 3.94s
WARNING:root: [*] Sat Dec 24 16:32:21 2022: Train Epoch: 3 [25600/50751 (50%)]	Loss: 0.123488 | F1-score: 0.97 | Elapsed: 3.96s
WARNING:root: [*] Sat Dec 24 16:32:25 2022: Train Epoch: 3 [32000/50751 (63%)]	Loss: 0.140461 | F1-score: 0.97 | Elapsed: 3.95s
WARNING:root: [*] Sat Dec 24 16:32:29 2022: Train Epoch: 3 [38400/50751 (76%)]	Loss: 0.112398 | F1-score: 0.97 | Elapsed: 4.07s
WARNING:root: [*] Sat Dec 24 16:32:33 2022: Train Epoch: 3 [44800/50751 (88%)]	Loss: 0.131164 | F1-score: 0.97 | Elapsed: 4.04s
WARNING:root: [*] Sat Dec 24 16:32:36 2022:    3    | Tr.loss: 0.110687 | Tr.F1.:   0.97    |   31.56  s
WARNING:root:
        [!] Sat Dec 24 16:32:36 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671895956-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671895956-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671895956-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671895956-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 50751, Validation set size: 25375
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 16:32:41 2022: Train Epoch: 1 [  0  /50751 (0 %)]	Loss: 0.724316 | F1-score: 0.46 | Elapsed: 0.04s
WARNING:root: [*] Sat Dec 24 16:32:45 2022: Train Epoch: 1 [6400 /50751 (13%)]	Loss: 0.247592 | F1-score: 0.88 | Elapsed: 3.93s
WARNING:root: [*] Sat Dec 24 16:32:49 2022: Train Epoch: 1 [12800/50751 (25%)]	Loss: 0.177052 | F1-score: 0.90 | Elapsed: 4.00s
WARNING:root: [*] Sat Dec 24 16:32:53 2022: Train Epoch: 1 [19200/50751 (38%)]	Loss: 0.345952 | F1-score: 0.92 | Elapsed: 3.94s
WARNING:root: [*] Sat Dec 24 16:32:57 2022: Train Epoch: 1 [25600/50751 (50%)]	Loss: 0.132518 | F1-score: 0.93 | Elapsed: 3.95s
WARNING:root: [*] Sat Dec 24 16:33:01 2022: Train Epoch: 1 [32000/50751 (63%)]	Loss: 0.154574 | F1-score: 0.93 | Elapsed: 3.93s
WARNING:root: [*] Sat Dec 24 16:33:05 2022: Train Epoch: 1 [38400/50751 (76%)]	Loss: 0.116766 | F1-score: 0.93 | Elapsed: 3.91s
WARNING:root: [*] Sat Dec 24 16:33:09 2022: Train Epoch: 1 [44800/50751 (88%)]	Loss: 0.076124 | F1-score: 0.94 | Elapsed: 3.92s
WARNING:root: [*] Sat Dec 24 16:33:12 2022:    1    | Tr.loss: 0.206922 | Tr.F1.:   0.94    |   31.24  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 16:33:12 2022: Train Epoch: 2 [  0  /50751 (0 %)]	Loss: 0.089129 | F1-score: 0.98 | Elapsed: 0.06s
WARNING:root: [*] Sat Dec 24 16:33:16 2022: Train Epoch: 2 [6400 /50751 (13%)]	Loss: 0.110719 | F1-score: 0.96 | Elapsed: 3.92s
WARNING:root: [*] Sat Dec 24 16:33:20 2022: Train Epoch: 2 [12800/50751 (25%)]	Loss: 0.069178 | F1-score: 0.96 | Elapsed: 3.94s
WARNING:root: [*] Sat Dec 24 16:33:24 2022: Train Epoch: 2 [19200/50751 (38%)]	Loss: 0.064294 | F1-score: 0.96 | Elapsed: 3.93s
WARNING:root: [*] Sat Dec 24 16:33:28 2022: Train Epoch: 2 [25600/50751 (50%)]	Loss: 0.123178 | F1-score: 0.96 | Elapsed: 3.93s
WARNING:root: [*] Sat Dec 24 16:33:32 2022: Train Epoch: 2 [32000/50751 (63%)]	Loss: 0.108521 | F1-score: 0.96 | Elapsed: 3.98s
WARNING:root: [*] Sat Dec 24 16:33:36 2022: Train Epoch: 2 [38400/50751 (76%)]	Loss: 0.140398 | F1-score: 0.96 | Elapsed: 3.95s
WARNING:root: [*] Sat Dec 24 16:33:40 2022: Train Epoch: 2 [44800/50751 (88%)]	Loss: 0.066293 | F1-score: 0.96 | Elapsed: 3.91s
WARNING:root: [*] Sat Dec 24 16:33:44 2022:    2    | Tr.loss: 0.126702 | Tr.F1.:   0.96    |   31.23  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 16:33:44 2022: Train Epoch: 3 [  0  /50751 (0 %)]	Loss: 0.088638 | F1-score: 0.99 | Elapsed: 0.04s
WARNING:root: [*] Sat Dec 24 16:33:48 2022: Train Epoch: 3 [6400 /50751 (13%)]	Loss: 0.128935 | F1-score: 0.96 | Elapsed: 3.92s
WARNING:root: [*] Sat Dec 24 16:33:51 2022: Train Epoch: 3 [12800/50751 (25%)]	Loss: 0.041561 | F1-score: 0.96 | Elapsed: 3.91s
WARNING:root: [*] Sat Dec 24 16:33:55 2022: Train Epoch: 3 [19200/50751 (38%)]	Loss: 0.102174 | F1-score: 0.96 | Elapsed: 3.90s
WARNING:root: [*] Sat Dec 24 16:33:59 2022: Train Epoch: 3 [25600/50751 (50%)]	Loss: 0.110381 | F1-score: 0.96 | Elapsed: 3.93s
WARNING:root: [*] Sat Dec 24 16:34:03 2022: Train Epoch: 3 [32000/50751 (63%)]	Loss: 0.119219 | F1-score: 0.96 | Elapsed: 3.92s
WARNING:root: [*] Sat Dec 24 16:34:07 2022: Train Epoch: 3 [38400/50751 (76%)]	Loss: 0.040606 | F1-score: 0.96 | Elapsed: 3.89s
WARNING:root: [*] Sat Dec 24 16:34:11 2022: Train Epoch: 3 [44800/50751 (88%)]	Loss: 0.096919 | F1-score: 0.96 | Elapsed: 3.92s
WARNING:root: [*] Sat Dec 24 16:34:15 2022:    3    | Tr.loss: 0.114072 | Tr.F1.:   0.96    |   31.03  s
WARNING:root:
        [!] Sat Dec 24 16:34:15 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896055-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896055-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896055-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896055-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\metrics_trainSize_76126_ep_3_cv_3_maxLen_1024_vocabSize_500_embeddingDim_64_hiddenNeurons_512_256_128_batchNormConv_False_batchNormFFNN_False_filterSizes_2_3_4_5.json
WARNING:root: [!] Average epoch time: 31.40s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.2471 -- F1: 0.3574
	FPR:  0.001 -- TPR: 0.8116 -- F1: 0.8957
	FPR:   0.01 -- TPR: 0.8990 -- F1: 0.9444
	FPR:    0.1 -- TPR: 0.9823 -- F1: 0.9677

WARNING:root: [!] Running Cross Validation with vocabSize: 500 | maxLen: 2048
WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 500, 'embeddingDim': 64, 'hiddenNeurons': [512, 256, 128], 'batchNormConv': False, 'batchNormFFNN': False, 'filterSizes': [2, 3, 4, 5]}
WARNING:root: [!] Fold 1/3 | Train set size: 50750, Validation set size: 25376
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 16:34:50 2022: Train Epoch: 1 [  0  /50750 (0 %)]	Loss: 0.691362 | F1-score: 0.59 | Elapsed: 0.32s
WARNING:root: [*] Sat Dec 24 16:34:57 2022: Train Epoch: 1 [6400 /50750 (13%)]	Loss: 0.226174 | F1-score: 0.87 | Elapsed: 7.04s
WARNING:root: [*] Sat Dec 24 16:35:04 2022: Train Epoch: 1 [12800/50750 (25%)]	Loss: 0.166213 | F1-score: 0.90 | Elapsed: 7.07s
WARNING:root: [*] Sat Dec 24 16:35:11 2022: Train Epoch: 1 [19200/50750 (38%)]	Loss: 0.148706 | F1-score: 0.91 | Elapsed: 7.10s
WARNING:root: [*] Sat Dec 24 16:35:18 2022: Train Epoch: 1 [25600/50750 (50%)]	Loss: 0.113108 | F1-score: 0.92 | Elapsed: 7.19s
WARNING:root: [*] Sat Dec 24 16:35:25 2022: Train Epoch: 1 [32000/50750 (63%)]	Loss: 0.113224 | F1-score: 0.93 | Elapsed: 7.18s
WARNING:root: [*] Sat Dec 24 16:35:33 2022: Train Epoch: 1 [38400/50750 (76%)]	Loss: 0.163219 | F1-score: 0.93 | Elapsed: 7.22s
WARNING:root: [*] Sat Dec 24 16:35:40 2022: Train Epoch: 1 [44800/50750 (88%)]	Loss: 0.067340 | F1-score: 0.94 | Elapsed: 7.23s
WARNING:root: [*] Sat Dec 24 16:35:47 2022:    1    | Tr.loss: 0.201357 | Tr.F1.:   0.94    |   57.11  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 16:35:47 2022: Train Epoch: 2 [  0  /50750 (0 %)]	Loss: 0.134205 | F1-score: 0.93 | Elapsed: 0.08s
WARNING:root: [*] Sat Dec 24 16:35:54 2022: Train Epoch: 2 [6400 /50750 (13%)]	Loss: 0.049285 | F1-score: 0.96 | Elapsed: 7.40s
WARNING:root: [*] Sat Dec 24 16:36:01 2022: Train Epoch: 2 [12800/50750 (25%)]	Loss: 0.066401 | F1-score: 0.96 | Elapsed: 7.33s
WARNING:root: [*] Sat Dec 24 16:36:09 2022: Train Epoch: 2 [19200/50750 (38%)]	Loss: 0.081515 | F1-score: 0.96 | Elapsed: 7.38s
WARNING:root: [*] Sat Dec 24 16:36:16 2022: Train Epoch: 2 [25600/50750 (50%)]	Loss: 0.052241 | F1-score: 0.96 | Elapsed: 7.29s
WARNING:root: [*] Sat Dec 24 16:36:23 2022: Train Epoch: 2 [32000/50750 (63%)]	Loss: 0.127404 | F1-score: 0.96 | Elapsed: 7.31s
WARNING:root: [*] Sat Dec 24 16:36:31 2022: Train Epoch: 2 [38400/50750 (76%)]	Loss: 0.090074 | F1-score: 0.96 | Elapsed: 7.42s
WARNING:root: [*] Sat Dec 24 16:36:38 2022: Train Epoch: 2 [44800/50750 (88%)]	Loss: 0.055432 | F1-score: 0.97 | Elapsed: 7.44s
WARNING:root: [*] Sat Dec 24 16:36:45 2022:    2    | Tr.loss: 0.112572 | Tr.F1.:   0.97    |   58.40  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 16:36:45 2022: Train Epoch: 3 [  0  /50750 (0 %)]	Loss: 0.172150 | F1-score: 0.95 | Elapsed: 0.09s
WARNING:root: [*] Sat Dec 24 16:36:52 2022: Train Epoch: 3 [6400 /50750 (13%)]	Loss: 0.143140 | F1-score: 0.97 | Elapsed: 7.24s
WARNING:root: [*] Sat Dec 24 16:37:00 2022: Train Epoch: 3 [12800/50750 (25%)]	Loss: 0.048206 | F1-score: 0.97 | Elapsed: 7.28s
WARNING:root: [*] Sat Dec 24 16:37:07 2022: Train Epoch: 3 [19200/50750 (38%)]	Loss: 0.056552 | F1-score: 0.97 | Elapsed: 7.36s
WARNING:root: [*] Sat Dec 24 16:37:14 2022: Train Epoch: 3 [25600/50750 (50%)]	Loss: 0.176151 | F1-score: 0.97 | Elapsed: 7.34s
WARNING:root: [*] Sat Dec 24 16:37:22 2022: Train Epoch: 3 [32000/50750 (63%)]	Loss: 0.048423 | F1-score: 0.97 | Elapsed: 7.32s
WARNING:root: [*] Sat Dec 24 16:37:29 2022: Train Epoch: 3 [38400/50750 (76%)]	Loss: 0.122950 | F1-score: 0.97 | Elapsed: 7.18s
WARNING:root: [*] Sat Dec 24 16:37:36 2022: Train Epoch: 3 [44800/50750 (88%)]	Loss: 0.052330 | F1-score: 0.97 | Elapsed: 7.20s
WARNING:root: [*] Sat Dec 24 16:37:43 2022:    3    | Tr.loss: 0.096242 | Tr.F1.:   0.97    |   57.73  s
WARNING:root:
        [!] Sat Dec 24 16:37:43 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896263-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896263-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896263-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896263-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 50751, Validation set size: 25375
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 16:37:52 2022: Train Epoch: 1 [  0  /50751 (0 %)]	Loss: 0.743086 | F1-score: 0.20 | Elapsed: 0.08s
WARNING:root: [*] Sat Dec 24 16:37:59 2022: Train Epoch: 1 [6400 /50751 (13%)]	Loss: 0.237202 | F1-score: 0.87 | Elapsed: 7.21s
WARNING:root: [*] Sat Dec 24 16:38:06 2022: Train Epoch: 1 [12800/50751 (25%)]	Loss: 0.242709 | F1-score: 0.90 | Elapsed: 7.22s
WARNING:root: [*] Sat Dec 24 16:38:13 2022: Train Epoch: 1 [19200/50751 (38%)]	Loss: 0.118975 | F1-score: 0.91 | Elapsed: 7.23s
WARNING:root: [*] Sat Dec 24 16:38:21 2022: Train Epoch: 1 [25600/50751 (50%)]	Loss: 0.115968 | F1-score: 0.92 | Elapsed: 7.26s
WARNING:root: [*] Sat Dec 24 16:38:28 2022: Train Epoch: 1 [32000/50751 (63%)]	Loss: 0.124486 | F1-score: 0.93 | Elapsed: 7.21s
WARNING:root: [*] Sat Dec 24 16:38:35 2022: Train Epoch: 1 [38400/50751 (76%)]	Loss: 0.116785 | F1-score: 0.93 | Elapsed: 7.19s
WARNING:root: [*] Sat Dec 24 16:38:42 2022: Train Epoch: 1 [44800/50751 (88%)]	Loss: 0.143344 | F1-score: 0.94 | Elapsed: 7.22s
WARNING:root: [*] Sat Dec 24 16:38:49 2022:    1    | Tr.loss: 0.201711 | Tr.F1.:   0.94    |   57.28  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 16:38:49 2022: Train Epoch: 2 [  0  /50751 (0 %)]	Loss: 0.084737 | F1-score: 0.97 | Elapsed: 0.08s
WARNING:root: [*] Sat Dec 24 16:38:56 2022: Train Epoch: 2 [6400 /50751 (13%)]	Loss: 0.170439 | F1-score: 0.96 | Elapsed: 7.24s
WARNING:root: [*] Sat Dec 24 16:39:04 2022: Train Epoch: 2 [12800/50751 (25%)]	Loss: 0.079779 | F1-score: 0.96 | Elapsed: 7.23s
WARNING:root: [*] Sat Dec 24 16:39:11 2022: Train Epoch: 2 [19200/50751 (38%)]	Loss: 0.118872 | F1-score: 0.96 | Elapsed: 7.24s
WARNING:root: [*] Sat Dec 24 16:39:18 2022: Train Epoch: 2 [25600/50751 (50%)]	Loss: 0.193916 | F1-score: 0.96 | Elapsed: 7.22s
WARNING:root: [*] Sat Dec 24 16:39:25 2022: Train Epoch: 2 [32000/50751 (63%)]	Loss: 0.094634 | F1-score: 0.96 | Elapsed: 7.22s
WARNING:root: [*] Sat Dec 24 16:39:32 2022: Train Epoch: 2 [38400/50751 (76%)]	Loss: 0.092989 | F1-score: 0.96 | Elapsed: 7.24s
WARNING:root: [*] Sat Dec 24 16:39:40 2022: Train Epoch: 2 [44800/50751 (88%)]	Loss: 0.096009 | F1-score: 0.96 | Elapsed: 7.22s
WARNING:root: [*] Sat Dec 24 16:39:46 2022:    2    | Tr.loss: 0.116904 | Tr.F1.:   0.96    |   57.33  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 16:39:46 2022: Train Epoch: 3 [  0  /50751 (0 %)]	Loss: 0.144638 | F1-score: 0.94 | Elapsed: 0.08s
WARNING:root: [*] Sat Dec 24 16:39:54 2022: Train Epoch: 3 [6400 /50751 (13%)]	Loss: 0.054879 | F1-score: 0.97 | Elapsed: 7.27s
WARNING:root: [*] Sat Dec 24 16:40:01 2022: Train Epoch: 3 [12800/50751 (25%)]	Loss: 0.076044 | F1-score: 0.97 | Elapsed: 7.22s
WARNING:root: [*] Sat Dec 24 16:40:08 2022: Train Epoch: 3 [19200/50751 (38%)]	Loss: 0.116906 | F1-score: 0.97 | Elapsed: 7.28s
WARNING:root: [*] Sat Dec 24 16:40:15 2022: Train Epoch: 3 [25600/50751 (50%)]	Loss: 0.039807 | F1-score: 0.97 | Elapsed: 7.30s
WARNING:root: [*] Sat Dec 24 16:40:23 2022: Train Epoch: 3 [32000/50751 (63%)]	Loss: 0.004416 | F1-score: 0.97 | Elapsed: 7.27s
WARNING:root: [*] Sat Dec 24 16:40:30 2022: Train Epoch: 3 [38400/50751 (76%)]	Loss: 0.082438 | F1-score: 0.97 | Elapsed: 7.31s
WARNING:root: [*] Sat Dec 24 16:40:37 2022: Train Epoch: 3 [44800/50751 (88%)]	Loss: 0.070782 | F1-score: 0.97 | Elapsed: 7.30s
WARNING:root: [*] Sat Dec 24 16:40:44 2022:    3    | Tr.loss: 0.096154 | Tr.F1.:   0.97    |   57.75  s
WARNING:root:
        [!] Sat Dec 24 16:40:44 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896444-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896444-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896444-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896444-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 50751, Validation set size: 25375
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 16:40:53 2022: Train Epoch: 1 [  0  /50751 (0 %)]	Loss: 0.648767 | F1-score: 0.80 | Elapsed: 0.07s
WARNING:root: [*] Sat Dec 24 16:41:00 2022: Train Epoch: 1 [6400 /50751 (13%)]	Loss: 0.251853 | F1-score: 0.88 | Elapsed: 7.27s
WARNING:root: [*] Sat Dec 24 16:41:08 2022: Train Epoch: 1 [12800/50751 (25%)]	Loss: 0.117073 | F1-score: 0.91 | Elapsed: 7.30s
WARNING:root: [*] Sat Dec 24 16:41:15 2022: Train Epoch: 1 [19200/50751 (38%)]	Loss: 0.130694 | F1-score: 0.92 | Elapsed: 7.33s
WARNING:root: [*] Sat Dec 24 16:41:22 2022: Train Epoch: 1 [25600/50751 (50%)]	Loss: 0.158602 | F1-score: 0.92 | Elapsed: 7.32s
WARNING:root: [*] Sat Dec 24 16:41:30 2022: Train Epoch: 1 [32000/50751 (63%)]	Loss: 0.135740 | F1-score: 0.93 | Elapsed: 7.35s
WARNING:root: [*] Sat Dec 24 16:41:37 2022: Train Epoch: 1 [38400/50751 (76%)]	Loss: 0.124982 | F1-score: 0.93 | Elapsed: 7.28s
WARNING:root: [*] Sat Dec 24 16:41:44 2022: Train Epoch: 1 [44800/50751 (88%)]	Loss: 0.135212 | F1-score: 0.94 | Elapsed: 7.28s
WARNING:root: [*] Sat Dec 24 16:41:51 2022:    1    | Tr.loss: 0.202482 | Tr.F1.:   0.94    |   57.87  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 16:41:51 2022: Train Epoch: 2 [  0  /50751 (0 %)]	Loss: 0.103939 | F1-score: 0.99 | Elapsed: 0.08s
WARNING:root: [*] Sat Dec 24 16:41:58 2022: Train Epoch: 2 [6400 /50751 (13%)]	Loss: 0.156449 | F1-score: 0.96 | Elapsed: 7.22s
WARNING:root: [*] Sat Dec 24 16:42:05 2022: Train Epoch: 2 [12800/50751 (25%)]	Loss: 0.139709 | F1-score: 0.96 | Elapsed: 7.27s
WARNING:root: [*] Sat Dec 24 16:42:13 2022: Train Epoch: 2 [19200/50751 (38%)]	Loss: 0.065658 | F1-score: 0.96 | Elapsed: 7.31s
WARNING:root: [*] Sat Dec 24 16:42:20 2022: Train Epoch: 2 [25600/50751 (50%)]	Loss: 0.098750 | F1-score: 0.96 | Elapsed: 7.22s
WARNING:root: [*] Sat Dec 24 16:42:27 2022: Train Epoch: 2 [32000/50751 (63%)]	Loss: 0.086565 | F1-score: 0.96 | Elapsed: 7.34s
WARNING:root: [*] Sat Dec 24 16:42:35 2022: Train Epoch: 2 [38400/50751 (76%)]	Loss: 0.070902 | F1-score: 0.96 | Elapsed: 7.35s
WARNING:root: [*] Sat Dec 24 16:42:42 2022: Train Epoch: 2 [44800/50751 (88%)]	Loss: 0.050277 | F1-score: 0.96 | Elapsed: 7.33s
WARNING:root: [*] Sat Dec 24 16:42:49 2022:    2    | Tr.loss: 0.113207 | Tr.F1.:   0.97    |   57.84  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 16:42:49 2022: Train Epoch: 3 [  0  /50751 (0 %)]	Loss: 0.052550 | F1-score: 0.97 | Elapsed: 0.07s
WARNING:root: [*] Sat Dec 24 16:42:56 2022: Train Epoch: 3 [6400 /50751 (13%)]	Loss: 0.098172 | F1-score: 0.97 | Elapsed: 7.24s
WARNING:root: [*] Sat Dec 24 16:43:03 2022: Train Epoch: 3 [12800/50751 (25%)]	Loss: 0.101334 | F1-score: 0.97 | Elapsed: 7.27s
WARNING:root: [*] Sat Dec 24 16:43:10 2022: Train Epoch: 3 [19200/50751 (38%)]	Loss: 0.095085 | F1-score: 0.97 | Elapsed: 7.23s
WARNING:root: [*] Sat Dec 24 16:43:18 2022: Train Epoch: 3 [25600/50751 (50%)]	Loss: 0.162518 | F1-score: 0.97 | Elapsed: 7.31s
WARNING:root: [*] Sat Dec 24 16:43:25 2022: Train Epoch: 3 [32000/50751 (63%)]	Loss: 0.092395 | F1-score: 0.97 | Elapsed: 7.35s
WARNING:root: [*] Sat Dec 24 16:43:32 2022: Train Epoch: 3 [38400/50751 (76%)]	Loss: 0.350573 | F1-score: 0.97 | Elapsed: 7.28s
WARNING:root: [*] Sat Dec 24 16:43:40 2022: Train Epoch: 3 [44800/50751 (88%)]	Loss: 0.041126 | F1-score: 0.97 | Elapsed: 7.27s
WARNING:root: [*] Sat Dec 24 16:43:46 2022:    3    | Tr.loss: 0.094980 | Tr.F1.:   0.97    |   57.75  s
WARNING:root:
        [!] Sat Dec 24 16:43:46 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896626-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896626-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896626-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671896626-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\metrics_trainSize_76126_ep_3_cv_3_maxLen_2048_vocabSize_500_embeddingDim_64_hiddenNeurons_512_256_128_batchNormConv_False_batchNormFFNN_False_filterSizes_2_3_4_5.json
WARNING:root: [!] Average epoch time: 57.67s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.3297 -- F1: 0.4858
	FPR:  0.001 -- TPR: 0.8301 -- F1: 0.9068
	FPR:   0.01 -- TPR: 0.9371 -- F1: 0.9652
	FPR:    0.1 -- TPR: 0.9881 -- F1: 0.9708

WARNING:root: [!] Running Cross Validation with vocabSize: 500 | maxLen: 4096
WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 500, 'embeddingDim': 64, 'hiddenNeurons': [512, 256, 128], 'batchNormConv': False, 'batchNormFFNN': False, 'filterSizes': [2, 3, 4, 5]}
WARNING:root: [!] Fold 1/3 | Train set size: 50750, Validation set size: 25376
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 16:44:27 2022: Train Epoch: 1 [  0  /50750 (0 %)]	Loss: 0.703887 | F1-score: 0.49 | Elapsed: 0.43s
WARNING:root: [*] Sat Dec 24 16:44:41 2022: Train Epoch: 1 [6400 /50750 (13%)]	Loss: 0.266897 | F1-score: 0.87 | Elapsed: 14.16s
WARNING:root: [*] Sat Dec 24 16:44:55 2022: Train Epoch: 1 [12800/50750 (25%)]	Loss: 0.167823 | F1-score: 0.90 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 16:45:09 2022: Train Epoch: 1 [19200/50750 (38%)]	Loss: 0.353677 | F1-score: 0.92 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 16:45:24 2022: Train Epoch: 1 [25600/50750 (50%)]	Loss: 0.116447 | F1-score: 0.93 | Elapsed: 14.22s
WARNING:root: [*] Sat Dec 24 16:45:38 2022: Train Epoch: 1 [32000/50750 (63%)]	Loss: 0.081712 | F1-score: 0.93 | Elapsed: 14.25s
WARNING:root: [*] Sat Dec 24 16:45:52 2022: Train Epoch: 1 [38400/50750 (76%)]	Loss: 0.098555 | F1-score: 0.93 | Elapsed: 14.35s
WARNING:root: [*] Sat Dec 24 16:46:07 2022: Train Epoch: 1 [44800/50750 (88%)]	Loss: 0.125930 | F1-score: 0.94 | Elapsed: 14.24s
WARNING:root: [*] Sat Dec 24 16:46:20 2022:    1    | Tr.loss: 0.200651 | Tr.F1.:   0.94    |  113.21  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 16:46:20 2022: Train Epoch: 2 [  0  /50750 (0 %)]	Loss: 0.179239 | F1-score: 0.92 | Elapsed: 0.15s
WARNING:root: [*] Sat Dec 24 16:46:34 2022: Train Epoch: 2 [6400 /50750 (13%)]	Loss: 0.077907 | F1-score: 0.96 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:46:48 2022: Train Epoch: 2 [12800/50750 (25%)]	Loss: 0.068395 | F1-score: 0.96 | Elapsed: 14.27s
WARNING:root: [*] Sat Dec 24 16:47:03 2022: Train Epoch: 2 [19200/50750 (38%)]	Loss: 0.067268 | F1-score: 0.96 | Elapsed: 14.27s
WARNING:root: [*] Sat Dec 24 16:47:17 2022: Train Epoch: 2 [25600/50750 (50%)]	Loss: 0.076446 | F1-score: 0.97 | Elapsed: 14.25s
WARNING:root: [*] Sat Dec 24 16:47:31 2022: Train Epoch: 2 [32000/50750 (63%)]	Loss: 0.074649 | F1-score: 0.96 | Elapsed: 14.34s
WARNING:root: [*] Sat Dec 24 16:47:45 2022: Train Epoch: 2 [38400/50750 (76%)]	Loss: 0.113352 | F1-score: 0.96 | Elapsed: 14.29s
WARNING:root: [*] Sat Dec 24 16:48:00 2022: Train Epoch: 2 [44800/50750 (88%)]	Loss: 0.092280 | F1-score: 0.97 | Elapsed: 14.31s
WARNING:root: [*] Sat Dec 24 16:48:13 2022:    2    | Tr.loss: 0.113456 | Tr.F1.:   0.97    |  113.24  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 16:48:13 2022: Train Epoch: 3 [  0  /50750 (0 %)]	Loss: 0.044099 | F1-score: 0.98 | Elapsed: 0.14s
WARNING:root: [*] Sat Dec 24 16:48:27 2022: Train Epoch: 3 [6400 /50750 (13%)]	Loss: 0.076866 | F1-score: 0.97 | Elapsed: 14.24s
WARNING:root: [*] Sat Dec 24 16:48:41 2022: Train Epoch: 3 [12800/50750 (25%)]	Loss: 0.070985 | F1-score: 0.97 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:48:56 2022: Train Epoch: 3 [19200/50750 (38%)]	Loss: 0.072880 | F1-score: 0.97 | Elapsed: 14.21s
WARNING:root: [*] Sat Dec 24 16:49:10 2022: Train Epoch: 3 [25600/50750 (50%)]	Loss: 0.074592 | F1-score: 0.97 | Elapsed: 14.24s
WARNING:root: [*] Sat Dec 24 16:49:24 2022: Train Epoch: 3 [32000/50750 (63%)]	Loss: 0.233660 | F1-score: 0.97 | Elapsed: 14.28s
WARNING:root: [*] Sat Dec 24 16:49:38 2022: Train Epoch: 3 [38400/50750 (76%)]	Loss: 0.062225 | F1-score: 0.97 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 16:49:53 2022: Train Epoch: 3 [44800/50750 (88%)]	Loss: 0.057966 | F1-score: 0.97 | Elapsed: 14.21s
WARNING:root: [*] Sat Dec 24 16:50:06 2022:    3    | Tr.loss: 0.094619 | Tr.F1.:   0.97    |  112.92  s
WARNING:root:
        [!] Sat Dec 24 16:50:06 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897006-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897006-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897006-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897006-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 50751, Validation set size: 25375
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 16:50:24 2022: Train Epoch: 1 [  0  /50751 (0 %)]	Loss: 0.683295 | F1-score: 0.73 | Elapsed: 0.18s
WARNING:root: [*] Sat Dec 24 16:50:38 2022: Train Epoch: 1 [6400 /50751 (13%)]	Loss: 0.162162 | F1-score: 0.87 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:50:52 2022: Train Epoch: 1 [12800/50751 (25%)]	Loss: 0.245826 | F1-score: 0.90 | Elapsed: 14.25s
WARNING:root: [*] Sat Dec 24 16:51:06 2022: Train Epoch: 1 [19200/50751 (38%)]	Loss: 0.179445 | F1-score: 0.91 | Elapsed: 14.28s
WARNING:root: [*] Sat Dec 24 16:51:21 2022: Train Epoch: 1 [25600/50751 (50%)]	Loss: 0.198388 | F1-score: 0.92 | Elapsed: 14.29s
WARNING:root: [*] Sat Dec 24 16:51:35 2022: Train Epoch: 1 [32000/50751 (63%)]	Loss: 0.109236 | F1-score: 0.93 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:51:49 2022: Train Epoch: 1 [38400/50751 (76%)]	Loss: 0.117680 | F1-score: 0.93 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:52:03 2022: Train Epoch: 1 [44800/50751 (88%)]	Loss: 0.206204 | F1-score: 0.93 | Elapsed: 14.27s
WARNING:root: [*] Sat Dec 24 16:52:17 2022:    1    | Tr.loss: 0.207015 | Tr.F1.:   0.94    |  113.19  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 16:52:17 2022: Train Epoch: 2 [  0  /50751 (0 %)]	Loss: 0.117908 | F1-score: 0.95 | Elapsed: 0.14s
WARNING:root: [*] Sat Dec 24 16:52:31 2022: Train Epoch: 2 [6400 /50751 (13%)]	Loss: 0.144036 | F1-score: 0.96 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:52:45 2022: Train Epoch: 2 [12800/50751 (25%)]	Loss: 0.141830 | F1-score: 0.96 | Elapsed: 14.25s
WARNING:root: [*] Sat Dec 24 16:53:00 2022: Train Epoch: 2 [19200/50751 (38%)]	Loss: 0.033437 | F1-score: 0.96 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:53:14 2022: Train Epoch: 2 [25600/50751 (50%)]	Loss: 0.192949 | F1-score: 0.96 | Elapsed: 14.29s
WARNING:root: [*] Sat Dec 24 16:53:28 2022: Train Epoch: 2 [32000/50751 (63%)]	Loss: 0.139039 | F1-score: 0.96 | Elapsed: 14.33s
WARNING:root: [*] Sat Dec 24 16:53:43 2022: Train Epoch: 2 [38400/50751 (76%)]	Loss: 0.176304 | F1-score: 0.96 | Elapsed: 14.36s
WARNING:root: [*] Sat Dec 24 16:53:57 2022: Train Epoch: 2 [44800/50751 (88%)]	Loss: 0.096258 | F1-score: 0.96 | Elapsed: 14.29s
WARNING:root: [*] Sat Dec 24 16:54:10 2022:    2    | Tr.loss: 0.116303 | Tr.F1.:   0.96    |  113.31  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 16:54:10 2022: Train Epoch: 3 [  0  /50751 (0 %)]	Loss: 0.167983 | F1-score: 0.97 | Elapsed: 0.16s
WARNING:root: [*] Sat Dec 24 16:54:24 2022: Train Epoch: 3 [6400 /50751 (13%)]	Loss: 0.069221 | F1-score: 0.97 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:54:39 2022: Train Epoch: 3 [12800/50751 (25%)]	Loss: 0.062575 | F1-score: 0.97 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 16:54:53 2022: Train Epoch: 3 [19200/50751 (38%)]	Loss: 0.190211 | F1-score: 0.97 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 16:55:07 2022: Train Epoch: 3 [25600/50751 (50%)]	Loss: 0.101481 | F1-score: 0.97 | Elapsed: 14.27s
WARNING:root: [*] Sat Dec 24 16:55:21 2022: Train Epoch: 3 [32000/50751 (63%)]	Loss: 0.098389 | F1-score: 0.97 | Elapsed: 14.28s
WARNING:root: [*] Sat Dec 24 16:55:36 2022: Train Epoch: 3 [38400/50751 (76%)]	Loss: 0.068697 | F1-score: 0.97 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:55:50 2022: Train Epoch: 3 [44800/50751 (88%)]	Loss: 0.039342 | F1-score: 0.97 | Elapsed: 14.29s
WARNING:root: [*] Sat Dec 24 16:56:03 2022:    3    | Tr.loss: 0.096713 | Tr.F1.:   0.97    |  113.08  s
WARNING:root:
        [!] Sat Dec 24 16:56:03 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897363-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897363-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897363-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897363-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 50751, Validation set size: 25375
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 16:56:21 2022: Train Epoch: 1 [  0  /50751 (0 %)]	Loss: 0.705687 | F1-score: 0.34 | Elapsed: 0.18s
WARNING:root: [*] Sat Dec 24 16:56:35 2022: Train Epoch: 1 [6400 /50751 (13%)]	Loss: 0.162782 | F1-score: 0.86 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 16:56:49 2022: Train Epoch: 1 [12800/50751 (25%)]	Loss: 0.222275 | F1-score: 0.90 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 16:57:04 2022: Train Epoch: 1 [19200/50751 (38%)]	Loss: 0.201816 | F1-score: 0.91 | Elapsed: 14.26s
WARNING:root: [*] Sat Dec 24 16:57:18 2022: Train Epoch: 1 [25600/50751 (50%)]	Loss: 0.181260 | F1-score: 0.92 | Elapsed: 14.32s
WARNING:root: [*] Sat Dec 24 16:57:32 2022: Train Epoch: 1 [32000/50751 (63%)]	Loss: 0.112558 | F1-score: 0.93 | Elapsed: 14.27s
WARNING:root: [*] Sat Dec 24 16:57:46 2022: Train Epoch: 1 [38400/50751 (76%)]	Loss: 0.123612 | F1-score: 0.93 | Elapsed: 14.25s
WARNING:root: [*] Sat Dec 24 16:58:01 2022: Train Epoch: 1 [44800/50751 (88%)]	Loss: 0.101447 | F1-score: 0.94 | Elapsed: 14.28s
WARNING:root: [*] Sat Dec 24 16:58:14 2022:    1    | Tr.loss: 0.198760 | Tr.F1.:   0.94    |  113.14  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 16:58:14 2022: Train Epoch: 2 [  0  /50751 (0 %)]	Loss: 0.136697 | F1-score: 0.95 | Elapsed: 0.15s
WARNING:root: [*] Sat Dec 24 16:58:28 2022: Train Epoch: 2 [6400 /50751 (13%)]	Loss: 0.108830 | F1-score: 0.96 | Elapsed: 14.24s
WARNING:root: [*] Sat Dec 24 16:58:42 2022: Train Epoch: 2 [12800/50751 (25%)]	Loss: 0.071216 | F1-score: 0.96 | Elapsed: 14.25s
WARNING:root: [*] Sat Dec 24 16:58:57 2022: Train Epoch: 2 [19200/50751 (38%)]	Loss: 0.171507 | F1-score: 0.96 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 16:59:11 2022: Train Epoch: 2 [25600/50751 (50%)]	Loss: 0.072134 | F1-score: 0.97 | Elapsed: 14.20s
WARNING:root: [*] Sat Dec 24 16:59:25 2022: Train Epoch: 2 [32000/50751 (63%)]	Loss: 0.089288 | F1-score: 0.96 | Elapsed: 14.28s
WARNING:root: [*] Sat Dec 24 16:59:39 2022: Train Epoch: 2 [38400/50751 (76%)]	Loss: 0.147086 | F1-score: 0.97 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 16:59:54 2022: Train Epoch: 2 [44800/50751 (88%)]	Loss: 0.072983 | F1-score: 0.97 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 17:00:07 2022:    2    | Tr.loss: 0.113637 | Tr.F1.:   0.97    |  112.98  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 17:00:07 2022: Train Epoch: 3 [  0  /50751 (0 %)]	Loss: 0.264440 | F1-score: 0.94 | Elapsed: 0.15s
WARNING:root: [*] Sat Dec 24 17:00:21 2022: Train Epoch: 3 [6400 /50751 (13%)]	Loss: 0.089634 | F1-score: 0.97 | Elapsed: 14.24s
WARNING:root: [*] Sat Dec 24 17:00:35 2022: Train Epoch: 3 [12800/50751 (25%)]	Loss: 0.164931 | F1-score: 0.97 | Elapsed: 14.29s
WARNING:root: [*] Sat Dec 24 17:00:50 2022: Train Epoch: 3 [19200/50751 (38%)]	Loss: 0.126591 | F1-score: 0.97 | Elapsed: 14.23s
WARNING:root: [*] Sat Dec 24 17:01:04 2022: Train Epoch: 3 [25600/50751 (50%)]	Loss: 0.071555 | F1-score: 0.97 | Elapsed: 14.25s
WARNING:root: [*] Sat Dec 24 17:01:18 2022: Train Epoch: 3 [32000/50751 (63%)]	Loss: 0.042712 | F1-score: 0.97 | Elapsed: 14.22s
WARNING:root: [*] Sat Dec 24 17:01:32 2022: Train Epoch: 3 [38400/50751 (76%)]	Loss: 0.064027 | F1-score: 0.97 | Elapsed: 14.20s
WARNING:root: [*] Sat Dec 24 17:01:47 2022: Train Epoch: 3 [44800/50751 (88%)]	Loss: 0.166770 | F1-score: 0.97 | Elapsed: 14.22s
WARNING:root: [*] Sat Dec 24 17:02:00 2022:    3    | Tr.loss: 0.092889 | Tr.F1.:   0.97    |  112.92  s
WARNING:root:
        [!] Sat Dec 24 17:02:00 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897720-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897720-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897720-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897720-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\metrics_trainSize_76126_ep_3_cv_3_maxLen_4096_vocabSize_500_embeddingDim_64_hiddenNeurons_512_256_128_batchNormConv_False_batchNormFFNN_False_filterSizes_2_3_4_5.json
WARNING:root: [!] Average epoch time: 113.11s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.2413 -- F1: 0.3544
	FPR:  0.001 -- TPR: 0.8164 -- F1: 0.8983
	FPR:   0.01 -- TPR: 0.9296 -- F1: 0.9614
	FPR:    0.1 -- TPR: 0.9869 -- F1: 0.9711

WARNING:root: [!] Running Cross Validation with vocabSize: 500 | maxLen: 512
WARNING:root: [!] Using device: cuda:0 | Dataset size: 76126
WARNING:root: [!] Epochs per fold: 3 | Model config: {'vocabSize': 500, 'embeddingDim': 64, 'hiddenNeurons': [512, 256, 128], 'batchNormConv': False, 'batchNormFFNN': False, 'filterSizes': [2, 3, 4, 5]}
WARNING:root: [!] Fold 1/3 | Train set size: 50750, Validation set size: 25376
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 17:02:47 2022: Train Epoch: 1 [  0  /50750 (0 %)]	Loss: 0.685566 | F1-score: 0.61 | Elapsed: 0.17s
WARNING:root: [*] Sat Dec 24 17:02:49 2022: Train Epoch: 1 [6400 /50750 (13%)]	Loss: 0.295682 | F1-score: 0.88 | Elapsed: 2.23s
WARNING:root: [*] Sat Dec 24 17:02:51 2022: Train Epoch: 1 [12800/50750 (25%)]	Loss: 0.091814 | F1-score: 0.91 | Elapsed: 2.21s
WARNING:root: [*] Sat Dec 24 17:02:54 2022: Train Epoch: 1 [19200/50750 (38%)]	Loss: 0.227079 | F1-score: 0.92 | Elapsed: 2.19s
WARNING:root: [*] Sat Dec 24 17:02:56 2022: Train Epoch: 1 [25600/50750 (50%)]	Loss: 0.326708 | F1-score: 0.92 | Elapsed: 2.21s
WARNING:root: [*] Sat Dec 24 17:02:58 2022: Train Epoch: 1 [32000/50750 (63%)]	Loss: 0.178814 | F1-score: 0.93 | Elapsed: 2.22s
WARNING:root: [*] Sat Dec 24 17:03:00 2022: Train Epoch: 1 [38400/50750 (76%)]	Loss: 0.144665 | F1-score: 0.93 | Elapsed: 2.22s
WARNING:root: [*] Sat Dec 24 17:03:03 2022: Train Epoch: 1 [44800/50750 (88%)]	Loss: 0.172859 | F1-score: 0.93 | Elapsed: 2.21s
WARNING:root: [*] Sat Dec 24 17:03:05 2022:    1    | Tr.loss: 0.211952 | Tr.F1.:   0.94    |   17.73  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 17:03:05 2022: Train Epoch: 2 [  0  /50750 (0 %)]	Loss: 0.076817 | F1-score: 1.00 | Elapsed: 0.03s
WARNING:root: [*] Sat Dec 24 17:03:07 2022: Train Epoch: 2 [6400 /50750 (13%)]	Loss: 0.156888 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:03:09 2022: Train Epoch: 2 [12800/50750 (25%)]	Loss: 0.150989 | F1-score: 0.95 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:11 2022: Train Epoch: 2 [19200/50750 (38%)]	Loss: 0.123941 | F1-score: 0.95 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:03:14 2022: Train Epoch: 2 [25600/50750 (50%)]	Loss: 0.187978 | F1-score: 0.95 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:16 2022: Train Epoch: 2 [32000/50750 (63%)]	Loss: 0.109833 | F1-score: 0.96 | Elapsed: 2.24s
WARNING:root: [*] Sat Dec 24 17:03:18 2022: Train Epoch: 2 [38400/50750 (76%)]	Loss: 0.131813 | F1-score: 0.96 | Elapsed: 2.24s
WARNING:root: [*] Sat Dec 24 17:03:20 2022: Train Epoch: 2 [44800/50750 (88%)]	Loss: 0.120219 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:03:22 2022:    2    | Tr.loss: 0.136054 | Tr.F1.:   0.96    |   17.86  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 17:03:22 2022: Train Epoch: 3 [  0  /50750 (0 %)]	Loss: 0.148351 | F1-score: 0.93 | Elapsed: 0.03s
WARNING:root: [*] Sat Dec 24 17:03:25 2022: Train Epoch: 3 [6400 /50750 (13%)]	Loss: 0.080954 | F1-score: 0.96 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:27 2022: Train Epoch: 3 [12800/50750 (25%)]	Loss: 0.162311 | F1-score: 0.96 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:29 2022: Train Epoch: 3 [19200/50750 (38%)]	Loss: 0.060473 | F1-score: 0.96 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:31 2022: Train Epoch: 3 [25600/50750 (50%)]	Loss: 0.113280 | F1-score: 0.96 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:34 2022: Train Epoch: 3 [32000/50750 (63%)]	Loss: 0.179698 | F1-score: 0.96 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:36 2022: Train Epoch: 3 [38400/50750 (76%)]	Loss: 0.100064 | F1-score: 0.96 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:38 2022: Train Epoch: 3 [44800/50750 (88%)]	Loss: 0.062903 | F1-score: 0.96 | Elapsed: 2.24s
WARNING:root: [*] Sat Dec 24 17:03:40 2022:    3    | Tr.loss: 0.119980 | Tr.F1.:   0.96    |   17.86  s
WARNING:root:
        [!] Sat Dec 24 17:03:40 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897820-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897820-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897820-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897820-duration.pickle
WARNING:root: [!] Fold 2/3 | Train set size: 50751, Validation set size: 25375
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 17:03:43 2022: Train Epoch: 1 [  0  /50751 (0 %)]	Loss: 0.698199 | F1-score: 0.56 | Elapsed: 0.03s
WARNING:root: [*] Sat Dec 24 17:03:45 2022: Train Epoch: 1 [6400 /50751 (13%)]	Loss: 0.369340 | F1-score: 0.88 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:47 2022: Train Epoch: 1 [12800/50751 (25%)]	Loss: 0.177098 | F1-score: 0.91 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:03:49 2022: Train Epoch: 1 [19200/50751 (38%)]	Loss: 0.107831 | F1-score: 0.92 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:03:52 2022: Train Epoch: 1 [25600/50751 (50%)]	Loss: 0.162400 | F1-score: 0.93 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:03:54 2022: Train Epoch: 1 [32000/50751 (63%)]	Loss: 0.157418 | F1-score: 0.93 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:03:56 2022: Train Epoch: 1 [38400/50751 (76%)]	Loss: 0.106347 | F1-score: 0.93 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:03:59 2022: Train Epoch: 1 [44800/50751 (88%)]	Loss: 0.071159 | F1-score: 0.94 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:04:01 2022:    1    | Tr.loss: 0.212085 | Tr.F1.:   0.94    |   17.94  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 17:04:01 2022: Train Epoch: 2 [  0  /50751 (0 %)]	Loss: 0.118087 | F1-score: 0.97 | Elapsed: 0.03s
WARNING:root: [*] Sat Dec 24 17:04:03 2022: Train Epoch: 2 [6400 /50751 (13%)]	Loss: 0.205446 | F1-score: 0.95 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:04:05 2022: Train Epoch: 2 [12800/50751 (25%)]	Loss: 0.102941 | F1-score: 0.95 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:04:07 2022: Train Epoch: 2 [19200/50751 (38%)]	Loss: 0.073121 | F1-score: 0.95 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:10 2022: Train Epoch: 2 [25600/50751 (50%)]	Loss: 0.146024 | F1-score: 0.96 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:04:12 2022: Train Epoch: 2 [32000/50751 (63%)]	Loss: 0.125221 | F1-score: 0.96 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:04:14 2022: Train Epoch: 2 [38400/50751 (76%)]	Loss: 0.183691 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:16 2022: Train Epoch: 2 [44800/50751 (88%)]	Loss: 0.256771 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:19 2022:    2    | Tr.loss: 0.140062 | Tr.F1.:   0.96    |   17.94  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 17:04:19 2022: Train Epoch: 3 [  0  /50751 (0 %)]	Loss: 0.079749 | F1-score: 0.97 | Elapsed: 0.04s
WARNING:root: [*] Sat Dec 24 17:04:21 2022: Train Epoch: 3 [6400 /50751 (13%)]	Loss: 0.244234 | F1-score: 0.96 | Elapsed: 2.29s
WARNING:root: [*] Sat Dec 24 17:04:23 2022: Train Epoch: 3 [12800/50751 (25%)]	Loss: 0.168813 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:25 2022: Train Epoch: 3 [19200/50751 (38%)]	Loss: 0.164049 | F1-score: 0.96 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:04:28 2022: Train Epoch: 3 [25600/50751 (50%)]	Loss: 0.095524 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:30 2022: Train Epoch: 3 [32000/50751 (63%)]	Loss: 0.131540 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:32 2022: Train Epoch: 3 [38400/50751 (76%)]	Loss: 0.108772 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:34 2022: Train Epoch: 3 [44800/50751 (88%)]	Loss: 0.105504 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:37 2022:    3    | Tr.loss: 0.122666 | Tr.F1.:   0.96    |   17.98  s
WARNING:root:
        [!] Sat Dec 24 17:04:37 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897877-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897877-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897877-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897877-duration.pickle
WARNING:root: [!] Fold 3/3 | Train set size: 50751, Validation set size: 25375
WARNING:root: [*] Started epoch: 1
WARNING:root: [*] Sat Dec 24 17:04:39 2022: Train Epoch: 1 [  0  /50751 (0 %)]	Loss: 0.685635 | F1-score: 0.63 | Elapsed: 0.03s
WARNING:root: [*] Sat Dec 24 17:04:41 2022: Train Epoch: 1 [6400 /50751 (13%)]	Loss: 0.290454 | F1-score: 0.88 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:44 2022: Train Epoch: 1 [12800/50751 (25%)]	Loss: 0.234919 | F1-score: 0.91 | Elapsed: 2.29s
WARNING:root: [*] Sat Dec 24 17:04:46 2022: Train Epoch: 1 [19200/50751 (38%)]	Loss: 0.208902 | F1-score: 0.92 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:04:48 2022: Train Epoch: 1 [25600/50751 (50%)]	Loss: 0.271000 | F1-score: 0.92 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:04:50 2022: Train Epoch: 1 [32000/50751 (63%)]	Loss: 0.343489 | F1-score: 0.93 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:04:53 2022: Train Epoch: 1 [38400/50751 (76%)]	Loss: 0.156020 | F1-score: 0.93 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:04:55 2022: Train Epoch: 1 [44800/50751 (88%)]	Loss: 0.115570 | F1-score: 0.94 | Elapsed: 2.25s
WARNING:root: [*] Sat Dec 24 17:04:57 2022:    1    | Tr.loss: 0.211842 | Tr.F1.:   0.94    |   17.99  s
WARNING:root: [*] Started epoch: 2
WARNING:root: [*] Sat Dec 24 17:04:57 2022: Train Epoch: 2 [  0  /50751 (0 %)]	Loss: 0.151661 | F1-score: 0.93 | Elapsed: 0.04s
WARNING:root: [*] Sat Dec 24 17:04:59 2022: Train Epoch: 2 [6400 /50751 (13%)]	Loss: 0.123702 | F1-score: 0.95 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:05:02 2022: Train Epoch: 2 [12800/50751 (25%)]	Loss: 0.102680 | F1-score: 0.96 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:05:04 2022: Train Epoch: 2 [19200/50751 (38%)]	Loss: 0.102158 | F1-score: 0.96 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:05:06 2022: Train Epoch: 2 [25600/50751 (50%)]	Loss: 0.141701 | F1-score: 0.96 | Elapsed: 2.26s
WARNING:root: [*] Sat Dec 24 17:05:08 2022: Train Epoch: 2 [32000/50751 (63%)]	Loss: 0.106942 | F1-score: 0.96 | Elapsed: 2.32s
WARNING:root: [*] Sat Dec 24 17:05:11 2022: Train Epoch: 2 [38400/50751 (76%)]	Loss: 0.091768 | F1-score: 0.96 | Elapsed: 2.27s
WARNING:root: [*] Sat Dec 24 17:05:13 2022: Train Epoch: 2 [44800/50751 (88%)]	Loss: 0.201145 | F1-score: 0.96 | Elapsed: 2.28s
WARNING:root: [*] Sat Dec 24 17:05:15 2022:    2    | Tr.loss: 0.137189 | Tr.F1.:   0.96    |   18.08  s
WARNING:root: [*] Started epoch: 3
WARNING:root: [*] Sat Dec 24 17:05:15 2022: Train Epoch: 3 [  0  /50751 (0 %)]	Loss: 0.099724 | F1-score: 0.96 | Elapsed: 0.04s
WARNING:root: [*] Sat Dec 24 17:05:17 2022: Train Epoch: 3 [6400 /50751 (13%)]	Loss: 0.149270 | F1-score: 0.96 | Elapsed: 2.32s
WARNING:root: [*] Sat Dec 24 17:05:20 2022: Train Epoch: 3 [12800/50751 (25%)]	Loss: 0.122870 | F1-score: 0.96 | Elapsed: 2.30s
WARNING:root: [*] Sat Dec 24 17:05:22 2022: Train Epoch: 3 [19200/50751 (38%)]	Loss: 0.090810 | F1-score: 0.96 | Elapsed: 2.31s
WARNING:root: [*] Sat Dec 24 17:05:24 2022: Train Epoch: 3 [25600/50751 (50%)]	Loss: 0.214430 | F1-score: 0.96 | Elapsed: 2.31s
WARNING:root: [*] Sat Dec 24 17:05:27 2022: Train Epoch: 3 [32000/50751 (63%)]	Loss: 0.301444 | F1-score: 0.96 | Elapsed: 2.31s
WARNING:root: [*] Sat Dec 24 17:05:29 2022: Train Epoch: 3 [38400/50751 (76%)]	Loss: 0.053007 | F1-score: 0.96 | Elapsed: 2.30s
WARNING:root: [*] Sat Dec 24 17:05:31 2022: Train Epoch: 3 [44800/50751 (88%)]	Loss: 0.052954 | F1-score: 0.96 | Elapsed: 2.29s
WARNING:root: [*] Sat Dec 24 17:05:33 2022:    3    | Tr.loss: 0.122571 | Tr.F1.:   0.96    |   18.29  s
WARNING:root:
        [!] Sat Dec 24 17:05:33 2022: Dumped results:
                model: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897933-model.torch
                train loss list: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897933-train_losses.pickle
                train metrics : C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897933-train_metrics.pickle
                duration: C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\trainingFiles\trainingFiles_1671897933-duration.pickle
WARNING:root: [!] Metrics saved to C:\Users\dtrizna\Code\nebula\evaluation\crossValidation_WithAPIargs\Cnn1DLinear_VocabSize_maxLen\metrics_trainSize_76126_ep_3_cv_3_maxLen_512_vocabSize_500_embeddingDim_64_hiddenNeurons_512_256_128_batchNormConv_False_batchNormFFNN_False_filterSizes_2_3_4_5.json
WARNING:root: [!] Average epoch time: 17.96s | Mean values over 3 folds:
	FPR: 0.0001 -- TPR: 0.4532 -- F1: 0.6214
	FPR:  0.001 -- TPR: 0.7717 -- F1: 0.8699
	FPR:   0.01 -- TPR: 0.8990 -- F1: 0.9445
	FPR:    0.1 -- TPR: 0.9763 -- F1: 0.9653

WARNING:root: [!] Skipping maxLen_6144_vocabSize_500_ as it already exists
