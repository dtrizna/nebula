2023-04-01 08:16:20,749 WARNING  [!] Working on full!
2023-04-01 08:16:20,749 WARNING  [!] Skipping since exists: out_fields_multiclass\full_vocab_50000_seqlen_512\x_train_full.npy
2023-04-01 08:16:20,831 WARNING  [!] Skipping since exists: out_fields_multiclass\full_vocab_50000_seqlen_512\x_test_full.npy
2023-04-01 08:16:20,871 WARNING  [!] full multiclass classification with 8 classes
2023-04-01 08:16:20,871 WARNING  [!!!] Starting CV over full!
2023-04-01 08:16:20,940 WARNING  [!] CV output folder out_fields_multiclass\cv_full_limNone_r1763_t5 already exists, skipping!
2023-04-01 08:16:20,940 WARNING  [!] Working on api_only_name!
2023-04-01 08:16:20,952 WARNING  [!] Skipping since exists: out_fields_multiclass\api_only_name_vocab_50000_seqlen_512\x_train_full.npy
2023-04-01 08:16:21,034 WARNING  [!] Skipping since exists: out_fields_multiclass\api_only_name_vocab_50000_seqlen_512\x_test_full.npy
2023-04-01 08:16:21,056 WARNING  [!] api_only_name multiclass classification with 8 classes
2023-04-01 08:16:21,057 WARNING  [!!!] Starting CV over api_only_name!
2023-04-01 08:16:21,136 WARNING  [!] CV output folder out_fields_multiclass\cv_api_only_name_limNone_r1763_t5 already exists, skipping!
2023-04-01 08:16:21,137 WARNING  [!] Working on api_only_full!
2023-04-01 08:16:21,137 WARNING  [!] Skipping since exists: out_fields_multiclass\api_only_full_vocab_50000_seqlen_512\x_train_full.npy
2023-04-01 08:16:21,217 WARNING  [!] Skipping since exists: out_fields_multiclass\api_only_full_vocab_50000_seqlen_512\x_test_full.npy
2023-04-01 08:16:21,252 WARNING  [!] api_only_full multiclass classification with 8 classes
2023-04-01 08:16:21,253 WARNING  [!!!] Starting CV over api_only_full!
2023-04-01 08:16:21,332 WARNING  [!] CV output folder out_fields_multiclass\cv_api_only_full_limNone_r1763_t5 already exists, skipping!
2023-04-01 08:16:21,332 WARNING  [!] Working on file!
2023-04-01 08:16:21,334 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-04-01 08:18:07,732 WARNING Finished... Took: 106.40s
2023-04-01 08:18:07,732 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-04-01 08:21:00,319 WARNING Finished... Took: 172.59s
2023-04-01 08:21:00,319 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-04-01 08:21:50,157 WARNING Finished... Took: 49.84s
2023-04-01 08:21:50,158 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-04-01 08:22:39,691 WARNING Finished... Took: 49.53s
2023-04-01 08:22:39,691 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-04-01 08:23:06,903 WARNING Finished... Took: 27.21s
2023-04-01 08:23:06,904 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-04-01 08:23:59,714 WARNING Finished... Took: 52.81s
2023-04-01 08:23:59,714 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-04-01 08:24:17,242 WARNING Finished... Took: 17.53s
2023-04-01 08:24:17,242 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-04-01 08:25:21,411 WARNING Finished... Took: 64.17s
2023-04-01 08:25:21,412 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-04-01 08:25:22,667 WARNING Finished... Took: 1.26s
2023-04-01 08:25:22,672 WARNING  [!] Saved Y as out_fields_multiclass\file_vocab_50000_seqlen_512\y_train_full.npy
2023-04-01 08:25:22,908 WARNING  [!] Saved Y names as out_fields_multiclass\file_vocab_50000_seqlen_512\y_names_train_full.json
2023-04-01 08:25:22,908 WARNING  [*] Initializing tokenizer training...
2023-04-01 08:25:23,632 WARNING Dumped vocab to out_fields_multiclass\file_vocab_50000_seqlen_512\tokenizer_50000_vocab.json
2023-04-01 08:25:23,646 WARNING Dumped vocab counter to out_fields_multiclass\file_vocab_50000_seqlen_512\tokenizer_50000_counter.json
2023-04-01 08:25:23,646 WARNING  [*] Encoding and padding...
2023-04-01 08:25:26,599 WARNING  [!] Saved X as out_fields_multiclass\file_vocab_50000_seqlen_512\x_train_full.npy
2023-04-01 08:25:26,645 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-04-01 08:26:03,923 WARNING Finished... Took: 37.28s
2023-04-01 08:26:03,923 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-04-01 08:27:45,013 WARNING Finished... Took: 101.09s
2023-04-01 08:27:45,014 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-04-01 08:28:12,159 WARNING Finished... Took: 27.15s
2023-04-01 08:28:12,159 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-04-01 08:28:15,060 WARNING Finished... Took: 2.90s
2023-04-01 08:28:15,060 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-04-01 08:28:26,269 WARNING Finished... Took: 11.21s
2023-04-01 08:28:26,269 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-04-01 08:30:42,223 WARNING Finished... Took: 135.95s
2023-04-01 08:30:42,223 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-04-01 08:31:01,557 WARNING Finished... Took: 19.33s
2023-04-01 08:31:01,558 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-04-01 08:31:14,055 WARNING Finished... Took: 12.50s
2023-04-01 08:31:14,056 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-04-01 08:31:14,583 WARNING Finished... Took: 0.53s
2023-04-01 08:31:14,585 WARNING  [!] Saved Y as out_fields_multiclass\file_vocab_50000_seqlen_512\y_test_full.npy
2023-04-01 08:31:14,629 WARNING  [!] Saved Y names as out_fields_multiclass\file_vocab_50000_seqlen_512\y_names_test_full.json
2023-04-01 08:31:14,632 WARNING  [*] Encoding and padding...
2023-04-01 08:31:34,401 WARNING  [!] Saved X as out_fields_multiclass\file_vocab_50000_seqlen_512\x_test_full.npy
2023-04-01 08:31:34,974 WARNING  [!] file multiclass classification with 8 classes
2023-04-01 08:31:34,974 WARNING  [!!!] Starting CV over file!
2023-04-01 08:31:35,064 WARNING  [!] Training time budget: 300min
2023-04-01 08:31:35,065 WARNING  [!] Model config: {'vocab_size': 5298, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 8, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-04-01 08:31:35,137 WARNING  [1/3] Train set size: 60730, Validation set size: 30366
2023-04-01 08:31:36,079 WARNING  [!] Saved dataset splits to dataset_splits_1680330695.npz
2023-04-01 08:31:36,314 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.5368e6
2023-04-01 08:31:36,315 WARNING  [*] Training time budget set: 5.0 min
2023-04-01 08:31:36,349 WARNING  [*] Started epoch: 1
2023-04-01 08:31:40,837 WARNING  [*] 08:31:40: Train Epoch: 1 [  0  / 633  (0 %)] | Loss: 9.031017 | Elapsed: 4.49s
2023-04-01 08:31:50,436 WARNING  [*] 08:31:50: Train Epoch: 1 [9600 / 633  (16%)] | Loss: 2.071496 | Elapsed: 9.60s
2023-04-01 08:32:00,065 WARNING  [*] 08:32:00: Train Epoch: 1 [19200/ 633  (32%)] | Loss: 2.045181 | Elapsed: 9.63s
2023-04-01 08:32:09,783 WARNING  [*] 08:32:09: Train Epoch: 1 [28800/ 633  (47%)] | Loss: 2.056001 | Elapsed: 9.72s
2023-04-01 08:32:19,651 WARNING  [*] 08:32:19: Train Epoch: 1 [38400/ 633  (63%)] | Loss: 2.048630 | Elapsed: 9.87s
2023-04-01 08:32:29,506 WARNING  [*] 08:32:29: Train Epoch: 1 [48000/ 633  (79%)] | Loss: 2.027306 | Elapsed: 9.85s
2023-04-01 08:32:39,418 WARNING  [*] 08:32:39: Train Epoch: 1 [57600/ 633  (95%)] | Loss: 2.029856 | Elapsed: 9.91s
2023-04-01 08:32:42,663 WARNING  [*] Sat Apr  1 08:32:42 2023:    1    | Tr.loss: 2.099838 | Elapsed:   66.31  s
2023-04-01 08:32:42,663 WARNING  [*] Started epoch: 2
2023-04-01 08:32:42,777 WARNING  [*] 08:32:42: Train Epoch: 2 [  0  / 633  (0 %)] | Loss: 2.020907 | Elapsed: 0.11s
2023-04-01 08:32:52,680 WARNING  [*] 08:32:52: Train Epoch: 2 [9600 / 633  (16%)] | Loss: 2.012142 | Elapsed: 9.90s
2023-04-01 08:33:02,574 WARNING  [*] 08:33:02: Train Epoch: 2 [19200/ 633  (32%)] | Loss: 2.047912 | Elapsed: 9.89s
2023-04-01 08:33:12,466 WARNING  [*] 08:33:12: Train Epoch: 2 [28800/ 633  (47%)] | Loss: 2.019057 | Elapsed: 9.89s
2023-04-01 08:33:22,416 WARNING  [*] 08:33:22: Train Epoch: 2 [38400/ 633  (63%)] | Loss: 2.004524 | Elapsed: 9.95s
2023-04-01 08:33:32,308 WARNING  [*] 08:33:32: Train Epoch: 2 [48000/ 633  (79%)] | Loss: 2.007907 | Elapsed: 9.89s
2023-04-01 08:33:42,222 WARNING  [*] 08:33:42: Train Epoch: 2 [57600/ 633  (95%)] | Loss: 2.011023 | Elapsed: 9.91s
2023-04-01 08:33:45,428 WARNING  [*] Sat Apr  1 08:33:45 2023:    2    | Tr.loss: 2.015572 | Elapsed:   62.77  s
2023-04-01 08:33:45,429 WARNING  [*] Started epoch: 3
2023-04-01 08:33:45,557 WARNING  [*] 08:33:45: Train Epoch: 3 [  0  / 633  (0 %)] | Loss: 2.015492 | Elapsed: 0.13s
2023-04-01 08:33:55,444 WARNING  [*] 08:33:55: Train Epoch: 3 [9600 / 633  (16%)] | Loss: 1.968949 | Elapsed: 9.89s
2023-04-01 08:34:05,388 WARNING  [*] 08:34:05: Train Epoch: 3 [19200/ 633  (32%)] | Loss: 1.965536 | Elapsed: 9.94s
2023-04-01 08:34:15,353 WARNING  [*] 08:34:15: Train Epoch: 3 [28800/ 633  (47%)] | Loss: 1.977826 | Elapsed: 9.96s
2023-04-01 08:34:25,246 WARNING  [*] 08:34:25: Train Epoch: 3 [38400/ 633  (63%)] | Loss: 1.974023 | Elapsed: 9.89s
2023-04-01 08:34:35,195 WARNING  [*] 08:34:35: Train Epoch: 3 [48000/ 633  (79%)] | Loss: 1.977333 | Elapsed: 9.95s
2023-04-01 08:34:45,127 WARNING  [*] 08:34:45: Train Epoch: 3 [57600/ 633  (95%)] | Loss: 2.020393 | Elapsed: 9.93s
2023-04-01 08:34:48,353 WARNING  [*] Sat Apr  1 08:34:48 2023:    3    | Tr.loss: 1.992644 | Elapsed:   62.92  s
2023-04-01 08:34:48,353 WARNING  [*] Started epoch: 4
2023-04-01 08:34:48,468 WARNING  [*] 08:34:48: Train Epoch: 4 [  0  / 633  (0 %)] | Loss: 1.949301 | Elapsed: 0.11s
2023-04-01 08:34:58,397 WARNING  [*] 08:34:58: Train Epoch: 4 [9600 / 633  (16%)] | Loss: 1.948235 | Elapsed: 9.93s
2023-04-01 08:35:08,381 WARNING  [*] 08:35:08: Train Epoch: 4 [19200/ 633  (32%)] | Loss: 2.041439 | Elapsed: 9.98s
2023-04-01 08:35:18,290 WARNING  [*] 08:35:18: Train Epoch: 4 [28800/ 633  (47%)] | Loss: 2.030326 | Elapsed: 9.91s
2023-04-01 08:35:28,200 WARNING  [*] 08:35:28: Train Epoch: 4 [38400/ 633  (63%)] | Loss: 1.986124 | Elapsed: 9.91s
2023-04-01 08:35:38,088 WARNING  [*] 08:35:38: Train Epoch: 4 [48000/ 633  (79%)] | Loss: 2.015433 | Elapsed: 9.89s
2023-04-01 08:35:47,969 WARNING  [*] 08:35:47: Train Epoch: 4 [57600/ 633  (95%)] | Loss: 2.029512 | Elapsed: 9.88s
2023-04-01 08:35:51,181 WARNING  [*] Sat Apr  1 08:35:51 2023:    4    | Tr.loss: 1.978891 | Elapsed:   62.83  s
2023-04-01 08:35:51,181 WARNING  [*] Started epoch: 5
2023-04-01 08:35:51,294 WARNING  [*] 08:35:51: Train Epoch: 5 [  0  / 633  (0 %)] | Loss: 1.928706 | Elapsed: 0.11s
2023-04-01 08:36:01,213 WARNING  [*] 08:36:01: Train Epoch: 5 [9600 / 633  (16%)] | Loss: 2.046785 | Elapsed: 9.92s
2023-04-01 08:36:11,101 WARNING  [*] 08:36:11: Train Epoch: 5 [19200/ 633  (32%)] | Loss: 1.925563 | Elapsed: 9.89s
2023-04-01 08:36:20,993 WARNING  [*] 08:36:20: Train Epoch: 5 [28800/ 633  (47%)] | Loss: 2.016565 | Elapsed: 9.89s
2023-04-01 08:36:30,911 WARNING  [*] 08:36:30: Train Epoch: 5 [38400/ 633  (63%)] | Loss: 1.974348 | Elapsed: 9.92s
2023-04-01 08:36:36,356 WARNING  [!] Time budget exceeded, training stopped.
2023-04-01 08:36:36,384 WARNING  [!] Sat Apr  1 08:36:36 2023: Dumped results:
                model       : 1680330695-model.torch
		train time  : 1680330695-trainTime.npy
		train losses: 1680330695-trainLosses.npy
		train AUC   : 1680330695-auc.npy
		train F1s   : 1680330695-trainF1s.npy
		train TPRs  : 1680330695-trainTPRs.npy
2023-04-01 08:36:36,428 WARNING  [!] Evaluating model on training set...
2023-04-01 08:36:52,763 WARNING  [!] This fold metrics on training set:
2023-04-01 08:36:52,802 WARNING 	AUC: 0.5000
2023-04-01 08:36:52,810 WARNING 	F1: 0.0546
2023-04-01 08:36:52,810 WARNING  [!] Evaluating model on validation set...
2023-04-01 08:37:00,963 WARNING  [!] This fold metrics on validation set:
2023-04-01 08:37:00,981 WARNING 	AUC: 0.5000
2023-04-01 08:37:00,986 WARNING 	F1: 0.0550
2023-04-01 08:37:01,186 WARNING  [2/3] Train set size: 60731, Validation set size: 30365
2023-04-01 08:37:02,131 WARNING  [!] Saved dataset splits to dataset_splits_1680331021.npz
2023-04-01 08:37:02,186 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.5368e6
2023-04-01 08:37:02,186 WARNING  [*] Training time budget set: 5.0 min
2023-04-01 08:37:02,218 WARNING  [*] Started epoch: 1
2023-04-01 08:37:02,345 WARNING  [*] 08:37:02: Train Epoch: 1 [  0  / 633  (0 %)] | Loss: 8.418325 | Elapsed: 0.13s
2023-04-01 08:37:12,263 WARNING  [*] 08:37:12: Train Epoch: 1 [9600 / 633  (16%)] | Loss: 2.048236 | Elapsed: 9.92s
2023-04-01 08:37:22,160 WARNING  [*] 08:37:22: Train Epoch: 1 [19200/ 633  (32%)] | Loss: 2.049090 | Elapsed: 9.90s
2023-04-01 08:37:32,092 WARNING  [*] 08:37:32: Train Epoch: 1 [28800/ 633  (47%)] | Loss: 2.038449 | Elapsed: 9.93s
2023-04-01 08:37:42,063 WARNING  [*] 08:37:42: Train Epoch: 1 [38400/ 633  (63%)] | Loss: 2.028497 | Elapsed: 9.97s
2023-04-01 08:37:52,067 WARNING  [*] 08:37:52: Train Epoch: 1 [48000/ 633  (79%)] | Loss: 2.004392 | Elapsed: 10.00s
2023-04-01 08:38:02,099 WARNING  [*] 08:38:02: Train Epoch: 1 [57600/ 633  (95%)] | Loss: 2.022731 | Elapsed: 10.03s
2023-04-01 08:38:05,362 WARNING  [*] Sat Apr  1 08:38:05 2023:    1    | Tr.loss: 2.087126 | Elapsed:   63.14  s
2023-04-01 08:38:05,363 WARNING  [*] Started epoch: 2
2023-04-01 08:38:05,474 WARNING  [*] 08:38:05: Train Epoch: 2 [  0  / 633  (0 %)] | Loss: 2.008193 | Elapsed: 0.11s
2023-04-01 08:38:15,405 WARNING  [*] 08:38:15: Train Epoch: 2 [9600 / 633  (16%)] | Loss: 1.990357 | Elapsed: 9.93s
2023-04-01 08:38:25,423 WARNING  [*] 08:38:25: Train Epoch: 2 [19200/ 633  (32%)] | Loss: 2.022270 | Elapsed: 10.02s
2023-04-01 08:38:35,430 WARNING  [*] 08:38:35: Train Epoch: 2 [28800/ 633  (47%)] | Loss: 2.002140 | Elapsed: 10.01s
2023-04-01 08:38:45,440 WARNING  [*] 08:38:45: Train Epoch: 2 [38400/ 633  (63%)] | Loss: 2.015497 | Elapsed: 10.01s
2023-04-01 08:38:55,479 WARNING  [*] 08:38:55: Train Epoch: 2 [48000/ 633  (79%)] | Loss: 1.999281 | Elapsed: 10.04s
2023-04-01 08:39:05,454 WARNING  [*] 08:39:05: Train Epoch: 2 [57600/ 633  (95%)] | Loss: 2.004109 | Elapsed: 9.97s
2023-04-01 08:39:08,723 WARNING  [*] Sat Apr  1 08:39:08 2023:    2    | Tr.loss: 2.005196 | Elapsed:   63.36  s
2023-04-01 08:39:08,723 WARNING  [*] Started epoch: 3
2023-04-01 08:39:08,839 WARNING  [*] 08:39:08: Train Epoch: 3 [  0  / 633  (0 %)] | Loss: 1.991514 | Elapsed: 0.12s
2023-04-01 08:39:18,907 WARNING  [*] 08:39:18: Train Epoch: 3 [9600 / 633  (16%)] | Loss: 1.976804 | Elapsed: 10.07s
2023-04-01 08:39:28,998 WARNING  [*] 08:39:28: Train Epoch: 3 [19200/ 633  (32%)] | Loss: 1.998893 | Elapsed: 10.09s
2023-04-01 08:39:39,023 WARNING  [*] 08:39:39: Train Epoch: 3 [28800/ 633  (47%)] | Loss: 1.975720 | Elapsed: 10.02s
2023-04-01 08:39:49,107 WARNING  [*] 08:39:49: Train Epoch: 3 [38400/ 633  (63%)] | Loss: 1.989347 | Elapsed: 10.08s
2023-04-01 08:39:59,179 WARNING  [*] 08:39:59: Train Epoch: 3 [48000/ 633  (79%)] | Loss: 1.977507 | Elapsed: 10.07s
2023-04-01 08:40:09,244 WARNING  [*] 08:40:09: Train Epoch: 3 [57600/ 633  (95%)] | Loss: 1.898077 | Elapsed: 10.06s
2023-04-01 08:40:12,547 WARNING  [*] Sat Apr  1 08:40:12 2023:    3    | Tr.loss: 1.983992 | Elapsed:   63.82  s
2023-04-01 08:40:12,548 WARNING  [*] Started epoch: 4
2023-04-01 08:40:12,658 WARNING  [*] 08:40:12: Train Epoch: 4 [  0  / 633  (0 %)] | Loss: 1.985336 | Elapsed: 0.11s
2023-04-01 08:40:22,717 WARNING  [*] 08:40:22: Train Epoch: 4 [9600 / 633  (16%)] | Loss: 1.998020 | Elapsed: 10.06s
2023-04-01 08:40:32,819 WARNING  [*] 08:40:32: Train Epoch: 4 [19200/ 633  (32%)] | Loss: 1.983785 | Elapsed: 10.10s
2023-04-01 08:40:42,964 WARNING  [*] 08:40:42: Train Epoch: 4 [28800/ 633  (47%)] | Loss: 2.028637 | Elapsed: 10.15s
2023-04-01 08:40:53,045 WARNING  [*] 08:40:53: Train Epoch: 4 [38400/ 633  (63%)] | Loss: 1.991866 | Elapsed: 10.08s
2023-04-01 08:41:03,133 WARNING  [*] 08:41:03: Train Epoch: 4 [48000/ 633  (79%)] | Loss: 1.980864 | Elapsed: 10.09s
2023-04-01 08:41:13,226 WARNING  [*] 08:41:13: Train Epoch: 4 [57600/ 633  (95%)] | Loss: 2.010329 | Elapsed: 10.09s
2023-04-01 08:41:16,534 WARNING  [*] Sat Apr  1 08:41:16 2023:    4    | Tr.loss: 1.972246 | Elapsed:   63.99  s
2023-04-01 08:41:16,535 WARNING  [*] Started epoch: 5
2023-04-01 08:41:16,645 WARNING  [*] 08:41:16: Train Epoch: 5 [  0  / 633  (0 %)] | Loss: 1.991269 | Elapsed: 0.11s
2023-04-01 08:41:26,824 WARNING  [*] 08:41:26: Train Epoch: 5 [9600 / 633  (16%)] | Loss: 1.984628 | Elapsed: 10.18s
2023-04-01 08:41:36,819 WARNING  [*] 08:41:36: Train Epoch: 5 [19200/ 633  (32%)] | Loss: 1.869363 | Elapsed: 10.00s
2023-04-01 08:41:46,768 WARNING  [*] 08:41:46: Train Epoch: 5 [28800/ 633  (47%)] | Loss: 1.938926 | Elapsed: 9.95s
2023-04-01 08:41:56,699 WARNING  [*] 08:41:56: Train Epoch: 5 [38400/ 633  (63%)] | Loss: 1.955166 | Elapsed: 9.93s
2023-04-01 08:42:02,222 WARNING  [!] Time budget exceeded, training stopped.
2023-04-01 08:42:02,244 WARNING  [!] Sat Apr  1 08:42:02 2023: Dumped results:
                model       : 1680331021-model.torch
		train time  : 1680331021-trainTime.npy
		train losses: 1680331021-trainLosses.npy
		train AUC   : 1680331021-auc.npy
		train F1s   : 1680331021-trainF1s.npy
		train TPRs  : 1680331021-trainTPRs.npy
2023-04-01 08:42:02,283 WARNING  [!] Evaluating model on training set...
2023-04-01 08:42:18,919 WARNING  [!] This fold metrics on training set:
2023-04-01 08:42:18,953 WARNING 	AUC: 0.5000
2023-04-01 08:42:18,961 WARNING 	F1: 0.0550
2023-04-01 08:42:18,961 WARNING  [!] Evaluating model on validation set...
2023-04-01 08:42:27,321 WARNING  [!] This fold metrics on validation set:
2023-04-01 08:42:27,339 WARNING 	AUC: 0.5000
2023-04-01 08:42:27,343 WARNING 	F1: 0.0542
2023-04-01 08:42:27,495 WARNING  [3/3] Train set size: 60731, Validation set size: 30365
2023-04-01 08:42:28,434 WARNING  [!] Saved dataset splits to dataset_splits_1680331347.npz
2023-04-01 08:42:28,489 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.5368e6
2023-04-01 08:42:28,490 WARNING  [*] Training time budget set: 5.0 min
2023-04-01 08:42:28,521 WARNING  [*] Started epoch: 1
2023-04-01 08:42:28,655 WARNING  [*] 08:42:28: Train Epoch: 1 [  0  / 633  (0 %)] | Loss: 11.757079 | Elapsed: 0.13s
2023-04-01 08:42:38,741 WARNING  [*] 08:42:38: Train Epoch: 1 [9600 / 633  (16%)] | Loss: 2.058000 | Elapsed: 10.09s
2023-04-01 08:42:48,805 WARNING  [*] 08:42:48: Train Epoch: 1 [19200/ 633  (32%)] | Loss: 2.064128 | Elapsed: 10.06s
2023-04-01 08:42:58,976 WARNING  [*] 08:42:58: Train Epoch: 1 [28800/ 633  (47%)] | Loss: 2.059635 | Elapsed: 10.17s
2023-04-01 08:43:09,073 WARNING  [*] 08:43:09: Train Epoch: 1 [38400/ 633  (63%)] | Loss: 2.033806 | Elapsed: 10.10s
2023-04-01 08:43:19,024 WARNING  [*] 08:43:19: Train Epoch: 1 [48000/ 633  (79%)] | Loss: 2.055325 | Elapsed: 9.95s
2023-04-01 08:43:29,079 WARNING  [*] 08:43:29: Train Epoch: 1 [57600/ 633  (95%)] | Loss: 2.028109 | Elapsed: 10.05s
2023-04-01 08:43:32,327 WARNING  [*] Sat Apr  1 08:43:32 2023:    1    | Tr.loss: 2.094135 | Elapsed:   63.81  s
2023-04-01 08:43:32,327 WARNING  [*] Started epoch: 2
2023-04-01 08:43:32,439 WARNING  [*] 08:43:32: Train Epoch: 2 [  0  / 633  (0 %)] | Loss: 2.028337 | Elapsed: 0.11s
2023-04-01 08:43:42,414 WARNING  [*] 08:43:42: Train Epoch: 2 [9600 / 633  (16%)] | Loss: 2.001320 | Elapsed: 9.97s
2023-04-01 08:43:52,577 WARNING  [*] 08:43:52: Train Epoch: 2 [19200/ 633  (32%)] | Loss: 2.014585 | Elapsed: 10.16s
2023-04-01 08:44:02,683 WARNING  [*] 08:44:02: Train Epoch: 2 [28800/ 633  (47%)] | Loss: 2.031019 | Elapsed: 10.11s
2023-04-01 08:44:12,805 WARNING  [*] 08:44:12: Train Epoch: 2 [38400/ 633  (63%)] | Loss: 2.015300 | Elapsed: 10.12s
2023-04-01 08:44:22,957 WARNING  [*] 08:44:22: Train Epoch: 2 [48000/ 633  (79%)] | Loss: 2.028835 | Elapsed: 10.15s
2023-04-01 08:44:33,001 WARNING  [*] 08:44:33: Train Epoch: 2 [57600/ 633  (95%)] | Loss: 1.977322 | Elapsed: 10.04s
2023-04-01 08:44:36,341 WARNING  [*] Sat Apr  1 08:44:36 2023:    2    | Tr.loss: 2.013106 | Elapsed:   64.01  s
2023-04-01 08:44:36,342 WARNING  [*] Started epoch: 3
2023-04-01 08:44:36,454 WARNING  [*] 08:44:36: Train Epoch: 3 [  0  / 633  (0 %)] | Loss: 2.013767 | Elapsed: 0.11s
2023-04-01 08:44:46,558 WARNING  [*] 08:44:46: Train Epoch: 3 [9600 / 633  (16%)] | Loss: 2.011435 | Elapsed: 10.10s
2023-04-01 08:44:56,757 WARNING  [*] 08:44:56: Train Epoch: 3 [19200/ 633  (32%)] | Loss: 1.976278 | Elapsed: 10.20s
2023-04-01 08:45:06,999 WARNING  [*] 08:45:06: Train Epoch: 3 [28800/ 633  (47%)] | Loss: 1.989844 | Elapsed: 10.24s
2023-04-01 08:45:17,119 WARNING  [*] 08:45:17: Train Epoch: 3 [38400/ 633  (63%)] | Loss: 1.999390 | Elapsed: 10.12s
2023-04-01 08:45:27,236 WARNING  [*] 08:45:27: Train Epoch: 3 [48000/ 633  (79%)] | Loss: 2.006358 | Elapsed: 10.12s
2023-04-01 08:45:37,332 WARNING  [*] 08:45:37: Train Epoch: 3 [57600/ 633  (95%)] | Loss: 2.015280 | Elapsed: 10.09s
2023-04-01 08:45:40,659 WARNING  [*] Sat Apr  1 08:45:40 2023:    3    | Tr.loss: 1.989789 | Elapsed:   64.32  s
2023-04-01 08:45:40,660 WARNING  [*] Started epoch: 4
2023-04-01 08:45:40,780 WARNING  [*] 08:45:40: Train Epoch: 4 [  0  / 633  (0 %)] | Loss: 1.964726 | Elapsed: 0.12s
2023-04-01 08:45:50,914 WARNING  [*] 08:45:50: Train Epoch: 4 [9600 / 633  (16%)] | Loss: 1.955009 | Elapsed: 10.13s
2023-04-01 08:46:01,123 WARNING  [*] 08:46:01: Train Epoch: 4 [19200/ 633  (32%)] | Loss: 1.893128 | Elapsed: 10.21s
2023-04-01 08:46:11,275 WARNING  [*] 08:46:11: Train Epoch: 4 [28800/ 633  (47%)] | Loss: 1.919084 | Elapsed: 10.15s
2023-04-01 08:46:21,430 WARNING  [*] 08:46:21: Train Epoch: 4 [38400/ 633  (63%)] | Loss: 1.924829 | Elapsed: 10.15s
2023-04-01 08:46:31,667 WARNING  [*] 08:46:31: Train Epoch: 4 [48000/ 633  (79%)] | Loss: 1.921073 | Elapsed: 10.24s
2023-04-01 08:46:41,785 WARNING  [*] 08:46:41: Train Epoch: 4 [57600/ 633  (95%)] | Loss: 2.026030 | Elapsed: 10.12s
2023-04-01 08:46:45,057 WARNING  [*] Sat Apr  1 08:46:45 2023:    4    | Tr.loss: 1.976429 | Elapsed:   64.40  s
2023-04-01 08:46:45,057 WARNING  [*] Started epoch: 5
2023-04-01 08:46:45,171 WARNING  [*] 08:46:45: Train Epoch: 5 [  0  / 633  (0 %)] | Loss: 1.963579 | Elapsed: 0.11s
2023-04-01 08:46:55,322 WARNING  [*] 08:46:55: Train Epoch: 5 [9600 / 633  (16%)] | Loss: 1.979872 | Elapsed: 10.15s
2023-04-01 08:47:05,485 WARNING  [*] 08:47:05: Train Epoch: 5 [19200/ 633  (32%)] | Loss: 1.980838 | Elapsed: 10.16s
2023-04-01 08:47:15,608 WARNING  [*] 08:47:15: Train Epoch: 5 [28800/ 633  (47%)] | Loss: 2.003946 | Elapsed: 10.12s
2023-04-01 08:47:25,732 WARNING  [*] 08:47:25: Train Epoch: 5 [38400/ 633  (63%)] | Loss: 1.992502 | Elapsed: 10.12s
2023-04-01 08:47:28,503 WARNING  [!] Time budget exceeded, training stopped.
2023-04-01 08:47:28,531 WARNING  [!] Sat Apr  1 08:47:28 2023: Dumped results:
                model       : 1680331347-model.torch
		train time  : 1680331347-trainTime.npy
		train losses: 1680331347-trainLosses.npy
		train AUC   : 1680331347-auc.npy
		train F1s   : 1680331347-trainF1s.npy
		train TPRs  : 1680331347-trainTPRs.npy
2023-04-01 08:47:28,586 WARNING  [!] Evaluating model on training set...
2023-04-01 08:47:45,193 WARNING  [!] This fold metrics on training set:
2023-04-01 08:47:45,227 WARNING 	AUC: 0.5000
2023-04-01 08:47:45,235 WARNING 	F1: 0.0546
2023-04-01 08:47:45,236 WARNING  [!] Evaluating model on validation set...
2023-04-01 08:47:53,581 WARNING  [!] This fold metrics on validation set:
2023-04-01 08:47:53,598 WARNING 	AUC: 0.5000
2023-04-01 08:47:53,602 WARNING 	F1: 0.0550
2023-04-01 08:47:53,680 WARNING  [!] Metrics saved to out_fields_multiclass\cv_file_limNone_r1763_t5\file_metrics_validation.json
2023-04-01 08:47:53,681 WARNING  [!] Metrics saved to out_fields_multiclass\cv_file_limNone_r1763_t5\file_metrics_training.json
2023-04-01 08:47:53,682 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.5000

2023-04-01 08:47:53,737 WARNING  [!] Working on network!
2023-04-01 08:47:53,746 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-04-01 08:49:08,407 WARNING Finished... Took: 74.66s
2023-04-01 08:49:08,408 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-04-01 08:51:10,231 WARNING Finished... Took: 121.82s
2023-04-01 08:51:10,231 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-04-01 08:51:46,882 WARNING Finished... Took: 36.65s
2023-04-01 08:51:46,882 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-04-01 08:52:23,844 WARNING Finished... Took: 36.96s
2023-04-01 08:52:23,845 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-04-01 08:52:45,827 WARNING Finished... Took: 21.98s
2023-04-01 08:52:45,827 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-04-01 08:53:33,769 WARNING Finished... Took: 47.94s
2023-04-01 08:53:33,769 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-04-01 08:53:49,233 WARNING Finished... Took: 15.46s
2023-04-01 08:53:49,233 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-04-01 08:54:42,864 WARNING Finished... Took: 53.63s
2023-04-01 08:54:42,865 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-04-01 08:54:43,965 WARNING Finished... Took: 1.10s
2023-04-01 08:54:43,970 WARNING  [!] Saved Y as out_fields_multiclass\network_vocab_50000_seqlen_512\y_train_full.npy
2023-04-01 08:54:44,210 WARNING  [!] Saved Y names as out_fields_multiclass\network_vocab_50000_seqlen_512\y_names_train_full.json
2023-04-01 08:54:44,210 WARNING  [*] Initializing tokenizer training...
2023-04-01 08:54:44,709 WARNING Dumped vocab to out_fields_multiclass\network_vocab_50000_seqlen_512\tokenizer_50000_vocab.json
2023-04-01 08:54:44,711 WARNING Dumped vocab counter to out_fields_multiclass\network_vocab_50000_seqlen_512\tokenizer_50000_counter.json
2023-04-01 08:54:44,711 WARNING  [*] Encoding and padding...
2023-04-01 08:54:47,362 WARNING  [!] Saved X as out_fields_multiclass\network_vocab_50000_seqlen_512\x_train_full.npy
2023-04-01 08:54:47,401 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
