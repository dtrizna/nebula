2023-03-18 09:35:42,073 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-03-18 09:39:40,604 WARNING Finished... Took: 238.53s
2023-03-18 09:39:40,604 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-03-18 09:45:22,934 WARNING Finished... Took: 342.33s
2023-03-18 09:45:22,934 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-03-18 09:47:05,694 WARNING Finished... Took: 102.76s
2023-03-18 09:47:05,694 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-03-18 09:48:36,882 WARNING Finished... Took: 91.19s
2023-03-18 09:48:36,882 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-03-18 09:49:42,951 WARNING Finished... Took: 66.07s
2023-03-18 09:49:42,951 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-03-18 09:52:48,898 WARNING Finished... Took: 185.95s
2023-03-18 09:52:48,899 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-03-18 09:53:33,799 WARNING Finished... Took: 44.90s
2023-03-18 09:53:33,799 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-03-18 09:55:56,434 WARNING Finished... Took: 142.63s
2023-03-18 09:55:56,435 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-03-18 09:55:58,209 WARNING Finished... Took: 1.77s
2023-03-18 09:55:58,220 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_5000_seqlen_512\y_train_full.npy
2023-03-18 09:55:58,415 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_5000_seqlen_512\y_names_train_full.json
2023-03-18 09:55:58,415 WARNING  [!] Initialized tokenizer without pre-trained model.
	You need to train tokenizer with .train() or specify 'model_path=' during initialization!
2023-03-18 09:55:58,415 WARNING  [*] Initializing tokenizer training...
2023-03-18 09:55:58,416 WARNING  [*] Data preparation for SentencePiece tokenizer...
2023-03-18 09:58:38,467 WARNING  [*] Saving to disk...
2023-03-18 09:58:43,341 WARNING  [!] Training tokenizer with command: --input=out_vocab_1679128542\nebula_vocab_5000_seqlen_512\tokenizer_5000_trainset_1679129918.txt --model_prefix=out_vocab_1679128542\nebula_vocab_5000_seqlen_512\tokenizer_5000 --vocab_size=5000 --model_type=bpe --split_by_number=False --max_sentence_length=4192 --max_sentencepiece_length=64
2023-03-18 10:00:05,042 WARNING  [!] Loaded vocab with size 5001 from out_vocab_1679128542\nebula_vocab_5000_seqlen_512\tokenizer_5000.vocab
2023-03-18 10:00:05,470 WARNING  [*] Encoding and padding...
2023-03-18 10:06:06,559 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_5000_seqlen_512\x_train_full.npy
2023-03-18 10:06:10,294 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-03-18 10:06:44,867 WARNING Finished... Took: 34.57s
2023-03-18 10:06:44,868 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-03-18 10:08:17,294 WARNING Finished... Took: 92.43s
2023-03-18 10:08:17,294 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-03-18 10:08:44,315 WARNING Finished... Took: 27.02s
2023-03-18 10:08:44,315 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-03-18 10:08:46,863 WARNING Finished... Took: 2.55s
2023-03-18 10:08:46,863 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-03-18 10:08:56,995 WARNING Finished... Took: 10.13s
2023-03-18 10:08:56,995 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-03-18 10:11:07,012 WARNING Finished... Took: 130.02s
2023-03-18 10:11:07,012 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-03-18 10:11:23,489 WARNING Finished... Took: 16.48s
2023-03-18 10:11:23,489 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-03-18 10:11:36,953 WARNING Finished... Took: 13.46s
2023-03-18 10:11:36,953 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-03-18 10:11:37,453 WARNING Finished... Took: 0.50s
2023-03-18 10:11:37,455 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_5000_seqlen_512\y_test_full.npy
2023-03-18 10:11:37,512 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_5000_seqlen_512\y_names_test_full.json
2023-03-18 10:11:37,521 WARNING  [!] Successfully loaded pre-trained tokenizer model!
2023-03-18 10:11:37,523 WARNING  [!] Loaded vocab with size 5001 from out_vocab_1679128542\nebula_vocab_5000_seqlen_512\tokenizer_5000_vocab.json
2023-03-18 10:11:37,523 WARNING  [*] Encoding and padding...
2023-03-18 10:12:46,289 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_5000_seqlen_512\x_test_full.npy
2023-03-18 10:12:47,128 WARNING  [!!!] Starting CV over 5000!
2023-03-18 10:12:47,237 WARNING  [!] Training time budget: 300min
2023-03-18 10:12:47,237 WARNING  [!] Model config: {'vocab_size': 5001, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-03-18 10:12:47,328 WARNING  [1/3] Train set size: 50750, Validation set size: 25376
2023-03-18 10:12:49,586 WARNING  [!] Saved dataset splits to dataset_splits_1679130767.npz
2023-03-18 10:12:49,759 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.5173e6
2023-03-18 10:12:49,759 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 10:12:49,786 WARNING  [*] Started epoch: 1
2023-03-18 10:12:51,607 WARNING  [*] 10:12:51: Train Epoch: 1 [  0  /50750 (0 %)] | Loss: 2.207826 | Elapsed: 1.81s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.4978
2023-03-18 10:13:01,252 WARNING  [*] 10:13:01: Train Epoch: 1 [9600 /50750 (19%)] | Loss: 0.366398 | Elapsed: 9.64s | FPR 0.0003 -> TPR 0.5588 & F1 0.7170 | AUC 0.9113
2023-03-18 10:13:10,943 WARNING  [*] 10:13:10: Train Epoch: 1 [19200/50750 (38%)] | Loss: 0.430572 | Elapsed: 9.68s | FPR 0.0003 -> TPR 0.4085 & F1 0.5800 | AUC 0.8562
2023-03-18 10:13:20,693 WARNING  [*] 10:13:20: Train Epoch: 1 [28800/50750 (57%)] | Loss: 0.301251 | Elapsed: 9.73s | FPR 0.0003 -> TPR 0.5882 & F1 0.7407 | AUC 0.9297
2023-03-18 10:13:30,492 WARNING  [*] 10:13:30: Train Epoch: 1 [38400/50750 (76%)] | Loss: 0.271539 | Elapsed: 9.78s | FPR 0.0003 -> TPR 0.6377 & F1 0.7788 | AUC 0.9411
2023-03-18 10:13:40,308 WARNING  [*] 10:13:40: Train Epoch: 1 [48000/50750 (95%)] | Loss: 0.192517 | Elapsed: 9.82s | FPR 0.0003 -> TPR 0.7538 & F1 0.8596 | AUC 0.9798
2023-03-18 10:13:44,100 WARNING  [*] Sat Mar 18 10:13:44 2023:    1    | Tr.loss: 0.369669 | Elapsed:   54.31  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.8988
2023-03-18 10:13:44,100 WARNING  [*] Started epoch: 2
2023-03-18 10:13:44,216 WARNING  [*] 10:13:44: Train Epoch: 2 [  0  /50750 (0 %)] | Loss: 0.306829 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.5333 & F1 0.6957 | AUC 0.9417
2023-03-18 10:13:54,142 WARNING  [*] 10:13:54: Train Epoch: 2 [9600 /50750 (19%)] | Loss: 0.164105 | Elapsed: 9.92s | FPR 0.0003 -> TPR 0.8235 & F1 0.9032 | AUC 0.9894
2023-03-18 10:14:04,050 WARNING  [*] 10:14:04: Train Epoch: 2 [19200/50750 (38%)] | Loss: 0.195922 | Elapsed: 9.91s | FPR 0.0003 -> TPR 0.6406 & F1 0.7810 | AUC 0.9701
2023-03-18 10:14:13,963 WARNING  [*] 10:14:13: Train Epoch: 2 [28800/50750 (57%)] | Loss: 0.056484 | Elapsed: 9.91s | FPR 0.0003 -> TPR 0.9726 & F1 0.9861 | AUC 0.9954
2023-03-18 10:14:23,879 WARNING  [*] 10:14:23: Train Epoch: 2 [38400/50750 (76%)] | Loss: 0.156415 | Elapsed: 9.90s | FPR 0.0003 -> TPR 0.8333 & F1 0.9091 | AUC 0.9871
2023-03-18 10:14:33,831 WARNING  [*] 10:14:33: Train Epoch: 2 [48000/50750 (95%)] | Loss: 0.225562 | Elapsed: 9.93s | FPR 0.0003 -> TPR 0.7069 & F1 0.8283 | AUC 0.9745
2023-03-18 10:14:37,660 WARNING  [*] Sat Mar 18 10:14:37 2023:    2    | Tr.loss: 0.160592 | Elapsed:   53.56  s | FPR 0.0003 -> TPR: 0.38 & F1: 0.55 | AUC: 0.9822
2023-03-18 10:14:37,661 WARNING  [*] Started epoch: 3
2023-03-18 10:14:37,783 WARNING  [*] 10:14:37: Train Epoch: 3 [  0  /50750 (0 %)] | Loss: 0.090527 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.9524 & F1 0.9756 | AUC 0.9976
2023-03-18 10:14:47,725 WARNING  [*] 10:14:47: Train Epoch: 3 [9600 /50750 (19%)] | Loss: 0.072282 | Elapsed: 9.93s | FPR 0.0003 -> TPR 0.9500 & F1 0.9744 | AUC 0.9975
2023-03-18 10:14:57,644 WARNING  [*] 10:14:57: Train Epoch: 3 [19200/50750 (38%)] | Loss: 0.128569 | Elapsed: 9.91s | FPR 0.0003 -> TPR 0.9444 & F1 0.9714 | AUC 0.9911
2023-03-18 10:15:07,576 WARNING  [*] 10:15:07: Train Epoch: 3 [28800/50750 (57%)] | Loss: 0.172989 | Elapsed: 9.93s | FPR 0.0003 -> TPR 0.8169 & F1 0.8992 | AUC 0.9830
2023-03-18 10:15:17,540 WARNING  [*] 10:15:17: Train Epoch: 3 [38400/50750 (76%)] | Loss: 0.088233 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.9714 & F1 0.9855 | AUC 0.9962
2023-03-18 10:15:27,494 WARNING  [*] 10:15:27: Train Epoch: 3 [48000/50750 (95%)] | Loss: 0.109048 | Elapsed: 9.94s | FPR 0.0003 -> TPR 0.9375 & F1 0.9677 | AUC 0.9931
2023-03-18 10:15:31,304 WARNING  [*] Sat Mar 18 10:15:31 2023:    3    | Tr.loss: 0.107466 | Elapsed:   53.64  s | FPR 0.0003 -> TPR: 0.62 & F1: 0.77 | AUC: 0.9920
2023-03-18 10:15:31,304 WARNING  [*] Started epoch: 4
2023-03-18 10:15:31,444 WARNING  [*] 10:15:31: Train Epoch: 4 [  0  /50750 (0 %)] | Loss: 0.139627 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.9365 & F1 0.9672 | AUC 0.9889
2023-03-18 10:15:41,430 WARNING  [*] 10:15:41: Train Epoch: 4 [9600 /50750 (19%)] | Loss: 0.076374 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.9344 & F1 0.9661 | AUC 0.9962
2023-03-18 10:15:51,364 WARNING  [*] 10:15:51: Train Epoch: 4 [19200/50750 (38%)] | Loss: 0.138439 | Elapsed: 9.93s | FPR 0.0003 -> TPR 0.7460 & F1 0.8545 | AUC 0.9910
2023-03-18 10:16:01,325 WARNING  [*] 10:16:01: Train Epoch: 4 [28800/50750 (57%)] | Loss: 0.058036 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.9722 & F1 0.9859 | AUC 0.9990
2023-03-18 10:16:11,270 WARNING  [*] 10:16:11: Train Epoch: 4 [38400/50750 (76%)] | Loss: 0.171867 | Elapsed: 9.94s | FPR 0.0003 -> TPR 0.7639 & F1 0.8661 | AUC 0.9866
2023-03-18 10:16:21,223 WARNING  [*] 10:16:21: Train Epoch: 4 [48000/50750 (95%)] | Loss: 0.021188 | Elapsed: 9.94s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 10:16:25,062 WARNING  [*] Sat Mar 18 10:16:25 2023:    4    | Tr.loss: 0.084232 | Elapsed:   53.76  s | FPR 0.0003 -> TPR: 0.65 & F1: 0.79 | AUC: 0.9951
2023-03-18 10:16:25,062 WARNING  [*] Started epoch: 5
2023-03-18 10:16:25,195 WARNING  [*] 10:16:25: Train Epoch: 5 [  0  /50750 (0 %)] | Loss: 0.037944 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9990
2023-03-18 10:16:35,162 WARNING  [*] 10:16:35: Train Epoch: 5 [9600 /50750 (19%)] | Loss: 0.041926 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.9853 & F1 0.9926 | AUC 0.9995
2023-03-18 10:16:45,136 WARNING  [*] 10:16:45: Train Epoch: 5 [19200/50750 (38%)] | Loss: 0.046451 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.9714 & F1 0.9855 | AUC 0.9990
2023-03-18 10:16:55,096 WARNING  [*] 10:16:55: Train Epoch: 5 [28800/50750 (57%)] | Loss: 0.041186 | Elapsed: 9.95s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 10:17:05,053 WARNING  [*] 10:17:05: Train Epoch: 5 [38400/50750 (76%)] | Loss: 0.059623 | Elapsed: 9.94s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9983
2023-03-18 10:17:15,018 WARNING  [*] 10:17:15: Train Epoch: 5 [48000/50750 (95%)] | Loss: 0.062792 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9982
2023-03-18 10:17:18,821 WARNING  [*] Sat Mar 18 10:17:18 2023:    5    | Tr.loss: 0.069891 | Elapsed:   53.76  s | FPR 0.0003 -> TPR: 0.71 & F1: 0.83 | AUC: 0.9965
2023-03-18 10:17:18,837 WARNING  [*] Started epoch: 6
2023-03-18 10:17:18,956 WARNING  [*] 10:17:18: Train Epoch: 6 [  0  /50750 (0 %)] | Loss: 0.083020 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9524 & F1 0.9756 | AUC 0.9947
2023-03-18 10:17:28,944 WARNING  [*] 10:17:28: Train Epoch: 6 [9600 /50750 (19%)] | Loss: 0.036713 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9996
2023-03-18 10:17:38,885 WARNING  [*] 10:17:38: Train Epoch: 6 [19200/50750 (38%)] | Loss: 0.043226 | Elapsed: 9.94s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9986
2023-03-18 10:17:48,873 WARNING  [*] 10:17:48: Train Epoch: 6 [28800/50750 (57%)] | Loss: 0.144404 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.8841 & F1 0.9385 | AUC 0.9897
2023-03-18 10:17:49,796 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 10:17:49,824 WARNING  [!] Sat Mar 18 10:17:49 2023: Dumped results:
                model       : 1679130767-model.torch
		train time  : 1679130767-trainTime.npy
		train losses: 1679130767-trainLosses.npy
		train AUC   : 1679130767-auc.npy
		train F1s   : 1679130767-trainF1s.npy
		train TPRs  : 1679130767-trainTPRs.npy
2023-03-18 10:17:49,861 WARNING  [!] Evaluating model on training set...
2023-03-18 10:18:03,555 WARNING  [!] This fold metrics on training set:
2023-03-18 10:18:03,563 WARNING 	AUC: 0.9985
2023-03-18 10:18:03,579 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 10:18:03,587 WARNING 	FPR: 0.0003 | TPR: 0.7173 | F1: 0.8353
2023-03-18 10:18:03,604 WARNING 	FPR: 0.001 | TPR: 0.9099 | F1: 0.9526
2023-03-18 10:18:03,617 WARNING 	FPR: 0.003 | TPR: 0.9409 | F1: 0.9688
2023-03-18 10:18:03,621 WARNING 	FPR: 0.01 | TPR: 0.9755 | F1: 0.9852
2023-03-18 10:18:03,644 WARNING 	FPR: 0.03 | TPR: 0.9902 | F1: 0.9880
2023-03-18 10:18:03,652 WARNING 	FPR: 0.1 | TPR: 0.9974 | F1: 0.9754
2023-03-18 10:18:03,652 WARNING  [!] Evaluating model on validation set...
2023-03-18 10:18:10,514 WARNING  [!] This fold metrics on validation set:
2023-03-18 10:18:10,518 WARNING 	AUC: 0.9965
2023-03-18 10:18:10,524 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 10:18:10,529 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 10:18:10,534 WARNING 	FPR: 0.001 | TPR: 0.7692 | F1: 0.8693
2023-03-18 10:18:10,540 WARNING 	FPR: 0.003 | TPR: 0.8905 | F1: 0.9414
2023-03-18 10:18:10,553 WARNING 	FPR: 0.01 | TPR: 0.9448 | F1: 0.9692
2023-03-18 10:18:10,563 WARNING 	FPR: 0.03 | TPR: 0.9758 | F1: 0.9807
2023-03-18 10:18:10,575 WARNING 	FPR: 0.1 | TPR: 0.9946 | F1: 0.9740
2023-03-18 10:18:10,766 WARNING  [2/3] Train set size: 50751, Validation set size: 25375
2023-03-18 10:18:13,198 WARNING  [!] Saved dataset splits to dataset_splits_1679131090.npz
2023-03-18 10:18:13,241 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.5173e6
2023-03-18 10:18:13,241 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 10:18:13,279 WARNING  [*] Started epoch: 1
2023-03-18 10:18:13,630 WARNING  [*] 10:18:13: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 3.182141 | Elapsed: 0.34s | FPR 0.0003 -> TPR 0.0312 & F1 0.0606 | AUC 0.4517
2023-03-18 10:18:23,598 WARNING  [*] 10:18:23: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.526420 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.3000 & F1 0.4615 | AUC 0.8142
2023-03-18 10:18:33,603 WARNING  [*] 10:18:33: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.296812 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.3429 & F1 0.5106 | AUC 0.9252
2023-03-18 10:18:43,564 WARNING  [*] 10:18:43: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.180349 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.7000 & F1 0.8235 | AUC 0.9662
2023-03-18 10:18:53,551 WARNING  [*] 10:18:53: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.252398 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.5441 & F1 0.7048 | AUC 0.9582
2023-03-18 10:19:03,535 WARNING  [*] 10:19:03: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.215261 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.5000 & F1 0.6667 | AUC 0.9623
2023-03-18 10:19:07,570 WARNING  [*] Sat Mar 18 10:19:07 2023:    1    | Tr.loss: 0.373245 | Elapsed:   54.29  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9002
2023-03-18 10:19:07,570 WARNING  [*] Started epoch: 2
2023-03-18 10:19:07,683 WARNING  [*] 10:19:07: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.216694 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.5606 & F1 0.7184 | AUC 0.9682
2023-03-18 10:19:17,678 WARNING  [*] 10:19:17: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.150335 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.8361 & F1 0.9107 | AUC 0.9870
2023-03-18 10:19:27,721 WARNING  [*] 10:19:27: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.141744 | Elapsed: 10.04s | FPR 0.0003 -> TPR 0.8485 & F1 0.9180 | AUC 0.9875
2023-03-18 10:19:37,756 WARNING  [*] 10:19:37: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.154128 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.9155 & F1 0.9559 | AUC 0.9840
2023-03-18 10:19:47,762 WARNING  [*] 10:19:47: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.086730 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.7971 & F1 0.8871 | AUC 0.9930
2023-03-18 10:19:57,756 WARNING  [*] 10:19:57: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.145982 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.7727 & F1 0.8718 | AUC 0.9889
2023-03-18 10:20:01,706 WARNING  [*] Sat Mar 18 10:20:01 2023:    2    | Tr.loss: 0.153566 | Elapsed:   54.14  s | FPR 0.0003 -> TPR: 0.34 & F1: 0.51 | AUC: 0.9836
2023-03-18 10:20:01,706 WARNING  [*] Started epoch: 3
2023-03-18 10:20:01,841 WARNING  [*] 10:20:01: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.137270 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.8714 & F1 0.9313 | AUC 0.9879
2023-03-18 10:20:11,886 WARNING  [*] 10:20:11: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.193385 | Elapsed: 10.04s | FPR 0.0003 -> TPR 0.7887 & F1 0.8819 | AUC 0.9825
2023-03-18 10:20:21,889 WARNING  [*] 10:20:21: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.029161 | Elapsed: 10.00s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 10:20:31,949 WARNING  [*] 10:20:31: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.077697 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9552 & F1 0.9771 | AUC 0.9973
2023-03-18 10:20:42,043 WARNING  [*] 10:20:42: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.061787 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.9429 & F1 0.9706 | AUC 0.9976
2023-03-18 10:20:52,089 WARNING  [*] 10:20:52: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.073732 | Elapsed: 10.04s | FPR 0.0003 -> TPR 0.9577 & F1 0.9784 | AUC 0.9961
2023-03-18 10:20:56,105 WARNING  [*] Sat Mar 18 10:20:56 2023:    3    | Tr.loss: 0.100942 | Elapsed:   54.40  s | FPR 0.0003 -> TPR: 0.50 & F1: 0.66 | AUC: 0.9929
2023-03-18 10:20:56,105 WARNING  [*] Started epoch: 4
2023-03-18 10:20:56,238 WARNING  [*] 10:20:56: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.060983 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9420 & F1 0.9701 | AUC 0.9979
2023-03-18 10:21:06,230 WARNING  [*] 10:21:06: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.064701 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9982
2023-03-18 10:21:16,264 WARNING  [*] 10:21:16: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.052718 | Elapsed: 10.03s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9986
2023-03-18 10:21:26,271 WARNING  [*] 10:21:26: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.113620 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.8095 & F1 0.8947 | AUC 0.9931
2023-03-18 10:21:36,279 WARNING  [*] 10:21:36: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.062573 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9740 & F1 0.9868 | AUC 0.9972
2023-03-18 10:21:46,340 WARNING  [*] 10:21:46: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.008112 | Elapsed: 10.05s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 10:21:50,276 WARNING  [*] Sat Mar 18 10:21:50 2023:    4    | Tr.loss: 0.079846 | Elapsed:   54.17  s | FPR 0.0003 -> TPR: 0.60 & F1: 0.75 | AUC: 0.9955
2023-03-18 10:21:50,276 WARNING  [*] Started epoch: 5
2023-03-18 10:21:50,394 WARNING  [*] 10:21:50: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.074916 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9960
2023-03-18 10:22:00,464 WARNING  [*] 10:22:00: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.092543 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.9859 & F1 0.9929 | AUC 0.9932
2023-03-18 10:22:10,445 WARNING  [*] 10:22:10: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.039123 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.9420 & F1 0.9701 | AUC 0.9981
2023-03-18 10:22:20,443 WARNING  [*] 10:22:20: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.157707 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.8451 & F1 0.9160 | AUC 0.9874
2023-03-18 10:22:30,472 WARNING  [*] 10:22:30: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.110197 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.9722 & F1 0.9859 | AUC 0.9931
2023-03-18 10:22:40,463 WARNING  [*] 10:22:40: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.025312 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.9863 & F1 0.9931 | AUC 0.9995
2023-03-18 10:22:44,455 WARNING  [*] Sat Mar 18 10:22:44 2023:    5    | Tr.loss: 0.068607 | Elapsed:   54.18  s | FPR 0.0003 -> TPR: 0.63 & F1: 0.78 | AUC: 0.9966
2023-03-18 10:22:44,456 WARNING  [*] Started epoch: 6
2023-03-18 10:22:44,595 WARNING  [*] 10:22:44: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.019260 | Elapsed: 0.13s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 10:22:54,610 WARNING  [*] 10:22:54: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.054559 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.9692 & F1 0.9844 | AUC 0.9987
2023-03-18 10:23:04,627 WARNING  [*] 10:23:04: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.109177 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9231 & F1 0.9600 | AUC 0.9947
2023-03-18 10:23:13,253 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 10:23:13,286 WARNING  [!] Sat Mar 18 10:23:13 2023: Dumped results:
                model       : 1679131090-model.torch
		train time  : 1679131090-trainTime.npy
		train losses: 1679131090-trainLosses.npy
		train AUC   : 1679131090-auc.npy
		train F1s   : 1679131090-trainF1s.npy
		train TPRs  : 1679131090-trainTPRs.npy
2023-03-18 10:23:13,319 WARNING  [!] Evaluating model on training set...
2023-03-18 10:23:27,057 WARNING  [!] This fold metrics on training set:
2023-03-18 10:23:27,065 WARNING 	AUC: 0.9986
2023-03-18 10:23:27,081 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 10:23:27,089 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 10:23:27,101 WARNING 	FPR: 0.001 | TPR: 0.9219 | F1: 0.9592
2023-03-18 10:23:27,120 WARNING 	FPR: 0.003 | TPR: 0.9520 | F1: 0.9747
2023-03-18 10:23:27,132 WARNING 	FPR: 0.01 | TPR: 0.9763 | F1: 0.9856
2023-03-18 10:23:27,137 WARNING 	FPR: 0.03 | TPR: 0.9914 | F1: 0.9885
2023-03-18 10:23:27,160 WARNING 	FPR: 0.1 | TPR: 0.9975 | F1: 0.9754
2023-03-18 10:23:27,161 WARNING  [!] Evaluating model on validation set...
2023-03-18 10:23:34,034 WARNING  [!] This fold metrics on validation set:
2023-03-18 10:23:34,039 WARNING 	AUC: 0.9965
2023-03-18 10:23:34,043 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 10:23:34,055 WARNING 	FPR: 0.0003 | TPR: 0.7884 | F1: 0.8816
2023-03-18 10:23:34,062 WARNING 	FPR: 0.001 | TPR: 0.8490 | F1: 0.9181
2023-03-18 10:23:34,070 WARNING 	FPR: 0.003 | TPR: 0.9011 | F1: 0.9473
2023-03-18 10:23:34,077 WARNING 	FPR: 0.01 | TPR: 0.9505 | F1: 0.9722
2023-03-18 10:23:34,084 WARNING 	FPR: 0.03 | TPR: 0.9775 | F1: 0.9815
2023-03-18 10:23:34,090 WARNING 	FPR: 0.1 | TPR: 0.9921 | F1: 0.9728
2023-03-18 10:23:34,258 WARNING  [3/3] Train set size: 50751, Validation set size: 25375
2023-03-18 10:23:36,662 WARNING  [!] Saved dataset splits to dataset_splits_1679131414.npz
2023-03-18 10:23:36,733 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.5173e6
2023-03-18 10:23:36,734 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 10:23:36,766 WARNING  [*] Started epoch: 1
2023-03-18 10:23:37,025 WARNING  [*] 10:23:37: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 2.152768 | Elapsed: 0.24s | FPR 0.0003 -> TPR 0.0455 & F1 0.0870 | AUC 0.5742
2023-03-18 10:23:47,048 WARNING  [*] 10:23:47: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.458241 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.2353 & F1 0.3810 | AUC 0.8621
2023-03-18 10:23:57,056 WARNING  [*] 10:23:57: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.386186 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.5821 & F1 0.7358 | AUC 0.8702
2023-03-18 10:24:07,094 WARNING  [*] 10:24:07: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.244762 | Elapsed: 10.03s | FPR 0.0003 -> TPR 0.4444 & F1 0.6154 | AUC 0.9474
2023-03-18 10:24:17,103 WARNING  [*] 10:24:17: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.294842 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.6000 & F1 0.7500 | AUC 0.9385
2023-03-18 10:24:27,110 WARNING  [*] 10:24:27: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.151836 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.6286 & F1 0.7719 | AUC 0.9786
2023-03-18 10:24:31,148 WARNING  [*] Sat Mar 18 10:24:31 2023:    1    | Tr.loss: 0.354773 | Elapsed:   54.38  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.02 | AUC: 0.9086
2023-03-18 10:24:31,148 WARNING  [*] Started epoch: 2
2023-03-18 10:24:31,287 WARNING  [*] 10:24:31: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.176701 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.8824 & F1 0.9375 | AUC 0.9816
2023-03-18 10:24:41,311 WARNING  [*] 10:24:41: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.134786 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.8594 & F1 0.9244 | AUC 0.9835
2023-03-18 10:24:51,364 WARNING  [*] 10:24:51: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.096450 | Elapsed: 10.04s | FPR 0.0003 -> TPR 0.9041 & F1 0.9496 | AUC 0.9954
2023-03-18 10:25:01,375 WARNING  [*] 10:25:01: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.125634 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9487 & F1 0.9737 | AUC 0.9889
2023-03-18 10:25:11,413 WARNING  [*] 10:25:11: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.226775 | Elapsed: 10.03s | FPR 0.0003 -> TPR 0.7083 & F1 0.8293 | AUC 0.9673
2023-03-18 10:25:21,417 WARNING  [*] 10:25:21: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.125439 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9868 & F1 0.9934 | AUC 0.9989
2023-03-18 10:25:25,393 WARNING  [*] Sat Mar 18 10:25:25 2023:    2    | Tr.loss: 0.160253 | Elapsed:   54.24  s | FPR 0.0003 -> TPR: 0.29 & F1: 0.45 | AUC: 0.9822
2023-03-18 10:25:25,394 WARNING  [*] Started epoch: 3
2023-03-18 10:25:25,550 WARNING  [*] 10:25:25: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.137848 | Elapsed: 0.14s | FPR 0.0003 -> TPR 0.9385 & F1 0.9683 | AUC 0.9856
2023-03-18 10:25:35,617 WARNING  [*] 10:25:35: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.132231 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.9306 & F1 0.9640 | AUC 0.9851
2023-03-18 10:25:45,633 WARNING  [*] 10:25:45: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.063260 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.9853 & F1 0.9926 | AUC 0.9977
2023-03-18 10:25:55,646 WARNING  [*] 10:25:55: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.145815 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.5333 & F1 0.6957 | AUC 0.9871
2023-03-18 10:26:05,672 WARNING  [*] 10:26:05: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.081403 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9733 & F1 0.9865 | AUC 0.9957
2023-03-18 10:26:15,700 WARNING  [*] 10:26:15: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.142156 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.9091 & F1 0.9524 | AUC 0.9866
2023-03-18 10:26:19,703 WARNING  [*] Sat Mar 18 10:26:19 2023:    3    | Tr.loss: 0.108402 | Elapsed:   54.31  s | FPR 0.0003 -> TPR: 0.48 & F1: 0.65 | AUC: 0.9918
2023-03-18 10:26:19,703 WARNING  [*] Started epoch: 4
2023-03-18 10:26:19,836 WARNING  [*] 10:26:19: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.130915 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.4483 & F1 0.6190 | AUC 0.9855
2023-03-18 10:26:29,829 WARNING  [*] 10:26:29: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.092534 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9474 & F1 0.9730 | AUC 0.9956
2023-03-18 10:26:39,869 WARNING  [*] 10:26:39: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.043534 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.9863 & F1 0.9931 | AUC 0.9990
2023-03-18 10:26:49,882 WARNING  [*] 10:26:49: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.033778 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.9839 & F1 0.9919 | AUC 0.9983
2023-03-18 10:26:59,896 WARNING  [*] 10:26:59: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.032771 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9706 & F1 0.9851 | AUC 0.9982
2023-03-18 10:27:09,949 WARNING  [*] 10:27:09: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.102934 | Elapsed: 10.04s | FPR 0.0003 -> TPR 0.9048 & F1 0.9500 | AUC 0.9966
2023-03-18 10:27:13,915 WARNING  [*] Sat Mar 18 10:27:13 2023:    4    | Tr.loss: 0.084757 | Elapsed:   54.21  s | FPR 0.0003 -> TPR: 0.53 & F1: 0.69 | AUC: 0.9949
2023-03-18 10:27:13,915 WARNING  [*] Started epoch: 5
2023-03-18 10:27:14,034 WARNING  [*] 10:27:14: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.071641 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9697 & F1 0.9846 | AUC 0.9949
2023-03-18 10:27:24,103 WARNING  [*] 10:27:24: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.020348 | Elapsed: 10.07s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 10:27:34,128 WARNING  [*] 10:27:34: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.086558 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.9242 & F1 0.9606 | AUC 0.9942
2023-03-18 10:27:44,157 WARNING  [*] 10:27:44: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.109692 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.8429 & F1 0.9147 | AUC 0.9933
2023-03-18 10:27:54,184 WARNING  [*] 10:27:54: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.013756 | Elapsed: 10.03s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 10:28:04,220 WARNING  [*] 10:28:04: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.066398 | Elapsed: 10.03s | FPR 0.0003 -> TPR 0.9571 & F1 0.9781 | AUC 0.9971
2023-03-18 10:28:08,248 WARNING  [*] Sat Mar 18 10:28:08 2023:    5    | Tr.loss: 0.069177 | Elapsed:   54.33  s | FPR 0.0003 -> TPR: 0.69 & F1: 0.82 | AUC: 0.9966
2023-03-18 10:28:08,248 WARNING  [*] Started epoch: 6
2023-03-18 10:28:08,410 WARNING  [*] 10:28:08: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.058527 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.9839 & F1 0.9919 | AUC 0.9991
2023-03-18 10:28:18,424 WARNING  [*] 10:28:18: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.074844 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.9500 & F1 0.9744 | AUC 0.9988
2023-03-18 10:28:28,432 WARNING  [*] 10:28:28: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.090137 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9857 & F1 0.9928 | AUC 0.9929
2023-03-18 10:28:36,788 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 10:28:36,823 WARNING  [!] Sat Mar 18 10:28:36 2023: Dumped results:
                model       : 1679131414-model.torch
		train time  : 1679131414-trainTime.npy
		train losses: 1679131414-trainLosses.npy
		train AUC   : 1679131414-auc.npy
		train F1s   : 1679131414-trainF1s.npy
		train TPRs  : 1679131414-trainTPRs.npy
2023-03-18 10:28:36,848 WARNING  [!] Evaluating model on training set...
2023-03-18 10:28:50,610 WARNING  [!] This fold metrics on training set:
2023-03-18 10:28:50,621 WARNING 	AUC: 0.9984
2023-03-18 10:28:50,640 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 10:28:50,646 WARNING 	FPR: 0.0003 | TPR: 0.7962 | F1: 0.8865
2023-03-18 10:28:50,657 WARNING 	FPR: 0.001 | TPR: 0.8889 | F1: 0.9410
2023-03-18 10:28:50,679 WARNING 	FPR: 0.003 | TPR: 0.9401 | F1: 0.9684
2023-03-18 10:28:50,686 WARNING 	FPR: 0.01 | TPR: 0.9714 | F1: 0.9831
2023-03-18 10:28:50,693 WARNING 	FPR: 0.03 | TPR: 0.9908 | F1: 0.9882
2023-03-18 10:28:50,717 WARNING 	FPR: 0.1 | TPR: 0.9973 | F1: 0.9757
2023-03-18 10:28:50,717 WARNING  [!] Evaluating model on validation set...
2023-03-18 10:28:57,600 WARNING  [!] This fold metrics on validation set:
2023-03-18 10:28:57,605 WARNING 	AUC: 0.9961
2023-03-18 10:28:57,608 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 10:28:57,614 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 10:28:57,625 WARNING 	FPR: 0.001 | TPR: 0.8001 | F1: 0.8887
2023-03-18 10:28:57,632 WARNING 	FPR: 0.003 | TPR: 0.8650 | F1: 0.9269
2023-03-18 10:28:57,638 WARNING 	FPR: 0.01 | TPR: 0.9373 | F1: 0.9652
2023-03-18 10:28:57,641 WARNING 	FPR: 0.03 | TPR: 0.9775 | F1: 0.9815
2023-03-18 10:28:57,648 WARNING 	FPR: 0.1 | TPR: 0.9930 | F1: 0.9731
2023-03-18 10:28:57,770 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_5000_limNone_r1763_t5\5000_metrics_validation.json
2023-03-18 10:28:57,772 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_5000_limNone_r1763_t5\5000_metrics_training.json
2023-03-18 10:28:57,772 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9964
	FPR: 0.0001 -- TPR: 0.0000 -- F1: 0.0000
	FPR: 0.0003 -- TPR: 0.2628 -- F1: 0.2939
	FPR:  0.001 -- TPR: 0.8061 -- F1: 0.8920
	FPR:  0.003 -- TPR: 0.8856 -- F1: 0.9385
	FPR:   0.01 -- TPR: 0.9442 -- F1: 0.9689
	FPR:   0.03 -- TPR: 0.9769 -- F1: 0.9812
	FPR:    0.1 -- TPR: 0.9932 -- F1: 0.9733

2023-03-18 10:28:57,839 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-03-18 10:33:01,562 WARNING Finished... Took: 243.72s
2023-03-18 10:33:01,562 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-03-18 10:38:17,593 WARNING Finished... Took: 316.03s
2023-03-18 10:38:17,593 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-03-18 10:39:59,089 WARNING Finished... Took: 101.50s
2023-03-18 10:39:59,089 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-03-18 10:41:36,342 WARNING Finished... Took: 97.25s
2023-03-18 10:41:36,342 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-03-18 10:42:35,998 WARNING Finished... Took: 59.66s
2023-03-18 10:42:35,998 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-03-18 10:45:40,741 WARNING Finished... Took: 184.74s
2023-03-18 10:45:40,741 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-03-18 10:46:26,141 WARNING Finished... Took: 45.40s
2023-03-18 10:46:26,141 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-03-18 10:48:47,108 WARNING Finished... Took: 140.97s
2023-03-18 10:48:47,108 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-03-18 10:48:48,868 WARNING Finished... Took: 1.76s
2023-03-18 10:48:48,882 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_10000_seqlen_512\y_train_full.npy
2023-03-18 10:48:49,082 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_10000_seqlen_512\y_names_train_full.json
2023-03-18 10:48:49,082 WARNING  [!] Initialized tokenizer without pre-trained model.
	You need to train tokenizer with .train() or specify 'model_path=' during initialization!
2023-03-18 10:48:49,082 WARNING  [*] Initializing tokenizer training...
2023-03-18 10:48:49,082 WARNING  [*] Data preparation for SentencePiece tokenizer...
2023-03-18 10:51:24,974 WARNING  [*] Saving to disk...
2023-03-18 10:51:29,898 WARNING  [!] Training tokenizer with command: --input=out_vocab_1679128542\nebula_vocab_10000_seqlen_512\tokenizer_10000_trainset_1679133084.txt --model_prefix=out_vocab_1679128542\nebula_vocab_10000_seqlen_512\tokenizer_10000 --vocab_size=10000 --model_type=bpe --split_by_number=False --max_sentence_length=4192 --max_sentencepiece_length=64
2023-03-18 10:53:04,800 WARNING  [!] Loaded vocab with size 10001 from out_vocab_1679128542\nebula_vocab_10000_seqlen_512\tokenizer_10000.vocab
2023-03-18 10:53:05,239 WARNING  [*] Encoding and padding...
2023-03-18 10:59:26,304 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_10000_seqlen_512\x_train_full.npy
2023-03-18 10:59:29,878 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-03-18 11:00:04,773 WARNING Finished... Took: 34.89s
2023-03-18 11:00:04,773 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-03-18 11:01:43,815 WARNING Finished... Took: 99.04s
2023-03-18 11:01:43,815 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-03-18 11:02:09,446 WARNING Finished... Took: 25.63s
2023-03-18 11:02:09,446 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-03-18 11:02:13,644 WARNING Finished... Took: 4.20s
2023-03-18 11:02:13,644 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-03-18 11:02:23,984 WARNING Finished... Took: 10.34s
2023-03-18 11:02:23,984 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-03-18 11:04:38,767 WARNING Finished... Took: 134.78s
2023-03-18 11:04:38,768 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-03-18 11:04:55,713 WARNING Finished... Took: 16.94s
2023-03-18 11:04:55,713 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-03-18 11:05:07,349 WARNING Finished... Took: 11.64s
2023-03-18 11:05:07,349 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-03-18 11:05:07,815 WARNING Finished... Took: 0.47s
2023-03-18 11:05:07,825 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_10000_seqlen_512\y_test_full.npy
2023-03-18 11:05:07,865 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_10000_seqlen_512\y_names_test_full.json
2023-03-18 11:05:07,899 WARNING  [!] Successfully loaded pre-trained tokenizer model!
2023-03-18 11:05:07,918 WARNING  [!] Loaded vocab with size 10001 from out_vocab_1679128542\nebula_vocab_10000_seqlen_512\tokenizer_10000_vocab.json
2023-03-18 11:05:07,918 WARNING  [*] Encoding and padding...
2023-03-18 11:06:19,953 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_10000_seqlen_512\x_test_full.npy
2023-03-18 11:06:20,778 WARNING  [!!!] Starting CV over 10000!
2023-03-18 11:06:20,874 WARNING  [!] Training time budget: 300min
2023-03-18 11:06:20,874 WARNING  [!] Model config: {'vocab_size': 10001, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-03-18 11:06:20,942 WARNING  [1/3] Train set size: 50750, Validation set size: 25376
2023-03-18 11:06:23,066 WARNING  [!] Saved dataset splits to dataset_splits_1679133980.npz
2023-03-18 11:06:23,125 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.8373e6
2023-03-18 11:06:23,125 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 11:06:23,155 WARNING  [*] Started epoch: 1
2023-03-18 11:06:23,543 WARNING  [*] 11:06:23: Train Epoch: 1 [  0  /50750 (0 %)] | Loss: 2.287597 | Elapsed: 0.39s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.4893
2023-03-18 11:06:33,217 WARNING  [*] 11:06:33: Train Epoch: 1 [9600 /50750 (19%)] | Loss: 0.417764 | Elapsed: 9.67s | FPR 0.0003 -> TPR 0.2308 & F1 0.3750 | AUC 0.8712
2023-03-18 11:06:42,970 WARNING  [*] 11:06:42: Train Epoch: 1 [19200/50750 (38%)] | Loss: 0.296157 | Elapsed: 9.74s | FPR 0.0003 -> TPR 0.4595 & F1 0.6296 | AUC 0.9090
2023-03-18 11:06:52,756 WARNING  [*] 11:06:52: Train Epoch: 1 [28800/50750 (57%)] | Loss: 0.303665 | Elapsed: 9.79s | FPR 0.0003 -> TPR 0.7101 & F1 0.8305 | AUC 0.9350
2023-03-18 11:07:02,575 WARNING  [*] 11:07:02: Train Epoch: 1 [38400/50750 (76%)] | Loss: 0.202877 | Elapsed: 9.81s | FPR 0.0003 -> TPR 0.5857 & F1 0.7387 | AUC 0.9624
2023-03-18 11:07:12,407 WARNING  [*] 11:07:12: Train Epoch: 1 [48000/50750 (95%)] | Loss: 0.149128 | Elapsed: 9.83s | FPR 0.0003 -> TPR 0.9014 & F1 0.9481 | AUC 0.9879
2023-03-18 11:07:16,158 WARNING  [*] Sat Mar 18 11:07:16 2023:    1    | Tr.loss: 0.364849 | Elapsed:   53.00  s | FPR 0.0003 -> TPR: 0.01 & F1: 0.01 | AUC: 0.9069
2023-03-18 11:07:16,158 WARNING  [*] Started epoch: 2
2023-03-18 11:07:16,280 WARNING  [*] 11:07:16: Train Epoch: 2 [  0  /50750 (0 %)] | Loss: 0.118984 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9710 & F1 0.9853 | AUC 0.9962
2023-03-18 11:07:26,186 WARNING  [*] 11:07:26: Train Epoch: 2 [9600 /50750 (19%)] | Loss: 0.165808 | Elapsed: 9.90s | FPR 0.0003 -> TPR 0.6818 & F1 0.8108 | AUC 0.9844
2023-03-18 11:07:36,072 WARNING  [*] 11:07:36: Train Epoch: 2 [19200/50750 (38%)] | Loss: 0.128692 | Elapsed: 9.89s | FPR 0.0003 -> TPR 0.8971 & F1 0.9457 | AUC 0.9926
2023-03-18 11:07:46,026 WARNING  [*] 11:07:46: Train Epoch: 2 [28800/50750 (57%)] | Loss: 0.077639 | Elapsed: 9.95s | FPR 0.0003 -> TPR 0.9444 & F1 0.9714 | AUC 0.9901
2023-03-18 11:07:55,958 WARNING  [*] 11:07:55: Train Epoch: 2 [38400/50750 (76%)] | Loss: 0.039883 | Elapsed: 9.92s | FPR 0.0003 -> TPR 0.9733 & F1 0.9865 | AUC 0.9952
2023-03-18 11:08:05,887 WARNING  [*] 11:08:05: Train Epoch: 2 [48000/50750 (95%)] | Loss: 0.038459 | Elapsed: 9.92s | FPR 0.0003 -> TPR 0.9853 & F1 0.9926 | AUC 0.9995
2023-03-18 11:08:09,926 WARNING  [*] Sat Mar 18 11:08:09 2023:    2    | Tr.loss: 0.142589 | Elapsed:   53.77  s | FPR 0.0003 -> TPR: 0.38 & F1: 0.55 | AUC: 0.9859
2023-03-18 11:08:09,926 WARNING  [*] Started epoch: 3
2023-03-18 11:08:10,082 WARNING  [*] 11:08:10: Train Epoch: 3 [  0  /50750 (0 %)] | Loss: 0.107118 | Elapsed: 0.15s | FPR 0.0003 -> TPR 0.9464 & F1 0.9725 | AUC 0.9911
2023-03-18 11:08:20,036 WARNING  [*] 11:08:20: Train Epoch: 3 [9600 /50750 (19%)] | Loss: 0.200283 | Elapsed: 9.95s | FPR 0.0003 -> TPR 0.8986 & F1 0.9466 | AUC 0.9785
2023-03-18 11:08:29,979 WARNING  [*] 11:08:29: Train Epoch: 3 [19200/50750 (38%)] | Loss: 0.117093 | Elapsed: 9.93s | FPR 0.0003 -> TPR 0.9275 & F1 0.9624 | AUC 0.9921
2023-03-18 11:08:39,927 WARNING  [*] 11:08:39: Train Epoch: 3 [28800/50750 (57%)] | Loss: 0.054634 | Elapsed: 9.94s | FPR 0.0003 -> TPR 0.9683 & F1 0.9839 | AUC 0.9987
2023-03-18 11:08:49,877 WARNING  [*] 11:08:49: Train Epoch: 3 [38400/50750 (76%)] | Loss: 0.038321 | Elapsed: 9.95s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9996
2023-03-18 11:08:59,923 WARNING  [*] 11:08:59: Train Epoch: 3 [48000/50750 (95%)] | Loss: 0.029729 | Elapsed: 10.03s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:09:03,866 WARNING  [*] Sat Mar 18 11:09:03 2023:    3    | Tr.loss: 0.087486 | Elapsed:   53.94  s | FPR 0.0003 -> TPR: 0.52 & F1: 0.68 | AUC: 0.9945
2023-03-18 11:09:03,866 WARNING  [*] Started epoch: 4
2023-03-18 11:09:03,983 WARNING  [*] 11:09:03: Train Epoch: 4 [  0  /50750 (0 %)] | Loss: 0.060882 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9706 & F1 0.9851 | AUC 0.9979
2023-03-18 11:09:13,940 WARNING  [*] 11:09:13: Train Epoch: 4 [9600 /50750 (19%)] | Loss: 0.050519 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.9483 & F1 0.9735 | AUC 0.9914
2023-03-18 11:09:23,907 WARNING  [*] 11:09:23: Train Epoch: 4 [19200/50750 (38%)] | Loss: 0.046835 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9991
2023-03-18 11:09:33,857 WARNING  [*] 11:09:33: Train Epoch: 4 [28800/50750 (57%)] | Loss: 0.116101 | Elapsed: 9.95s | FPR 0.0003 -> TPR 0.9180 & F1 0.9573 | AUC 0.9912
2023-03-18 11:09:43,835 WARNING  [*] 11:09:43: Train Epoch: 4 [38400/50750 (76%)] | Loss: 0.029427 | Elapsed: 9.98s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:09:53,830 WARNING  [*] 11:09:53: Train Epoch: 4 [48000/50750 (95%)] | Loss: 0.019795 | Elapsed: 9.99s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:09:57,869 WARNING  [*] Sat Mar 18 11:09:57 2023:    4    | Tr.loss: 0.063498 | Elapsed:   54.00  s | FPR 0.0003 -> TPR: 0.72 & F1: 0.84 | AUC: 0.9971
2023-03-18 11:09:57,869 WARNING  [*] Started epoch: 5
2023-03-18 11:09:57,987 WARNING  [*] 11:09:57: Train Epoch: 5 [  0  /50750 (0 %)] | Loss: 0.083432 | Elapsed: 0.10s | FPR 0.0003 -> TPR 0.9467 & F1 0.9726 | AUC 0.9943
2023-03-18 11:10:07,959 WARNING  [*] 11:10:07: Train Epoch: 5 [9600 /50750 (19%)] | Loss: 0.102699 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.9394 & F1 0.9688 | AUC 0.9955
2023-03-18 11:10:17,927 WARNING  [*] 11:10:17: Train Epoch: 5 [19200/50750 (38%)] | Loss: 0.015023 | Elapsed: 9.95s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9995
2023-03-18 11:10:27,895 WARNING  [*] 11:10:27: Train Epoch: 5 [28800/50750 (57%)] | Loss: 0.068571 | Elapsed: 9.96s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:10:37,873 WARNING  [*] 11:10:37: Train Epoch: 5 [38400/50750 (76%)] | Loss: 0.037523 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9995
2023-03-18 11:10:47,852 WARNING  [*] 11:10:47: Train Epoch: 5 [48000/50750 (95%)] | Loss: 0.002721 | Elapsed: 9.98s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:10:51,894 WARNING  [*] Sat Mar 18 11:10:51 2023:    5    | Tr.loss: 0.051167 | Elapsed:   54.03  s | FPR 0.0003 -> TPR: 0.82 & F1: 0.90 | AUC: 0.9980
2023-03-18 11:10:51,894 WARNING  [*] Started epoch: 6
2023-03-18 11:10:52,049 WARNING  [*] 11:10:52: Train Epoch: 6 [  0  /50750 (0 %)] | Loss: 0.042256 | Elapsed: 0.14s | FPR 0.0003 -> TPR 0.9859 & F1 0.9929 | AUC 0.9989
2023-03-18 11:11:02,029 WARNING  [*] 11:11:02: Train Epoch: 6 [9600 /50750 (19%)] | Loss: 0.027081 | Elapsed: 9.97s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:11:11,991 WARNING  [*] 11:11:11: Train Epoch: 6 [19200/50750 (38%)] | Loss: 0.043373 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.9672 & F1 0.9833 | AUC 0.9983
2023-03-18 11:11:21,968 WARNING  [*] 11:11:21: Train Epoch: 6 [28800/50750 (57%)] | Loss: 0.013889 | Elapsed: 9.98s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:11:23,168 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 11:11:23,197 WARNING  [!] Sat Mar 18 11:11:23 2023: Dumped results:
                model       : 1679133980-model.torch
		train time  : 1679133980-trainTime.npy
		train losses: 1679133980-trainLosses.npy
		train AUC   : 1679133980-auc.npy
		train F1s   : 1679133980-trainF1s.npy
		train TPRs  : 1679133980-trainTPRs.npy
2023-03-18 11:11:23,234 WARNING  [!] Evaluating model on training set...
2023-03-18 11:11:36,909 WARNING  [!] This fold metrics on training set:
2023-03-18 11:11:36,934 WARNING 	AUC: 0.9991
2023-03-18 11:11:36,942 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:11:36,947 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:11:36,971 WARNING 	FPR: 0.001 | TPR: 0.9571 | F1: 0.9779
2023-03-18 11:11:36,975 WARNING 	FPR: 0.003 | TPR: 0.9761 | F1: 0.9872
2023-03-18 11:11:36,987 WARNING 	FPR: 0.01 | TPR: 0.9883 | F1: 0.9918
2023-03-18 11:11:37,011 WARNING 	FPR: 0.03 | TPR: 0.9947 | F1: 0.9902
2023-03-18 11:11:37,016 WARNING 	FPR: 0.1 | TPR: 0.9985 | F1: 0.9765
2023-03-18 11:11:37,016 WARNING  [!] Evaluating model on validation set...
2023-03-18 11:11:43,877 WARNING  [!] This fold metrics on validation set:
2023-03-18 11:11:43,895 WARNING 	AUC: 0.9963
2023-03-18 11:11:43,903 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:11:43,912 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:11:43,919 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:11:43,926 WARNING 	FPR: 0.003 | TPR: 0.8950 | F1: 0.9439
2023-03-18 11:11:43,931 WARNING 	FPR: 0.01 | TPR: 0.9597 | F1: 0.9770
2023-03-18 11:11:43,938 WARNING 	FPR: 0.03 | TPR: 0.9840 | F1: 0.9849
2023-03-18 11:11:43,952 WARNING 	FPR: 0.1 | TPR: 0.9939 | F1: 0.9735
2023-03-18 11:11:44,118 WARNING  [2/3] Train set size: 50751, Validation set size: 25375
2023-03-18 11:11:46,467 WARNING  [!] Saved dataset splits to dataset_splits_1679134304.npz
2023-03-18 11:11:46,513 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.8373e6
2023-03-18 11:11:46,513 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 11:11:46,551 WARNING  [*] Started epoch: 1
2023-03-18 11:11:47,001 WARNING  [*] 11:11:47: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 2.220006 | Elapsed: 0.43s | FPR 0.0003 -> TPR 0.0333 & F1 0.0645 | AUC 0.5028
2023-03-18 11:11:56,976 WARNING  [*] 11:11:56: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.543345 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.2836 & F1 0.4419 | AUC 0.8431
2023-03-18 11:12:06,954 WARNING  [*] 11:12:06: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.344202 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.7246 & F1 0.8403 | AUC 0.9374
2023-03-18 11:12:16,929 WARNING  [*] 11:12:16: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.349644 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.3030 & F1 0.4651 | AUC 0.9109
2023-03-18 11:12:26,901 WARNING  [*] 11:12:26: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.283630 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.3611 & F1 0.5306 | AUC 0.9325
2023-03-18 11:12:36,886 WARNING  [*] 11:12:36: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.252388 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.6912 & F1 0.8174 | AUC 0.9674
2023-03-18 11:12:40,931 WARNING  [*] Sat Mar 18 11:12:40 2023:    1    | Tr.loss: 0.345877 | Elapsed:   54.38  s | FPR 0.0003 -> TPR: 0.05 & F1: 0.09 | AUC: 0.9135
2023-03-18 11:12:40,932 WARNING  [*] Started epoch: 2
2023-03-18 11:12:41,075 WARNING  [*] 11:12:41: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.072176 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.9242 & F1 0.9606 | AUC 0.9975
2023-03-18 11:12:51,094 WARNING  [*] 11:12:51: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.210589 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.8033 & F1 0.8909 | AUC 0.9811
2023-03-18 11:13:01,084 WARNING  [*] 11:13:01: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.264322 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.6301 & F1 0.7731 | AUC 0.9630
2023-03-18 11:13:11,080 WARNING  [*] 11:13:11: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.113611 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.8551 & F1 0.9219 | AUC 0.9921
2023-03-18 11:13:21,072 WARNING  [*] 11:13:21: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.075608 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9692 & F1 0.9844 | AUC 0.9978
2023-03-18 11:13:31,080 WARNING  [*] 11:13:31: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.048093 | Elapsed: 10.00s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:13:35,052 WARNING  [*] Sat Mar 18 11:13:35 2023:    2    | Tr.loss: 0.129582 | Elapsed:   54.12  s | FPR 0.0003 -> TPR: 0.38 & F1: 0.55 | AUC: 0.9882
2023-03-18 11:13:35,052 WARNING  [*] Started epoch: 3
2023-03-18 11:13:35,183 WARNING  [*] 11:13:35: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.135692 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.8889 & F1 0.9412 | AUC 0.9861
2023-03-18 11:13:45,222 WARNING  [*] 11:13:45: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.137678 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.8788 & F1 0.9355 | AUC 0.9911
2023-03-18 11:13:55,212 WARNING  [*] 11:13:55: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.085438 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9048 & F1 0.9500 | AUC 0.9957
2023-03-18 11:14:05,216 WARNING  [*] 11:14:05: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.230279 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.4154 & F1 0.5870 | AUC 0.9723
2023-03-18 11:14:15,247 WARNING  [*] 11:14:15: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.062151 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.9589 & F1 0.9790 | AUC 0.9980
2023-03-18 11:14:25,247 WARNING  [*] 11:14:25: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.099414 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9667 & F1 0.9831 | AUC 0.9975
2023-03-18 11:14:29,253 WARNING  [*] Sat Mar 18 11:14:29 2023:    3    | Tr.loss: 0.084708 | Elapsed:   54.20  s | FPR 0.0003 -> TPR: 0.43 & F1: 0.60 | AUC: 0.9948
2023-03-18 11:14:29,254 WARNING  [*] Started epoch: 4
2023-03-18 11:14:29,367 WARNING  [*] 11:14:29: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.104750 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9508 & F1 0.9748 | AUC 0.9930
2023-03-18 11:14:39,374 WARNING  [*] 11:14:39: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.051198 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9991
2023-03-18 11:14:49,418 WARNING  [*] 11:14:49: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.036829 | Elapsed: 10.04s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:14:59,419 WARNING  [*] 11:14:59: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.038568 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9859 & F1 0.9929 | AUC 0.9995
2023-03-18 11:15:09,414 WARNING  [*] 11:15:09: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.085233 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9718 & F1 0.9857 | AUC 0.9947
2023-03-18 11:15:19,410 WARNING  [*] 11:15:19: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.052369 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9718 & F1 0.9857 | AUC 0.9976
2023-03-18 11:15:23,355 WARNING  [*] Sat Mar 18 11:15:23 2023:    4    | Tr.loss: 0.065344 | Elapsed:   54.10  s | FPR 0.0003 -> TPR: 0.57 & F1: 0.73 | AUC: 0.9968
2023-03-18 11:15:23,355 WARNING  [*] Started epoch: 5
2023-03-18 11:15:23,492 WARNING  [*] 11:15:23: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.040588 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9985
2023-03-18 11:15:33,556 WARNING  [*] 11:15:33: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.059760 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9841 & F1 0.9920 | AUC 0.9979
2023-03-18 11:15:43,574 WARNING  [*] 11:15:43: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.078541 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.9589 & F1 0.9790 | AUC 0.9964
2023-03-18 11:15:53,587 WARNING  [*] 11:15:53: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.050392 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9987
2023-03-18 11:16:03,575 WARNING  [*] 11:16:03: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.050718 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.9710 & F1 0.9853 | AUC 0.9991
2023-03-18 11:16:13,555 WARNING  [*] 11:16:13: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.023635 | Elapsed: 9.98s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:16:17,566 WARNING  [*] Sat Mar 18 11:16:17 2023:    5    | Tr.loss: 0.053516 | Elapsed:   54.21  s | FPR 0.0003 -> TPR: 0.62 & F1: 0.77 | AUC: 0.9978
2023-03-18 11:16:17,566 WARNING  [*] Started epoch: 6
2023-03-18 11:16:17,682 WARNING  [*] 11:16:17: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.009019 | Elapsed: 0.11s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:16:27,676 WARNING  [*] 11:16:27: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.040140 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9996
2023-03-18 11:16:37,637 WARNING  [*] 11:16:37: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.065189 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.9538 & F1 0.9764 | AUC 0.9987
2023-03-18 11:16:46,529 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 11:16:46,556 WARNING  [!] Sat Mar 18 11:16:46 2023: Dumped results:
                model       : 1679134304-model.torch
		train time  : 1679134304-trainTime.npy
		train losses: 1679134304-trainLosses.npy
		train AUC   : 1679134304-auc.npy
		train F1s   : 1679134304-trainF1s.npy
		train TPRs  : 1679134304-trainTPRs.npy
2023-03-18 11:16:46,582 WARNING  [!] Evaluating model on training set...
2023-03-18 11:17:00,322 WARNING  [!] This fold metrics on training set:
2023-03-18 11:17:00,346 WARNING 	AUC: 0.9991
2023-03-18 11:17:00,359 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:17:00,371 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:17:00,376 WARNING 	FPR: 0.001 | TPR: 0.9401 | F1: 0.9689
2023-03-18 11:17:00,401 WARNING 	FPR: 0.003 | TPR: 0.9644 | F1: 0.9812
2023-03-18 11:17:00,404 WARNING 	FPR: 0.01 | TPR: 0.9881 | F1: 0.9916
2023-03-18 11:17:00,416 WARNING 	FPR: 0.03 | TPR: 0.9948 | F1: 0.9903
2023-03-18 11:17:00,442 WARNING 	FPR: 0.1 | TPR: 0.9984 | F1: 0.9771
2023-03-18 11:17:00,443 WARNING  [!] Evaluating model on validation set...
2023-03-18 11:17:07,314 WARNING  [!] This fold metrics on validation set:
2023-03-18 11:17:07,320 WARNING 	AUC: 0.9971
2023-03-18 11:17:07,324 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:17:07,335 WARNING 	FPR: 0.0003 | TPR: 0.8089 | F1: 0.8943
2023-03-18 11:17:07,342 WARNING 	FPR: 0.001 | TPR: 0.8587 | F1: 0.9237
2023-03-18 11:17:07,350 WARNING 	FPR: 0.003 | TPR: 0.9097 | F1: 0.9520
2023-03-18 11:17:07,355 WARNING 	FPR: 0.01 | TPR: 0.9644 | F1: 0.9795
2023-03-18 11:17:07,365 WARNING 	FPR: 0.03 | TPR: 0.9840 | F1: 0.9848
2023-03-18 11:17:07,371 WARNING 	FPR: 0.1 | TPR: 0.9929 | F1: 0.9732
2023-03-18 11:17:07,509 WARNING  [3/3] Train set size: 50751, Validation set size: 25375
2023-03-18 11:17:09,827 WARNING  [!] Saved dataset splits to dataset_splits_1679134627.npz
2023-03-18 11:17:09,871 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 2.8373e6
2023-03-18 11:17:09,871 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 11:17:09,910 WARNING  [*] Started epoch: 1
2023-03-18 11:17:10,160 WARNING  [*] 11:17:10: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 1.874421 | Elapsed: 0.23s | FPR 0.0003 -> TPR 0.0135 & F1 0.0267 | AUC 0.5347
2023-03-18 11:17:20,147 WARNING  [*] 11:17:20: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.420277 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.2923 & F1 0.4524 | AUC 0.8782
2023-03-18 11:17:30,114 WARNING  [*] 11:17:30: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.236276 | Elapsed: 9.95s | FPR 0.0003 -> TPR 0.7164 & F1 0.8348 | AUC 0.9593
2023-03-18 11:17:40,089 WARNING  [*] 11:17:40: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.280302 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.7973 & F1 0.8872 | AUC 0.9501
2023-03-18 11:17:50,142 WARNING  [*] 11:17:50: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.232719 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.7895 & F1 0.8824 | AUC 0.9846
2023-03-18 11:18:00,134 WARNING  [*] 11:18:00: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.089863 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9982
2023-03-18 11:18:04,153 WARNING  [*] Sat Mar 18 11:18:04 2023:    1    | Tr.loss: 0.350264 | Elapsed:   54.24  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9152
2023-03-18 11:18:04,154 WARNING  [*] Started epoch: 2
2023-03-18 11:18:04,296 WARNING  [*] 11:18:04: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.283071 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.8182 & F1 0.9000 | AUC 0.9591
2023-03-18 11:18:14,302 WARNING  [*] 11:18:14: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.199059 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.8611 & F1 0.9254 | AUC 0.9782
2023-03-18 11:18:24,281 WARNING  [*] 11:18:24: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.114576 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.8889 & F1 0.9412 | AUC 0.9901
2023-03-18 11:18:34,255 WARNING  [*] 11:18:34: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.172027 | Elapsed: 9.97s | FPR 0.0003 -> TPR 0.8788 & F1 0.9355 | AUC 0.9857
2023-03-18 11:18:44,231 WARNING  [*] 11:18:44: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.113970 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.9552 & F1 0.9771 | AUC 0.9914
2023-03-18 11:18:54,243 WARNING  [*] 11:18:54: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.100793 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9077 & F1 0.9516 | AUC 0.9960
2023-03-18 11:18:58,218 WARNING  [*] Sat Mar 18 11:18:58 2023:    2    | Tr.loss: 0.131000 | Elapsed:   54.06  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9881
2023-03-18 11:18:58,218 WARNING  [*] Started epoch: 3
2023-03-18 11:18:58,336 WARNING  [*] 11:18:58: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.112673 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.8696 & F1 0.9302 | AUC 0.9925
2023-03-18 11:19:08,387 WARNING  [*] 11:19:08: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.110542 | Elapsed: 10.03s | FPR 0.0003 -> TPR 0.9315 & F1 0.9645 | AUC 0.9944
2023-03-18 11:19:18,370 WARNING  [*] 11:19:18: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.058344 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9995
2023-03-18 11:19:28,362 WARNING  [*] 11:19:28: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.126769 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9905
2023-03-18 11:19:38,358 WARNING  [*] 11:19:38: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.115791 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.8966 & F1 0.9455 | AUC 0.9922
2023-03-18 11:19:48,374 WARNING  [*] 11:19:48: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.026293 | Elapsed: 10.01s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:19:52,425 WARNING  [*] Sat Mar 18 11:19:52 2023:    3    | Tr.loss: 0.086566 | Elapsed:   54.21  s | FPR 0.0003 -> TPR: 0.50 & F1: 0.66 | AUC: 0.9947
2023-03-18 11:19:52,425 WARNING  [*] Started epoch: 4
2023-03-18 11:19:52,552 WARNING  [*] 11:19:52: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.072802 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9143 & F1 0.9552 | AUC 0.9967
2023-03-18 11:20:02,530 WARNING  [*] 11:20:02: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.066674 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9968
2023-03-18 11:20:12,503 WARNING  [*] 11:20:12: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.033365 | Elapsed: 9.97s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:20:22,506 WARNING  [*] 11:20:22: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.099702 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9718 & F1 0.9857 | AUC 0.9927
2023-03-18 11:20:32,508 WARNING  [*] 11:20:32: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.059751 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.9559 & F1 0.9774 | AUC 0.9977
2023-03-18 11:20:42,560 WARNING  [*] 11:20:42: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.050331 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9996
2023-03-18 11:20:46,532 WARNING  [*] Sat Mar 18 11:20:46 2023:    4    | Tr.loss: 0.067965 | Elapsed:   54.11  s | FPR 0.0003 -> TPR: 0.57 & F1: 0.73 | AUC: 0.9966
2023-03-18 11:20:46,532 WARNING  [*] Started epoch: 5
2023-03-18 11:20:46,687 WARNING  [*] 11:20:46: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.060841 | Elapsed: 0.14s | FPR 0.0003 -> TPR 0.9516 & F1 0.9752 | AUC 0.9986
2023-03-18 11:20:56,749 WARNING  [*] 11:20:56: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.044129 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.9706 & F1 0.9851 | AUC 0.9991
2023-03-18 11:21:06,744 WARNING  [*] 11:21:06: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.019185 | Elapsed: 9.98s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:21:16,723 WARNING  [*] 11:21:16: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.017312 | Elapsed: 9.97s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:21:26,704 WARNING  [*] 11:21:26: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.014886 | Elapsed: 9.98s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:21:36,708 WARNING  [*] 11:21:36: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.080178 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9577 & F1 0.9784 | AUC 0.9947
2023-03-18 11:21:40,871 WARNING  [*] Sat Mar 18 11:21:40 2023:    5    | Tr.loss: 0.057356 | Elapsed:   54.34  s | FPR 0.0003 -> TPR: 0.70 & F1: 0.82 | AUC: 0.9976
2023-03-18 11:21:40,871 WARNING  [*] Started epoch: 6
2023-03-18 11:21:41,015 WARNING  [*] 11:21:41: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.070135 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.9048 & F1 0.9500 | AUC 0.9966
2023-03-18 11:21:51,035 WARNING  [*] 11:21:51: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.008208 | Elapsed: 10.02s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 11:22:01,024 WARNING  [*] 11:22:01: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.061111 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.9861 & F1 0.9930 | AUC 0.9995
2023-03-18 11:22:09,897 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 11:22:09,932 WARNING  [!] Sat Mar 18 11:22:09 2023: Dumped results:
                model       : 1679134627-model.torch
		train time  : 1679134627-trainTime.npy
		train losses: 1679134627-trainLosses.npy
		train AUC   : 1679134627-auc.npy
		train F1s   : 1679134627-trainF1s.npy
		train TPRs  : 1679134627-trainTPRs.npy
2023-03-18 11:22:09,970 WARNING  [!] Evaluating model on training set...
2023-03-18 11:22:23,726 WARNING  [!] This fold metrics on training set:
2023-03-18 11:22:23,734 WARNING 	AUC: 0.9990
2023-03-18 11:22:23,746 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:22:23,749 WARNING 	FPR: 0.0003 | TPR: 0.8943 | F1: 0.9441
2023-03-18 11:22:23,771 WARNING 	FPR: 0.001 | TPR: 0.9433 | F1: 0.9706
2023-03-18 11:22:23,778 WARNING 	FPR: 0.003 | TPR: 0.9634 | F1: 0.9806
2023-03-18 11:22:23,786 WARNING 	FPR: 0.01 | TPR: 0.9839 | F1: 0.9895
2023-03-18 11:22:23,809 WARNING 	FPR: 0.03 | TPR: 0.9943 | F1: 0.9901
2023-03-18 11:22:23,815 WARNING 	FPR: 0.1 | TPR: 0.9984 | F1: 0.9762
2023-03-18 11:22:23,815 WARNING  [!] Evaluating model on validation set...
2023-03-18 11:22:30,694 WARNING  [!] This fold metrics on validation set:
2023-03-18 11:22:30,699 WARNING 	AUC: 0.9967
2023-03-18 11:22:30,703 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:22:30,709 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 11:22:30,721 WARNING 	FPR: 0.001 | TPR: 0.8068 | F1: 0.8929
2023-03-18 11:22:30,728 WARNING 	FPR: 0.003 | TPR: 0.8745 | F1: 0.9323
2023-03-18 11:22:30,732 WARNING 	FPR: 0.01 | TPR: 0.9554 | F1: 0.9748
2023-03-18 11:22:30,737 WARNING 	FPR: 0.03 | TPR: 0.9788 | F1: 0.9822
2023-03-18 11:22:30,743 WARNING 	FPR: 0.1 | TPR: 0.9938 | F1: 0.9736
2023-03-18 11:22:30,832 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_10000_limNone_r1763_t5\10000_metrics_validation.json
2023-03-18 11:22:30,837 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_10000_limNone_r1763_t5\10000_metrics_training.json
2023-03-18 11:22:30,837 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9967
	FPR: 0.0001 -- TPR: 0.0000 -- F1: 0.0000
	FPR: 0.0003 -- TPR: 0.2696 -- F1: 0.2981
	FPR:  0.001 -- TPR: 0.5552 -- F1: 0.6055
	FPR:  0.003 -- TPR: 0.8931 -- F1: 0.9427
	FPR:   0.01 -- TPR: 0.9598 -- F1: 0.9771
	FPR:   0.03 -- TPR: 0.9823 -- F1: 0.9840
	FPR:    0.1 -- TPR: 0.9935 -- F1: 0.9734

2023-03-18 11:22:30,903 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-03-18 11:26:30,846 WARNING Finished... Took: 239.94s
2023-03-18 11:26:30,846 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-03-18 11:31:45,166 WARNING Finished... Took: 314.32s
2023-03-18 11:31:45,166 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-03-18 11:33:26,700 WARNING Finished... Took: 101.53s
2023-03-18 11:33:26,700 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-03-18 11:35:03,874 WARNING Finished... Took: 97.17s
2023-03-18 11:35:03,874 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-03-18 11:36:03,449 WARNING Finished... Took: 59.58s
2023-03-18 11:36:03,449 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-03-18 11:39:09,624 WARNING Finished... Took: 186.18s
2023-03-18 11:39:09,625 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-03-18 11:39:54,443 WARNING Finished... Took: 44.82s
2023-03-18 11:39:54,443 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-03-18 11:42:15,138 WARNING Finished... Took: 140.69s
2023-03-18 11:42:15,138 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-03-18 11:42:16,918 WARNING Finished... Took: 1.78s
2023-03-18 11:42:16,937 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_30000_seqlen_512\y_train_full.npy
2023-03-18 11:42:17,139 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_30000_seqlen_512\y_names_train_full.json
2023-03-18 11:42:17,139 WARNING  [!] Initialized tokenizer without pre-trained model.
	You need to train tokenizer with .train() or specify 'model_path=' during initialization!
2023-03-18 11:42:17,140 WARNING  [*] Initializing tokenizer training...
2023-03-18 11:42:17,140 WARNING  [*] Data preparation for SentencePiece tokenizer...
2023-03-18 11:44:49,836 WARNING  [*] Saving to disk...
2023-03-18 11:44:54,759 WARNING  [!] Training tokenizer with command: --input=out_vocab_1679128542\nebula_vocab_30000_seqlen_512\tokenizer_30000_trainset_1679136289.txt --model_prefix=out_vocab_1679128542\nebula_vocab_30000_seqlen_512\tokenizer_30000 --vocab_size=30000 --model_type=bpe --split_by_number=False --max_sentence_length=4192 --max_sentencepiece_length=64
2023-03-18 11:47:21,525 WARNING  [!] Loaded vocab with size 30001 from out_vocab_1679128542\nebula_vocab_30000_seqlen_512\tokenizer_30000.vocab
2023-03-18 11:47:21,981 WARNING  [*] Encoding and padding...
2023-03-18 11:53:56,600 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_30000_seqlen_512\x_train_full.npy
2023-03-18 11:54:00,209 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-03-18 11:54:36,547 WARNING Finished... Took: 36.34s
2023-03-18 11:54:36,547 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-03-18 11:56:13,216 WARNING Finished... Took: 96.67s
2023-03-18 11:56:13,218 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-03-18 11:56:38,572 WARNING Finished... Took: 25.35s
2023-03-18 11:56:38,572 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-03-18 11:56:42,759 WARNING Finished... Took: 4.19s
2023-03-18 11:56:42,759 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-03-18 11:56:52,912 WARNING Finished... Took: 10.15s
2023-03-18 11:56:52,912 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-03-18 11:58:59,536 WARNING Finished... Took: 126.62s
2023-03-18 11:58:59,537 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-03-18 11:59:15,652 WARNING Finished... Took: 16.11s
2023-03-18 11:59:15,652 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-03-18 11:59:26,909 WARNING Finished... Took: 11.26s
2023-03-18 11:59:26,909 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-03-18 11:59:27,399 WARNING Finished... Took: 0.49s
2023-03-18 11:59:27,421 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_30000_seqlen_512\y_test_full.npy
2023-03-18 11:59:27,459 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_30000_seqlen_512\y_names_test_full.json
2023-03-18 11:59:27,489 WARNING  [!] Successfully loaded pre-trained tokenizer model!
2023-03-18 11:59:27,507 WARNING  [!] Loaded vocab with size 30001 from out_vocab_1679128542\nebula_vocab_30000_seqlen_512\tokenizer_30000_vocab.json
2023-03-18 11:59:27,509 WARNING  [*] Encoding and padding...
2023-03-18 12:00:41,706 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_30000_seqlen_512\x_test_full.npy
2023-03-18 12:00:42,514 WARNING  [!!!] Starting CV over 30000!
2023-03-18 12:00:42,603 WARNING  [!] Training time budget: 300min
2023-03-18 12:00:42,603 WARNING  [!] Model config: {'vocab_size': 30001, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-03-18 12:00:42,653 WARNING  [1/3] Train set size: 50750, Validation set size: 25376
2023-03-18 12:00:44,757 WARNING  [!] Saved dataset splits to dataset_splits_1679137242.npz
2023-03-18 12:00:44,865 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 4.1173e6
2023-03-18 12:00:44,865 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 12:00:44,897 WARNING  [*] Started epoch: 1
2023-03-18 12:00:45,235 WARNING  [*] 12:00:45: Train Epoch: 1 [  0  /50750 (0 %)] | Loss: 2.381340 | Elapsed: 0.33s | FPR 0.0003 -> TPR 0.0588 & F1 0.1111 | AUC 0.5872
2023-03-18 12:00:54,959 WARNING  [*] 12:00:54: Train Epoch: 1 [9600 /50750 (19%)] | Loss: 0.295554 | Elapsed: 9.71s | FPR 0.0003 -> TPR 0.6866 & F1 0.8142 | AUC 0.9276
2023-03-18 12:01:04,730 WARNING  [*] 12:01:04: Train Epoch: 1 [19200/50750 (38%)] | Loss: 0.352488 | Elapsed: 9.76s | FPR 0.0003 -> TPR 0.5479 & F1 0.7080 | AUC 0.9001
2023-03-18 12:01:14,558 WARNING  [*] 12:01:14: Train Epoch: 1 [28800/50750 (57%)] | Loss: 0.164986 | Elapsed: 9.82s | FPR 0.0003 -> TPR 0.6892 & F1 0.8160 | AUC 0.9735
2023-03-18 12:01:24,424 WARNING  [*] 12:01:24: Train Epoch: 1 [38400/50750 (76%)] | Loss: 0.168719 | Elapsed: 9.86s | FPR 0.0003 -> TPR 0.7465 & F1 0.8548 | AUC 0.9772
2023-03-18 12:01:34,324 WARNING  [*] 12:01:34: Train Epoch: 1 [48000/50750 (95%)] | Loss: 0.317188 | Elapsed: 9.89s | FPR 0.0003 -> TPR 0.4348 & F1 0.6061 | AUC 0.9294
2023-03-18 12:01:38,102 WARNING  [*] Sat Mar 18 12:01:38 2023:    1    | Tr.loss: 0.350303 | Elapsed:   53.20  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9150
2023-03-18 12:01:38,102 WARNING  [*] Started epoch: 2
2023-03-18 12:01:38,236 WARNING  [*] 12:01:38: Train Epoch: 2 [  0  /50750 (0 %)] | Loss: 0.240611 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9611
2023-03-18 12:01:48,235 WARNING  [*] 12:01:48: Train Epoch: 2 [9600 /50750 (19%)] | Loss: 0.196989 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.8030 & F1 0.8908 | AUC 0.9759
2023-03-18 12:01:58,188 WARNING  [*] 12:01:58: Train Epoch: 2 [19200/50750 (38%)] | Loss: 0.163703 | Elapsed: 9.95s | FPR 0.0003 -> TPR 0.8485 & F1 0.9180 | AUC 0.9853
2023-03-18 12:02:08,174 WARNING  [*] 12:02:08: Train Epoch: 2 [28800/50750 (57%)] | Loss: 0.081379 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.7681 & F1 0.8689 | AUC 0.9888
2023-03-18 12:02:18,140 WARNING  [*] 12:02:18: Train Epoch: 2 [38400/50750 (76%)] | Loss: 0.120642 | Elapsed: 9.95s | FPR 0.0003 -> TPR 0.9375 & F1 0.9677 | AUC 0.9900
2023-03-18 12:02:28,115 WARNING  [*] 12:02:28: Train Epoch: 2 [48000/50750 (95%)] | Loss: 0.199691 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.2273 & F1 0.3704 | AUC 0.9759
2023-03-18 12:02:32,167 WARNING  [*] Sat Mar 18 12:02:32 2023:    2    | Tr.loss: 0.139546 | Elapsed:   54.06  s | FPR 0.0003 -> TPR: 0.53 & F1: 0.69 | AUC: 0.9866
2023-03-18 12:02:32,167 WARNING  [*] Started epoch: 3
2023-03-18 12:02:32,280 WARNING  [*] 12:02:32: Train Epoch: 3 [  0  /50750 (0 %)] | Loss: 0.099623 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9437 & F1 0.9710 | AUC 0.9915
2023-03-18 12:02:42,314 WARNING  [*] 12:02:42: Train Epoch: 3 [9600 /50750 (19%)] | Loss: 0.078744 | Elapsed: 10.03s | FPR 0.0003 -> TPR 0.9836 & F1 0.9917 | AUC 0.9954
2023-03-18 12:02:52,291 WARNING  [*] 12:02:52: Train Epoch: 3 [19200/50750 (38%)] | Loss: 0.048128 | Elapsed: 9.98s | FPR 0.0003 -> TPR 0.9625 & F1 0.9809 | AUC 0.9981
2023-03-18 12:03:02,320 WARNING  [*] 12:03:02: Train Epoch: 3 [28800/50750 (57%)] | Loss: 0.090323 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.9455 & F1 0.9720 | AUC 0.9980
2023-03-18 12:03:12,324 WARNING  [*] 12:03:12: Train Epoch: 3 [38400/50750 (76%)] | Loss: 0.036003 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9865 & F1 0.9932 | AUC 0.9990
2023-03-18 12:03:22,396 WARNING  [*] 12:03:22: Train Epoch: 3 [48000/50750 (95%)] | Loss: 0.129691 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.8052 & F1 0.8921 | AUC 0.9881
2023-03-18 12:03:26,368 WARNING  [*] Sat Mar 18 12:03:26 2023:    3    | Tr.loss: 0.082546 | Elapsed:   54.20  s | FPR 0.0003 -> TPR: 0.65 & F1: 0.79 | AUC: 0.9952
2023-03-18 12:03:26,368 WARNING  [*] Started epoch: 4
2023-03-18 12:03:26,503 WARNING  [*] 12:03:26: Train Epoch: 4 [  0  /50750 (0 %)] | Loss: 0.037075 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9995
2023-03-18 12:03:36,529 WARNING  [*] 12:03:36: Train Epoch: 4 [9600 /50750 (19%)] | Loss: 0.144679 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.9315 & F1 0.9645 | AUC 0.9909
2023-03-18 12:03:46,565 WARNING  [*] 12:03:46: Train Epoch: 4 [19200/50750 (38%)] | Loss: 0.123467 | Elapsed: 10.03s | FPR 0.0003 -> TPR 0.9091 & F1 0.9524 | AUC 0.9902
2023-03-18 12:03:56,568 WARNING  [*] 12:03:56: Train Epoch: 4 [28800/50750 (57%)] | Loss: 0.058913 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.9863 & F1 0.9931 | AUC 0.9995
2023-03-18 12:04:06,599 WARNING  [*] 12:04:06: Train Epoch: 4 [38400/50750 (76%)] | Loss: 0.062500 | Elapsed: 10.02s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9977
2023-03-18 12:04:16,609 WARNING  [*] 12:04:16: Train Epoch: 4 [48000/50750 (95%)] | Loss: 0.013984 | Elapsed: 9.99s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:04:20,627 WARNING  [*] Sat Mar 18 12:04:20 2023:    4    | Tr.loss: 0.054817 | Elapsed:   54.26  s | FPR 0.0003 -> TPR: 0.77 & F1: 0.87 | AUC: 0.9978
2023-03-18 12:04:20,627 WARNING  [*] Started epoch: 5
2023-03-18 12:04:20,757 WARNING  [*] 12:04:20: Train Epoch: 5 [  0  /50750 (0 %)] | Loss: 0.072764 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9839 & F1 0.9919 | AUC 0.9986
2023-03-18 12:04:30,803 WARNING  [*] 12:04:30: Train Epoch: 5 [9600 /50750 (19%)] | Loss: 0.127909 | Elapsed: 10.04s | FPR 0.0003 -> TPR 0.9516 & F1 0.9752 | AUC 0.9941
2023-03-18 12:04:40,823 WARNING  [*] 12:04:40: Train Epoch: 5 [19200/50750 (38%)] | Loss: 0.009288 | Elapsed: 10.01s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:04:50,841 WARNING  [*] 12:04:50: Train Epoch: 5 [28800/50750 (57%)] | Loss: 0.025843 | Elapsed: 10.00s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:05:00,911 WARNING  [*] 12:05:00: Train Epoch: 5 [38400/50750 (76%)] | Loss: 0.065778 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.9403 & F1 0.9692 | AUC 0.9964
2023-03-18 12:05:10,962 WARNING  [*] 12:05:10: Train Epoch: 5 [48000/50750 (95%)] | Loss: 0.036048 | Elapsed: 10.04s | FPR 0.0003 -> TPR 0.9861 & F1 0.9930 | AUC 0.9995
2023-03-18 12:05:14,994 WARNING  [*] Sat Mar 18 12:05:14 2023:    5    | Tr.loss: 0.043682 | Elapsed:   54.37  s | FPR 0.0003 -> TPR: 0.76 & F1: 0.86 | AUC: 0.9985
2023-03-18 12:05:14,994 WARNING  [*] Started epoch: 6
2023-03-18 12:05:15,124 WARNING  [*] 12:05:15: Train Epoch: 6 [  0  /50750 (0 %)] | Loss: 0.037800 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9995
2023-03-18 12:05:25,153 WARNING  [*] 12:05:25: Train Epoch: 6 [9600 /50750 (19%)] | Loss: 0.065136 | Elapsed: 10.01s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9982
2023-03-18 12:05:35,187 WARNING  [*] 12:05:35: Train Epoch: 6 [19200/50750 (38%)] | Loss: 0.025913 | Elapsed: 10.03s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:05:44,933 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 12:05:44,970 WARNING  [!] Sat Mar 18 12:05:44 2023: Dumped results:
                model       : 1679137242-model.torch
		train time  : 1679137242-trainTime.npy
		train losses: 1679137242-trainLosses.npy
		train AUC   : 1679137242-auc.npy
		train F1s   : 1679137242-trainF1s.npy
		train TPRs  : 1679137242-trainTPRs.npy
2023-03-18 12:05:45,002 WARNING  [!] Evaluating model on training set...
2023-03-18 12:05:58,727 WARNING  [!] This fold metrics on training set:
2023-03-18 12:05:58,732 WARNING 	AUC: 0.9995
2023-03-18 12:05:58,743 WARNING 	FPR: 0.0001 | TPR: 0.8583 | F1: 0.9237
2023-03-18 12:05:58,760 WARNING 	FPR: 0.0003 | TPR: 0.9259 | F1: 0.9615
2023-03-18 12:05:58,766 WARNING 	FPR: 0.001 | TPR: 0.9562 | F1: 0.9774
2023-03-18 12:05:58,775 WARNING 	FPR: 0.003 | TPR: 0.9788 | F1: 0.9886
2023-03-18 12:05:58,797 WARNING 	FPR: 0.01 | TPR: 0.9927 | F1: 0.9939
2023-03-18 12:05:58,804 WARNING 	FPR: 0.03 | TPR: 0.9970 | F1: 0.9914
2023-03-18 12:05:58,810 WARNING 	FPR: 0.1 | TPR: 0.9994 | F1: 0.9770
2023-03-18 12:05:58,810 WARNING  [!] Evaluating model on validation set...
2023-03-18 12:06:05,675 WARNING  [!] This fold metrics on validation set:
2023-03-18 12:06:05,692 WARNING 	AUC: 0.9972
2023-03-18 12:06:05,699 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:06:05,705 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:06:05,707 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:06:05,713 WARNING 	FPR: 0.003 | TPR: 0.9229 | F1: 0.9592
2023-03-18 12:06:05,720 WARNING 	FPR: 0.01 | TPR: 0.9687 | F1: 0.9817
2023-03-18 12:06:05,730 WARNING 	FPR: 0.03 | TPR: 0.9852 | F1: 0.9854
2023-03-18 12:06:05,743 WARNING 	FPR: 0.1 | TPR: 0.9954 | F1: 0.9744
2023-03-18 12:06:05,917 WARNING  [2/3] Train set size: 50751, Validation set size: 25375
2023-03-18 12:06:08,101 WARNING  [!] Saved dataset splits to dataset_splits_1679137565.npz
2023-03-18 12:06:08,172 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 4.1173e6
2023-03-18 12:06:08,173 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 12:06:08,206 WARNING  [*] Started epoch: 1
2023-03-18 12:06:08,471 WARNING  [*] 12:06:08: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 2.329991 | Elapsed: 0.25s | FPR 0.0003 -> TPR 0.0455 & F1 0.0870 | AUC 0.4581
2023-03-18 12:06:18,528 WARNING  [*] 12:06:18: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.416227 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.1000 & F1 0.1818 | AUC 0.8390
2023-03-18 12:06:28,576 WARNING  [*] 12:06:28: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.338419 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.4242 & F1 0.5957 | AUC 0.9104
2023-03-18 12:06:38,691 WARNING  [*] 12:06:38: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.282089 | Elapsed: 10.10s | FPR 0.0003 -> TPR 0.5634 & F1 0.7207 | AUC 0.9339
2023-03-18 12:06:48,754 WARNING  [*] 12:06:48: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.182402 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.8507 & F1 0.9194 | AUC 0.9806
2023-03-18 12:06:58,818 WARNING  [*] 12:06:58: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.310955 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.6377 & F1 0.7788 | AUC 0.9476
2023-03-18 12:07:02,907 WARNING  [*] Sat Mar 18 12:07:02 2023:    1    | Tr.loss: 0.360960 | Elapsed:   54.70  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9089
2023-03-18 12:07:02,907 WARNING  [*] Started epoch: 2
2023-03-18 12:07:03,014 WARNING  [*] 12:07:03: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.184582 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.5286 & F1 0.6916 | AUC 0.9698
2023-03-18 12:07:13,086 WARNING  [*] 12:07:13: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.201118 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.8462 & F1 0.9167 | AUC 0.9749
2023-03-18 12:07:23,147 WARNING  [*] 12:07:23: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.120092 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.8923 & F1 0.9431 | AUC 0.9851
2023-03-18 12:07:33,205 WARNING  [*] 12:07:33: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.188933 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.8088 & F1 0.8943 | AUC 0.9743
2023-03-18 12:07:43,276 WARNING  [*] 12:07:43: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.097153 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.9429 & F1 0.9706 | AUC 0.9957
2023-03-18 12:07:53,353 WARNING  [*] 12:07:53: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.100055 | Elapsed: 10.08s | FPR 0.0003 -> TPR 0.9000 & F1 0.9474 | AUC 0.9913
2023-03-18 12:07:57,337 WARNING  [*] Sat Mar 18 12:07:57 2023:    2    | Tr.loss: 0.147762 | Elapsed:   54.43  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9849
2023-03-18 12:07:57,337 WARNING  [*] Started epoch: 3
2023-03-18 12:07:57,485 WARNING  [*] 12:07:57: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.084656 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.8955 & F1 0.9449 | AUC 0.9954
2023-03-18 12:08:07,614 WARNING  [*] 12:08:07: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.046872 | Elapsed: 10.12s | FPR 0.0003 -> TPR 0.9868 & F1 0.9934 | AUC 0.9995
2023-03-18 12:08:17,670 WARNING  [*] 12:08:17: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.049971 | Elapsed: 10.04s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9996
2023-03-18 12:08:27,742 WARNING  [*] 12:08:27: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.121924 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9375 & F1 0.9677 | AUC 0.9948
2023-03-18 12:08:37,798 WARNING  [*] 12:08:37: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.079324 | Elapsed: 10.03s | FPR 0.0003 -> TPR 0.7931 & F1 0.8846 | AUC 0.9951
2023-03-18 12:08:47,854 WARNING  [*] 12:08:47: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.042385 | Elapsed: 10.05s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:08:51,890 WARNING  [*] Sat Mar 18 12:08:51 2023:    3    | Tr.loss: 0.082591 | Elapsed:   54.55  s | FPR 0.0003 -> TPR: 0.57 & F1: 0.73 | AUC: 0.9951
2023-03-18 12:08:51,890 WARNING  [*] Started epoch: 4
2023-03-18 12:08:52,012 WARNING  [*] 12:08:52: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.034952 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9990
2023-03-18 12:09:02,091 WARNING  [*] 12:09:02: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.017170 | Elapsed: 10.06s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:09:12,162 WARNING  [*] 12:09:12: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.165458 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.8254 & F1 0.9043 | AUC 0.9897
2023-03-18 12:09:22,224 WARNING  [*] 12:09:22: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.009656 | Elapsed: 10.05s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:09:32,278 WARNING  [*] 12:09:32: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.050126 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9857 & F1 0.9928 | AUC 0.9962
2023-03-18 12:09:42,363 WARNING  [*] 12:09:42: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.035041 | Elapsed: 10.08s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9996
2023-03-18 12:09:46,337 WARNING  [*] Sat Mar 18 12:09:46 2023:    4    | Tr.loss: 0.057697 | Elapsed:   54.45  s | FPR 0.0003 -> TPR: 0.59 & F1: 0.74 | AUC: 0.9975
2023-03-18 12:09:46,337 WARNING  [*] Started epoch: 5
2023-03-18 12:09:46,476 WARNING  [*] 12:09:46: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.019713 | Elapsed: 0.13s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:09:56,599 WARNING  [*] 12:09:56: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.015552 | Elapsed: 10.11s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:10:06,658 WARNING  [*] 12:10:06: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.041995 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.9853 & F1 0.9926 | AUC 0.9995
2023-03-18 12:10:16,714 WARNING  [*] 12:10:16: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.113057 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9714 & F1 0.9855 | AUC 0.9981
2023-03-18 12:10:26,757 WARNING  [*] 12:10:26: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.015672 | Elapsed: 10.04s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:10:36,808 WARNING  [*] 12:10:36: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.071669 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9821 & F1 0.9910 | AUC 0.9955
2023-03-18 12:10:40,873 WARNING  [*] Sat Mar 18 12:10:40 2023:    5    | Tr.loss: 0.044729 | Elapsed:   54.54  s | FPR 0.0003 -> TPR: 0.66 & F1: 0.80 | AUC: 0.9984
2023-03-18 12:10:40,888 WARNING  [*] Started epoch: 6
2023-03-18 12:10:41,009 WARNING  [*] 12:10:41: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.018854 | Elapsed: 0.12s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:10:51,073 WARNING  [*] 12:10:51: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.096841 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.9041 & F1 0.9496 | AUC 0.9954
2023-03-18 12:11:01,131 WARNING  [*] 12:11:01: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.023022 | Elapsed: 10.05s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:11:08,181 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 12:11:08,213 WARNING  [!] Sat Mar 18 12:11:08 2023: Dumped results:
                model       : 1679137565-model.torch
		train time  : 1679137565-trainTime.npy
		train losses: 1679137565-trainLosses.npy
		train AUC   : 1679137565-auc.npy
		train F1s   : 1679137565-trainF1s.npy
		train TPRs  : 1679137565-trainTPRs.npy
2023-03-18 12:11:08,246 WARNING  [!] Evaluating model on training set...
2023-03-18 12:11:22,000 WARNING  [!] This fold metrics on training set:
2023-03-18 12:11:22,012 WARNING 	AUC: 0.9995
2023-03-18 12:11:22,029 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:11:22,038 WARNING 	FPR: 0.0003 | TPR: 0.8862 | F1: 0.9396
2023-03-18 12:11:22,044 WARNING 	FPR: 0.001 | TPR: 0.9687 | F1: 0.9839
2023-03-18 12:11:22,066 WARNING 	FPR: 0.003 | TPR: 0.9842 | F1: 0.9913
2023-03-18 12:11:22,073 WARNING 	FPR: 0.01 | TPR: 0.9934 | F1: 0.9943
2023-03-18 12:11:22,080 WARNING 	FPR: 0.03 | TPR: 0.9969 | F1: 0.9913
2023-03-18 12:11:22,104 WARNING 	FPR: 0.1 | TPR: 0.9992 | F1: 0.9764
2023-03-18 12:11:22,104 WARNING  [!] Evaluating model on validation set...
2023-03-18 12:11:28,985 WARNING  [!] This fold metrics on validation set:
2023-03-18 12:11:28,990 WARNING 	AUC: 0.9978
2023-03-18 12:11:28,995 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:11:29,003 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:11:29,005 WARNING 	FPR: 0.001 | TPR: 0.9057 | F1: 0.9503
2023-03-18 12:11:29,020 WARNING 	FPR: 0.003 | TPR: 0.9450 | F1: 0.9710
2023-03-18 12:11:29,027 WARNING 	FPR: 0.01 | TPR: 0.9726 | F1: 0.9837
2023-03-18 12:11:29,030 WARNING 	FPR: 0.03 | TPR: 0.9865 | F1: 0.9861
2023-03-18 12:11:29,035 WARNING 	FPR: 0.1 | TPR: 0.9952 | F1: 0.9744
2023-03-18 12:11:29,193 WARNING  [3/3] Train set size: 50751, Validation set size: 25375
2023-03-18 12:11:31,365 WARNING  [!] Saved dataset splits to dataset_splits_1679137889.npz
2023-03-18 12:11:31,434 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 4.1173e6
2023-03-18 12:11:31,435 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 12:11:31,466 WARNING  [*] Started epoch: 1
2023-03-18 12:11:31,735 WARNING  [*] 12:11:31: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 1.364123 | Elapsed: 0.27s | FPR 0.0003 -> TPR 0.1333 & F1 0.2353 | AUC 0.5731
2023-03-18 12:11:41,864 WARNING  [*] 12:11:41: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.297896 | Elapsed: 10.12s | FPR 0.0003 -> TPR 0.6429 & F1 0.7826 | AUC 0.9210
2023-03-18 12:11:51,978 WARNING  [*] 12:11:51: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.236549 | Elapsed: 10.10s | FPR 0.0003 -> TPR 0.6438 & F1 0.7833 | AUC 0.9609
2023-03-18 12:12:02,040 WARNING  [*] 12:12:02: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.254979 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.8358 & F1 0.9106 | AUC 0.9507
2023-03-18 12:12:12,114 WARNING  [*] 12:12:12: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.203872 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.7917 & F1 0.8837 | AUC 0.9702
2023-03-18 12:12:22,192 WARNING  [*] 12:12:22: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.212300 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.5753 & F1 0.7304 | AUC 0.9574
2023-03-18 12:12:26,240 WARNING  [*] Sat Mar 18 12:12:26 2023:    1    | Tr.loss: 0.342903 | Elapsed:   54.77  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9160
2023-03-18 12:12:26,240 WARNING  [*] Started epoch: 2
2023-03-18 12:12:26,372 WARNING  [*] 12:12:26: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.171217 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.8095 & F1 0.8947 | AUC 0.9817
2023-03-18 12:12:36,422 WARNING  [*] 12:12:36: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.080149 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9296 & F1 0.9635 | AUC 0.9966
2023-03-18 12:12:46,519 WARNING  [*] 12:12:46: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.169602 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.8857 & F1 0.9394 | AUC 0.9762
2023-03-18 12:12:56,604 WARNING  [*] 12:12:56: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.067803 | Elapsed: 10.08s | FPR 0.0003 -> TPR 0.8873 & F1 0.9403 | AUC 0.9908
2023-03-18 12:13:06,687 WARNING  [*] 12:13:06: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.190647 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.8676 & F1 0.9291 | AUC 0.9775
2023-03-18 12:13:16,762 WARNING  [*] 12:13:16: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.024820 | Elapsed: 10.06s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:13:20,771 WARNING  [*] Sat Mar 18 12:13:20 2023:    2    | Tr.loss: 0.135311 | Elapsed:   54.53  s | FPR 0.0003 -> TPR: 0.37 & F1: 0.54 | AUC: 0.9873
2023-03-18 12:13:20,772 WARNING  [*] Started epoch: 3
2023-03-18 12:13:20,880 WARNING  [*] 12:13:20: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.063457 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9974
2023-03-18 12:13:31,017 WARNING  [*] 12:13:31: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.104861 | Elapsed: 10.11s | FPR 0.0003 -> TPR 0.9552 & F1 0.9771 | AUC 0.9914
2023-03-18 12:13:41,084 WARNING  [*] 12:13:41: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.075317 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.9730 & F1 0.9863 | AUC 0.9958
2023-03-18 12:13:51,155 WARNING  [*] 12:13:51: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.114325 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.8971 & F1 0.9457 | AUC 0.9949
2023-03-18 12:14:01,224 WARNING  [*] 12:14:01: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.026170 | Elapsed: 10.06s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:14:11,333 WARNING  [*] 12:14:11: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.060366 | Elapsed: 10.10s | FPR 0.0003 -> TPR 0.9861 & F1 0.9930 | AUC 0.9985
2023-03-18 12:14:15,380 WARNING  [*] Sat Mar 18 12:14:15 2023:    3    | Tr.loss: 0.074648 | Elapsed:   54.61  s | FPR 0.0003 -> TPR: 0.64 & F1: 0.78 | AUC: 0.9960
2023-03-18 12:14:15,380 WARNING  [*] Started epoch: 4
2023-03-18 12:14:15,538 WARNING  [*] 12:14:15: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.061653 | Elapsed: 0.14s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9985
2023-03-18 12:14:25,595 WARNING  [*] 12:14:25: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.088869 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9524 & F1 0.9756 | AUC 0.9957
2023-03-18 12:14:35,660 WARNING  [*] 12:14:35: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.047031 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.9667 & F1 0.9831 | AUC 0.9992
2023-03-18 12:14:45,741 WARNING  [*] 12:14:45: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.035832 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.9841 & F1 0.9920 | AUC 0.9991
2023-03-18 12:14:55,802 WARNING  [*] 12:14:55: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.037557 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.9839 & F1 0.9919 | AUC 0.9992
2023-03-18 12:15:05,882 WARNING  [*] 12:15:05: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.043018 | Elapsed: 10.08s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9991
2023-03-18 12:15:09,900 WARNING  [*] Sat Mar 18 12:15:09 2023:    4    | Tr.loss: 0.055604 | Elapsed:   54.52  s | FPR 0.0003 -> TPR: 0.62 & F1: 0.76 | AUC: 0.9977
2023-03-18 12:15:09,900 WARNING  [*] Started epoch: 5
2023-03-18 12:15:10,030 WARNING  [*] 12:15:10: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.035564 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9995
2023-03-18 12:15:20,147 WARNING  [*] 12:15:20: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.041761 | Elapsed: 10.11s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9991
2023-03-18 12:15:30,213 WARNING  [*] 12:15:30: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.052872 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.9706 & F1 0.9851 | AUC 0.9991
2023-03-18 12:15:40,276 WARNING  [*] 12:15:40: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.003829 | Elapsed: 10.06s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:15:50,369 WARNING  [*] 12:15:50: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.103801 | Elapsed: 10.09s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9941
2023-03-18 12:16:00,420 WARNING  [*] 12:16:00: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.001175 | Elapsed: 10.05s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:16:04,457 WARNING  [*] Sat Mar 18 12:16:04 2023:    5    | Tr.loss: 0.043307 | Elapsed:   54.56  s | FPR 0.0003 -> TPR: 0.73 & F1: 0.84 | AUC: 0.9986
2023-03-18 12:16:04,458 WARNING  [*] Started epoch: 6
2023-03-18 12:16:04,587 WARNING  [*] 12:16:04: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.027052 | Elapsed: 0.11s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:16:14,669 WARNING  [*] 12:16:14: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.064956 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.9429 & F1 0.9706 | AUC 0.9967
2023-03-18 12:16:24,740 WARNING  [*] 12:16:24: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.021628 | Elapsed: 10.06s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 12:16:31,472 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 12:16:31,509 WARNING  [!] Sat Mar 18 12:16:31 2023: Dumped results:
                model       : 1679137889-model.torch
		train time  : 1679137889-trainTime.npy
		train losses: 1679137889-trainLosses.npy
		train AUC   : 1679137889-auc.npy
		train F1s   : 1679137889-trainF1s.npy
		train TPRs  : 1679137889-trainTPRs.npy
2023-03-18 12:16:31,541 WARNING  [!] Evaluating model on training set...
2023-03-18 12:16:45,291 WARNING  [!] This fold metrics on training set:
2023-03-18 12:16:45,311 WARNING 	AUC: 0.9993
2023-03-18 12:16:45,323 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:16:45,344 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:16:45,356 WARNING 	FPR: 0.001 | TPR: 0.9447 | F1: 0.9713
2023-03-18 12:16:45,360 WARNING 	FPR: 0.003 | TPR: 0.9647 | F1: 0.9813
2023-03-18 12:16:45,370 WARNING 	FPR: 0.01 | TPR: 0.9929 | F1: 0.9941
2023-03-18 12:16:45,392 WARNING 	FPR: 0.03 | TPR: 0.9970 | F1: 0.9914
2023-03-18 12:16:45,399 WARNING 	FPR: 0.1 | TPR: 0.9990 | F1: 0.9762
2023-03-18 12:16:45,399 WARNING  [!] Evaluating model on validation set...
2023-03-18 12:16:52,289 WARNING  [!] This fold metrics on validation set:
2023-03-18 12:16:52,295 WARNING 	AUC: 0.9972
2023-03-18 12:16:52,299 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:16:52,310 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:16:52,322 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 12:16:52,331 WARNING 	FPR: 0.003 | TPR: 0.9269 | F1: 0.9614
2023-03-18 12:16:52,338 WARNING 	FPR: 0.01 | TPR: 0.9664 | F1: 0.9806
2023-03-18 12:16:52,341 WARNING 	FPR: 0.03 | TPR: 0.9882 | F1: 0.9870
2023-03-18 12:16:52,347 WARNING 	FPR: 0.1 | TPR: 0.9957 | F1: 0.9746
2023-03-18 12:16:52,439 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_30000_limNone_r1763_t5\30000_metrics_validation.json
2023-03-18 12:16:52,443 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_30000_limNone_r1763_t5\30000_metrics_training.json
2023-03-18 12:16:52,443 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9974
	FPR: 0.0001 -- TPR: 0.0000 -- F1: 0.0000
	FPR: 0.0003 -- TPR: 0.0000 -- F1: 0.0000
	FPR:  0.001 -- TPR: 0.3019 -- F1: 0.3168
	FPR:  0.003 -- TPR: 0.9316 -- F1: 0.9639
	FPR:   0.01 -- TPR: 0.9693 -- F1: 0.9820
	FPR:   0.03 -- TPR: 0.9866 -- F1: 0.9862
	FPR:    0.1 -- TPR: 0.9954 -- F1: 0.9745

2023-03-18 12:16:52,507 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-03-18 12:20:50,402 WARNING Finished... Took: 237.89s
2023-03-18 12:20:50,402 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-03-18 12:26:07,034 WARNING Finished... Took: 316.63s
2023-03-18 12:26:07,034 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-03-18 12:27:47,582 WARNING Finished... Took: 100.55s
2023-03-18 12:27:47,582 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-03-18 12:29:23,573 WARNING Finished... Took: 95.99s
2023-03-18 12:29:23,573 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-03-18 12:30:23,716 WARNING Finished... Took: 60.14s
2023-03-18 12:30:23,716 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-03-18 12:33:31,657 WARNING Finished... Took: 187.94s
2023-03-18 12:33:31,657 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-03-18 12:34:17,148 WARNING Finished... Took: 45.49s
2023-03-18 12:34:17,148 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-03-18 12:36:38,138 WARNING Finished... Took: 140.99s
2023-03-18 12:36:38,138 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-03-18 12:36:39,848 WARNING Finished... Took: 1.71s
2023-03-18 12:36:39,855 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_50000_seqlen_512\y_train_full.npy
2023-03-18 12:36:40,071 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_50000_seqlen_512\y_names_train_full.json
2023-03-18 12:36:40,071 WARNING  [!] Initialized tokenizer without pre-trained model.
	You need to train tokenizer with .train() or specify 'model_path=' during initialization!
2023-03-18 12:36:40,086 WARNING  [*] Initializing tokenizer training...
2023-03-18 12:36:40,086 WARNING  [*] Data preparation for SentencePiece tokenizer...
2023-03-18 12:39:13,503 WARNING  [*] Saving to disk...
2023-03-18 12:39:18,358 WARNING  [!] Training tokenizer with command: --input=out_vocab_1679128542\nebula_vocab_50000_seqlen_512\tokenizer_50000_trainset_1679139553.txt --model_prefix=out_vocab_1679128542\nebula_vocab_50000_seqlen_512\tokenizer_50000 --vocab_size=50000 --model_type=bpe --split_by_number=False --max_sentence_length=4192 --max_sentencepiece_length=64
2023-03-18 12:42:49,631 WARNING  [!] Loaded vocab with size 50001 from out_vocab_1679128542\nebula_vocab_50000_seqlen_512\tokenizer_50000.vocab
2023-03-18 12:42:50,130 WARNING  [*] Encoding and padding...
2023-03-18 12:49:31,765 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_50000_seqlen_512\x_train_full.npy
2023-03-18 12:49:35,285 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-03-18 12:50:09,470 WARNING Finished... Took: 34.19s
2023-03-18 12:50:09,470 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-03-18 12:51:46,531 WARNING Finished... Took: 97.06s
2023-03-18 12:51:46,531 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-03-18 12:52:11,778 WARNING Finished... Took: 25.25s
2023-03-18 12:52:11,779 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-03-18 12:52:15,859 WARNING Finished... Took: 4.08s
2023-03-18 12:52:15,859 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-03-18 12:52:26,035 WARNING Finished... Took: 10.18s
2023-03-18 12:52:26,035 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-03-18 12:54:33,481 WARNING Finished... Took: 127.45s
2023-03-18 12:54:33,481 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-03-18 12:54:49,705 WARNING Finished... Took: 16.22s
2023-03-18 12:54:49,720 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-03-18 12:55:02,478 WARNING Finished... Took: 12.76s
2023-03-18 12:55:02,478 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-03-18 12:55:02,948 WARNING Finished... Took: 0.47s
2023-03-18 12:55:02,952 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_50000_seqlen_512\y_test_full.npy
2023-03-18 12:55:03,004 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_50000_seqlen_512\y_names_test_full.json
2023-03-18 12:55:03,022 WARNING  [!] Successfully loaded pre-trained tokenizer model!
2023-03-18 12:55:03,054 WARNING  [!] Loaded vocab with size 50001 from out_vocab_1679128542\nebula_vocab_50000_seqlen_512\tokenizer_50000_vocab.json
2023-03-18 12:55:03,054 WARNING  [*] Encoding and padding...
2023-03-18 12:56:21,141 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_50000_seqlen_512\x_test_full.npy
2023-03-18 12:56:21,949 WARNING  [!!!] Starting CV over 50000!
2023-03-18 12:56:22,035 WARNING  [!] Training time budget: 300min
2023-03-18 12:56:22,035 WARNING  [!] Model config: {'vocab_size': 50001, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-03-18 12:56:22,088 WARNING  [1/3] Train set size: 50750, Validation set size: 25376
2023-03-18 12:56:24,078 WARNING  [!] Saved dataset splits to dataset_splits_1679140582.npz
2023-03-18 12:56:24,176 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3973e6
2023-03-18 12:56:24,176 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 12:56:24,210 WARNING  [*] Started epoch: 1
2023-03-18 12:56:24,594 WARNING  [*] 12:56:24: Train Epoch: 1 [  0  /50750 (0 %)] | Loss: 3.402544 | Elapsed: 0.38s | FPR 0.0003 -> TPR 0.0615 & F1 0.1159 | AUC 0.6283
2023-03-18 12:56:34,433 WARNING  [*] 12:56:34: Train Epoch: 1 [9600 /50750 (19%)] | Loss: 0.414278 | Elapsed: 9.82s | FPR 0.0003 -> TPR 0.4333 & F1 0.6047 | AUC 0.8721
2023-03-18 12:56:44,319 WARNING  [*] 12:56:44: Train Epoch: 1 [19200/50750 (38%)] | Loss: 0.277653 | Elapsed: 9.88s | FPR 0.0003 -> TPR 0.5139 & F1 0.6789 | AUC 0.9375
2023-03-18 12:56:54,252 WARNING  [*] 12:56:54: Train Epoch: 1 [28800/50750 (57%)] | Loss: 0.258136 | Elapsed: 9.91s | FPR 0.0003 -> TPR 0.8116 & F1 0.8960 | AUC 0.9490
2023-03-18 12:57:04,216 WARNING  [*] 12:57:04: Train Epoch: 1 [38400/50750 (76%)] | Loss: 0.246128 | Elapsed: 9.96s | FPR 0.0003 -> TPR 0.5634 & F1 0.7207 | AUC 0.9359
2023-03-18 12:57:14,216 WARNING  [*] 12:57:14: Train Epoch: 1 [48000/50750 (95%)] | Loss: 0.204383 | Elapsed: 10.00s | FPR 0.0003 -> TPR 0.7812 & F1 0.8772 | AUC 0.9718
2023-03-18 12:57:18,055 WARNING  [*] Sat Mar 18 12:57:18 2023:    1    | Tr.loss: 0.350183 | Elapsed:   53.84  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9140
2023-03-18 12:57:18,055 WARNING  [*] Started epoch: 2
2023-03-18 12:57:18,191 WARNING  [*] 12:57:18: Train Epoch: 2 [  0  /50750 (0 %)] | Loss: 0.228129 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.7313 & F1 0.8448 | AUC 0.9732
2023-03-18 12:57:28,241 WARNING  [*] 12:57:28: Train Epoch: 2 [9600 /50750 (19%)] | Loss: 0.086396 | Elapsed: 10.05s | FPR 0.0003 -> TPR 0.9714 & F1 0.9855 | AUC 0.9971
2023-03-18 12:57:38,317 WARNING  [*] 12:57:38: Train Epoch: 2 [19200/50750 (38%)] | Loss: 0.231378 | Elapsed: 10.07s | FPR 0.0003 -> TPR 0.6508 & F1 0.7885 | AUC 0.9695
2023-03-18 12:57:48,403 WARNING  [*] 12:57:48: Train Epoch: 2 [28800/50750 (57%)] | Loss: 0.096966 | Elapsed: 10.06s | FPR 0.0003 -> TPR 0.9041 & F1 0.9496 | AUC 0.9934
2023-03-18 12:57:58,504 WARNING  [*] 12:57:58: Train Epoch: 2 [38400/50750 (76%)] | Loss: 0.177450 | Elapsed: 10.09s | FPR 0.0003 -> TPR 0.6333 & F1 0.7755 | AUC 0.9846
2023-03-18 12:58:08,599 WARNING  [*] 12:58:08: Train Epoch: 2 [48000/50750 (95%)] | Loss: 0.077115 | Elapsed: 10.09s | FPR 0.0003 -> TPR 0.9577 & F1 0.9784 | AUC 0.9937
2023-03-18 12:58:12,681 WARNING  [*] Sat Mar 18 12:58:12 2023:    2    | Tr.loss: 0.137369 | Elapsed:   54.63  s | FPR 0.0003 -> TPR: 0.42 & F1: 0.59 | AUC: 0.9869
2023-03-18 12:58:12,681 WARNING  [*] Started epoch: 3
2023-03-18 12:58:12,793 WARNING  [*] 12:58:12: Train Epoch: 3 [  0  /50750 (0 %)] | Loss: 0.090132 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9722 & F1 0.9859 | AUC 0.9919
2023-03-18 12:58:22,916 WARNING  [*] 12:58:22: Train Epoch: 3 [9600 /50750 (19%)] | Loss: 0.122946 | Elapsed: 10.11s | FPR 0.0003 -> TPR 0.9206 & F1 0.9587 | AUC 0.9901
2023-03-18 12:58:33,014 WARNING  [*] 12:58:33: Train Epoch: 3 [19200/50750 (38%)] | Loss: 0.100747 | Elapsed: 10.09s | FPR 0.0003 -> TPR 0.9836 & F1 0.9917 | AUC 0.9954
2023-03-18 12:58:43,129 WARNING  [*] 12:58:43: Train Epoch: 3 [28800/50750 (57%)] | Loss: 0.075523 | Elapsed: 10.11s | FPR 0.0003 -> TPR 0.9333 & F1 0.9655 | AUC 0.9975
2023-03-18 12:58:53,251 WARNING  [*] 12:58:53: Train Epoch: 3 [38400/50750 (76%)] | Loss: 0.057136 | Elapsed: 10.12s | FPR 0.0003 -> TPR 0.9833 & F1 0.9916 | AUC 0.9971
2023-03-18 12:59:03,424 WARNING  [*] 12:59:03: Train Epoch: 3 [48000/50750 (95%)] | Loss: 0.052913 | Elapsed: 10.16s | FPR 0.0003 -> TPR 0.9733 & F1 0.9865 | AUC 0.9989
2023-03-18 12:59:07,440 WARNING  [*] Sat Mar 18 12:59:07 2023:    3    | Tr.loss: 0.077358 | Elapsed:   54.76  s | FPR 0.0003 -> TPR: 0.62 & F1: 0.77 | AUC: 0.9958
2023-03-18 12:59:07,440 WARNING  [*] Started epoch: 4
2023-03-18 12:59:07,575 WARNING  [*] 12:59:07: Train Epoch: 4 [  0  /50750 (0 %)] | Loss: 0.051007 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9688 & F1 0.9841 | AUC 0.9990
2023-03-18 12:59:17,682 WARNING  [*] 12:59:17: Train Epoch: 4 [9600 /50750 (19%)] | Loss: 0.069287 | Elapsed: 10.10s | FPR 0.0003 -> TPR 0.9516 & F1 0.9752 | AUC 0.9979
2023-03-18 12:59:27,831 WARNING  [*] 12:59:27: Train Epoch: 4 [19200/50750 (38%)] | Loss: 0.047350 | Elapsed: 10.13s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9991
2023-03-18 12:59:37,962 WARNING  [*] 12:59:37: Train Epoch: 4 [28800/50750 (57%)] | Loss: 0.079052 | Elapsed: 10.11s | FPR 0.0003 -> TPR 0.9857 & F1 0.9928 | AUC 0.9962
2023-03-18 12:59:48,089 WARNING  [*] 12:59:48: Train Epoch: 4 [38400/50750 (76%)] | Loss: 0.046425 | Elapsed: 10.12s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9991
2023-03-18 12:59:58,224 WARNING  [*] 12:59:58: Train Epoch: 4 [48000/50750 (95%)] | Loss: 0.019964 | Elapsed: 10.13s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:00:02,301 WARNING  [*] Sat Mar 18 13:00:02 2023:    4    | Tr.loss: 0.053260 | Elapsed:   54.86  s | FPR 0.0003 -> TPR: 0.70 & F1: 0.82 | AUC: 0.9979
2023-03-18 13:00:02,301 WARNING  [*] Started epoch: 5
2023-03-18 13:00:02,433 WARNING  [*] 13:00:02: Train Epoch: 5 [  0  /50750 (0 %)] | Loss: 0.002563 | Elapsed: 0.13s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:00:12,567 WARNING  [*] 13:00:12: Train Epoch: 5 [9600 /50750 (19%)] | Loss: 0.006935 | Elapsed: 10.12s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:00:22,686 WARNING  [*] 13:00:22: Train Epoch: 5 [19200/50750 (38%)] | Loss: 0.027127 | Elapsed: 10.12s | FPR 0.0003 -> TPR 0.9863 & F1 0.9931 | AUC 0.9990
2023-03-18 13:00:32,824 WARNING  [*] 13:00:32: Train Epoch: 5 [28800/50750 (57%)] | Loss: 0.025359 | Elapsed: 10.12s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9996
2023-03-18 13:00:42,962 WARNING  [*] 13:00:42: Train Epoch: 5 [38400/50750 (76%)] | Loss: 0.049433 | Elapsed: 10.12s | FPR 0.0003 -> TPR 0.9365 & F1 0.9672 | AUC 0.9983
2023-03-18 13:00:53,101 WARNING  [*] 13:00:53: Train Epoch: 5 [48000/50750 (95%)] | Loss: 0.004936 | Elapsed: 10.12s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:00:57,152 WARNING  [*] Sat Mar 18 13:00:57 2023:    5    | Tr.loss: 0.041280 | Elapsed:   54.85  s | FPR 0.0003 -> TPR: 0.84 & F1: 0.91 | AUC: 0.9987
2023-03-18 13:00:57,153 WARNING  [*] Started epoch: 6
2023-03-18 13:00:57,268 WARNING  [*] 13:00:57: Train Epoch: 6 [  0  /50750 (0 %)] | Loss: 0.015669 | Elapsed: 0.11s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:01:07,394 WARNING  [*] 13:01:07: Train Epoch: 6 [9600 /50750 (19%)] | Loss: 0.030968 | Elapsed: 10.13s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:01:17,531 WARNING  [*] 13:01:17: Train Epoch: 6 [19200/50750 (38%)] | Loss: 0.071096 | Elapsed: 10.13s | FPR 0.0003 -> TPR 0.9750 & F1 0.9873 | AUC 0.9969
2023-03-18 13:01:24,229 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 13:01:24,287 WARNING  [!] Sat Mar 18 13:01:24 2023: Dumped results:
                model       : 1679140582-model.torch
		train time  : 1679140582-trainTime.npy
		train losses: 1679140582-trainLosses.npy
		train AUC   : 1679140582-auc.npy
		train F1s   : 1679140582-trainF1s.npy
		train TPRs  : 1679140582-trainTPRs.npy
2023-03-18 13:01:24,310 WARNING  [!] Evaluating model on training set...
2023-03-18 13:01:38,048 WARNING  [!] This fold metrics on training set:
2023-03-18 13:01:38,051 WARNING 	AUC: 0.9997
2023-03-18 13:01:38,059 WARNING 	FPR: 0.0001 | TPR: 0.9286 | F1: 0.9630
2023-03-18 13:01:38,081 WARNING 	FPR: 0.0003 | TPR: 0.9479 | F1: 0.9732
2023-03-18 13:01:38,087 WARNING 	FPR: 0.001 | TPR: 0.9617 | F1: 0.9802
2023-03-18 13:01:38,095 WARNING 	FPR: 0.003 | TPR: 0.9835 | F1: 0.9910
2023-03-18 13:01:38,117 WARNING 	FPR: 0.01 | TPR: 0.9945 | F1: 0.9949
2023-03-18 13:01:38,132 WARNING 	FPR: 0.03 | TPR: 0.9982 | F1: 0.9920
2023-03-18 13:01:38,136 WARNING 	FPR: 0.1 | TPR: 0.9996 | F1: 0.9767
2023-03-18 13:01:38,136 WARNING  [!] Evaluating model on validation set...
2023-03-18 13:01:45,020 WARNING  [!] This fold metrics on validation set:
2023-03-18 13:01:45,028 WARNING 	AUC: 0.9966
2023-03-18 13:01:45,033 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:01:45,044 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:01:45,050 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:01:45,056 WARNING 	FPR: 0.003 | TPR: 0.9202 | F1: 0.9577
2023-03-18 13:01:45,059 WARNING 	FPR: 0.01 | TPR: 0.9626 | F1: 0.9786
2023-03-18 13:01:45,066 WARNING 	FPR: 0.03 | TPR: 0.9861 | F1: 0.9859
2023-03-18 13:01:45,077 WARNING 	FPR: 0.1 | TPR: 0.9948 | F1: 0.9741
2023-03-18 13:01:45,252 WARNING  [2/3] Train set size: 50751, Validation set size: 25375
2023-03-18 13:01:47,767 WARNING  [!] Saved dataset splits to dataset_splits_1679140905.npz
2023-03-18 13:01:47,869 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3973e6
2023-03-18 13:01:47,869 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 13:01:47,905 WARNING  [*] Started epoch: 1
2023-03-18 13:01:48,167 WARNING  [*] 13:01:48: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 1.835796 | Elapsed: 0.24s | FPR 0.0003 -> TPR 0.1571 & F1 0.2716 | AUC 0.6484
2023-03-18 13:01:58,335 WARNING  [*] 13:01:58: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.349335 | Elapsed: 10.16s | FPR 0.0003 -> TPR 0.5286 & F1 0.6916 | AUC 0.9045
2023-03-18 13:02:08,483 WARNING  [*] 13:02:08: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.370249 | Elapsed: 10.14s | FPR 0.0003 -> TPR 0.3889 & F1 0.5600 | AUC 0.8829
2023-03-18 13:02:18,626 WARNING  [*] 13:02:18: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.205020 | Elapsed: 10.12s | FPR 0.0003 -> TPR 0.7200 & F1 0.8372 | AUC 0.9541
2023-03-18 13:02:28,789 WARNING  [*] 13:02:28: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.261608 | Elapsed: 10.16s | FPR 0.0003 -> TPR 0.6562 & F1 0.7925 | AUC 0.9449
2023-03-18 13:02:38,944 WARNING  [*] 13:02:38: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.218739 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9687
2023-03-18 13:02:43,046 WARNING  [*] Sat Mar 18 13:02:43 2023:    1    | Tr.loss: 0.369284 | Elapsed:   55.14  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9077
2023-03-18 13:02:43,046 WARNING  [*] Started epoch: 2
2023-03-18 13:02:43,179 WARNING  [*] 13:02:43: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.182013 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.8824 & F1 0.9375 | AUC 0.9816
2023-03-18 13:02:53,327 WARNING  [*] 13:02:53: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.185392 | Elapsed: 10.14s | FPR 0.0003 -> TPR 0.8308 & F1 0.9076 | AUC 0.9837
2023-03-18 13:03:03,459 WARNING  [*] 13:03:03: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.116248 | Elapsed: 10.13s | FPR 0.0003 -> TPR 0.9429 & F1 0.9706 | AUC 0.9943
2023-03-18 13:03:13,617 WARNING  [*] 13:03:13: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.115310 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9091 & F1 0.9524 | AUC 0.9902
2023-03-18 13:03:23,770 WARNING  [*] 13:03:23: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.113246 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9385 & F1 0.9683 | AUC 0.9930
2023-03-18 13:03:33,951 WARNING  [*] 13:03:33: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.176235 | Elapsed: 10.18s | FPR 0.0003 -> TPR 0.6667 & F1 0.8000 | AUC 0.9817
2023-03-18 13:03:37,978 WARNING  [*] Sat Mar 18 13:03:37 2023:    2    | Tr.loss: 0.166052 | Elapsed:   54.93  s | FPR 0.0003 -> TPR: 0.39 & F1: 0.56 | AUC: 0.9810
2023-03-18 13:03:37,979 WARNING  [*] Started epoch: 3
2023-03-18 13:03:38,114 WARNING  [*] 13:03:38: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.098964 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9016 & F1 0.9483 | AUC 0.9948
2023-03-18 13:03:48,310 WARNING  [*] 13:03:48: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.097419 | Elapsed: 10.19s | FPR 0.0003 -> TPR 0.9583 & F1 0.9787 | AUC 0.9965
2023-03-18 13:03:58,443 WARNING  [*] 13:03:58: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.057467 | Elapsed: 10.13s | FPR 0.0003 -> TPR 0.9559 & F1 0.9774 | AUC 0.9972
2023-03-18 13:04:08,610 WARNING  [*] 13:04:08: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.121389 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9420 & F1 0.9701 | AUC 0.9892
2023-03-18 13:04:18,789 WARNING  [*] 13:04:18: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.058373 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.9722 & F1 0.9859 | AUC 0.9985
2023-03-18 13:04:28,938 WARNING  [*] 13:04:28: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.092929 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9206 & F1 0.9587 | AUC 0.9940
2023-03-18 13:04:33,035 WARNING  [*] Sat Mar 18 13:04:33 2023:    3    | Tr.loss: 0.092716 | Elapsed:   55.06  s | FPR 0.0003 -> TPR: 0.63 & F1: 0.77 | AUC: 0.9939
2023-03-18 13:04:33,035 WARNING  [*] Started epoch: 4
2023-03-18 13:04:33,169 WARNING  [*] 13:04:33: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.064251 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.9692 & F1 0.9844 | AUC 0.9975
2023-03-18 13:04:43,322 WARNING  [*] 13:04:43: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.012557 | Elapsed: 10.15s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:04:53,498 WARNING  [*] 13:04:53: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.067656 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.9697 & F1 0.9846 | AUC 0.9987
2023-03-18 13:05:03,656 WARNING  [*] 13:05:03: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.026887 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9697 & F1 0.9846 | AUC 0.9991
2023-03-18 13:05:13,813 WARNING  [*] 13:05:13: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.027253 | Elapsed: 10.16s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:05:24,049 WARNING  [*] 13:05:24: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.038667 | Elapsed: 10.23s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9991
2023-03-18 13:05:28,044 WARNING  [*] Sat Mar 18 13:05:28 2023:    4    | Tr.loss: 0.063236 | Elapsed:   55.01  s | FPR 0.0003 -> TPR: 0.71 & F1: 0.83 | AUC: 0.9970
2023-03-18 13:05:28,044 WARNING  [*] Started epoch: 5
2023-03-18 13:05:28,187 WARNING  [*] 13:05:28: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.051838 | Elapsed: 0.14s | FPR 0.0003 -> TPR 0.9683 & F1 0.9839 | AUC 0.9990
2023-03-18 13:05:38,408 WARNING  [*] 13:05:38: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.109229 | Elapsed: 10.20s | FPR 0.0003 -> TPR 0.9178 & F1 0.9571 | AUC 0.9919
2023-03-18 13:05:48,595 WARNING  [*] 13:05:48: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.019369 | Elapsed: 10.17s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:05:58,752 WARNING  [*] 13:05:58: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.035748 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9987
2023-03-18 13:06:08,906 WARNING  [*] 13:06:08: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.008106 | Elapsed: 10.15s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:06:19,084 WARNING  [*] 13:06:19: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.053794 | Elapsed: 10.16s | FPR 0.0003 -> TPR 0.9841 & F1 0.9920 | AUC 0.9979
2023-03-18 13:06:23,215 WARNING  [*] Sat Mar 18 13:06:23 2023:    5    | Tr.loss: 0.047737 | Elapsed:   55.17  s | FPR 0.0003 -> TPR: 0.66 & F1: 0.79 | AUC: 0.9982
2023-03-18 13:06:23,216 WARNING  [*] Started epoch: 6
2023-03-18 13:06:23,335 WARNING  [*] 13:06:23: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.094208 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9375 & F1 0.9677 | AUC 0.9966
2023-03-18 13:06:33,496 WARNING  [*] 13:06:33: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.036219 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9706 & F1 0.9851 | AUC 0.9991
2023-03-18 13:06:43,658 WARNING  [*] 13:06:43: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.023828 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9865 & F1 0.9932 | AUC 0.9995
2023-03-18 13:06:47,912 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 13:06:47,973 WARNING  [!] Sat Mar 18 13:06:47 2023: Dumped results:
                model       : 1679140905-model.torch
		train time  : 1679140905-trainTime.npy
		train losses: 1679140905-trainLosses.npy
		train AUC   : 1679140905-auc.npy
		train F1s   : 1679140905-trainF1s.npy
		train TPRs  : 1679140905-trainTPRs.npy
2023-03-18 13:06:48,026 WARNING  [!] Evaluating model on training set...
2023-03-18 13:07:01,785 WARNING  [!] This fold metrics on training set:
2023-03-18 13:07:01,793 WARNING 	AUC: 0.9994
2023-03-18 13:07:01,798 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:07:01,806 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:07:01,828 WARNING 	FPR: 0.001 | TPR: 0.9611 | F1: 0.9799
2023-03-18 13:07:01,834 WARNING 	FPR: 0.003 | TPR: 0.9818 | F1: 0.9901
2023-03-18 13:07:01,842 WARNING 	FPR: 0.01 | TPR: 0.9925 | F1: 0.9939
2023-03-18 13:07:01,864 WARNING 	FPR: 0.03 | TPR: 0.9973 | F1: 0.9916
2023-03-18 13:07:01,870 WARNING 	FPR: 0.1 | TPR: 0.9996 | F1: 0.9764
2023-03-18 13:07:01,870 WARNING  [!] Evaluating model on validation set...
2023-03-18 13:07:08,758 WARNING  [!] This fold metrics on validation set:
2023-03-18 13:07:08,766 WARNING 	AUC: 0.9978
2023-03-18 13:07:08,770 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:07:08,781 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:07:08,788 WARNING 	FPR: 0.001 | TPR: 0.8932 | F1: 0.9433
2023-03-18 13:07:08,792 WARNING 	FPR: 0.003 | TPR: 0.9489 | F1: 0.9731
2023-03-18 13:07:08,802 WARNING 	FPR: 0.01 | TPR: 0.9736 | F1: 0.9842
2023-03-18 13:07:08,816 WARNING 	FPR: 0.03 | TPR: 0.9867 | F1: 0.9862
2023-03-18 13:07:08,823 WARNING 	FPR: 0.1 | TPR: 0.9961 | F1: 0.9747
2023-03-18 13:07:08,979 WARNING  [3/3] Train set size: 50751, Validation set size: 25375
2023-03-18 13:07:11,148 WARNING  [!] Saved dataset splits to dataset_splits_1679141228.npz
2023-03-18 13:07:11,241 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 5.3973e6
2023-03-18 13:07:11,241 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 13:07:11,277 WARNING  [*] Started epoch: 1
2023-03-18 13:07:11,523 WARNING  [*] 13:07:11: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 2.589274 | Elapsed: 0.24s | FPR 0.0003 -> TPR 0.0000 & F1 0.0000 | AUC 0.5090
2023-03-18 13:07:21,723 WARNING  [*] 13:07:21: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.378136 | Elapsed: 10.20s | FPR 0.0003 -> TPR 0.1515 & F1 0.2632 | AUC 0.9051
2023-03-18 13:07:31,905 WARNING  [*] 13:07:31: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.388735 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.5312 & F1 0.6939 | AUC 0.8802
2023-03-18 13:07:42,065 WARNING  [*] 13:07:42: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.233380 | Elapsed: 10.16s | FPR 0.0003 -> TPR 0.7465 & F1 0.8548 | AUC 0.9684
2023-03-18 13:07:52,246 WARNING  [*] 13:07:52: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.272443 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.7576 & F1 0.8621 | AUC 0.9528
2023-03-18 13:08:02,422 WARNING  [*] 13:08:02: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.175600 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.7288 & F1 0.8431 | AUC 0.9810
2023-03-18 13:08:06,528 WARNING  [*] Sat Mar 18 13:08:06 2023:    1    | Tr.loss: 0.344974 | Elapsed:   55.25  s | FPR 0.0003 -> TPR: 0.03 & F1: 0.05 | AUC: 0.9159
2023-03-18 13:08:06,528 WARNING  [*] Started epoch: 2
2023-03-18 13:08:06,672 WARNING  [*] 13:08:06: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.349545 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.6508 & F1 0.7885 | AUC 0.9360
2023-03-18 13:08:16,860 WARNING  [*] 13:08:16: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.179933 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.8750 & F1 0.9333 | AUC 0.9846
2023-03-18 13:08:27,040 WARNING  [*] 13:08:27: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.173431 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.8500 & F1 0.9189 | AUC 0.9808
2023-03-18 13:08:37,210 WARNING  [*] 13:08:37: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.096080 | Elapsed: 10.16s | FPR 0.0003 -> TPR 0.8750 & F1 0.9333 | AUC 0.9950
2023-03-18 13:08:47,396 WARNING  [*] 13:08:47: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.114285 | Elapsed: 10.18s | FPR 0.0003 -> TPR 0.8611 & F1 0.9254 | AUC 0.9911
2023-03-18 13:08:57,565 WARNING  [*] 13:08:57: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.132163 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.8382 & F1 0.9120 | AUC 0.9890
2023-03-18 13:09:01,614 WARNING  [*] Sat Mar 18 13:09:01 2023:    2    | Tr.loss: 0.139452 | Elapsed:   55.09  s | FPR 0.0003 -> TPR: 0.30 & F1: 0.46 | AUC: 0.9866
2023-03-18 13:09:01,614 WARNING  [*] Started epoch: 3
2023-03-18 13:09:01,777 WARNING  [*] 13:09:01: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.133568 | Elapsed: 0.14s | FPR 0.0003 -> TPR 0.9231 & F1 0.9600 | AUC 0.9911
2023-03-18 13:09:12,014 WARNING  [*] 13:09:12: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.064087 | Elapsed: 10.23s | FPR 0.0003 -> TPR 0.9412 & F1 0.9697 | AUC 0.9963
2023-03-18 13:09:22,180 WARNING  [*] 13:09:22: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.035894 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9996
2023-03-18 13:09:32,353 WARNING  [*] 13:09:32: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.070682 | Elapsed: 10.16s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9977
2023-03-18 13:09:42,523 WARNING  [*] 13:09:42: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.014049 | Elapsed: 10.15s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:09:52,674 WARNING  [*] 13:09:52: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.062527 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9986
2023-03-18 13:09:56,770 WARNING  [*] Sat Mar 18 13:09:56 2023:    3    | Tr.loss: 0.077627 | Elapsed:   55.16  s | FPR 0.0003 -> TPR: 0.56 & F1: 0.72 | AUC: 0.9957
2023-03-18 13:09:56,771 WARNING  [*] Started epoch: 4
2023-03-18 13:09:56,888 WARNING  [*] 13:09:56: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.031599 | Elapsed: 0.11s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:10:07,042 WARNING  [*] 13:10:07: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.017662 | Elapsed: 10.15s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:10:17,220 WARNING  [*] 13:10:17: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.030626 | Elapsed: 10.18s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:10:27,408 WARNING  [*] 13:10:27: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.028004 | Elapsed: 10.18s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:10:37,556 WARNING  [*] 13:10:37: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.026146 | Elapsed: 10.14s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:10:47,758 WARNING  [*] 13:10:47: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.037565 | Elapsed: 10.18s | FPR 0.0003 -> TPR 0.9733 & F1 0.9865 | AUC 0.9989
2023-03-18 13:10:51,783 WARNING  [*] Sat Mar 18 13:10:51 2023:    4    | Tr.loss: 0.054025 | Elapsed:   55.01  s | FPR 0.0003 -> TPR: 0.68 & F1: 0.81 | AUC: 0.9978
2023-03-18 13:10:51,783 WARNING  [*] Started epoch: 5
2023-03-18 13:10:51,912 WARNING  [*] 13:10:51: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.039434 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9828 & F1 0.9913 | AUC 0.9991
2023-03-18 13:11:02,102 WARNING  [*] 13:11:02: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.043039 | Elapsed: 10.19s | FPR 0.0003 -> TPR 0.9836 & F1 0.9917 | AUC 0.9987
2023-03-18 13:11:12,274 WARNING  [*] 13:11:12: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.007943 | Elapsed: 10.17s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:11:22,451 WARNING  [*] 13:11:22: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.007725 | Elapsed: 10.17s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:11:32,631 WARNING  [*] 13:11:32: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.061670 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.9722 & F1 0.9859 | AUC 0.9970
2023-03-18 13:11:42,845 WARNING  [*] 13:11:42: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.010617 | Elapsed: 10.21s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:11:46,934 WARNING  [*] Sat Mar 18 13:11:46 2023:    5    | Tr.loss: 0.042902 | Elapsed:   55.15  s | FPR 0.0003 -> TPR: 0.67 & F1: 0.80 | AUC: 0.9986
2023-03-18 13:11:46,934 WARNING  [*] Started epoch: 6
2023-03-18 13:11:47,078 WARNING  [*] 13:11:47: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.047551 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.9688 & F1 0.9841 | AUC 0.9990
2023-03-18 13:11:57,250 WARNING  [*] 13:11:57: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.157081 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.9014 & F1 0.9481 | AUC 0.9913
2023-03-18 13:12:07,402 WARNING  [*] 13:12:07: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.043765 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9991
2023-03-18 13:12:11,271 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 13:12:11,311 WARNING  [!] Sat Mar 18 13:12:11 2023: Dumped results:
                model       : 1679141228-model.torch
		train time  : 1679141228-trainTime.npy
		train losses: 1679141228-trainLosses.npy
		train AUC   : 1679141228-auc.npy
		train F1s   : 1679141228-trainF1s.npy
		train TPRs  : 1679141228-trainTPRs.npy
2023-03-18 13:12:11,346 WARNING  [!] Evaluating model on training set...
2023-03-18 13:12:25,106 WARNING  [!] This fold metrics on training set:
2023-03-18 13:12:25,123 WARNING 	AUC: 0.9995
2023-03-18 13:12:25,133 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:12:25,146 WARNING 	FPR: 0.0003 | TPR: 0.8849 | F1: 0.9389
2023-03-18 13:12:25,163 WARNING 	FPR: 0.001 | TPR: 0.9667 | F1: 0.9828
2023-03-18 13:12:25,175 WARNING 	FPR: 0.003 | TPR: 0.9839 | F1: 0.9912
2023-03-18 13:12:25,189 WARNING 	FPR: 0.01 | TPR: 0.9934 | F1: 0.9943
2023-03-18 13:12:25,205 WARNING 	FPR: 0.03 | TPR: 0.9978 | F1: 0.9918
2023-03-18 13:12:25,230 WARNING 	FPR: 0.1 | TPR: 0.9994 | F1: 0.9764
2023-03-18 13:12:25,230 WARNING  [!] Evaluating model on validation set...
2023-03-18 13:12:32,118 WARNING  [!] This fold metrics on validation set:
2023-03-18 13:12:32,123 WARNING 	AUC: 0.9975
2023-03-18 13:12:32,129 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:12:32,131 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:12:32,139 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:12:32,150 WARNING 	FPR: 0.003 | TPR: 0.9417 | F1: 0.9693
2023-03-18 13:12:32,156 WARNING 	FPR: 0.01 | TPR: 0.9708 | F1: 0.9829
2023-03-18 13:12:32,162 WARNING 	FPR: 0.03 | TPR: 0.9859 | F1: 0.9858
2023-03-18 13:12:32,165 WARNING 	FPR: 0.1 | TPR: 0.9955 | F1: 0.9751
2023-03-18 13:12:32,261 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_50000_limNone_r1763_t5\50000_metrics_validation.json
2023-03-18 13:12:32,262 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_50000_limNone_r1763_t5\50000_metrics_training.json
2023-03-18 13:12:32,263 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9973
	FPR: 0.0001 -- TPR: 0.0000 -- F1: 0.0000
	FPR: 0.0003 -- TPR: 0.0000 -- F1: 0.0000
	FPR:  0.001 -- TPR: 0.2977 -- F1: 0.3144
	FPR:  0.003 -- TPR: 0.9369 -- F1: 0.9667
	FPR:   0.01 -- TPR: 0.9690 -- F1: 0.9819
	FPR:   0.03 -- TPR: 0.9862 -- F1: 0.9860
	FPR:    0.1 -- TPR: 0.9955 -- F1: 0.9746

2023-03-18 13:12:32,333 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_backdoor
2023-03-18 13:16:32,110 WARNING Finished... Took: 239.78s
2023-03-18 13:16:32,111 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_clean
2023-03-18 13:21:50,109 WARNING Finished... Took: 318.00s
2023-03-18 13:21:50,109 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_coinminer
2023-03-18 13:23:29,885 WARNING Finished... Took: 99.78s
2023-03-18 13:23:29,885 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_dropper
2023-03-18 13:25:05,519 WARNING Finished... Took: 95.63s
2023-03-18 13:25:05,520 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_keylogger
2023-03-18 13:26:05,367 WARNING Finished... Took: 59.85s
2023-03-18 13:26:05,382 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_ransomware
2023-03-18 13:29:11,521 WARNING Finished... Took: 186.14s
2023-03-18 13:29:11,521 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_rat
2023-03-18 13:29:56,545 WARNING Finished... Took: 45.02s
2023-03-18 13:29:56,545 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_trojan
2023-03-18 13:32:17,764 WARNING Finished... Took: 141.22s
2023-03-18 13:32:17,764 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_trainset\report_windows_syswow64
2023-03-18 13:32:19,581 WARNING Finished... Took: 1.82s
2023-03-18 13:32:19,587 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_70000_seqlen_512\y_train_full.npy
2023-03-18 13:32:19,792 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_70000_seqlen_512\y_names_train_full.json
2023-03-18 13:32:19,793 WARNING  [!] Initialized tokenizer without pre-trained model.
	You need to train tokenizer with .train() or specify 'model_path=' during initialization!
2023-03-18 13:32:19,794 WARNING  [*] Initializing tokenizer training...
2023-03-18 13:32:19,794 WARNING  [*] Data preparation for SentencePiece tokenizer...
2023-03-18 13:34:54,518 WARNING  [*] Saving to disk...
2023-03-18 13:34:59,382 WARNING  [!] Training tokenizer with command: --input=out_vocab_1679128542\nebula_vocab_70000_seqlen_512\tokenizer_70000_trainset_1679142894.txt --model_prefix=out_vocab_1679128542\nebula_vocab_70000_seqlen_512\tokenizer_70000 --vocab_size=70000 --model_type=bpe --split_by_number=False --max_sentence_length=4192 --max_sentencepiece_length=64
2023-03-18 13:39:50,224 WARNING  [!] Loaded vocab with size 70001 from out_vocab_1679128542\nebula_vocab_70000_seqlen_512\tokenizer_70000.vocab
2023-03-18 13:39:50,795 WARNING  [*] Encoding and padding...
2023-03-18 13:46:42,014 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_70000_seqlen_512\x_train_full.npy
2023-03-18 13:46:45,487 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_backdoor
2023-03-18 13:47:20,650 WARNING Finished... Took: 35.16s
2023-03-18 13:47:20,650 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_clean
2023-03-18 13:48:57,647 WARNING Finished... Took: 97.00s
2023-03-18 13:48:57,647 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_coinminer
2023-03-18 13:49:22,922 WARNING Finished... Took: 25.28s
2023-03-18 13:49:22,922 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_dropper
2023-03-18 13:49:27,116 WARNING Finished... Took: 4.19s
2023-03-18 13:49:27,116 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_keylogger
2023-03-18 13:49:37,388 WARNING Finished... Took: 10.27s
2023-03-18 13:49:37,388 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_ransomware
2023-03-18 13:51:47,477 WARNING Finished... Took: 130.09s
2023-03-18 13:51:47,477 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_rat
2023-03-18 13:52:03,760 WARNING Finished... Took: 16.28s
2023-03-18 13:52:03,760 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_trojan
2023-03-18 13:52:15,078 WARNING Finished... Took: 11.32s
2023-03-18 13:52:15,078 WARNING Filtering and normalizing C:\Users\dtrizna\Code\nebula\evaluation\preprocessing\..\..\data\data_raw\windows_emulation_testset\report_windows_syswow64
2023-03-18 13:52:15,570 WARNING Finished... Took: 0.49s
2023-03-18 13:52:15,573 WARNING  [!] Saved Y as out_vocab_1679128542\nebula_vocab_70000_seqlen_512\y_test_full.npy
2023-03-18 13:52:15,624 WARNING  [!] Saved Y names as out_vocab_1679128542\nebula_vocab_70000_seqlen_512\y_names_test_full.json
2023-03-18 13:52:15,666 WARNING  [!] Successfully loaded pre-trained tokenizer model!
2023-03-18 13:52:15,705 WARNING  [!] Loaded vocab with size 70001 from out_vocab_1679128542\nebula_vocab_70000_seqlen_512\tokenizer_70000_vocab.json
2023-03-18 13:52:15,707 WARNING  [*] Encoding and padding...
2023-03-18 13:53:33,318 WARNING  [!] Saved X as out_vocab_1679128542\nebula_vocab_70000_seqlen_512\x_test_full.npy
2023-03-18 13:53:34,205 WARNING  [!!!] Starting CV over 70000!
2023-03-18 13:53:34,303 WARNING  [!] Training time budget: 300min
2023-03-18 13:53:34,304 WARNING  [!] Model config: {'vocab_size': 70001, 'maxlen': 512, 'chunk_size': 64, 'dModel': 64, 'nHeads': 8, 'dHidden': 256, 'nLayers': 2, 'numClasses': 1, 'hiddenNeurons': [64], 'layerNorm': False, 'dropout': 0.3, 'mean_over_sequence': False, 'norm_first': True}
2023-03-18 13:53:34,377 WARNING  [1/3] Train set size: 50750, Validation set size: 25376
2023-03-18 13:53:36,319 WARNING  [!] Saved dataset splits to dataset_splits_1679144014.npz
2023-03-18 13:53:36,422 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 6.6773e6
2023-03-18 13:53:36,422 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 13:53:36,454 WARNING  [*] Started epoch: 1
2023-03-18 13:53:36,843 WARNING  [*] 13:53:36: Train Epoch: 1 [  0  /50750 (0 %)] | Loss: 4.435430 | Elapsed: 0.37s | FPR 0.0003 -> TPR 0.0806 & F1 0.1493 | AUC 0.4597
2023-03-18 13:53:46,806 WARNING  [*] 13:53:46: Train Epoch: 1 [9600 /50750 (19%)] | Loss: 0.338192 | Elapsed: 9.94s | FPR 0.0003 -> TPR 0.6216 & F1 0.7667 | AUC 0.9038
2023-03-18 13:53:56,768 WARNING  [*] 13:53:56: Train Epoch: 1 [19200/50750 (38%)] | Loss: 0.364940 | Elapsed: 9.95s | FPR 0.0003 -> TPR 0.4167 & F1 0.5882 | AUC 0.9175
2023-03-18 13:54:06,770 WARNING  [*] 13:54:06: Train Epoch: 1 [28800/50750 (57%)] | Loss: 0.210745 | Elapsed: 9.99s | FPR 0.0003 -> TPR 0.7302 & F1 0.8440 | AUC 0.9734
2023-03-18 13:54:16,797 WARNING  [*] 13:54:16: Train Epoch: 1 [38400/50750 (76%)] | Loss: 0.169754 | Elapsed: 10.03s | FPR 0.0003 -> TPR 0.7222 & F1 0.8387 | AUC 0.9747
2023-03-18 13:54:26,905 WARNING  [*] 13:54:26: Train Epoch: 1 [48000/50750 (95%)] | Loss: 0.172011 | Elapsed: 10.08s | FPR 0.0003 -> TPR 0.8406 & F1 0.9134 | AUC 0.9860
2023-03-18 13:54:30,724 WARNING  [*] Sat Mar 18 13:54:30 2023:    1    | Tr.loss: 0.353665 | Elapsed:   54.27  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9131
2023-03-18 13:54:30,724 WARNING  [*] Started epoch: 2
2023-03-18 13:54:30,853 WARNING  [*] 13:54:30: Train Epoch: 2 [  0  /50750 (0 %)] | Loss: 0.243006 | Elapsed: 0.12s | FPR 0.0003 -> TPR 0.7931 & F1 0.8846 | AUC 0.9651
2023-03-18 13:54:41,005 WARNING  [*] 13:54:41: Train Epoch: 2 [9600 /50750 (19%)] | Loss: 0.247182 | Elapsed: 10.13s | FPR 0.0003 -> TPR 0.6986 & F1 0.8226 | AUC 0.9680
2023-03-18 13:54:51,161 WARNING  [*] 13:54:51: Train Epoch: 2 [19200/50750 (38%)] | Loss: 0.134445 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.8485 & F1 0.9180 | AUC 0.9898
2023-03-18 13:55:01,326 WARNING  [*] 13:55:01: Train Epoch: 2 [28800/50750 (57%)] | Loss: 0.092306 | Elapsed: 10.15s | FPR 0.0003 -> TPR 0.9104 & F1 0.9531 | AUC 0.9946
2023-03-18 13:55:11,506 WARNING  [*] 13:55:11: Train Epoch: 2 [38400/50750 (76%)] | Loss: 0.057904 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.9677 & F1 0.9836 | AUC 0.9987
2023-03-18 13:55:21,667 WARNING  [*] 13:55:21: Train Epoch: 2 [48000/50750 (95%)] | Loss: 0.087561 | Elapsed: 10.14s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9987
2023-03-18 13:55:25,775 WARNING  [*] Sat Mar 18 13:55:25 2023:    2    | Tr.loss: 0.139352 | Elapsed:   55.05  s | FPR 0.0003 -> TPR: 0.53 & F1: 0.69 | AUC: 0.9866
2023-03-18 13:55:25,775 WARNING  [*] Started epoch: 3
2023-03-18 13:55:25,904 WARNING  [*] 13:55:25: Train Epoch: 3 [  0  /50750 (0 %)] | Loss: 0.049457 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9726 & F1 0.9861 | AUC 0.9988
2023-03-18 13:55:36,049 WARNING  [*] 13:55:36: Train Epoch: 3 [9600 /50750 (19%)] | Loss: 0.054245 | Elapsed: 10.14s | FPR 0.0003 -> TPR 0.9677 & F1 0.9836 | AUC 0.9979
2023-03-18 13:55:46,237 WARNING  [*] 13:55:46: Train Epoch: 3 [19200/50750 (38%)] | Loss: 0.050781 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.9688 & F1 0.9841 | AUC 0.9991
2023-03-18 13:55:56,417 WARNING  [*] 13:55:56: Train Epoch: 3 [28800/50750 (57%)] | Loss: 0.083953 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.9692 & F1 0.9844 | AUC 0.9952
2023-03-18 13:56:06,579 WARNING  [*] 13:56:06: Train Epoch: 3 [38400/50750 (76%)] | Loss: 0.149125 | Elapsed: 10.14s | FPR 0.0003 -> TPR 0.7833 & F1 0.8785 | AUC 0.9921
2023-03-18 13:56:16,801 WARNING  [*] 13:56:16: Train Epoch: 3 [48000/50750 (95%)] | Loss: 0.042767 | Elapsed: 10.21s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9996
2023-03-18 13:56:20,791 WARNING  [*] Sat Mar 18 13:56:20 2023:    3    | Tr.loss: 0.072860 | Elapsed:   55.02  s | FPR 0.0003 -> TPR: 0.68 & F1: 0.81 | AUC: 0.9962
2023-03-18 13:56:20,791 WARNING  [*] Started epoch: 4
2023-03-18 13:56:20,938 WARNING  [*] 13:56:20: Train Epoch: 4 [  0  /50750 (0 %)] | Loss: 0.033108 | Elapsed: 0.14s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9995
2023-03-18 13:56:31,132 WARNING  [*] 13:56:31: Train Epoch: 4 [9600 /50750 (19%)] | Loss: 0.016954 | Elapsed: 10.19s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:56:41,316 WARNING  [*] 13:56:41: Train Epoch: 4 [19200/50750 (38%)] | Loss: 0.045326 | Elapsed: 10.18s | FPR 0.0003 -> TPR 0.9718 & F1 0.9857 | AUC 0.9981
2023-03-18 13:56:51,499 WARNING  [*] 13:56:51: Train Epoch: 4 [28800/50750 (57%)] | Loss: 0.056455 | Elapsed: 10.18s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9978
2023-03-18 13:57:01,670 WARNING  [*] 13:57:01: Train Epoch: 4 [38400/50750 (76%)] | Loss: 0.011156 | Elapsed: 10.16s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:57:11,849 WARNING  [*] 13:57:11: Train Epoch: 4 [48000/50750 (95%)] | Loss: 0.096871 | Elapsed: 10.18s | FPR 0.0003 -> TPR 0.8824 & F1 0.9375 | AUC 0.9954
2023-03-18 13:57:15,952 WARNING  [*] Sat Mar 18 13:57:15 2023:    4    | Tr.loss: 0.047472 | Elapsed:   55.16  s | FPR 0.0003 -> TPR: 0.81 & F1: 0.89 | AUC: 0.9983
2023-03-18 13:57:15,953 WARNING  [*] Started epoch: 5
2023-03-18 13:57:16,081 WARNING  [*] 13:57:16: Train Epoch: 5 [  0  /50750 (0 %)] | Loss: 0.030462 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9995
2023-03-18 13:57:26,256 WARNING  [*] 13:57:26: Train Epoch: 5 [9600 /50750 (19%)] | Loss: 0.060921 | Elapsed: 10.17s | FPR 0.0003 -> TPR 0.9718 & F1 0.9857 | AUC 0.9981
2023-03-18 13:57:36,432 WARNING  [*] 13:57:36: Train Epoch: 5 [19200/50750 (38%)] | Loss: 0.014683 | Elapsed: 10.16s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:57:46,632 WARNING  [*] 13:57:46: Train Epoch: 5 [28800/50750 (57%)] | Loss: 0.013623 | Elapsed: 10.18s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:57:56,833 WARNING  [*] 13:57:56: Train Epoch: 5 [38400/50750 (76%)] | Loss: 0.007990 | Elapsed: 10.20s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:58:07,027 WARNING  [*] 13:58:07: Train Epoch: 5 [48000/50750 (95%)] | Loss: 0.028960 | Elapsed: 10.19s | FPR 0.0003 -> TPR 0.9844 & F1 0.9921 | AUC 0.9991
2023-03-18 13:58:11,072 WARNING  [*] Sat Mar 18 13:58:11 2023:    5    | Tr.loss: 0.035671 | Elapsed:   55.12  s | FPR 0.0003 -> TPR: 0.83 & F1: 0.91 | AUC: 0.9990
2023-03-18 13:58:11,072 WARNING  [*] Started epoch: 6
2023-03-18 13:58:11,196 WARNING  [*] 13:58:11: Train Epoch: 6 [  0  /50750 (0 %)] | Loss: 0.012013 | Elapsed: 0.10s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 13:58:21,358 WARNING  [*] 13:58:21: Train Epoch: 6 [9600 /50750 (19%)] | Loss: 0.029819 | Elapsed: 10.16s | FPR 0.0003 -> TPR 0.9833 & F1 0.9916 | AUC 0.9996
2023-03-18 13:58:31,561 WARNING  [*] 13:58:31: Train Epoch: 6 [19200/50750 (38%)] | Loss: 0.064895 | Elapsed: 10.19s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9978
2023-03-18 13:58:36,472 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 13:58:36,512 WARNING  [!] Sat Mar 18 13:58:36 2023: Dumped results:
                model       : 1679144014-model.torch
		train time  : 1679144014-trainTime.npy
		train losses: 1679144014-trainLosses.npy
		train AUC   : 1679144014-auc.npy
		train F1s   : 1679144014-trainF1s.npy
		train TPRs  : 1679144014-trainTPRs.npy
2023-03-18 13:58:36,558 WARNING  [!] Evaluating model on training set...
2023-03-18 13:58:50,273 WARNING  [!] This fold metrics on training set:
2023-03-18 13:58:50,282 WARNING 	AUC: 0.9996
2023-03-18 13:58:50,290 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:58:50,297 WARNING 	FPR: 0.0003 | TPR: 0.9214 | F1: 0.9590
2023-03-18 13:58:50,318 WARNING 	FPR: 0.001 | TPR: 0.9558 | F1: 0.9771
2023-03-18 13:58:50,324 WARNING 	FPR: 0.003 | TPR: 0.9801 | F1: 0.9892
2023-03-18 13:58:50,333 WARNING 	FPR: 0.01 | TPR: 0.9949 | F1: 0.9951
2023-03-18 13:58:50,353 WARNING 	FPR: 0.03 | TPR: 0.9980 | F1: 0.9920
2023-03-18 13:58:50,360 WARNING 	FPR: 0.1 | TPR: 0.9997 | F1: 0.9765
2023-03-18 13:58:50,360 WARNING  [!] Evaluating model on validation set...
2023-03-18 13:58:57,226 WARNING  [!] This fold metrics on validation set:
2023-03-18 13:58:57,229 WARNING 	AUC: 0.9969
2023-03-18 13:58:57,233 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:58:57,239 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:58:57,250 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 13:58:57,256 WARNING 	FPR: 0.003 | TPR: 0.9199 | F1: 0.9576
2023-03-18 13:58:57,262 WARNING 	FPR: 0.01 | TPR: 0.9644 | F1: 0.9795
2023-03-18 13:58:57,265 WARNING 	FPR: 0.03 | TPR: 0.9846 | F1: 0.9851
2023-03-18 13:58:57,271 WARNING 	FPR: 0.1 | TPR: 0.9953 | F1: 0.9744
2023-03-18 13:58:57,459 WARNING  [2/3] Train set size: 50751, Validation set size: 25375
2023-03-18 13:58:59,617 WARNING  [!] Saved dataset splits to dataset_splits_1679144337.npz
2023-03-18 13:58:59,736 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 6.6773e6
2023-03-18 13:58:59,736 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 13:58:59,755 WARNING  [*] Started epoch: 1
2023-03-18 13:59:00,189 WARNING  [*] 13:59:00: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 2.682284 | Elapsed: 0.43s | FPR 0.0003 -> TPR 0.0345 & F1 0.0667 | AUC 0.5390
2023-03-18 13:59:10,489 WARNING  [*] 13:59:10: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.445884 | Elapsed: 10.28s | FPR 0.0003 -> TPR 0.3288 & F1 0.4948 | AUC 0.8305
2023-03-18 13:59:20,811 WARNING  [*] 13:59:20: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.289428 | Elapsed: 10.30s | FPR 0.0003 -> TPR 0.7000 & F1 0.8235 | AUC 0.9367
2023-03-18 13:59:31,007 WARNING  [*] 13:59:31: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.430721 | Elapsed: 10.18s | FPR 0.0003 -> TPR 0.4912 & F1 0.6588 | AUC 0.9074
2023-03-18 13:59:41,226 WARNING  [*] 13:59:41: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.241528 | Elapsed: 10.22s | FPR 0.0003 -> TPR 0.5429 & F1 0.7037 | AUC 0.9476
2023-03-18 13:59:51,441 WARNING  [*] 13:59:51: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.202170 | Elapsed: 10.20s | FPR 0.0003 -> TPR 0.8873 & F1 0.9403 | AUC 0.9713
2023-03-18 13:59:55,544 WARNING  [*] Sat Mar 18 13:59:55 2023:    1    | Tr.loss: 0.346836 | Elapsed:   55.79  s | FPR 0.0003 -> TPR: 0.00 & F1: 0.00 | AUC: 0.9183
2023-03-18 13:59:55,544 WARNING  [*] Started epoch: 2
2023-03-18 13:59:55,672 WARNING  [*] 13:59:55: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.193516 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9000 & F1 0.9474 | AUC 0.9759
2023-03-18 14:00:05,875 WARNING  [*] 14:00:05: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.094060 | Elapsed: 10.20s | FPR 0.0003 -> TPR 0.5833 & F1 0.7368 | AUC 0.9875
2023-03-18 14:00:16,086 WARNING  [*] 14:00:16: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.173494 | Elapsed: 10.20s | FPR 0.0003 -> TPR 0.8788 & F1 0.9355 | AUC 0.9835
2023-03-18 14:00:26,298 WARNING  [*] 14:00:26: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.117374 | Elapsed: 10.21s | FPR 0.0003 -> TPR 0.8400 & F1 0.9130 | AUC 0.9883
2023-03-18 14:00:36,509 WARNING  [*] 14:00:36: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.116927 | Elapsed: 10.20s | FPR 0.0003 -> TPR 0.8594 & F1 0.9244 | AUC 0.9961
2023-03-18 14:00:46,743 WARNING  [*] 14:00:46: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.070908 | Elapsed: 10.23s | FPR 0.0003 -> TPR 0.9275 & F1 0.9624 | AUC 0.9977
2023-03-18 14:00:50,768 WARNING  [*] Sat Mar 18 14:00:50 2023:    2    | Tr.loss: 0.123049 | Elapsed:   55.22  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.61 | AUC: 0.9894
2023-03-18 14:00:50,768 WARNING  [*] Started epoch: 3
2023-03-18 14:00:50,884 WARNING  [*] 14:00:50: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.121127 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.8382 & F1 0.9120 | AUC 0.9900
2023-03-18 14:01:01,151 WARNING  [*] 14:01:01: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.041056 | Elapsed: 10.25s | FPR 0.0003 -> TPR 0.9867 & F1 0.9933 | AUC 0.9989
2023-03-18 14:01:11,346 WARNING  [*] 14:01:11: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.074573 | Elapsed: 10.19s | FPR 0.0003 -> TPR 0.9559 & F1 0.9774 | AUC 0.9986
2023-03-18 14:01:21,564 WARNING  [*] 14:01:21: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.026690 | Elapsed: 10.22s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:01:31,768 WARNING  [*] 14:01:31: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.024105 | Elapsed: 10.19s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:01:42,074 WARNING  [*] 14:01:42: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.032248 | Elapsed: 10.29s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:01:46,162 WARNING  [*] Sat Mar 18 14:01:46 2023:    3    | Tr.loss: 0.064063 | Elapsed:   55.39  s | FPR 0.0003 -> TPR: 0.51 & F1: 0.67 | AUC: 0.9969
2023-03-18 14:01:46,162 WARNING  [*] Started epoch: 4
2023-03-18 14:01:46,288 WARNING  [*] 14:01:46: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.056728 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9286 & F1 0.9630 | AUC 0.9973
2023-03-18 14:01:56,499 WARNING  [*] 14:01:56: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.083414 | Elapsed: 10.21s | FPR 0.0003 -> TPR 0.9683 & F1 0.9839 | AUC 0.9979
2023-03-18 14:02:06,710 WARNING  [*] 14:02:06: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.036039 | Elapsed: 10.21s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9996
2023-03-18 14:02:16,922 WARNING  [*] 14:02:16: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.028954 | Elapsed: 10.19s | FPR 0.0003 -> TPR 0.9863 & F1 0.9931 | AUC 0.9995
2023-03-18 14:02:27,126 WARNING  [*] 14:02:27: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.020652 | Elapsed: 10.19s | FPR 0.0003 -> TPR 0.9848 & F1 0.9924 | AUC 0.9996
2023-03-18 14:02:37,321 WARNING  [*] 14:02:37: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.026694 | Elapsed: 10.20s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:02:41,398 WARNING  [*] Sat Mar 18 14:02:41 2023:    4    | Tr.loss: 0.045472 | Elapsed:   55.24  s | FPR 0.0003 -> TPR: 0.66 & F1: 0.79 | AUC: 0.9984
2023-03-18 14:02:41,399 WARNING  [*] Started epoch: 5
2023-03-18 14:02:41,542 WARNING  [*] 14:02:41: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.039745 | Elapsed: 0.14s | FPR 0.0003 -> TPR 0.9701 & F1 0.9848 | AUC 0.9990
2023-03-18 14:02:51,782 WARNING  [*] 14:02:51: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.012396 | Elapsed: 10.24s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:03:01,984 WARNING  [*] 14:03:01: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.045599 | Elapsed: 10.20s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:03:12,189 WARNING  [*] 14:03:12: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.043427 | Elapsed: 10.20s | FPR 0.0003 -> TPR 0.9853 & F1 0.9926 | AUC 0.9991
2023-03-18 14:03:22,393 WARNING  [*] 14:03:22: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.052123 | Elapsed: 10.20s | FPR 0.0003 -> TPR 0.9836 & F1 0.9917 | AUC 0.9979
2023-03-18 14:03:32,612 WARNING  [*] 14:03:32: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.015147 | Elapsed: 10.21s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:03:36,720 WARNING  [*] Sat Mar 18 14:03:36 2023:    5    | Tr.loss: 0.036127 | Elapsed:   55.32  s | FPR 0.0003 -> TPR: 0.67 & F1: 0.80 | AUC: 0.9989
2023-03-18 14:03:36,720 WARNING  [*] Started epoch: 6
2023-03-18 14:03:36,850 WARNING  [*] 14:03:36: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.120301 | Elapsed: 0.13s | FPR 0.0003 -> TPR 0.7414 & F1 0.8515 | AUC 0.9932
2023-03-18 14:03:47,078 WARNING  [*] 14:03:47: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.003818 | Elapsed: 10.22s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:03:57,283 WARNING  [*] 14:03:57: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.095113 | Elapsed: 10.18s | FPR 0.0003 -> TPR 0.9718 & F1 0.9857 | AUC 0.9961
2023-03-18 14:03:59,737 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 14:03:59,777 WARNING  [!] Sat Mar 18 14:03:59 2023: Dumped results:
                model       : 1679144337-model.torch
		train time  : 1679144337-trainTime.npy
		train losses: 1679144337-trainLosses.npy
		train AUC   : 1679144337-auc.npy
		train F1s   : 1679144337-trainF1s.npy
		train TPRs  : 1679144337-trainTPRs.npy
2023-03-18 14:03:59,824 WARNING  [!] Evaluating model on training set...
2023-03-18 14:04:13,578 WARNING  [!] This fold metrics on training set:
2023-03-18 14:04:13,581 WARNING 	AUC: 0.9995
2023-03-18 14:04:13,588 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 14:04:13,610 WARNING 	FPR: 0.0003 | TPR: 0.9009 | F1: 0.9478
2023-03-18 14:04:13,616 WARNING 	FPR: 0.001 | TPR: 0.9635 | F1: 0.9812
2023-03-18 14:04:13,624 WARNING 	FPR: 0.003 | TPR: 0.9800 | F1: 0.9892
2023-03-18 14:04:13,648 WARNING 	FPR: 0.01 | TPR: 0.9951 | F1: 0.9951
2023-03-18 14:04:13,653 WARNING 	FPR: 0.03 | TPR: 0.9984 | F1: 0.9921
2023-03-18 14:04:13,662 WARNING 	FPR: 0.1 | TPR: 0.9996 | F1: 0.9768
2023-03-18 14:04:13,662 WARNING  [!] Evaluating model on validation set...
2023-03-18 14:04:20,542 WARNING  [!] This fold metrics on validation set:
2023-03-18 14:04:20,546 WARNING 	AUC: 0.9972
2023-03-18 14:04:20,549 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 14:04:20,557 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 14:04:20,566 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 14:04:20,573 WARNING 	FPR: 0.003 | TPR: 0.9309 | F1: 0.9635
2023-03-18 14:04:20,578 WARNING 	FPR: 0.01 | TPR: 0.9725 | F1: 0.9837
2023-03-18 14:04:20,582 WARNING 	FPR: 0.03 | TPR: 0.9872 | F1: 0.9864
2023-03-18 14:04:20,588 WARNING 	FPR: 0.1 | TPR: 0.9949 | F1: 0.9746
2023-03-18 14:04:20,739 WARNING  [3/3] Train set size: 50751, Validation set size: 25375
2023-03-18 14:04:22,869 WARNING  [!] Saved dataset splits to dataset_splits_1679144660.npz
2023-03-18 14:04:22,985 WARNING  [!] Iniatialized TransformerEncoderChunks. Total trainable parameters: 6.6773e6
2023-03-18 14:04:22,985 WARNING  [*] Training time budget set: 5.0 min
2023-03-18 14:04:23,023 WARNING  [*] Started epoch: 1
2023-03-18 14:04:23,340 WARNING  [*] 14:04:23: Train Epoch: 1 [  0  /50751 (0 %)] | Loss: 2.212895 | Elapsed: 0.31s | FPR 0.0003 -> TPR 0.0469 & F1 0.0896 | AUC 0.5112
2023-03-18 14:04:33,571 WARNING  [*] 14:04:33: Train Epoch: 1 [9600 /50751 (19%)] | Loss: 0.327113 | Elapsed: 10.22s | FPR 0.0003 -> TPR 0.6324 & F1 0.7748 | AUC 0.9095
2023-03-18 14:04:43,826 WARNING  [*] 14:04:43: Train Epoch: 1 [19200/50751 (38%)] | Loss: 0.389301 | Elapsed: 10.24s | FPR 0.0003 -> TPR 0.4107 & F1 0.5823 | AUC 0.8925
2023-03-18 14:04:54,061 WARNING  [*] 14:04:54: Train Epoch: 1 [28800/50751 (57%)] | Loss: 0.246580 | Elapsed: 10.23s | FPR 0.0003 -> TPR 0.7465 & F1 0.8548 | AUC 0.9471
2023-03-18 14:05:04,281 WARNING  [*] 14:05:04: Train Epoch: 1 [38400/50751 (76%)] | Loss: 0.132862 | Elapsed: 10.21s | FPR 0.0003 -> TPR 0.9028 & F1 0.9489 | AUC 0.9936
2023-03-18 14:05:14,513 WARNING  [*] 14:05:14: Train Epoch: 1 [48000/50751 (95%)] | Loss: 0.194405 | Elapsed: 10.23s | FPR 0.0003 -> TPR 0.7746 & F1 0.8730 | AUC 0.9791
2023-03-18 14:05:18,622 WARNING  [*] Sat Mar 18 14:05:18 2023:    1    | Tr.loss: 0.333679 | Elapsed:   55.60  s | FPR 0.0003 -> TPR: 0.02 & F1: 0.04 | AUC: 0.9209
2023-03-18 14:05:18,622 WARNING  [*] Started epoch: 2
2023-03-18 14:05:18,751 WARNING  [*] 14:05:18: Train Epoch: 2 [  0  /50751 (0 %)] | Loss: 0.270455 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.7679 & F1 0.8687 | AUC 0.9571
2023-03-18 14:05:28,993 WARNING  [*] 14:05:28: Train Epoch: 2 [9600 /50751 (19%)] | Loss: 0.088254 | Elapsed: 10.24s | FPR 0.0003 -> TPR 0.9726 & F1 0.9861 | AUC 0.9934
2023-03-18 14:05:39,235 WARNING  [*] 14:05:39: Train Epoch: 2 [19200/50751 (38%)] | Loss: 0.272089 | Elapsed: 10.22s | FPR 0.0003 -> TPR 0.2344 & F1 0.3797 | AUC 0.9640
2023-03-18 14:05:49,485 WARNING  [*] 14:05:49: Train Epoch: 2 [28800/50751 (57%)] | Loss: 0.150357 | Elapsed: 10.24s | FPR 0.0003 -> TPR 0.8305 & F1 0.9074 | AUC 0.9888
2023-03-18 14:05:59,694 WARNING  [*] 14:05:59: Train Epoch: 2 [38400/50751 (76%)] | Loss: 0.044602 | Elapsed: 10.21s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:06:09,929 WARNING  [*] 14:06:09: Train Epoch: 2 [48000/50751 (95%)] | Loss: 0.104131 | Elapsed: 10.23s | FPR 0.0003 -> TPR 0.9688 & F1 0.9841 | AUC 0.9952
2023-03-18 14:06:13,962 WARNING  [*] Sat Mar 18 14:06:13 2023:    2    | Tr.loss: 0.135026 | Elapsed:   55.34  s | FPR 0.0003 -> TPR: 0.44 & F1: 0.62 | AUC: 0.9873
2023-03-18 14:06:13,963 WARNING  [*] Started epoch: 3
2023-03-18 14:06:14,107 WARNING  [*] 14:06:14: Train Epoch: 3 [  0  /50751 (0 %)] | Loss: 0.046351 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9846 & F1 0.9922 | AUC 0.9990
2023-03-18 14:06:24,375 WARNING  [*] 14:06:24: Train Epoch: 3 [9600 /50751 (19%)] | Loss: 0.082819 | Elapsed: 10.27s | FPR 0.0003 -> TPR 0.9200 & F1 0.9583 | AUC 0.9957
2023-03-18 14:06:34,631 WARNING  [*] 14:06:34: Train Epoch: 3 [19200/50751 (38%)] | Loss: 0.135572 | Elapsed: 10.25s | FPR 0.0003 -> TPR 0.9385 & F1 0.9683 | AUC 0.9965
2023-03-18 14:06:44,865 WARNING  [*] 14:06:44: Train Epoch: 3 [28800/50751 (57%)] | Loss: 0.044610 | Elapsed: 10.23s | FPR 0.0003 -> TPR 0.9667 & F1 0.9831 | AUC 0.9987
2023-03-18 14:06:55,098 WARNING  [*] 14:06:55: Train Epoch: 3 [38400/50751 (76%)] | Loss: 0.015954 | Elapsed: 10.21s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:07:05,310 WARNING  [*] 14:07:05: Train Epoch: 3 [48000/50751 (95%)] | Loss: 0.029987 | Elapsed: 10.21s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:07:09,412 WARNING  [*] Sat Mar 18 14:07:09 2023:    3    | Tr.loss: 0.073607 | Elapsed:   55.45  s | FPR 0.0003 -> TPR: 0.52 & F1: 0.69 | AUC: 0.9960
2023-03-18 14:07:09,413 WARNING  [*] Started epoch: 4
2023-03-18 14:07:09,531 WARNING  [*] 14:07:09: Train Epoch: 4 [  0  /50751 (0 %)] | Loss: 0.041955 | Elapsed: 0.11s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:07:19,753 WARNING  [*] 14:07:19: Train Epoch: 4 [9600 /50751 (19%)] | Loss: 0.043025 | Elapsed: 10.21s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9995
2023-03-18 14:07:29,969 WARNING  [*] 14:07:29: Train Epoch: 4 [19200/50751 (38%)] | Loss: 0.037166 | Elapsed: 10.20s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:07:40,185 WARNING  [*] 14:07:40: Train Epoch: 4 [28800/50751 (57%)] | Loss: 0.042550 | Elapsed: 10.21s | FPR 0.0003 -> TPR 0.9833 & F1 0.9916 | AUC 0.9996
2023-03-18 14:07:50,444 WARNING  [*] 14:07:50: Train Epoch: 4 [38400/50751 (76%)] | Loss: 0.045063 | Elapsed: 10.25s | FPR 0.0003 -> TPR 0.9589 & F1 0.9790 | AUC 0.9980
2023-03-18 14:08:00,638 WARNING  [*] 14:08:00: Train Epoch: 4 [48000/50751 (95%)] | Loss: 0.015436 | Elapsed: 10.19s | FPR 0.0003 -> TPR 0.9559 & F1 0.9774 | AUC 0.9982
2023-03-18 14:08:04,684 WARNING  [*] Sat Mar 18 14:08:04 2023:    4    | Tr.loss: 0.048645 | Elapsed:   55.27  s | FPR 0.0003 -> TPR: 0.73 & F1: 0.85 | AUC: 0.9982
2023-03-18 14:08:04,684 WARNING  [*] Started epoch: 5
2023-03-18 14:08:04,812 WARNING  [*] 14:08:04: Train Epoch: 5 [  0  /50751 (0 %)] | Loss: 0.063430 | Elapsed: 0.11s | FPR 0.0003 -> TPR 0.9855 & F1 0.9927 | AUC 0.9936
2023-03-18 14:08:15,069 WARNING  [*] 14:08:15: Train Epoch: 5 [9600 /50751 (19%)] | Loss: 0.018306 | Elapsed: 10.25s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:08:25,285 WARNING  [*] 14:08:25: Train Epoch: 5 [19200/50751 (38%)] | Loss: 0.014852 | Elapsed: 10.21s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:08:35,517 WARNING  [*] 14:08:35: Train Epoch: 5 [28800/50751 (57%)] | Loss: 0.010432 | Elapsed: 10.22s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:08:45,762 WARNING  [*] 14:08:45: Train Epoch: 5 [38400/50751 (76%)] | Loss: 0.017061 | Elapsed: 10.22s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:08:55,970 WARNING  [*] 14:08:55: Train Epoch: 5 [48000/50751 (95%)] | Loss: 0.024384 | Elapsed: 10.20s | FPR 0.0003 -> TPR 0.9851 & F1 0.9925 | AUC 0.9995
2023-03-18 14:09:00,022 WARNING  [*] Sat Mar 18 14:09:00 2023:    5    | Tr.loss: 0.035593 | Elapsed:   55.34  s | FPR 0.0003 -> TPR: 0.90 & F1: 0.94 | AUC: 0.9990
2023-03-18 14:09:00,022 WARNING  [*] Started epoch: 6
2023-03-18 14:09:00,158 WARNING  [*] 14:09:00: Train Epoch: 6 [  0  /50751 (0 %)] | Loss: 0.012440 | Elapsed: 0.14s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:09:10,392 WARNING  [*] 14:09:10: Train Epoch: 6 [9600 /50751 (19%)] | Loss: 0.031324 | Elapsed: 10.23s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:09:20,628 WARNING  [*] 14:09:20: Train Epoch: 6 [19200/50751 (38%)] | Loss: 0.008658 | Elapsed: 10.22s | FPR 0.0003 -> TPR 1.0000 & F1 1.0000 | AUC 1.0000
2023-03-18 14:09:23,080 WARNING  [!] Time budget exceeded, training stopped.
2023-03-18 14:09:23,125 WARNING  [!] Sat Mar 18 14:09:23 2023: Dumped results:
                model       : 1679144660-model.torch
		train time  : 1679144660-trainTime.npy
		train losses: 1679144660-trainLosses.npy
		train AUC   : 1679144660-auc.npy
		train F1s   : 1679144660-trainF1s.npy
		train TPRs  : 1679144660-trainTPRs.npy
2023-03-18 14:09:23,162 WARNING  [!] Evaluating model on training set...
2023-03-18 14:09:36,946 WARNING  [!] This fold metrics on training set:
2023-03-18 14:09:36,954 WARNING 	AUC: 0.9996
2023-03-18 14:09:36,958 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 14:09:36,967 WARNING 	FPR: 0.0003 | TPR: 0.9348 | F1: 0.9662
2023-03-18 14:09:36,988 WARNING 	FPR: 0.001 | TPR: 0.9749 | F1: 0.9871
2023-03-18 14:09:36,996 WARNING 	FPR: 0.003 | TPR: 0.9875 | F1: 0.9930
2023-03-18 14:09:37,002 WARNING 	FPR: 0.01 | TPR: 0.9953 | F1: 0.9955
2023-03-18 14:09:37,023 WARNING 	FPR: 0.03 | TPR: 0.9988 | F1: 0.9922
2023-03-18 14:09:37,036 WARNING 	FPR: 0.1 | TPR: 0.9995 | F1: 0.9764
2023-03-18 14:09:37,037 WARNING  [!] Evaluating model on validation set...
2023-03-18 14:09:43,935 WARNING  [!] This fold metrics on validation set:
2023-03-18 14:09:43,939 WARNING 	AUC: 0.9978
2023-03-18 14:09:43,945 WARNING 	FPR: 0.0001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 14:09:43,949 WARNING 	FPR: 0.0003 | TPR: 0.0000 | F1: 0.0000
2023-03-18 14:09:43,955 WARNING 	FPR: 0.001 | TPR: 0.0000 | F1: 0.0000
2023-03-18 14:09:43,966 WARNING 	FPR: 0.003 | TPR: 0.9489 | F1: 0.9731
2023-03-18 14:09:43,972 WARNING 	FPR: 0.01 | TPR: 0.9760 | F1: 0.9855
2023-03-18 14:09:43,979 WARNING 	FPR: 0.03 | TPR: 0.9886 | F1: 0.9872
2023-03-18 14:09:43,980 WARNING 	FPR: 0.1 | TPR: 0.9965 | F1: 0.9750
2023-03-18 14:09:44,109 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_70000_limNone_r1763_t5\70000_metrics_validation.json
2023-03-18 14:09:44,111 WARNING  [!] Metrics saved to out_vocab_1679128542\cv_70000_limNone_r1763_t5\70000_metrics_training.json
2023-03-18 14:09:44,112 WARNING  [!] Average epoch time: 0.03s | Mean values over 3 folds:
	AUC: 0.9973
	FPR: 0.0001 -- TPR: 0.0000 -- F1: 0.0000
	FPR: 0.0003 -- TPR: 0.0000 -- F1: 0.0000
	FPR:  0.001 -- TPR: 0.0000 -- F1: 0.0000
	FPR:  0.003 -- TPR: 0.9332 -- F1: 0.9647
	FPR:   0.01 -- TPR: 0.9710 -- F1: 0.9829
	FPR:   0.03 -- TPR: 0.9868 -- F1: 0.9862
	FPR:    0.1 -- TPR: 0.9956 -- F1: 0.9746

