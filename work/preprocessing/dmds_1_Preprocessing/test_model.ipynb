{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data = r\"..\\dataset\"\n",
    "LIMIT = 5 \n",
    "X = [np.load(os.path.join(data, x)) for x in os.listdir(data)[:LIMIT] if x.endswith(\".npy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 102)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape # each sample represents 1000 API calls, each 102 dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "class FeatureType(object):\n",
    "    ''' Base class from which each feature type may inherit '''\n",
    "\n",
    "    name = ''\n",
    "    dim = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, self.dim)\n",
    "\n",
    "    def raw_features(self, input_dict):\n",
    "        ''' Generate a JSON-able representation of the file '''\n",
    "        raise (NotImplemented)\n",
    "\n",
    "    def process_features(self, raw_obj):\n",
    "        ''' Generate a feature vector from the raw features '''\n",
    "        raise (NotImplemented)\n",
    "    \n",
    "    def process_raw_features(self, raw_obj):\n",
    "        ''' Generate a feature vector from the raw features '''\n",
    "        raise (NotImplemented)\n",
    "\n",
    "    def feature_vector(self, input_dict):\n",
    "        ''' Directly calculate the feature vector from the sample itself. This should only be implemented differently\n",
    "        if there are significant speedups to be gained from combining the two functions. '''\n",
    "        return self.process_raw_features(self.raw_features(input_dict))\n",
    "\n",
    "\n",
    "class APIName(FeatureType):\n",
    "    ''' api_name hash info '''\n",
    "\n",
    "    name = 'api_name'\n",
    "    dim = 8\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self._name = re.compile('^[a-z]+|[A-Z][^A-Z]*')\n",
    "\n",
    "    def raw_features(self, input_dict):\n",
    "        \"\"\"\n",
    "        input_dict: string\n",
    "        \"\"\"\n",
    "        tmp = self._name.findall(input_dict)\n",
    "        hasher = FeatureHasher(self.dim, input_type=\"string\").transform([tmp]).toarray()[0]\n",
    "        return hasher\n",
    "    \n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return raw_obj\n",
    "\n",
    "\n",
    "class IntInfo(FeatureType):\n",
    "    ''' int hash info '''\n",
    "\n",
    "    name = 'int'\n",
    "    dim = 16\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, input_dict):\n",
    "        hasher = FeatureHasher(self.dim).transform([input_dict]).toarray()[0]\n",
    "        return hasher\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return raw_obj\n",
    "\n",
    "\n",
    "class PRUIInfo(FeatureType):\n",
    "    ''' Path, Registry, Urls, IPs hash info '''\n",
    "\n",
    "    name = 'prui'\n",
    "    dim = 16 + 8 + 12 + 16 + 12\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self._paths = re.compile('^c:\\\\\\\\', re.IGNORECASE)\n",
    "        self._dlls = re.compile('.+\\.dll$', re.IGNORECASE)\n",
    "        self._urls = re.compile('^https?://(.+?)[/|\\s|:]', re.IGNORECASE)\n",
    "        self._registry = re.compile('^HKEY_')\n",
    "        self._ips = re.compile('^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}')\n",
    "\n",
    "    def raw_features(self, input_dict):\n",
    "        paths = np.zeros((16,), dtype=np.float32)\n",
    "        dlls = np.zeros((8,), dtype=np.float32)\n",
    "        registry = np.zeros((12,), dtype=np.float32)\n",
    "        urls = np.zeros((16,), dtype=np.float32)\n",
    "        ips = np.zeros((12,), dtype=np.float32)\n",
    "        for str_name, str_value in input_dict.items():\n",
    "            if self._dlls.match(str_value):\n",
    "                tmp = re.split('//|\\\\\\\\|\\.', str_value)[:-1]\n",
    "                tmp = ['\\\\'.join(tmp[:i]) for i in range(1, len(tmp) + 1)]\n",
    "                dlls += FeatureHasher(8, input_type=\"string\").transform([tmp]).toarray()[0]\n",
    "            if self._paths.match(str_value):\n",
    "                tmp = re.split('//|\\\\\\\\|\\.', str_value)[:-1]\n",
    "                tmp = ['\\\\'.join(tmp[:i]) for i in range(1, len(tmp) + 1)]\n",
    "                paths += FeatureHasher(16, input_type=\"string\").transform([tmp]).toarray()[0]\n",
    "            elif self._registry.match(str_value):\n",
    "                tmp = str_value.split('\\\\')[:6]\n",
    "                tmp = ['\\\\'.join(tmp[:i]) for i in range(1, len(tmp) + 1)]\n",
    "                registry += FeatureHasher(12, input_type=\"string\").transform([tmp]).toarray()[0]\n",
    "            elif self._urls.match(str_value):\n",
    "                tmp = self._urls.split(str_value + \"/\")[1]\n",
    "                tmp = tmp.split('.')[::-1]\n",
    "                tmp = ['.'.join(tmp[:i][::-1]) for i in range(1, len(tmp) + 1)]\n",
    "                urls += FeatureHasher(16, input_type=\"string\").transform([tmp]).toarray()[0]\n",
    "            elif self._ips.match(str_value):\n",
    "                tmp = str_value.split('.')\n",
    "                tmp = ['.'.join(tmp[:i]) for i in range(1, len(tmp) + 1)]\n",
    "                ips += FeatureHasher(12, input_type=\"string\").transform([tmp]).toarray()[0]\n",
    "        return np.hstack([paths, dlls, registry, urls, ips]).astype(np.float32)\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return raw_obj\n",
    "\n",
    "\n",
    "class StringsInfo(FeatureType):\n",
    "    ''' Other printable strings hash info '''\n",
    "\n",
    "    name = 'strings'\n",
    "    dim = 8\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FeatureType, self).__init__()\n",
    "        self._allstrings = re.compile(b'[\\x20-\\x7f]{5,}')\n",
    "        self._paths = re.compile(b'c:\\\\\\\\', re.IGNORECASE)\n",
    "        self._dlls = re.compile(b'\\\\.dll', re.IGNORECASE)\n",
    "        self._urls = re.compile(b'https?://', re.IGNORECASE)\n",
    "        self._registry = re.compile(b'HKEY_')\n",
    "        self._mz = re.compile(b'MZ')\n",
    "        self._ips = re.compile(b'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}')\n",
    "        super(FeatureType, self).__init__()\n",
    "\n",
    "    def raw_features(self, input_dict):\n",
    "        bytez = '\\x11'.join(input_dict.values()).encode('UTF-8', 'ignore')\n",
    "        allstrings = self._allstrings.findall(bytez)\n",
    "        if allstrings:\n",
    "            # statistics about strings:\n",
    "            string_lengths = [len(s) for s in allstrings]\n",
    "            avlength = sum(string_lengths) / len(string_lengths)\n",
    "            # map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive\n",
    "            as_shifted_string = [b - ord(b'\\x20') for b in b''.join(allstrings)]\n",
    "            c = np.bincount(as_shifted_string, minlength=96)  # histogram count\n",
    "            # distribution of characters in printable strings\n",
    "            csum = c.sum()\n",
    "            p = c.astype(np.float32) / csum\n",
    "            wh = np.where(c)[0]\n",
    "            H = np.sum(-p[wh] * np.log2(p[wh]))  # entropy\n",
    "        else:\n",
    "            avlength = 0\n",
    "            c = np.zeros((96,), dtype=np.float32)\n",
    "            H = 0\n",
    "            csum = 0\n",
    "        return {\n",
    "            'numstrings': len(allstrings),\n",
    "            'avlength': avlength,\n",
    "            'printables': int(csum),\n",
    "            'entropy': float(H),\n",
    "            'paths': len(self._paths.findall(bytez)),\n",
    "            'dlls': len(self._dlls.findall(bytez)),\n",
    "            'urls': len(self._urls.findall(bytez)),\n",
    "            'registry': len(self._registry.findall(bytez)),\n",
    "            'ips': len(self._ips.findall(bytez)),\n",
    "            'MZ': len(self._mz.findall(bytez))\n",
    "        }\n",
    "\n",
    "    def process_raw_features(self, raw_obj):\n",
    "        return np.hstack([\n",
    "            raw_obj['numstrings'], raw_obj['avlength'], raw_obj['printables'],\n",
    "            raw_obj['entropy'], raw_obj['paths'], raw_obj['dlls'], raw_obj['urls'],\n",
    "            raw_obj['registry'], raw_obj['ips'], raw_obj['MZ']\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "features = dict((fe.name, fe) for fe in\n",
    "                [APIName(), IntInfo(), PRUIInfo(), StringsInfo()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_attributes': 0.6931471805599453}\n",
      "{'filepath_r': 'C:\\\\Users\\\\cuckoo\\\\AppData\\\\Local\\\\Temp\\\\1F60EAF2E2F2E0B2F816BB7EE54D094510D9163F9896937F2711FC7D6E4E192F_7526400.dll.manifest', 'filepath': 'C:\\\\Users\\\\cuckoo\\\\AppData\\\\Local\\\\Temp\\\\1F60EAF2E2F2E0B2F816BB7EE54D094510D9163F9896937F2711FC7D6E4E192F_7526400.dll.manifest'}\n",
      "[ -2.          0.          0.          0.          0.          0.\n",
      "  -1.          1.          0.         -0.6931472   0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -2.          0.          0.          0.\n",
      "   2.          0.          0.          0.          0.         -2.\n",
      "   2.         -4.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          2.        120.\n",
      " 240.          4.968205    2.          2.          0.          0.\n",
      "   0.          0.       ]\n",
      "(98,)\n"
     ]
    }
   ],
   "source": [
    "arguments = {\n",
    "    \"file_attributes\": -1,\n",
    "    \"filepath_r\": \"C:\\\\Users\\\\cuckoo\\\\AppData\\\\Local\\\\Temp\\\\1F60EAF2E2F2E0B2F816BB7EE54D094510D9163F9896937F2711FC7D6E4E192F_7526400.dll.manifest\",\n",
    "    \"filepath\": \"C:\\\\Users\\\\cuckoo\\\\AppData\\\\Local\\\\Temp\\\\1F60EAF2E2F2E0B2F816BB7EE54D094510D9163F9896937F2711FC7D6E4E192F_7526400.dll.manifest\"\n",
    "}\n",
    "\n",
    "api_parser = APIName()\n",
    "api_example = \"GetFileAttributesW\"\n",
    "api_name_hashed = api_parser.feature_vector(api_example)\n",
    "\n",
    "api_int_dict, api_str_dict = {}, {}\n",
    "for c_n, c_v in arguments.items():\n",
    "    if isinstance(c_v, (list, dict, tuple)):\n",
    "        continue\n",
    "    if isinstance(c_v, (int, float)):\n",
    "        api_int_dict[c_n] = np.log(np.abs(c_v) + 1)\n",
    "    else:\n",
    "        if c_v[:2] == '0x':\n",
    "            continue\n",
    "        api_str_dict[c_n] = c_v\n",
    "\n",
    "print(api_int_dict)\n",
    "print(api_str_dict)\n",
    "\n",
    "api_int_hashed = features['int'].feature_vector(api_int_dict)\n",
    "api_prui_hashed = features['prui'].feature_vector(\n",
    "    api_str_dict)\n",
    "api_str_hashed = features['strings'].feature_vector(\n",
    "    api_str_dict)\n",
    "hashed_feature = np.hstack(\n",
    "    [api_name_hashed, api_int_hashed, api_prui_hashed, api_str_hashed]).astype(\n",
    "    np.float32)\n",
    "\n",
    "print(hashed_feature)\n",
    "print(hashed_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000, 102)\n",
      "(1, 1000, 128)\n"
     ]
    }
   ],
   "source": [
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization, Conv1D, Multiply\n",
    "\n",
    "x = Input(shape=(1000, 102), batch_size=1)\n",
    "y = BatchNormalization()(x)\n",
    "m = Model(x, y)\n",
    "\n",
    "x_0 = Conv1D(128, 2, strides=1, padding='same')(y)\n",
    "x_1 = Conv1D(128, 2, strides=1, activation=\"sigmoid\", padding='same')(y)\n",
    "gated_0 = Multiply()([x_0, x_1])\n",
    "m2 = Model(x, gated_0)\n",
    "\n",
    "print(m(X[0].reshape(1, 1000, 102)).shape)\n",
    "print(m2(X[0].reshape(1, 1000,102)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0264]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from functools import reduce\n",
    "from operator import __add__\n",
    "\n",
    "class Conv2dSamePadding(nn.Conv1d):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.zero_pad_2d = nn.ZeroPad2d(reduce(__add__,\n",
    "                  [(k // 2 + (k - 2 * (k // 2)) - 1, k // 2) for k in self.kernel_size[::-1]]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return  self._conv_forward(self.zero_pad_2d(input), self.weight, self.bias)\n",
    "\n",
    "class GatedCNN(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ndim=102,\n",
    "            seq_len=1000,\n",
    "            conv_out_dim=128,\n",
    "            lstm_hidden=100,\n",
    "            dense_hidden=64,\n",
    "            dropout=0.5,\n",
    "            num_classes=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2dSamePadding(ndim, conv_out_dim, kernel_size=2)#, stride=1, padding=1)\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.conv2 = Conv2dSamePadding(ndim, conv_out_dim, kernel_size=2)#, stride=1, padding=1)\n",
    "        self.sig2 = nn.Sigmoid()\n",
    "        \n",
    "        self.conv3 = Conv2dSamePadding(ndim, conv_out_dim, kernel_size=3)#, stride=1, padding=1)\n",
    "        self.sig3 = nn.Sigmoid()\n",
    "        self.conv4 = Conv2dSamePadding(ndim, conv_out_dim, kernel_size=3)#, stride=1, padding=1)\n",
    "        self.sig4 = nn.Sigmoid()\n",
    "        \n",
    "        self.lstm = nn.LSTM(conv_out_dim*2, lstm_hidden, bidirectional=True, batch_first=True)\n",
    "        self.dense1 = nn.Linear(lstm_hidden*2, dense_hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.dense2 = nn.Linear(dense_hidden, num_classes)\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm1d(seq_len)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input: (B, L, C) where B - batch size, L - length of sequence, C - feature dim of each sequence element \n",
    "        \"\"\"\n",
    "        x = self.batch_norm1(x)\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "\n",
    "        gated_0 = self.conv1(x)\n",
    "        gated_0 = self.sig1(gated_0)\n",
    "        gated_0 = gated_0 * self.conv2(x)\n",
    "        \n",
    "        gated_1 = self.conv3(x)\n",
    "        gated_1 = self.sig3(gated_1)\n",
    "        gated_1 = gated_1 * self.conv4(x)\n",
    "        \n",
    "        x = torch.cat([gated_0, gated_1], dim=1)\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        x = self.batch_norm2(x)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        x = torch.max(x, dim=1)[0]\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "mt = GatedCNN()\n",
    "x = torch.Tensor(X[0].reshape(-1, 1000, 102))\n",
    "\n",
    "mt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0350],\n",
       "        [ 0.0240],\n",
       "        [-0.0925],\n",
       "        [ 0.1308]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(np.stack(X))\n",
    "mt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
