{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root: [!] Loaded data and vocab. X train size: (13000, 2048), X test size: (17407, 2048), vocab size: 10000\n",
      "WARNING:root:Masking sequences...\n",
      "100%|██████████| 10400/10400 [00:02<00:00, 4600.94it/s]\n",
      "WARNING:root:Pre-training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/10400 (0%)]\tLoss: 465.383728\n",
      "Train Epoch: 1 [2560/10400 (24%)]\tLoss: 376.569275\n",
      "Train Epoch: 1 [5120/10400 (49%)]\tLoss: 378.656982\n",
      "Train Epoch: 1 [7680/10400 (73%)]\tLoss: 357.453705\n",
      "Train Epoch: 1 [6400/10400 (98%)]\tLoss: 347.796448\n",
      "Train Epoch: 2 [0/10400 (0%)]\tLoss: 375.825684\n",
      "Train Epoch: 2 [2560/10400 (24%)]\tLoss: 372.419800\n",
      "Train Epoch: 2 [5120/10400 (49%)]\tLoss: 356.096802\n",
      "Train Epoch: 2 [7680/10400 (73%)]\tLoss: 362.398804\n",
      "Train Epoch: 2 [6400/10400 (98%)]\tLoss: 364.035645\n",
      "Train Epoch: 3 [0/10400 (0%)]\tLoss: 360.333893\n",
      "Train Epoch: 3 [2560/10400 (24%)]\tLoss: 345.647339\n",
      "Train Epoch: 3 [5120/10400 (49%)]\tLoss: 341.048584\n",
      "Train Epoch: 3 [7680/10400 (73%)]\tLoss: 358.625031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Training pre-trained model on downstream task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [6400/10400 (98%)]\tLoss: 340.023987\n",
      "Train Epoch: 1 [0/2600 (0%)]\tLoss: 0.693352\tF1: 0.526302\n",
      "Train Epoch: 1 [400/2600 (91%)]\tLoss: 0.406161\tF1: 0.713262\n",
      "Train Epoch: 2 [0/2600 (0%)]\tLoss: 0.484427\tF1: 0.671434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Training model on downstream task without pre-training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [400/2600 (91%)]\tLoss: 0.346188\tF1: 0.765625\n",
      "Train Epoch: 1 [0/2600 (0%)]\tLoss: 0.722794\tF1: 0.437466\n",
      "Train Epoch: 1 [400/2600 (91%)]\tLoss: 0.568851\tF1: 0.525692\n",
      "Train Epoch: 2 [0/2600 (0%)]\tLoss: 0.541061\tF1: 0.496888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Training new model on downstream task on full dataset as benchmark...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [400/2600 (91%)]\tLoss: 0.448866\tF1: 0.810683\n",
      "Train Epoch: 1 [0/13000 (0%)]\tLoss: 0.703211\tF1: 0.452247\n",
      "Train Epoch: 1 [2560/13000 (20%)]\tLoss: 0.587433\tF1: 0.512821\n",
      "Train Epoch: 1 [5120/13000 (39%)]\tLoss: 0.418562\tF1: 0.823465\n",
      "Train Epoch: 1 [7680/13000 (59%)]\tLoss: 0.262040\tF1: 0.865460\n",
      "Train Epoch: 1 [10240/13000 (78%)]\tLoss: 0.187548\tF1: 0.932049\n",
      "Train Epoch: 1 [10000/13000 (98%)]\tLoss: 0.197079\tF1: 0.878792\n",
      "Train Epoch: 2 [0/13000 (0%)]\tLoss: 0.168509\tF1: 0.933996\n",
      "Train Epoch: 2 [2560/13000 (20%)]\tLoss: 0.141542\tF1: 0.921204\n",
      "Train Epoch: 2 [5120/13000 (39%)]\tLoss: 0.144487\tF1: 0.933606\n",
      "Train Epoch: 2 [7680/13000 (59%)]\tLoss: 0.117239\tF1: 0.937098\n",
      "Train Epoch: 2 [10240/13000 (78%)]\tLoss: 0.111415\tF1: 0.941530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Evaluating all models on test set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [10000/13000 (98%)]\tLoss: 0.082355\tF1: 0.963938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:06<00:00, 11.19it/s]\n",
      "100%|██████████| 68/68 [00:06<00:00, 10.99it/s]\n",
      "100%|██████████| 68/68 [00:06<00:00, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FPR:  0.01\n",
      "0.29 +- 0.13 -- pretrained_U_Lx\n",
      "0.28 +- 0.15 -- non_pretrained_Lx\n",
      "0.68 +- 0.14 -- full_data_X\n",
      "\n",
      "FPR:  0.001\n",
      "0.20 +- 0.09 -- pretrained_U_Lx\n",
      "0.17 +- 0.13 -- non_pretrained_Lx\n",
      "0.56 +- 0.16 -- full_data_X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from nebula.models import Cnn1DLinearLM\n",
    "from nebula.pretraining import SSLPretraining\n",
    "from sklearn.utils import shuffle\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "train_limit = 13000\n",
    "\n",
    "xTrainFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_trainset_WithAPIargs\\speakeasy_VocabSize_10000_maxLen_2048_x.npy\"\n",
    "xTrain = np.load(xTrainFile)\n",
    "yTrainFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_trainset_WithAPIargs\\speakeasy_y.npy\"\n",
    "yTrain = np.load(yTrainFile)\n",
    "xTestFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_testset_WithAPIargs\\speakeasy_VocabSize_10000_maxLen_2048_x.npy\"\n",
    "xTest = np.load(xTestFile)\n",
    "yTestFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_testset_WithAPIargs\\speakeasy_y.npy\"\n",
    "yTest = np.load(yTestFile)\n",
    "\n",
    "if train_limit:\n",
    "    xTrain, yTrain = shuffle(xTrain, yTrain, random_state=42)\n",
    "    xTrain = xTrain[:train_limit]\n",
    "    yTrain = yTrain[:train_limit]\n",
    "\n",
    "\n",
    "vocabFile = r\"C:\\Users\\dtrizna\\Code\\nebula\\data\\data_filtered\\speakeasy_trainset_WithAPIargs\\speakeasy_VocabSize_10000.pkl\"\n",
    "with open(vocabFile, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "logging.warning(f\" [!] Loaded data and vocab. X train size: {xTrain.shape}, X test size: {xTest.shape}, vocab size: {len(vocab)}\")\n",
    "\n",
    "modelConfig = {\n",
    "    \"vocabSize\": len(vocab)\n",
    "}\n",
    "modelClass = Cnn1DLinearLM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\" [!] Using device: {device}\")\n",
    "# device=device, pretrinEpochs=1, downstreamEpochs=1, =100, batchSize=256\n",
    "# converth previous line to a dict\n",
    "config = {\n",
    "    \"device\": device,\n",
    "    \"pretraingEpochs\": 3,\n",
    "    \"downstreamEpochs\": 2,\n",
    "    \"batchSize\": 256,\n",
    "    \"\": 10\n",
    "}\n",
    "\n",
    "pretrain = SSLPretraining(xTrain, yTrain, xTest, yTest, vocab, modelClass, modelConfig, **config)\n",
    "metrics = pretrain.run(xTrain, yTrain, xTest, yTest, vocab, modelClass, modelConfig, **config)\n",
    "\n",
    "fpr = 0.01\n",
    "print(\"\\nFPR: \", fpr)\n",
    "for m in metrics:\n",
    "    print(f\"{metrics[m][fpr]['tpr']:.2f} +- {metrics[m][fpr]['tpr_std']:.2f} -- {m}\")\n",
    "\n",
    "fpr = 0.001\n",
    "print(\"\\nFPR: \", fpr)\n",
    "for m in metrics:\n",
    "    print(f\"{metrics[m][fpr]['tpr']:.2f} +- {metrics[m][fpr]['tpr_std']:.2f} -- {m}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e123ac7eba4d44924a894b1be2fc564282b1d2645e9d64ed33bc5003b6c2a87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
